[
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2406966",
                "VG_object_id": "2379643",
                "bbox": [112, 35, 308, 322],
                "image": "data\\images\\2406966.jpg"
            },
            {
                "VG_image_id": "2339173",
                "VG_object_id": "2264697",
                "bbox": [133, 235, 192, 371],
                "image": "data\\images\\2339173.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman doing", 2],
            ["how many people are there in the picture", 2],
            ["what color is the woman's shirt", 1],
            ["what color is the background", 1],
            ["what is the woman holding", 1],
            ["Where is the woman", 1],
            ["what is the woman wearing", 1],
            ["what is the person doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what color is the background", 1],
            ["what is the woman holding", 1],
            ["How many people are there", -1],
            ["Where is the woman", 1],
            ["what is the woman wearing on the head", -1],
            ["What is woman doing", 2],
            ["what is the woman wearing", 1],
            ["when was the photo taken", -1],
            ["what is the gender of the person", -1],
            ["who is in the photo", -1],
            ["what is the person doing", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a woman riding a horse in a dirt arena.",
            "a group of people standing on top of a stage."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2371276",
                "VG_object_id": "2443415",
                "bbox": [119, 75, 250, 430],
                "image": "data\\images\\2371276.jpg"
            },
            {
                "VG_image_id": "2347938",
                "VG_object_id": "3608311",
                "bbox": [2, 325, 67, 373],
                "image": "data\\images\\2347938.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many person's are there in the photo", 2],
            ["where is the woman", 1],
            ["what is in the distance", 1],
            ["what is the woman wearing", 1],
            ["who is wearing a pink shirt", 1]
        ],
        "org_questions": [
            ["where is the woman", 1],
            ["how many people are there on the photo", -1],
            ["what is in the distance", 1],
            ["what color is the woman's hair", -1],
            ["What season is it", -1],
            ["what is the lady doing", -1],
            ["what is the woman wearing", 1],
            ["how many person's are there in the photo", 2],
            ["who is wearing a pink shirt", 1],
            ["when was this photo taken", -1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a woman in a pink dress is waiting on a bench.",
            "a large group of people on a beach flying kites."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2401444",
                "VG_object_id": "1147817",
                "bbox": [221, 139, 296, 211],
                "image": "data\\images\\2401444.jpg"
            },
            {
                "VG_image_id": "2389476",
                "VG_object_id": "1260510",
                "bbox": [218, 101, 499, 332],
                "image": "data\\images\\2389476.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what sport is the man playing", 2],
            ["what are the people doing", 1],
            ["what is the player wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what color is the background", -1],
            ["how many people are there", -1],
            ["what are the people doing", 1],
            ["what is the man in the shirt wearing on head", -1],
            ["what is behind the person", -1],
            ["what is the land covered with", -1],
            ["when was the picture taken", -1],
            ["what is the man holding", 2],
            ["what is the player wearing", 1],
            ["what sport is the man playing", 2]
        ],
        "context": [
            "a baseball player walking on a field",
            "a man in a hat holding a frisbee."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2333819",
                "VG_object_id": "3461698",
                "bbox": [106, 41, 252, 151],
                "image": "data\\images\\2333819.jpg"
            },
            {
                "VG_image_id": "2364839",
                "VG_object_id": "2105777",
                "bbox": [66, 62, 216, 247],
                "image": "data\\images\\2364839.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color are the man's pants", 2],
            ["how many people are there in the picture", 2],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color are the man's pants", 2],
            ["what color is the background", -1],
            ["how many people are there", -1],
            ["what is the man doing", -1],
            ["What sports is man doing", -1],
            ["how many shirts are there", -1],
            ["what is on the man's head", 1],
            ["who is in the photo", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man swinging a tennis racket at a tennis match.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2409107",
                "VG_object_id": "248907",
                "bbox": [237, 129, 329, 264],
                "image": "data\\images\\2409107.jpg"
            },
            {
                "VG_image_id": "2332559",
                "VG_object_id": "2705067",
                "bbox": [352, 67, 458, 272],
                "image": "data\\images\\2332559.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what is the woman doing", 1],
            ["what color is the woman's clothes", 1],
            ["how many people are there", 1],
            ["What is woman doing", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what color is the woman's clothes", 1],
            ["how many people are there", 1],
            ["what is the woman wearing on her face", -1],
            ["where is the woman", -1],
            ["what is the woman holding", 2],
            ["What is woman doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a person in a winter coat throwing a frisbee.",
            "a horse is pulling a carriage with two men in it."
        ]
    },
    {
        "object_category": "book",
        "images": [
            {
                "VG_image_id": "2339296",
                "VG_object_id": "951883",
                "bbox": [17, 134, 192, 292],
                "image": "data\\images\\2339296.jpg"
            },
            {
                "VG_image_id": "2380533",
                "VG_object_id": "1345516",
                "bbox": [272, 2, 352, 50],
                "image": "data\\images\\2380533.jpg"
            }
        ],
        "questions_with_scores": [["how many people are there", 1]],
        "org_questions": [
            ["how many people are there", 1],
            ["what color is the book", -1],
            ["where is the book", -1],
            ["what is beside the books", -1],
            ["how many books are there", -1],
            ["what is on the back of the shelf", -1],
            ["where are books", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a woman playing a game with a wii controller.",
            "a woman and a baby sitting at a table."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2337082",
                "VG_object_id": "2327037",
                "bbox": [25, 12, 302, 253],
                "image": "data\\images\\2337082.jpg"
            },
            {
                "VG_image_id": "2405438",
                "VG_object_id": "332062",
                "bbox": [75, 53, 228, 212],
                "image": "data\\images\\2405438.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the cat", 2],
            ["What color is the table", 2]
        ],
        "org_questions": [
            ["What color is the cat", 2],
            ["What color is the table", 2],
            ["What is cat looking at", -1],
            ["how many cats are in the picture", -1],
            ["where is the cat", -1],
            ["what is the cat doing", -1],
            ["what is the cat on", -1],
            ["what is beside the cat", -1],
            ["what kind of animal is in the picture", -1],
            ["who is in the picture", -1],
            ["what is on the desk", -1],
            ["what is in front of the cat", -1]
        ],
        "context": [
            "a cat sleeping next to a computer keyboard.",
            "a cat sitting on a keyboard"
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2362878",
                "VG_object_id": "770340",
                "bbox": [1, 235, 498, 334],
                "image": "data\\images\\2362878.jpg"
            },
            {
                "VG_image_id": "2368438",
                "VG_object_id": "1828129",
                "bbox": [0, 0, 372, 497],
                "image": "data\\images\\2368438.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many screens are there", 2],
            ["how man screens are there on the desk", 1],
            ["what is on the desk", 1],
            ["how many screens are there on the desk", 1],
            ["how many computers are in the picture", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how man screens are there on the desk", 1],
            ["what color is the desk", -1],
            ["what is the desk made of", -1],
            ["how many people are there in the picture", -1],
            ["what is on the desk", 1],
            ["how many screens are there on the desk", 1],
            ["how many computers are in the picture", 1],
            ["where is the picture taken", -1],
            ["what is in the background", 1],
            ["how many screens are there", 2]
        ],
        "context": [
            "a laptop computer sitting on top of a desk.",
            "a cell phone sitting on top of a wooden table."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2318523",
                "VG_object_id": "1009859",
                "bbox": [303, 196, 375, 287],
                "image": "data\\images\\2318523.jpg"
            },
            {
                "VG_image_id": "2412885",
                "VG_object_id": "3802565",
                "bbox": [246, 101, 345, 199],
                "image": "data\\images\\2412885.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["what is the man wearing on his head", 1],
            ["who is wearing the shirt", 1],
            ["what is the persion holding", 1],
            ["how many people are there", 1],
            ["HOw many people are there", 1],
            ["what is the persion wearing on the head", 1],
            ["what is in front of the person", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["what is the man wearing on his head", 1],
            ["who is wearing the shirt", 1],
            ["what gender is the person", -1],
            ["what is the persion holding", 1],
            ["when was the photo taken", -1],
            ["how many people are there", 1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the persion wearing", -1],
            ["what gender is the person in the shirt", -1],
            ["HOw many people are there", 1],
            ["what is the persion wearing on the head", 1],
            ["what is in front of the person", 1]
        ],
        "context": [
            "a boy holding a tennis ball and racket in a park.",
            "a man riding a skateboard up the side of a ramp."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2408507",
                "VG_object_id": "3809165",
                "bbox": [18, 7, 405, 252],
                "image": "data\\images\\2408507.jpg"
            },
            {
                "VG_image_id": "2393410",
                "VG_object_id": "2490582",
                "bbox": [119, 146, 399, 361],
                "image": "data\\images\\2393410.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the color of table", 2],
            ["what color is the food in the bowl", 2],
            ["What is in the bowl", 1],
            ["what is under the bowl", 1]
        ],
        "org_questions": [
            ["What is in the bowl", 1],
            ["How many bowls are there in the image", -1],
            ["What is the color of table", 2],
            ["what is the bowl made of", -1],
            ["what the bowl is on", -1],
            ["what color is the food in the bowl", 2],
            ["how many people are there", -1],
            ["what is beside the bowl", -1],
            ["where is the plate", -1],
            ["what utensil is in the bowl", -1],
            ["what is on the table", -1],
            ["what is under the bowl", 1]
        ],
        "context": [
            "a plate with a bowl of soup and a sandwich.",
            "a table with a bowl of salad and a bottle of wine."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2369255",
                "VG_object_id": "2058275",
                "bbox": [39, 146, 499, 326],
                "image": "data\\images\\2369255.jpg"
            },
            {
                "VG_image_id": "2393144",
                "VG_object_id": "474545",
                "bbox": [0, 189, 321, 402],
                "image": "data\\images\\2393144.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bench", 2],
            ["how old is the woman", 2],
            ["what is the woman wearing on her head", 1],
            ["what season is the photo taken in", 1],
            ["What is in the background of image", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the bench", 2],
            ["how old is the woman", 2],
            ["what is the woman wearing on her head", 1],
            ["what season is the photo taken in", 1],
            ["who is sitting on the chair", -1],
            ["What is in the background of image", 1],
            ["what is the bench made of", -1],
            ["WHat is on the bench", -1],
            ["where was the photo taken", 1],
            ["how many people are there", -1],
            ["what is the woman sitting on", -1],
            ["where is the person sitting", -1]
        ],
        "context": [
            "a woman sitting on a blue bench holding a cell phone.",
            "a beautiful girl sitting on a bench in a park."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2347158",
                "VG_object_id": "2051835",
                "bbox": [321, 0, 428, 281],
                "image": "data\\images\\2347158.jpg"
            },
            {
                "VG_image_id": "2358138",
                "VG_object_id": "3152638",
                "bbox": [184, 26, 463, 148],
                "image": "data\\images\\2358138.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the board", 2],
            ["how many people are there", 2],
            ["what is the main color of the board", 1],
            ["what is in front of the board", 1],
            ["what is the board on", 1],
            ["what is the building made of", 1],
            ["what is the white object", 1]
        ],
        "org_questions": [
            ["what is the main color of the board", 1],
            ["where is the board", 2],
            ["what is in front of the board", 1],
            ["what is the board on", 1],
            ["how many people are there", 2],
            ["what is the building made of", 1],
            ["what is the white object", 1]
        ],
        "context": [
            "a car show room with a car and a sign",
            "a group of sheep laying in a field of grass."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2414752",
                "VG_object_id": "152617",
                "bbox": [4, 89, 443, 276],
                "image": "data\\images\\2414752.jpg"
            },
            {
                "VG_image_id": "2367517",
                "VG_object_id": "621902",
                "bbox": [69, 117, 474, 252],
                "image": "data\\images\\2367517.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the tail of the plane", 1],
            ["what is the plane doing", 1],
            ["What is the color of  sky", 1],
            ["what color is the sky", 1]
        ],
        "org_questions": [
            ["what color is the tail of the plane", 1],
            ["what is the weather like", -1],
            ["where is the airplane", -1],
            ["what is the plane doing", 1],
            ["What is the color of  sky", 1],
            ["what color is the sky", 1],
            ["what color is the airplane", -1],
            ["when was the photo taken", -1],
            ["what is on the side of the plane", -1],
            ["what kind of plane is this", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a small airplane parked in front of a building.",
            "a plane is taking off from the runway."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2356544",
                "VG_object_id": "819528",
                "bbox": [241, 3, 374, 218],
                "image": "data\\images\\2356544.jpg"
            },
            {
                "VG_image_id": "2378752",
                "VG_object_id": "1365230",
                "bbox": [96, 0, 300, 53],
                "image": "data\\images\\2378752.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["what is in the background", 1],
            ["where is the box", 1],
            ["how large is the box", 1],
            ["what is the box placed on", 1]
        ],
        "org_questions": [
            ["how many people are in the picture", 1],
            ["what is in the background", 1],
            ["where is the box", 1],
            ["how large is the box", 1],
            ["what is the box placed on", 1],
            ["where is the photo taken", -1]
        ],
        "context": [
            "a blender and a blender on a table.",
            "a little girl in a dog bed"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2377931",
                "VG_object_id": "2224277",
                "bbox": [239, 134, 294, 302],
                "image": "data\\images\\2377931.jpg"
            },
            {
                "VG_image_id": "2369221",
                "VG_object_id": "613067",
                "bbox": [56, 150, 120, 293],
                "image": "data\\images\\2369221.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the man's shirt", 2],
            ["what color are the man's pants", 2],
            ["what is the man on", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["what is the man standing at", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what is the color of the man's shirt", 2],
            ["what color are the man's pants", 2],
            ["what is the man on", 1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["what is the man doing", -1],
            ["what is the man standing at", 1],
            ["when was the photo taken", -1],
            ["what is on the man's head", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a small plane on a runway with people standing on it.",
            "a group of men standing next to a brick wall."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2380952",
                "VG_object_id": "542898",
                "bbox": [117, 133, 336, 382],
                "image": "data\\images\\2380952.jpg"
            },
            {
                "VG_image_id": "2347049",
                "VG_object_id": "2588286",
                "bbox": [260, 73, 370, 203],
                "image": "data\\images\\2347049.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color is the man's shoes", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the man's shoes", 1],
            ["How many people are there", -1],
            ["where is the person", -1],
            ["what is the boy holding", -1],
            ["what is the boy wearing", -1],
            ["What color is child's hair", -1],
            ["when was the photo taken", -1],
            ["what is the skateboarder doing", -1],
            ["what kind of pants is the boy wearing", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a boy doing a trick on a skateboard.",
            "a man riding a skateboard on top of a wall."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2344650",
                "VG_object_id": "3156766",
                "bbox": [3, 195, 484, 373],
                "image": "data\\images\\2344650.jpg"
            },
            {
                "VG_image_id": "2407334",
                "VG_object_id": "281471",
                "bbox": [257, 429, 313, 499],
                "image": "data\\images\\2407334.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the field", 2],
            ["what animals are in the field", 2],
            ["how many animals are on the field", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the field", 2],
            ["what animals are in the field", 2],
            ["how many animals are on the field", 1],
            ["what is on the field", -1],
            ["what is in the field", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", 1],
            ["where is the shadow", -1]
        ],
        "context": [
            "a group of zebras standing around in a fenced in area.",
            "two giraffes are standing in a field near a tree."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2394123",
                "VG_object_id": "467019",
                "bbox": [104, 142, 180, 200],
                "image": "data\\images\\2394123.jpg"
            },
            {
                "VG_image_id": "2382697",
                "VG_object_id": "1326775",
                "bbox": [49, 169, 112, 209],
                "image": "data\\images\\2382697.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the pillows", 2],
            ["where are the pillows placed on", 1],
            ["where are the pillows", 1],
            ["how many pillows are there", 1],
            ["what is the pillow on", 1],
            ["where is the pillow", 1],
            ["what is the main color of the pillow", 1]
        ],
        "org_questions": [
            ["what color are the pillows", 2],
            ["where are the pillows placed on", 1],
            ["where are the pillows", 1],
            ["how many pillows are there", 1],
            ["what is the pillow on", 1],
            ["where is the pillow", 1],
            ["what is the main color of the pillow", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a bedroom with a chair and a bed.",
            "a living room with a couch, desk and chair."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2382883",
                "VG_object_id": "1325374",
                "bbox": [287, 6, 413, 267],
                "image": "data\\images\\2382883.jpg"
            },
            {
                "VG_image_id": "2344386",
                "VG_object_id": "2881824",
                "bbox": [123, 14, 334, 331],
                "image": "data\\images\\2344386.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the person wearing", 1],
            ["how many people are in the picture", 1],
            ["what gender is the player", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person wearing", 1],
            ["how many people are in the picture", 1],
            ["what gender is the player", 1],
            ["what is the player doing", -1],
            ["how any players are there", -1],
            ["how many players are in the picture", -1],
            ["when was the picture taken", -1],
            ["what is on the player's wrist", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a woman holding a tennis racket on a tennis court.",
            "a man swinging a tennis racket on a court."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2398056",
                "VG_object_id": "1183375",
                "bbox": [397, 127, 462, 227],
                "image": "data\\images\\2398056.jpg"
            },
            {
                "VG_image_id": "2371276",
                "VG_object_id": "2370172",
                "bbox": [120, 73, 252, 419],
                "image": "data\\images\\2371276.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["where is the woman", 2],
            ["how many women are there", 1]
        ],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what color is the woman's shirt", 2],
            ["how many women are there", 1],
            ["what is the woman wearing on the head", -1],
            ["where is the woman", 2],
            ["what is the woman holding", -1],
            ["what is the woman wearing", -1],
            ["what color is the background", -1],
            ["what is the gender of the person in the foreground", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball",
            "a woman in a pink dress is waiting on a bench."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2319615",
                "VG_object_id": "3085438",
                "bbox": [334, 29, 485, 340],
                "image": "data\\images\\2319615.jpg"
            },
            {
                "VG_image_id": "2343895",
                "VG_object_id": "918972",
                "bbox": [269, 83, 333, 158],
                "image": "data\\images\\2343895.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["what is the woman doing", 2],
            ["what is the woman holding", 1],
            ["what gesture is the woman", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 2],
            ["how many people are there", -1],
            ["what is the woman wearing on head", -1],
            ["what is the woman doing", 2],
            ["what is the woman holding", 1],
            ["what gesture is the woman", 1],
            ["who is in the photo", -1],
            ["when was the photo taken", 1],
            ["what is the woman wearing", -1],
            ["where is the woman", -1]
        ],
        "context": [
            "a group of people waiting for a subway train.",
            "a horse drawn carriage on a city street"
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2376273",
                "VG_object_id": "2221933",
                "bbox": [1, 317, 373, 497],
                "image": "data\\images\\2376273.jpg"
            },
            {
                "VG_image_id": "2351093",
                "VG_object_id": "2674540",
                "bbox": [76, 118, 461, 298],
                "image": "data\\images\\2351093.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["when is the picture taken", 2],
            ["what is the weather like", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", -1],
            ["what is in the distance", -1],
            ["what is the weather like", 1],
            ["who is in the picture", -1],
            ["where was this picture taken", -1],
            ["what is on the ground", -1],
            ["where is the photo taken", -1],
            ["when is the picture taken", 2]
        ],
        "context": [
            "a man holding a frisbee and giving a thumbs up sign.",
            "a person holding a surfboard walking into the ocean."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2317621",
                "VG_object_id": "1019092",
                "bbox": [194, 78, 386, 289],
                "image": "data\\images\\2317621.jpg"
            },
            {
                "VG_image_id": "2377329",
                "VG_object_id": "566228",
                "bbox": [114, 1, 357, 280],
                "image": "data\\images\\2377329.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall behind the boy", 2],
            ["what color of the boys's shirt", 1],
            ["how many boys are there", 1],
            ["What is man doing", 1],
            ["what is the boy doing", 1],
            ["how many people are in this picture", 1]
        ],
        "org_questions": [
            ["what color of the boys's shirt", 1],
            ["how many boys are there", 1],
            ["what color is the wall behind the boy", 2],
            ["What is man doing", 1],
            ["what is the child wearing on head", -1],
            ["Where is the man", -1],
            ["what is the boy doing", 1],
            ["who is in the photo", -1],
            ["what is the gender of the person", -1],
            ["where was the photo taken", -1],
            ["how many people are in this picture", 1]
        ],
        "context": [
            "two young boys playing with a plastic bat.",
            "a young boy smiles while lighting sparklers."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2320367",
                "VG_object_id": "3085751",
                "bbox": [1, 195, 498, 389],
                "image": "data\\images\\2320367.jpg"
            },
            {
                "VG_image_id": "2318494",
                "VG_object_id": "1010119",
                "bbox": [76, 217, 376, 374],
                "image": "data\\images\\2318494.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the floor", 2],
            ["What is on the floor", 1],
            ["what is the floor made of", 1],
            ["what type of room is this", 1],
            ["what is the flooring", 1],
            ["what type of floor is this", 1],
            ["what is in the room", 1]
        ],
        "org_questions": [
            ["What color is the floor", 2],
            ["What is on the floor", 1],
            ["Where is the picture taken", -1],
            ["how many people are standing on the floor", -1],
            ["What is the pattern of the floor", -1],
            ["what is the floor made of", 1],
            ["what type of room is this", 1],
            ["what shape is the floor", -1],
            ["what is the flooring", 1],
            ["what type of floor is this", 1],
            ["what is in the room", 1]
        ],
        "context": [
            "a bed that is made up of mattresses.",
            "a living room with a couch, television, and a couch."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2374602",
                "VG_object_id": "725837",
                "bbox": [65, 167, 257, 348],
                "image": "data\\images\\2374602.jpg"
            },
            {
                "VG_image_id": "2374424",
                "VG_object_id": "2677288",
                "bbox": [16, 37, 358, 218],
                "image": "data\\images\\2374424.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cat", 2],
            ["where is the cat", 1],
            ["what is the cat sitting on", 1],
            ["what is in front of the cat", 1]
        ],
        "org_questions": [
            ["what color is the cat", 2],
            ["where is the cat", 1],
            ["what is the cat doing", -1],
            ["what is the cat sitting on", 1],
            ["what gesture is the cat", -1],
            ["what is in front of the cat", 1],
            ["what is the cat on", -1],
            ["what kind of animal is in the picture", -1],
            ["how many cats are there", -1],
            ["who is in the photo", -1],
            ["what animal is shown", -1]
        ],
        "context": [
            "a black and white cat laying on a wooden floor.",
            "a cat is laying on a wooden bench."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2338180",
                "VG_object_id": "2163655",
                "bbox": [178, 356, 272, 440],
                "image": "data\\images\\2338180.jpg"
            },
            {
                "VG_image_id": "2368297",
                "VG_object_id": "2352493",
                "bbox": [154, 184, 226, 251],
                "image": "data\\images\\2368297.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the clock", 1],
            ["how many clocks are there", 1],
            ["where is the photo taken", 1],
            ["what time is showed on the clock", 1],
            ["what color is the clock", 1],
            ["what time of day is it", 1],
            ["what is behind the clock", 1],
            ["what time is it", 1],
            ["how many clocks", 1]
        ],
        "org_questions": [
            ["where is the clock", 1],
            ["how many clocks are there", 1],
            ["where is the photo taken", 1],
            ["what shape is the clock", -1],
            ["what time is showed on the clock", 1],
            ["what color is the clock", 1],
            ["what is on the clock", -1],
            ["what time of day is it", 1],
            ["what is behind the clock", 1],
            ["what time is it", 1],
            ["how many clocks", 1]
        ],
        "context": [
            "a white clock tower with a gold top and a gold dome.",
            "a library with a lot of clocks on the wall"
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2372823",
                "VG_object_id": "1688296",
                "bbox": [6, 52, 278, 331],
                "image": "data\\images\\2372823.jpg"
            },
            {
                "VG_image_id": "2357187",
                "VG_object_id": "1786229",
                "bbox": [87, 221, 233, 341],
                "image": "data\\images\\2357187.jpg"
            }
        ],
        "questions_with_scores": [["what is the horse doing", 1]],
        "org_questions": [
            ["what is the horse doing", 1],
            ["where is the horse", -1],
            ["what is on the horse's neck", -1],
            ["how many horses are there", -1],
            ["what color is the ground", -1],
            ["what is in the background", -1],
            ["what is the land made of", -1],
            ["what kind of animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["what animal is shown", -1]
        ],
        "context": [
            "a woman standing next to a white horse.",
            "a white horse grazing in a field with mountains in the background."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2352145",
                "VG_object_id": "1664633",
                "bbox": [1, 265, 302, 498],
                "image": "data\\images\\2352145.jpg"
            },
            {
                "VG_image_id": "2363684",
                "VG_object_id": "2647047",
                "bbox": [0, 120, 497, 330],
                "image": "data\\images\\2363684.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is on the ground", 2],
            ["What is the ground made of", 1],
            ["what is in the distance", 1],
            ["how is the weather", 1],
            ["where was this picture taken", 1],
            ["what is the weather like", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", 2],
            ["what is on the ground", 2],
            ["how many people are there", -1],
            ["where is the land", -1],
            ["What is the ground made of", 1],
            ["what is in the distance", 1],
            ["how is the weather", 1],
            ["where was this picture taken", 1],
            ["what is the weather like", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a young man is jumping a skateboard over a hill.",
            "a couple of wooden benches sitting on top of a brick sidewalk."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2317475",
                "VG_object_id": "3427915",
                "bbox": [1, 336, 318, 498],
                "image": "data\\images\\2317475.jpg"
            },
            {
                "VG_image_id": "2376130",
                "VG_object_id": "575911",
                "bbox": [3, 287, 330, 499],
                "image": "data\\images\\2376130.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the court", 1],
            ["What gender is the player on the court", 1]
        ],
        "org_questions": [
            ["What color is the court", 1],
            ["What is the court made of", -1],
            ["What gender is the player on the court", 1],
            ["how many people are there in the picture", -1],
            ["what is in the background", -1],
            ["what is on the ground", -1],
            ["where was this photo taken", -1],
            ["where is the tennis court", -1],
            ["what kind of court is this", -1],
            ["where is this scene", -1]
        ],
        "context": [
            "two men in white shirts and white shirts playing tennis.",
            "a woman in a blue tennis outfit is playing tennis"
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2315901",
                "VG_object_id": "3267683",
                "bbox": [22, 248, 113, 382],
                "image": "data\\images\\2315901.jpg"
            },
            {
                "VG_image_id": "2353799",
                "VG_object_id": "3406549",
                "bbox": [1, 106, 182, 191],
                "image": "data\\images\\2353799.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shelf", 2],
            ["where is the shelf placing", 1],
            ["what is the pattern of the wall", 1],
            ["what is the shelf made of", 1],
            ["what is on the shelf", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the shelf", 2],
            ["where is the shelf placing", 1],
            ["what is in front of the shelf", -1],
            ["what is the pattern of the wall", 1],
            ["what is the shelf made of", 1],
            ["what is on the shelf", 1],
            ["what is near the shelf", -1],
            ["how many people are there", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a man with a guitar in his hands.",
            "a bathroom with a sink, toilet, and a bathtub."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2348347",
                "VG_object_id": "1744097",
                "bbox": [295, 5, 396, 164],
                "image": "data\\images\\2348347.jpg"
            },
            {
                "VG_image_id": "2335223",
                "VG_object_id": "3615271",
                "bbox": [126, 13, 250, 276],
                "image": "data\\images\\2335223.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man riding", 2],
            ["what is in front of the boy", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is in the background", -1],
            ["how many boys are there", -1],
            ["what color are the boy's trousers", -1],
            ["Where is the man", -1],
            ["what is the boy holding", -1],
            ["what is in front of the boy", 1],
            ["when was the photo taken", 1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1],
            ["where was the photo taken", -1],
            ["what is the man riding", 2]
        ],
        "context": [
            "a skateboarder is riding on the ground in a skate park.",
            "a man riding a bike through a flooded street."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2340765",
                "VG_object_id": "2110339",
                "bbox": [134, 93, 245, 223],
                "image": "data\\images\\2340765.jpg"
            },
            {
                "VG_image_id": "2317042",
                "VG_object_id": "3431742",
                "bbox": [138, 91, 230, 180],
                "image": "data\\images\\2317042.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the ground", 2],
            ["What color is the man's shirt", 2],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is the ground", 2],
            ["What color is the man's shirt", 2],
            ["what gender is the person in the shirt", -1],
            ["what sport is the man playing", -1],
            ["what is the man holding", -1],
            ["what is the man doing", -1],
            ["when was the picture taken", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man swinging a tennis racket on a tennis court.",
            "a man hitting a tennis ball with a racquet."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2406943",
                "VG_object_id": "287811",
                "bbox": [160, 240, 213, 280],
                "image": "data\\images\\2406943.jpg"
            },
            {
                "VG_image_id": "2336570",
                "VG_object_id": "2220648",
                "bbox": [66, 168, 118, 269],
                "image": "data\\images\\2336570.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is wearing the trousers", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the trousers", -1],
            ["who is wearing the trousers", 1],
            ["where is the person", -1],
            ["how many people are there", -1],
            ["when was the picture taken", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a woman is throwing a frisbee on the beach.",
            "a man looks at a display of laptops."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2349557",
                "VG_object_id": "3028317",
                "bbox": [2, 2, 499, 374],
                "image": "data\\images\\2349557.jpg"
            },
            {
                "VG_image_id": "2332170",
                "VG_object_id": "3713465",
                "bbox": [2, 34, 498, 372],
                "image": "data\\images\\2332170.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bowls are on the table", 2],
            ["what main color is the table", 1],
            ["what kind of food is that on the table", 1],
            ["what is on the table", 1],
            ["what color is the thing on the table", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["what main color is the table", 1],
            ["how many bowls are on the table", 2],
            ["what kind of food is that on the table", 1],
            ["where is the food", -1],
            ["what is on the table", 1],
            ["what color is the thing on the table", 1],
            ["what is the table made of", -1],
            ["what is the plate sitting on", -1],
            ["where was this photo taken", -1],
            ["what is next to the plate", 1]
        ],
        "context": [
            "a grill with hot dogs and a tinfoil.",
            "a plate of cheese, tomatoes, cheese and cheese."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2320536",
                "VG_object_id": "2953819",
                "bbox": [219, 107, 325, 341],
                "image": "data\\images\\2320536.jpg"
            },
            {
                "VG_image_id": "2369245",
                "VG_object_id": "612843",
                "bbox": [229, 28, 398, 332],
                "image": "data\\images\\2369245.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on the head", 2],
            ["what is the man doing", 1],
            ["what is the color of the man's shirt", 1],
            ["what is the man holding", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man wearing on the head", 2],
            ["what is the color of the man's shirt", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the ground the man standing on made of", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what sport is being played", 1],
            ["where is the man", -1]
        ],
        "context": [
            "a man is swinging a bat at a ball.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2379670",
                "VG_object_id": "3834750",
                "bbox": [26, 77, 424, 251],
                "image": "data\\images\\2379670.jpg"
            },
            {
                "VG_image_id": "2354869",
                "VG_object_id": "2875267",
                "bbox": [10, 50, 364, 225],
                "image": "data\\images\\2354869.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the plane doing", 1],
            ["how many airplanes are in the sky", 1],
            ["what is behind the plane", 1]
        ],
        "org_questions": [
            ["what color is the sky", -1],
            ["what color is the land", -1],
            ["what is the plane doing", 1],
            ["how many airplanes are in the sky", 1],
            ["where is the plane", -1],
            ["what is the weather like", -1],
            ["what color is the ground", -1],
            ["when was the picture taken", -1],
            ["what is on the ground", -1],
            ["what is behind the plane", 1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a large jetliner flying through the air on a runway.",
            "a large jetliner sitting on top of an airport tarmac."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2396075",
                "VG_object_id": "2347471",
                "bbox": [68, 59, 245, 373],
                "image": "data\\images\\2396075.jpg"
            },
            {
                "VG_image_id": "2345637",
                "VG_object_id": "2348460",
                "bbox": [214, 3, 494, 330],
                "image": "data\\images\\2345637.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the photo", 2],
            ["what color is the woman's hair", 1],
            ["what is the woman looking at", 1],
            ["what is the woman holding", 1],
            ["who is in the picture", 1],
            ["what is around the woman's neck", 1],
            ["how is the woman's hair", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", -1],
            ["how many people are in the photo", 2],
            ["what color is the woman's hair", 1],
            ["Where are people", -1],
            ["what is the woman looking at", 1],
            ["What is lady wearing on her head", -1],
            ["what is the woman holding", 1],
            ["who is in the picture", 1],
            ["what is around the woman's neck", 1],
            ["how is the woman's hair", 1]
        ],
        "context": [
            "person, person, and person, from left, are pictured with people.",
            "a woman with a cat on her shoulder."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2417821",
                "VG_object_id": "2853420",
                "bbox": [89, 154, 254, 461],
                "image": "data\\images\\2417821.jpg"
            },
            {
                "VG_image_id": "2417133",
                "VG_object_id": "2996508",
                "bbox": [18, 44, 153, 263],
                "image": "data\\images\\2417133.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["where is the photo taken", 2],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["where is the photo taken", 2],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what is the race of the man", -1],
            ["what is the man doing", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man sitting on a bench in a shopping mall.",
            "two men standing in a living room."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2392444",
                "VG_object_id": "668052",
                "bbox": [210, 392, 260, 495],
                "image": "data\\images\\2392444.jpg"
            },
            {
                "VG_image_id": "2364556",
                "VG_object_id": "1920705",
                "bbox": [28, 404, 66, 498],
                "image": "data\\images\\2364556.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1],
            ["where was the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is the weather like", 1],
            ["what is in the distance", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the persion doing", 1],
            ["What color is person's shirt", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", 1],
            ["what is the persion wearing", -1],
            ["where was the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is the weather like", 1],
            ["what is in the distance", 1],
            ["how many people are there in the picture", 1]
        ],
        "context": [
            "a clock on a pole in a city.",
            "a red frisbee is in the air near a tree."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2414003",
                "VG_object_id": "161645",
                "bbox": [95, 211, 418, 437],
                "image": "data\\images\\2414003.jpg"
            },
            {
                "VG_image_id": "2408507",
                "VG_object_id": "3809165",
                "bbox": [18, 7, 405, 252],
                "image": "data\\images\\2408507.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the food in the bowl", 1],
            ["what is in the bowl", 1],
            ["what is inside the bowl", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the food in the bowl", 1],
            ["what color is the table", -1],
            ["how many plates are there in the picture", -1],
            ["what is the table made of", -1],
            ["where is the bowl", -1],
            ["what is in the bowl", 1],
            ["what is inside the bowl", 1],
            ["what is the bowl sitting on", -1],
            ["what is on the plate", -1],
            ["what is the table color", -1],
            ["what color is the plate", -1],
            ["what is on the table", 1]
        ],
        "context": [
            "a bowl of soup with meat and vegetables.",
            "a plate with a bowl of soup and a sandwich."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2342027",
                "VG_object_id": "937946",
                "bbox": [25, 266, 487, 387],
                "image": "data\\images\\2342027.jpg"
            },
            {
                "VG_image_id": "2397580",
                "VG_object_id": "1188558",
                "bbox": [193, 429, 332, 494],
                "image": "data\\images\\2397580.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["where is the picture taken", 1],
            ["What is the ground made of", 1],
            ["what is the condition of the ground", 1]
        ],
        "org_questions": [
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["where is the picture taken", 1],
            ["How many people are there", -1],
            ["how many zebras are there on the ground", -1],
            ["What is the ground made of", 1],
            ["how is the weather", -1],
            ["when was the photo taken", -1],
            ["what is the condition of the ground", 1]
        ],
        "context": [
            "a small airplane flying over a field.",
            "a white cow with red and green horns."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2388394",
                "VG_object_id": "3213183",
                "bbox": [0, 48, 492, 272],
                "image": "data\\images\\2388394.jpg"
            },
            {
                "VG_image_id": "2379675",
                "VG_object_id": "1355410",
                "bbox": [74, 98, 400, 192],
                "image": "data\\images\\2379675.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plane", 2],
            ["how many planes are there", 2],
            ["where is the plane", 1],
            ["what is the plane doing", 1],
            ["what color is the sky", 1],
            ["what is in the background", 1],
            ["where was the photo taken", 1],
            ["how many planes", 1]
        ],
        "org_questions": [
            ["what color is the plane", 2],
            ["where is the plane", 1],
            ["how many planes are there", 2],
            ["what is the plane doing", 1],
            ["what color is the sky", 1],
            ["what is in the background", 1],
            ["what kind of plane is this", -1],
            ["where was the photo taken", 1],
            ["how many planes", 1]
        ],
        "context": [
            "a fighter jet flying through a cloudy sky.",
            "a yellow jet sitting on top of an airport tarmac."
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2375632",
                "VG_object_id": "580840",
                "bbox": [0, 1, 494, 332],
                "image": "data\\images\\2375632.jpg"
            },
            {
                "VG_image_id": "2363119",
                "VG_object_id": "3747154",
                "bbox": [3, 12, 369, 496],
                "image": "data\\images\\2363119.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the bed", 2],
            ["What is the color of the ground", 1],
            ["How many windows are there", 1],
            ["what is the ground covered with", 1],
            ["what is on the left of the room", 1]
        ],
        "org_questions": [
            ["What color is the bed", 2],
            ["What is the color of the ground", 1],
            ["How many windows are there", 1],
            ["what is the ground covered with", 1],
            ["what is on the left of the room", 1],
            ["what is in the background", -1],
            ["what is on the wall", -1],
            ["what are in the room", -1],
            ["where was this taken", -1],
            ["what type of room is this", -1],
            ["where is the bed", -1],
            ["what is covering the bed", -1]
        ],
        "context": [
            "a bed with a white comforter and a blue light.",
            "a bed in a room with a large bed."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2408623",
                "VG_object_id": "257992",
                "bbox": [55, 1, 461, 107],
                "image": "data\\images\\2408623.jpg"
            },
            {
                "VG_image_id": "2406989",
                "VG_object_id": "287125",
                "bbox": [96, 3, 306, 160],
                "image": "data\\images\\2406989.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the umbrella", 1],
            ["what is the umbrella on", 1],
            ["what is on the umbrella", 1],
            ["what is on the left side of the photo", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 1],
            ["what is the umbrella on", 1],
            ["what is the man doing", -1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["where is the umbrella", -1],
            ["what is the weather like", -1],
            ["what is on the umbrella", 1],
            ["what is the umbrella made of", -1],
            ["what is on the left side of the photo", 1],
            ["what is in the background", 1],
            ["how is the weather", -1]
        ],
        "context": [
            "two men sitting under an umbrella on a sidewalk.",
            "a man and a woman sitting at a table with plates of food."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2332009",
                "VG_object_id": "2929515",
                "bbox": [49, 310, 291, 453],
                "image": "data\\images\\2332009.jpg"
            },
            {
                "VG_image_id": "2331878",
                "VG_object_id": "3398873",
                "bbox": [2, 1, 242, 75],
                "image": "data\\images\\2331878.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many sheeps are there in the picture", 2],
            ["how many boards are there in the picture", 2],
            ["what color is the board", 1]
        ],
        "org_questions": [
            ["what color is the board", 1],
            ["how many characters are there on the board", -1],
            ["where is the photo taken", -1],
            ["What is the background of image", -1],
            ["when was the picture taken", -1],
            ["what is in the background", -1],
            ["how many sheeps are there in the picture", 2],
            ["how many boards are there in the picture", 2]
        ],
        "context": [
            "a green and white sign",
            "a couple of sheep standing next to each other."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2356099",
                "VG_object_id": "2692917",
                "bbox": [90, 28, 374, 291],
                "image": "data\\images\\2356099.jpg"
            },
            {
                "VG_image_id": "2399691",
                "VG_object_id": "414440",
                "bbox": [5, 120, 122, 350],
                "image": "data\\images\\2399691.jpg"
            }
        ],
        "questions_with_scores": [["what color is the floor", 1]],
        "org_questions": [
            ["what color is the floor", 1],
            ["How many people are there", -1],
            ["What is in front of the shelf", -1],
            ["Where  is the shelf", -1],
            ["where was the photo taken", -1],
            ["what is next to the shelf", -1],
            ["how many books are there", -1],
            ["what is on the wall", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a desk with a computer and a laptop on it.",
            "a laptop computer sitting on top of a desk."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2400937",
                "VG_object_id": "404036",
                "bbox": [148, 95, 256, 196],
                "image": "data\\images\\2400937.jpg"
            },
            {
                "VG_image_id": "2408541",
                "VG_object_id": "259701",
                "bbox": [29, 58, 152, 177],
                "image": "data\\images\\2408541.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is on the screen", 1]
        ],
        "org_questions": [
            ["what is on the screen", 1],
            ["where is the television", -1],
            ["how many people are there in the picture", 2],
            ["what color is the table", -1],
            ["what kind of TV is it", -1],
            ["what is behind the television", -1],
            ["what is turned on", -1],
            ["where is the picture taken", -1],
            ["what is on the back of the tv", -1]
        ],
        "context": [
            "a little girl playing a game on the television.",
            "a desk with a pizza box and a tv on it"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2334586",
                "VG_object_id": "3926326",
                "bbox": [0, 109, 374, 498],
                "image": "data\\images\\2334586.jpg"
            },
            {
                "VG_image_id": "2326560",
                "VG_object_id": "982058",
                "bbox": [0, 339, 329, 499],
                "image": "data\\images\\2326560.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 2],
            ["what color is the table", 2],
            ["how many forks are there on the plate", 2],
            ["what kind of food is on the plate", 1],
            ["what is the persion doing", 1],
            ["what is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the plate", 2],
            ["what color is the table", 2],
            ["how many forks are there on the plate", 2],
            ["what kind of food is on the plate", 1],
            ["what is the persion doing", 1],
            ["what is on the plate", 1],
            ["what is the table made of", -1],
            ["what is under the table", -1],
            ["where was the photo taken", -1],
            ["what is in the table", -1]
        ],
        "context": [
            "a plate of food with meat and vegetables",
            "a young boy is eating a piece of pizza."
        ]
    },
    {
        "object_category": "lamp",
        "images": [
            {
                "VG_image_id": "2352877",
                "VG_object_id": "2346994",
                "bbox": [3, 53, 70, 113],
                "image": "data\\images\\2352877.jpg"
            },
            {
                "VG_image_id": "2412133",
                "VG_object_id": "202168",
                "bbox": [137, 1, 205, 61],
                "image": "data\\images\\2412133.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the lamp", 1],
            ["what color is the lamp", 1],
            ["What color is lamp", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["where is the lamp", 1],
            ["what color is the lamp", 1],
            ["how many people are there in the picture", -1],
            ["when is this picture taken", -1],
            ["What color is lamp", 1],
            ["where is the picture taken", 1],
            ["how many lights are there", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a street sign on a pole in front of a tall building.",
            "two women sitting at a table with a pizza on it."
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2361618",
                "VG_object_id": "3754000",
                "bbox": [262, 135, 368, 326],
                "image": "data\\images\\2361618.jpg"
            },
            {
                "VG_image_id": "2350382",
                "VG_object_id": "1855297",
                "bbox": [307, 87, 461, 246],
                "image": "data\\images\\2350382.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many motorcycles are there", 2],
            ["How many people are there", 2],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["How many motorcycles are there", 2],
            ["How many people are there", 2],
            ["What color is the image", -1],
            ["what is the ground covered with", -1],
            ["where are the motor ", -1],
            ["what is in the background", -1],
            ["where is the motorcycle", -1],
            ["what is the man doing", -1],
            ["what type of vehicle is shown", -1],
            ["who is on the motorcycle", -1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a group of people riding bikes down a street.",
            "a man riding a motorcycle down a street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2359153",
                "VG_object_id": "795606",
                "bbox": [3, 215, 500, 356],
                "image": "data\\images\\2359153.jpg"
            },
            {
                "VG_image_id": "2405813",
                "VG_object_id": "371814",
                "bbox": [21, 217, 493, 330],
                "image": "data\\images\\2405813.jpg"
            }
        ],
        "questions_with_scores": [
            ["when is the picture taken", 1],
            ["what color is the train", 1]
        ],
        "org_questions": [
            ["what color is the ground ", -1],
            ["when is the picture taken", 1],
            ["what color is the train", 1],
            ["where is this photo taken", -1],
            ["what is on the land", -1],
            ["where are the tracks", -1],
            ["what is next to the train", -1],
            ["what is on the side of the train", -1]
        ],
        "context": [
            "a train is traveling down the tracks near a field.",
            "a train is coming down the tracks at night."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2370662",
                "VG_object_id": "1784343",
                "bbox": [0, 2, 497, 323],
                "image": "data\\images\\2370662.jpg"
            },
            {
                "VG_image_id": "2331876",
                "VG_object_id": "971126",
                "bbox": [1, 26, 497, 334],
                "image": "data\\images\\2331876.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of food is that on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what kind of food is that on the table", 1],
            ["how many plates are there on the table", -1],
            ["what is the table made of", -1],
            ["What is on the table", -1],
            ["How many people are there", -1],
            ["what is the plate sitting on", -1],
            ["how is the table made", -1],
            ["what type of table is this", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "a plate of food on a table",
            "a hot dog with cheese and toppings on a bun."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2414143",
                "VG_object_id": "159136",
                "bbox": [229, 99, 400, 320],
                "image": "data\\images\\2414143.jpg"
            },
            {
                "VG_image_id": "2322280",
                "VG_object_id": "2819261",
                "bbox": [214, 127, 370, 412],
                "image": "data\\images\\2322280.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is player's shirt", 1],
            ["What color is the ground", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is player's shirt", 1],
            ["What color is the ground", 1],
            ["what is the player doing", -1],
            ["what is the player wearing", -1],
            ["what is the player holding", -1],
            ["how many players are there in the picture", -1],
            ["where is the ball", -1],
            ["who is playing tennis", -1],
            ["what sport is the man playing", -1],
            ["why is the man holding a racket", -1]
        ],
        "context": [
            "a man playing tennis on a clay court.",
            "a man swinging a tennis racket at a ball"
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2338600",
                "VG_object_id": "2720335",
                "bbox": [178, 267, 279, 400],
                "image": "data\\images\\2338600.jpg"
            },
            {
                "VG_image_id": "2363162",
                "VG_object_id": "767592",
                "bbox": [23, 185, 195, 386],
                "image": "data\\images\\2363162.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color of the building", 1]
        ],
        "org_questions": [
            ["what is the main color of the clock", -1],
            ["what is in front of the clock", -1],
            ["what is the main color of the building", 1],
            ["how many clocks are there", -1],
            ["what time is it", -1],
            ["what is the shape of the clock", -1],
            ["where is the clock", -1],
            ["what is the clock made of", -1],
            ["when was the photo taken", -1],
            ["what is above the clock", -1],
            ["what is on the clock", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a clock tower with a sky background",
            "a clock with a golden sun and a gold sun dial."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2404938",
                "VG_object_id": "336347",
                "bbox": [123, 53, 480, 280],
                "image": "data\\images\\2404938.jpg"
            },
            {
                "VG_image_id": "2373666",
                "VG_object_id": "588218",
                "bbox": [158, 186, 210, 430],
                "image": "data\\images\\2373666.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bears are there", 2],
            ["what is the bear doing", 1],
            ["where is the bear", 1],
            ["what is the bear standing on", 1]
        ],
        "org_questions": [
            ["how many bears are there", 2],
            ["what is the bear doing", 1],
            ["where is the bear", 1],
            ["what is the weather like", -1],
            ["what is in the distance", -1],
            ["what is in front of the bear", -1],
            ["when was this picture taken", -1],
            ["what type of animal is shown", -1],
            ["what is the bear looking at", -1],
            ["what is the bear standing on", 1],
            ["what is on the bear", -1]
        ],
        "context": [
            "a black bear walking around a wooden fence.",
            "two black bears climbing a tree in a forest."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2348758",
                "VG_object_id": "2239708",
                "bbox": [35, 132, 110, 362],
                "image": "data\\images\\2348758.jpg"
            },
            {
                "VG_image_id": "2388800",
                "VG_object_id": "1266753",
                "bbox": [19, 30, 441, 325],
                "image": "data\\images\\2388800.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the background", 1],
            ["how many people are there in the picture", 1],
            ["what is the man doing", 1],
            ["how many persons are there", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the background", 1],
            ["how many people are there in the picture", 1],
            ["What is the man wearing on his head", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", -1],
            ["how many persons are there", 1],
            ["what is the gender of the person in the photo", -1]
        ],
        "context": [
            "a group of people standing around a counter.",
            "a man wearing a tie and a tie holding up a small pink box."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2356856",
                "VG_object_id": "2329479",
                "bbox": [0, 180, 496, 330],
                "image": "data\\images\\2356856.jpg"
            },
            {
                "VG_image_id": "2373586",
                "VG_object_id": "3850889",
                "bbox": [72, 86, 499, 329],
                "image": "data\\images\\2373586.jpg"
            }
        ],
        "questions_with_scores": [["what is on the beach", 1]],
        "org_questions": [
            ["what color is the beach", -1],
            ["how many people are there on the beach", -1],
            ["what is on the beach", 1],
            ["what are the people doing in the picture", -1],
            ["what is the man doing", -1],
            ["how is the weather", -1],
            ["where was this picture taken", -1],
            ["what is in the background", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a man walking on the beach with a surfboard.",
            "a boat is pulling a boat out of the water."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2334700",
                "VG_object_id": "2846670",
                "bbox": [4, 97, 497, 372],
                "image": "data\\images\\2334700.jpg"
            },
            {
                "VG_image_id": "2319184",
                "VG_object_id": "2983687",
                "bbox": [0, 264, 332, 499],
                "image": "data\\images\\2319184.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the player doing", 2],
            ["what color is the land", 1],
            ["what is the man on the land doing", 1],
            ["how many shoes are there", 1],
            ["what is the land made of", 1],
            ["What is on the land", 1],
            ["how many people are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is the man on the land doing", 1],
            ["how many shoes are there", 1],
            ["what is the land made of", 1],
            ["what is the weather like", -1],
            ["What is on the land", 1],
            ["where was this photo taken", -1],
            ["where are the white lines", -1],
            ["how many people are in the picture", 1],
            ["what is the player doing", 2]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a man playing tennis on a clay court"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2372378",
                "VG_object_id": "2265353",
                "bbox": [263, 4, 472, 336],
                "image": "data\\images\\2372378.jpg"
            },
            {
                "VG_image_id": "2396488",
                "VG_object_id": "443721",
                "bbox": [187, 4, 280, 189],
                "image": "data\\images\\2396488.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what color is the man's pants", -1],
            ["what is the man wearing on his face", -1],
            ["where is the photo taken", -1],
            ["what is the man doing", 2],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what kind of pants is the man wearing", -1],
            ["what is the man holding", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man standing next to a parking meter.",
            "a man riding a skateboard on a ledge."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2327909",
                "VG_object_id": "3226675",
                "bbox": [207, 323, 280, 418],
                "image": "data\\images\\2327909.jpg"
            },
            {
                "VG_image_id": "2415042",
                "VG_object_id": "147171",
                "bbox": [146, 29, 218, 114],
                "image": "data\\images\\2415042.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what gesture is the man", 1],
            ["what is the man riding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is on the head on the man", -1],
            ["what is the man wearing", -1],
            ["what gesture is the man", 1],
            ["What is on person's head", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man riding", 1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "three people sitting on a motorcycle in front of a building.",
            "a group of people standing on top of a van."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2379064",
                "VG_object_id": "713292",
                "bbox": [3, 14, 389, 332],
                "image": "data\\images\\2379064.jpg"
            },
            {
                "VG_image_id": "2411432",
                "VG_object_id": "1079920",
                "bbox": [21, 140, 499, 242],
                "image": "data\\images\\2411432.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants are in the picture", 2],
            ["what color is the elephant", 1]
        ],
        "org_questions": [
            ["how many elephants are in the picture", 2],
            ["what color is the elephant", 1],
            ["what is the elephant standing on", -1],
            ["where are the elephants", -1],
            ["what is on the elephant's feet", -1],
            ["what is the ground covered with", -1],
            ["when was the photo taken", -1],
            ["what is in the background", -1],
            ["where was this photo taken", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "an elephant is walking through tall grass in the wild.",
            "a herd of elephants grazing in a field."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2333575",
                "VG_object_id": "3219409",
                "bbox": [154, 52, 305, 274],
                "image": "data\\images\\2333575.jpg"
            },
            {
                "VG_image_id": "2356544",
                "VG_object_id": "819528",
                "bbox": [241, 3, 374, 218],
                "image": "data\\images\\2356544.jpg"
            }
        ],
        "questions_with_scores": [
            ["how large is the box", 1],
            ["where is the box", 1],
            ["what is on the side of the box", 1],
            ["what is the box placed on", 1],
            ["where was this photo taken", 1],
            ["where is the picture taken", 1],
            ["what is beside the box", 1],
            ["what is in the background", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the box", -1],
            ["how large is the box", 1],
            ["where is the box", 1],
            ["what is on the side of the box", 1],
            ["how many people are there", -1],
            ["what is the box placed on", 1],
            ["when was the photo taken", -1],
            ["where was this photo taken", 1],
            ["what is in the middle of the room", -1],
            ["where is the picture taken", 1],
            ["what is beside the box", 1],
            ["what is in the background", 1],
            ["what is the box made of", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a living room with a couch, couch, and television.",
            "a blender and a blender on a table."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2382890",
                "VG_object_id": "1325281",
                "bbox": [2, 106, 374, 499],
                "image": "data\\images\\2382890.jpg"
            },
            {
                "VG_image_id": "2368610",
                "VG_object_id": "617084",
                "bbox": [63, 164, 165, 237],
                "image": "data\\images\\2368610.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the dog wearing on its head", 1],
            ["Where is the dog", 1],
            ["what is the ground covered with", 1],
            ["what is in front of the dog", 1],
            ["what is the dog standing on", 1],
            ["what is the dog looking at", 1]
        ],
        "org_questions": [
            ["What is the dog wearing on its head", 1],
            ["Where is the dog", 1],
            ["What color is the background", -1],
            ["how many dogs are there in the picture", -1],
            ["what is the ground covered with", 1],
            ["what is the dog doing", -1],
            ["what is in front of the dog", 1],
            ["what is the dog standing on", 1],
            ["what animal is in the picture", -1],
            ["when was this photo taken", -1],
            ["what is the dog looking at", 1],
            ["what is on the dog's face", -1]
        ],
        "context": [
            "a dog wearing a hat and wearing a hat.",
            "a man and a dog on the beach with a frisbee."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2358579",
                "VG_object_id": "2328168",
                "bbox": [74, 415, 299, 499],
                "image": "data\\images\\2358579.jpg"
            },
            {
                "VG_image_id": "2401722",
                "VG_object_id": "1144958",
                "bbox": [226, 384, 370, 500],
                "image": "data\\images\\2401722.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 1],
            ["where is the floor", 1],
            ["what is the color of the rug", 1],
            ["how is the floor made", 1],
            ["what kind of flooring is in the room", 1],
            ["what material is the floor", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 1],
            ["where is the floor", 1],
            ["what shape is the rug", -1],
            ["how many people are there in the photo", -1],
            ["what is the color of the rug", 1],
            ["what is on the rug", -1],
            ["what pattern is on the floor", -1],
            ["how is the floor made", 1],
            ["what kind of flooring is in the room", 1],
            ["what material is the floor", 1]
        ],
        "context": [
            "a bathroom with a tub, toilet, and a shower.",
            "a bedroom with a view of a grassy field."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2353825",
                "VG_object_id": "2019301",
                "bbox": [40, 56, 293, 458],
                "image": "data\\images\\2353825.jpg"
            },
            {
                "VG_image_id": "2345808",
                "VG_object_id": "1889913",
                "bbox": [46, 2, 448, 326],
                "image": "data\\images\\2345808.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many giraffes are there", 2],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["how many giraffes are there", 2],
            ["where is the tree", -1],
            ["what kinds of animal is it", -1],
            ["what is the ground covered with", 1],
            ["what is the color of the grass", -1],
            ["when was the photo taken", -1],
            ["where was this picture taken", -1],
            ["what is in the background", -1],
            ["where are the giraffes", -1]
        ],
        "context": [
            "a giraffe standing next to a tree in a zoo.",
            "two giraffes standing next to each other in a forest."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2402959",
                "VG_object_id": "384650",
                "bbox": [92, 74, 198, 213],
                "image": "data\\images\\2402959.jpg"
            },
            {
                "VG_image_id": "2341976",
                "VG_object_id": "3345323",
                "bbox": [292, 29, 426, 289],
                "image": "data\\images\\2341976.jpg"
            }
        ],
        "questions_with_scores": [["what is the woman doing", 1]],
        "org_questions": [
            ["how many people are there", -1],
            ["what is the woman doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man in a wet suit is surfing in a river.",
            "a woman riding a horse on the beach."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2409959",
                "VG_object_id": "229377",
                "bbox": [361, 180, 467, 227],
                "image": "data\\images\\2409959.jpg"
            },
            {
                "VG_image_id": "2380815",
                "VG_object_id": "1341935",
                "bbox": [258, 239, 495, 297],
                "image": "data\\images\\2380815.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the bench", 2],
            ["what is in the background", 1],
            ["what is the ground covered with", 1],
            ["where was the photo taken", 1],
            ["what is on the bench", 1],
            ["What is in the background of image", 1],
            ["what is the bench sitting on", 1],
            ["where is the bench", 1],
            ["what is under the bench", 1]
        ],
        "org_questions": [
            ["what is in the background", 1],
            ["how many people are there on the bench", 2],
            ["what is the ground covered with", 1],
            ["where was the photo taken", 1],
            ["what is on the bench", 1],
            ["What is in the background of image", 1],
            ["what is the bench made of", -1],
            ["when was the picture taken", -1],
            ["what is the bench sitting on", 1],
            ["where is the bench", 1],
            ["what is under the bench", 1]
        ],
        "context": [
            "a bench sitting on a hill in front of a mountain.",
            "a couple sitting on a bench in the city."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2341886",
                "VG_object_id": "2434580",
                "bbox": [327, 68, 429, 285],
                "image": "data\\images\\2341886.jpg"
            },
            {
                "VG_image_id": "2399876",
                "VG_object_id": "1165202",
                "bbox": [207, 49, 336, 112],
                "image": "data\\images\\2399876.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["where is the picture taken", 2],
            ["how many people are there", 1],
            ["what color is the pants", 1],
            ["what is the person holding", 1],
            ["What is person doing", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what are the people doing", 2],
            ["what color is the pants", 1],
            ["where is the picture taken", 2],
            ["what is the person holding", 1],
            ["What is person doing", 1],
            ["what is the man doing", 1],
            ["who is wearing a helmet", -1],
            ["what is the man wearing on the head", -1],
            ["when was the photo taken", -1],
            ["what is the person wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a family on skis posing for a picture.",
            "a man is doing a trick on a dirt bike."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2320263",
                "VG_object_id": "994679",
                "bbox": [1, 162, 498, 333],
                "image": "data\\images\\2320263.jpg"
            },
            {
                "VG_image_id": "2366267",
                "VG_object_id": "3880043",
                "bbox": [2, 364, 490, 494],
                "image": "data\\images\\2366267.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 2],
            ["where is the photo taken", 2],
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["Whatis the background of image", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["what is in the background", 2],
            ["how many cows are there on the ground", -1],
            ["where is the photo taken", 2],
            ["Whatis the background of image", 1],
            ["how is the weather", -1],
            ["what is the weather like", -1],
            ["how many people are there", 1]
        ],
        "context": [
            "a motorcycle parked on the side of a street.",
            "a couple of people standing on top of a beach."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2396638",
                "VG_object_id": "442057",
                "bbox": [182, 219, 337, 372],
                "image": "data\\images\\2396638.jpg"
            },
            {
                "VG_image_id": "2417370",
                "VG_object_id": "3081215",
                "bbox": [160, 195, 322, 332],
                "image": "data\\images\\2417370.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's pants", 2],
            ["what sport is being played", 2],
            ["what sport is the man playing", 2],
            ["how many people are there", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what color are the man's pants", 2],
            ["what color is the background", -1],
            ["what is the man wearing on the head", -1],
            ["what is the man doing", 1],
            ["what is the weather like", -1],
            ["what color is the ground", -1],
            ["when was this picture taken", -1],
            ["what sport is being played", 2],
            ["what is the player wearing", -1],
            ["what is the man wearing on his feet", -1],
            ["what sport is the man playing", 2]
        ],
        "context": [
            "a man is playing cricket on a field.",
            "person dribbles the ball down the field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2339403",
                "VG_object_id": "951495",
                "bbox": [346, 98, 453, 257],
                "image": "data\\images\\2339403.jpg"
            },
            {
                "VG_image_id": "2367238",
                "VG_object_id": "3874850",
                "bbox": [16, 110, 252, 305],
                "image": "data\\images\\2367238.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 2],
            ["how many people are in the photo", 2],
            ["how many shirts are there", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 2],
            ["Where is the image", -1],
            ["how many shirts are there", 1],
            ["what is the gender of the person", -1],
            ["what is the man doing", -1],
            ["what is in the distance", -1],
            ["where is the photo taken", 1],
            ["what is the man sitting on", -1],
            ["what is the man wearing", -1],
            ["how many people are in the photo", 2]
        ],
        "context": [
            "a man and a woman sitting at a table with food.",
            "a man and a woman sitting at a table with a child."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2370610",
                "VG_object_id": "1995110",
                "bbox": [217, 207, 312, 314],
                "image": "data\\images\\2370610.jpg"
            },
            {
                "VG_image_id": "2318523",
                "VG_object_id": "1009846",
                "bbox": [2, 132, 105, 302],
                "image": "data\\images\\2318523.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is person doing", 2],
            ["WHat color is ground", 1],
            ["WHat color is person's shirt", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["WHat color is ground", 1],
            ["WHat color is person's shirt", 1],
            ["What is person doing", 2],
            ["How many people are there", -1],
            ["what time is it", -1],
            ["where is the person", -1],
            ["what is the person wearing", -1],
            ["what is the man doing", -1],
            ["who is in the photo", -1],
            ["what sport is being played", -1],
            ["when was the photo taken", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a young boy playing tennis on a tennis court.",
            "a boy holding a tennis ball and racket in a park."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2365648",
                "VG_object_id": "2575086",
                "bbox": [3, 39, 499, 373],
                "image": "data\\images\\2365648.jpg"
            },
            {
                "VG_image_id": "2317591",
                "VG_object_id": "3417594",
                "bbox": [19, 1, 331, 497],
                "image": "data\\images\\2317591.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is the persion in the middle holding", 1],
            ["what is the woman wearing", 1],
            ["what is the woman doing", 1],
            ["What is woman holding", 1],
            ["Where is the woman", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 2],
            ["what is the persion in the middle holding", 1],
            ["Where is the woman standing", -1],
            ["what is the woman wearing", 1],
            ["what is the woman doing", 1],
            ["What is woman holding", 1],
            ["Where is the woman", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is behind the woman", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "three women standing next to each other holding wine glasses.",
            "a woman brushing her teeth with a toothbrush."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2380971",
                "VG_object_id": "708236",
                "bbox": [76, 411, 299, 486],
                "image": "data\\images\\2380971.jpg"
            },
            {
                "VG_image_id": "2385596",
                "VG_object_id": "1294862",
                "bbox": [3, 387, 332, 500],
                "image": "data\\images\\2385596.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 2],
            ["where is the table", 1],
            ["what is in the background", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 2],
            ["what is the table made of", -1],
            ["how many chairs are there beside the table", -1],
            ["where is the table", 1],
            ["what is in the background", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a young boy eating a donut outside.",
            "a red flower in a vase on a table."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2332086",
                "VG_object_id": "3418874",
                "bbox": [70, 100, 195, 301],
                "image": "data\\images\\2332086.jpg"
            },
            {
                "VG_image_id": "2329855",
                "VG_object_id": "3266243",
                "bbox": [82, 52, 403, 371],
                "image": "data\\images\\2329855.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["what is in the woman's left hand", 1]
        ],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what is the woman holding", 1],
            ["how many people are there", -1],
            ["Where is the person", -1],
            ["What is the background of image", -1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is behind the woman", -1],
            ["what is in the woman's left hand", 1]
        ],
        "context": [
            "a woman holding a toothbrush in front of a display.",
            "a woman holding a hot dog in her hand."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2342602",
                "VG_object_id": "932539",
                "bbox": [11, 109, 174, 209],
                "image": "data\\images\\2342602.jpg"
            },
            {
                "VG_image_id": "2407462",
                "VG_object_id": "1097594",
                "bbox": [28, 104, 103, 177],
                "image": "data\\images\\2407462.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the pillow", 2],
            ["how many people are there", 2],
            ["what is the pillow placed on", 1],
            ["where is the pillow placed on", 1],
            ["where was the photo taken", 1],
            ["where are the pillows", 1]
        ],
        "org_questions": [
            ["what color is the pillow", 2],
            ["what is the pillow placed on", 1],
            ["how many people are there", 2],
            ["where is the pillow", -1],
            ["where is the pillow placed on", 1],
            ["how many pillows are in the picture", -1],
            ["where was the photo taken", 1],
            ["how many pillows", -1],
            ["where are the pillows", 1]
        ],
        "context": [
            "a person laying on a bed with a bag on their shoulder",
            "a living room with a white table and pink flowers on it."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2329084",
                "VG_object_id": "3700710",
                "bbox": [0, 0, 498, 372],
                "image": "data\\images\\2329084.jpg"
            },
            {
                "VG_image_id": "2408338",
                "VG_object_id": "263659",
                "bbox": [2, 12, 497, 373],
                "image": "data\\images\\2408338.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many glasses are there on the table", 2],
            ["what is the cup on", 1],
            ["what food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", -1],
            ["what is the cup on", 1],
            ["how many people are there", -1],
            ["what is the table made of", -1],
            ["what food is on the plate", 1],
            ["how many glasses are there on the table", 2],
            ["what color is the plate on the table", -1],
            ["where was the picture taken", -1],
            ["where is the cup", -1],
            ["what is next to the plate", -1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a plate of food with eggs, bacon, and a sandwich.",
            "a plate of food and a cup of coffee on a table."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2358594",
                "VG_object_id": "1676762",
                "bbox": [373, 112, 446, 227],
                "image": "data\\images\\2358594.jpg"
            },
            {
                "VG_image_id": "2353977",
                "VG_object_id": "2129053",
                "bbox": [7, 8, 234, 277],
                "image": "data\\images\\2353977.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["where is the woman", 2],
            ["What is the woman doing", 1],
            ["what color is the woman's shirt", 1],
            ["what is the persion sitting on", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what color is the floor", -1],
            ["where is the woman", 2],
            ["What is the woman doing", 1],
            ["what is the woman wearing", -1],
            ["what color is the woman's shirt", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1],
            ["where are the people", 1],
            ["what are the people doing", -1]
        ],
        "context": [
            "a library with a lot of laptops on the table",
            "a man sitting on a bench with his head on his head."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2346651",
                "VG_object_id": "897473",
                "bbox": [158, 134, 289, 283],
                "image": "data\\images\\2346651.jpg"
            },
            {
                "VG_image_id": "2409049",
                "VG_object_id": "250007",
                "bbox": [358, 164, 483, 275],
                "image": "data\\images\\2409049.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["how many people are there", 2],
            ["what is the gesture of the person", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["how many persons are there in the picture", -1],
            ["what is the gesture of the person", 1],
            ["who is wearing the shirt", -1],
            ["what is the pattern of the boy's shirt", -1],
            ["what is the persion doing", 1],
            ["how many people are there", 2],
            ["what are the people wearing", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a man standing in the woods talking on a cell phone.",
            "three children sitting on a bench with their hands on their knees."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2356825",
                "VG_object_id": "2560573",
                "bbox": [34, 214, 141, 341],
                "image": "data\\images\\2356825.jpg"
            },
            {
                "VG_image_id": "2336000",
                "VG_object_id": "2623423",
                "bbox": [2, 0, 499, 374],
                "image": "data\\images\\2336000.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants", 2],
            ["How many elephants are there", 1]
        ],
        "org_questions": [
            ["How many elephants are there", 1],
            ["What is on the elephant", -1],
            ["How many people are there", -1],
            ["where is picture taken", -1],
            ["what color is the elephants", -1],
            ["what is the elephant doing", -1],
            ["where are the elephants", -1],
            ["what is in front of the elephant", -1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["who is in the picture", -1],
            ["how many elephants", 2]
        ],
        "context": [
            "a group of people riding on the back of an elephant.",
            "a man standing next to a herd of elephants."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2342134",
                "VG_object_id": "936887",
                "bbox": [119, 265, 197, 374],
                "image": "data\\images\\2342134.jpg"
            },
            {
                "VG_image_id": "2412390",
                "VG_object_id": "195346",
                "bbox": [53, 310, 213, 455],
                "image": "data\\images\\2412390.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["who is wearing the trousers", 1],
            ["what color is the ground", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["who is wearing the trousers", 1],
            ["what color is the ground", 1],
            ["what is the man standing on", 1]
        ],
        "context": [
            "a woman standing in a kitchen with a plate of food.",
            "a man holding a frisbee while standing on a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2403021",
                "VG_object_id": "657653",
                "bbox": [355, 160, 493, 333],
                "image": "data\\images\\2403021.jpg"
            },
            {
                "VG_image_id": "2392207",
                "VG_object_id": "1232798",
                "bbox": [222, 16, 384, 184],
                "image": "data\\images\\2392207.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's hair", 1],
            ["what is the man wearing", 1],
            ["How many people are there", 1],
            ["how old is the man", 1],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1]
        ],
        "org_questions": [
            ["what color is the man's hair", 1],
            ["where is the man", -1],
            ["what is the man wearing", 1],
            ["How many people are there", 1],
            ["how old is the man", 1],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man wearing a suit and sunglasses",
            "a man is standing in front of a laptop."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2343251",
                "VG_object_id": "925818",
                "bbox": [0, 281, 360, 498],
                "image": "data\\images\\2343251.jpg"
            },
            {
                "VG_image_id": "2320360",
                "VG_object_id": "3174357",
                "bbox": [13, 391, 349, 498],
                "image": "data\\images\\2320360.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the land", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what main color is the land", -1],
            ["what is on the land", 1],
            ["where is the photo taken", 1],
            ["how many people are there on the ground", -1],
            ["what is the ground covered with", 1],
            ["how is the weather", -1],
            ["when was the picture taken", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a tree with yellow leaves on it sitting under a tree.",
            "a skateboard with stickers on it"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2410508",
                "VG_object_id": "215815",
                "bbox": [133, 196, 180, 320],
                "image": "data\\images\\2410508.jpg"
            },
            {
                "VG_image_id": "2320502",
                "VG_object_id": "3325470",
                "bbox": [135, 232, 201, 348],
                "image": "data\\images\\2320502.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man's posture", 2],
            ["when was this photo taken", 1],
            ["what is the person doing", 1]
        ],
        "org_questions": [
            ["what is the man's posture", 2],
            ["what color is the sky", -1],
            ["how many people are there", -1],
            ["where is the man standing ", -1],
            ["what is the weather like", -1],
            ["what is the persion wearing around his neck", -1],
            ["when was this photo taken", 1],
            ["what is the person doing", 1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a person jumping in the air to catch a frisbee.",
            "a man sitting on a surfboard on the beach."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2348830",
                "VG_object_id": "879487",
                "bbox": [43, 38, 337, 499],
                "image": "data\\images\\2348830.jpg"
            },
            {
                "VG_image_id": "2403496",
                "VG_object_id": "352512",
                "bbox": [206, 31, 476, 331],
                "image": "data\\images\\2403496.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding in her right hand", 2],
            ["how many women are there in the picture", 2],
            ["What is woman doing", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman holding in her right hand", 2],
            ["how many women are there in the picture", 2],
            ["where is the woman", -1],
            ["What is woman doing", 1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["what is the woman holding", 1]
        ],
        "context": [
            "a woman holding a cell phone in her hand.",
            "a man and woman holding champagne glasses in their hands."
        ]
    },
    {
        "object_category": "lamp",
        "images": [
            {
                "VG_image_id": "2380948",
                "VG_object_id": "708296",
                "bbox": [0, 80, 73, 194],
                "image": "data\\images\\2380948.jpg"
            },
            {
                "VG_image_id": "2325694",
                "VG_object_id": "2760734",
                "bbox": [411, 37, 491, 210],
                "image": "data\\images\\2325694.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is under the lamp", 2],
            ["What color is lamp", 1],
            ["where is the lamp", 1],
            ["What is next to the lamp", 1],
            ["where is the picture taken", 1],
            ["what is the lamp putting on", 1],
            ["Where is the lamp standing on", 1],
            ["what is hanging on the wall", 1]
        ],
        "org_questions": [
            ["What color is lamp", 1],
            ["where is the lamp", 1],
            ["What is next to the lamp", 1],
            ["how many lamps are on", -1],
            ["where is the picture taken", 1],
            ["what is the lamp putting on", 1],
            ["Where is the lamp standing on", 1],
            ["what is under the lamp", 2],
            ["what is hanging on the wall", 1],
            ["what is on top of the lamp", -1]
        ],
        "context": [
            "a bathroom with a tub, toilet, and sink.",
            "a bed with a black bag on it and a lamp"
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2352382",
                "VG_object_id": "3783499",
                "bbox": [123, 257, 414, 365],
                "image": "data\\images\\2352382.jpg"
            },
            {
                "VG_image_id": "2410929",
                "VG_object_id": "318803",
                "bbox": [246, 291, 494, 423],
                "image": "data\\images\\2410929.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the sheets", 2],
            ["what is the color of the bed", 2],
            ["how many pillows are on the bed", 1]
        ],
        "org_questions": [
            ["what color is the blanket", -1],
            ["how many light sources are there", -1],
            ["what is on the bed", -1],
            ["where is the blanket", -1],
            ["how many people are on the blanket", -1],
            ["what is next to the bed", -1],
            ["where are the pillows", -1],
            ["how many pillows are on the bed", 1],
            ["what color are the sheets", 2],
            ["what is the color of the bed", 2]
        ],
        "context": [
            "a bedroom with a bed and a chair.",
            "a man jumping in the air in a bedroom."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2367517",
                "VG_object_id": "621902",
                "bbox": [69, 117, 474, 252],
                "image": "data\\images\\2367517.jpg"
            },
            {
                "VG_image_id": "2317073",
                "VG_object_id": "3348577",
                "bbox": [9, 109, 396, 288],
                "image": "data\\images\\2317073.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 2],
            ["what color is the ground", 1],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["what is the main color of the plane", -1],
            ["what color is the ground", 1],
            ["What is the number of plane", -1],
            ["what is the plane doing", -1],
            ["what color is the background", 1],
            ["where was the photo taken", -1],
            ["what is on the ground", 2],
            ["when was the picture taken", -1],
            ["what kind of plane is this", -1],
            ["what is behind the plane", -1]
        ],
        "context": [
            "a plane is taking off from the runway.",
            "a small airplane is on a runway with a cloudy sky in the background."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2353648",
                "VG_object_id": "844201",
                "bbox": [15, 282, 320, 486],
                "image": "data\\images\\2353648.jpg"
            },
            {
                "VG_image_id": "2390186",
                "VG_object_id": "3828222",
                "bbox": [5, 190, 481, 340],
                "image": "data\\images\\2390186.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the color of the shirt of the player", 2],
            ["What is the color of the ground", 1],
            ["What color is the court", 1],
            ["what is the main color of the court", 1]
        ],
        "org_questions": [
            ["What is the color of the ground", 1],
            ["What is the court made of", -1],
            ["What is the color of the shirt of the player", 2],
            ["how many people are there", -1],
            ["what is the persion wearing on his head", -1],
            ["What color is the court", 1],
            ["what is the main color of the court", 1],
            ["how many people are there on the court", -1],
            ["where was this picture taken", -1],
            ["what is on the ground", -1],
            ["where is the tennis court", -1]
        ],
        "context": [
            "a woman hitting a tennis ball with a tennis racket.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2380894",
                "VG_object_id": "543159",
                "bbox": [159, 359, 280, 447],
                "image": "data\\images\\2380894.jpg"
            },
            {
                "VG_image_id": "2373456",
                "VG_object_id": "588756",
                "bbox": [68, 102, 499, 346],
                "image": "data\\images\\2373456.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the photo", 2],
            ["what color is the plate", 1],
            ["what is the plate made of", 1],
            ["what shape is the tray", 1],
            ["what kind of food is on the tray", 1],
            ["What food is on the plate", 1],
            ["what is the food on", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what is the plate made of", 1],
            ["what is on the plate", -1],
            ["what shape is the tray", 1],
            ["what kind of food is on the tray", 1],
            ["what is in the tray", -1],
            ["What food is on the plate", 1],
            ["what color is the table", -1],
            ["how many people are in the photo", 2],
            ["where is the food", -1],
            ["what is the food on", 1]
        ],
        "context": [
            "a man is preparing a meal at a buffet.",
            "a pan of blueberry muffins on a stove."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2393599",
                "VG_object_id": "1218536",
                "bbox": [81, 221, 267, 450],
                "image": "data\\images\\2393599.jpg"
            },
            {
                "VG_image_id": "2316066",
                "VG_object_id": "3364722",
                "bbox": [32, 24, 265, 272],
                "image": "data\\images\\2316066.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many guys are there", 2],
            ["What color is guy's shirt", 2],
            ["What is guy doing", 1],
            ["what is the field made of", 1],
            ["what is the person holding", 1],
            ["what are on the guy's feet", 1],
            ["what is the man doing", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["How many guys are there", 2],
            ["What color is guy's shirt", 2],
            ["What is guy doing", 1],
            ["what is the field made of", 1],
            ["what is the person holding", 1],
            ["what are on the guy's feet", 1],
            ["what is the man wearing", -1],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a man walking on a beach carrying a surfboard.",
            "a man riding a skateboard on a ramp."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2322853",
                "VG_object_id": "3314128",
                "bbox": [66, 209, 348, 281],
                "image": "data\\images\\2322853.jpg"
            },
            {
                "VG_image_id": "498289",
                "VG_object_id": "1076176",
                "bbox": [83, 213, 991, 634],
                "image": "data\\images\\498289.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 1],
            ["what is beside the train", 1],
            ["what color is the sky", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the train", 1],
            ["what is beside the train", 1],
            ["what color is the sky", 1],
            ["How many trains are there", -1],
            ["what is the weather like", -1],
            ["what is the train doing", -1],
            ["where is the train", -1],
            ["what is the ground covered with", 1],
            ["what kind of train is this", -1],
            ["what is on the tracks", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a train is traveling through a lush green forest.",
            "a train is pulling into a station with cars."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2336373",
                "VG_object_id": "2220043",
                "bbox": [94, 78, 356, 331],
                "image": "data\\images\\2336373.jpg"
            },
            {
                "VG_image_id": "2378191",
                "VG_object_id": "1778580",
                "bbox": [68, 175, 163, 331],
                "image": "data\\images\\2378191.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 2],
            ["what is the woman holding", 2],
            ["what is the woman wearing", 1],
            ["how many people are there", 1],
            ["Where is the woman", 1],
            ["what is the weather like", 1],
            ["what is the woman's posture", 1],
            ["what is the persion on the right wearing", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 2],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 2],
            ["how many people are there", 1],
            ["Where is the woman", 1],
            ["what is the weather like", 1],
            ["what is the woman's posture", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion on the right wearing", 1],
            ["where are the people", 1]
        ],
        "context": [
            "a woman in a black headband catching a frisbee.",
            "a woman standing in a parking lot holding an umbrella."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2338494",
                "VG_object_id": "3321642",
                "bbox": [67, 177, 251, 332],
                "image": "data\\images\\2338494.jpg"
            },
            {
                "VG_image_id": "2357750",
                "VG_object_id": "2225184",
                "bbox": [142, 112, 265, 253],
                "image": "data\\images\\2357750.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the man's helmet", 1]
        ],
        "org_questions": [
            ["what color is the horse", -1],
            ["what color is the man's shirt", 1],
            ["what color is the man's helmet", 1],
            ["how many people are there", -1],
            ["what is behind the horses", -1],
            ["what are the horses doing", -1],
            ["where is the man", -1],
            ["what is the land made of", -1],
            ["what kind of animal is this", -1],
            ["what is on the horse's head", -1],
            ["who is on the horse", -1],
            ["what is the horse on", -1]
        ],
        "context": [
            "a person riding a horse in a corral.",
            "a man riding a horse through a forest."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2401269",
                "VG_object_id": "1149455",
                "bbox": [298, 267, 381, 495],
                "image": "data\\images\\2401269.jpg"
            },
            {
                "VG_image_id": "2396574",
                "VG_object_id": "1196869",
                "bbox": [58, 101, 194, 297],
                "image": "data\\images\\2396574.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 2],
            ["what is the woman wearing", 2],
            ["What is woman holding", 1],
            ["what is the persion on the left holding", 1],
            ["what is the woman standing on", 1],
            ["what is the persion riding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 2],
            ["what is on the woman's head", -1],
            ["How many people are there", -1],
            ["Where is the woman standing", -1],
            ["What is woman holding", 1],
            ["what is the woman wearing", 2],
            ["what is the persion on the left holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman standing on", 1],
            ["what is the persion riding", 1]
        ],
        "context": [
            "a woman flying a kite in a field.",
            "a woman standing next to a fence with cows in the background."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2346364",
                "VG_object_id": "900175",
                "bbox": [233, 55, 287, 227],
                "image": "data\\images\\2346364.jpg"
            },
            {
                "VG_image_id": "2334865",
                "VG_object_id": "3673614",
                "bbox": [159, 7, 310, 290],
                "image": "data\\images\\2334865.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's trousers", 1],
            ["what is the man riding", 1],
            ["what is the man wearing on the head", 1],
            ["How many people are there", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["what color is the man's trousers", 1],
            ["what is the man riding", 1],
            ["what is the man wearing on the head", 1],
            ["How many people are there", 1],
            ["how is the weather", -1],
            ["where is the man", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man on", 1]
        ],
        "context": [
            "a man riding a horse with a flag on it.",
            "a man sitting on a motorcycle in the middle of a street."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2396704",
                "VG_object_id": "441327",
                "bbox": [15, 78, 492, 395],
                "image": "data\\images\\2396704.jpg"
            },
            {
                "VG_image_id": "2380222",
                "VG_object_id": "1349236",
                "bbox": [249, 22, 331, 94],
                "image": "data\\images\\2380222.jpg"
            }
        ],
        "questions_with_scores": [
            ["where was this picture taken", 2],
            ["what is in the box", 1],
            ["what is the box on", 1],
            ["where is the photo taken", 1],
            ["where is the box", 1],
            ["what is in the background", 1],
            ["where are the boxes", 1]
        ],
        "org_questions": [
            ["what is in the box", 1],
            ["what color is the box", -1],
            ["what is the box on", 1],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["how large is the box", -1],
            ["where is the box", 1],
            ["where was this picture taken", 2],
            ["what is in the background", 1],
            ["where are the boxes", 1]
        ],
        "context": [
            "a box of doughnuts on a table",
            "a woman sitting on the ground next to a bunch of bananas."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2377273",
                "VG_object_id": "566757",
                "bbox": [26, 27, 317, 500],
                "image": "data\\images\\2377273.jpg"
            },
            {
                "VG_image_id": "2354683",
                "VG_object_id": "1699445",
                "bbox": [105, 240, 185, 409],
                "image": "data\\images\\2354683.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many ladies are there", 2],
            ["what is in the background", 2]
        ],
        "org_questions": [
            ["how many ladies are there", 2],
            ["who is in the photo", -1],
            ["what is in the background", 2]
        ],
        "context": [
            "two women standing next to each other holding wine glasses.",
            "a stop sign with a woman walking on it."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2373506",
                "VG_object_id": "2074716",
                "bbox": [213, 286, 333, 363],
                "image": "data\\images\\2373506.jpg"
            },
            {
                "VG_image_id": "2346902",
                "VG_object_id": "2914199",
                "bbox": [2, 182, 498, 329],
                "image": "data\\images\\2346902.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the ground", 2],
            ["what is on the land", 2],
            ["what is the ground covered with", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is the color of the ground", 2],
            ["what is on the land", 2],
            ["what time is the picture taken", -1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["how many motorcycles are there on the ground", -1],
            ["where is the picture taken", 1],
            ["how is the weather", -1]
        ],
        "context": [
            "a man flying through the air while riding a skateboard.",
            "a dog sitting on a trailer with a car in the background."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2391641",
                "VG_object_id": "1239186",
                "bbox": [117, 232, 182, 314],
                "image": "data\\images\\2391641.jpg"
            },
            {
                "VG_image_id": "2387191",
                "VG_object_id": "1278387",
                "bbox": [161, 134, 231, 185],
                "image": "data\\images\\2387191.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the clock", 1],
            ["where is the clock", 1],
            ["how many clocks are shown", 1],
            ["what time is on the clock", 1]
        ],
        "org_questions": [
            ["what color is the clock", 1],
            ["where is the clock", 1],
            ["what time is it", -1],
            ["what is the shape of the clock", -1],
            ["what is behind the clock", -1],
            ["what is the clock on", -1],
            ["Where is the clock located", -1],
            ["how many clocks are shown", 1],
            ["when was this picture taken", -1],
            ["what is on the clock", -1],
            ["what time is on the clock", 1],
            ["what time of day is it", -1]
        ],
        "context": [
            "a clock tower with a weather vane on top of it.",
            "a view of a clock tower from the ceiling."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2403980",
                "VG_object_id": "347513",
                "bbox": [73, 48, 162, 154],
                "image": "data\\images\\2403980.jpg"
            },
            {
                "VG_image_id": "2354154",
                "VG_object_id": "1743742",
                "bbox": [129, 2, 396, 345],
                "image": "data\\images\\2354154.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["how many people are there", 1],
            ["what is the woman wearing", 1],
            ["where was this photo taken", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", -1],
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["how many people are there", 1],
            ["what is the woman wearing", 1],
            ["What is woman holding", -1],
            ["what is the woman holding", -1],
            ["who is in the photo", -1],
            ["where was this photo taken", 1],
            ["what are the people doing", 1],
            ["what is the persion standing on", -1],
            ["what is the persion holding", -1],
            ["what is the color of woman's hair", -1],
            ["when was the photo taken", -1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a man standing in front of a display of doughnuts.",
            "a person blowing out a candle on a cake."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2394890",
                "VG_object_id": "459945",
                "bbox": [316, 166, 474, 294],
                "image": "data\\images\\2394890.jpg"
            },
            {
                "VG_image_id": "2333676",
                "VG_object_id": "2939059",
                "bbox": [224, 138, 449, 331],
                "image": "data\\images\\2333676.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["who is wearing the shirt", 1],
            ["what is the person doing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["how many elephants are in the background", -1],
            ["who is wearing the shirt", 1],
            ["what is the person doing", 1],
            ["what is the main color of the shirt", 1],
            ["what is the person wearing", -1],
            ["how many people are there", -1]
        ],
        "context": [
            "a man sitting at a desk with a computer.",
            "two women in red shirts are eating hot dogs."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2358666",
                "VG_object_id": "2623907",
                "bbox": [156, 108, 336, 276],
                "image": "data\\images\\2358666.jpg"
            },
            {
                "VG_image_id": "2382659",
                "VG_object_id": "696230",
                "bbox": [63, 0, 391, 316],
                "image": "data\\images\\2382659.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the dog doing", 2],
            ["how many dogs are in the picture", 2]
        ],
        "org_questions": [
            ["what is the dog doing", 2],
            ["how many dogs are in the picture", 2],
            ["where is the dog", -1],
            ["what color is the grass", -1],
            ["what is the ground covered with", -1],
            ["what is the dog standing on", -1],
            ["what color is the ground", -1],
            ["what animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a dog jumping up to catch a frisbee.",
            "three dogs sitting on the grass with their mouths open."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2334883",
                "VG_object_id": "964126",
                "bbox": [249, 9, 414, 333],
                "image": "data\\images\\2334883.jpg"
            },
            {
                "VG_image_id": "2376106",
                "VG_object_id": "2196102",
                "bbox": [165, 2, 498, 157],
                "image": "data\\images\\2376106.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the cat sitting on", 1],
            ["what is behind the cat", 1],
            ["where is the cat looking", 1]
        ],
        "org_questions": [
            ["what color is the cat", -1],
            ["what is the cat sitting on", 1],
            ["how many cats are there", -1],
            ["where are the cats", -1],
            ["what kind of animal is in the picture", -1],
            ["what is the cat doing", -1],
            ["what is behind the cat", 1],
            ["what is the cat's color", -1],
            ["where is the cat looking", 1]
        ],
        "context": [
            "a cat sitting on a table with a lot of stuff.",
            "a cat sitting on a couch next to a cup of coffee."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2370858",
                "VG_object_id": "2455737",
                "bbox": [278, 46, 449, 371],
                "image": "data\\images\\2370858.jpg"
            },
            {
                "VG_image_id": "2404739",
                "VG_object_id": "3814737",
                "bbox": [23, 3, 276, 498],
                "image": "data\\images\\2404739.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many players are there in the photo", 2],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the player's helmet", -1],
            ["how many players are there in the photo", 2],
            ["what color is the player's shoes", -1],
            ["what gender is the player", -1],
            ["What is the person holding", -1],
            ["what is in the background", 1],
            ["what color is the ground", -1],
            ["what color is the man's shirt", -1],
            ["when was the photo taken", -1],
            ["who is holding the bat", -1],
            ["what is the batter doing", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a baseball player is getting ready to hit a ball."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2360583",
                "VG_object_id": "787247",
                "bbox": [0, 342, 332, 462],
                "image": "data\\images\\2360583.jpg"
            },
            {
                "VG_image_id": "2358683",
                "VG_object_id": "2700296",
                "bbox": [85, 234, 298, 483],
                "image": "data\\images\\2358683.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are there", 2],
            ["What food is on the plate", 1],
            ["What food  in the plate", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["how many plates are there", 2],
            ["what is on the plates", -1],
            ["where are the plates", -1],
            ["what pattern is on the plate", -1],
            ["What food is on the plate", 1],
            ["What food  in the plate", 1],
            ["what is the shape of the plate", -1],
            ["what is the plate on", -1],
            ["what is next to the plate", 1]
        ],
        "context": [
            "a sandwich and salad on a plate at a restaurant.",
            "a person is taking a slice of pizza from a plate."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2364730",
                "VG_object_id": "639371",
                "bbox": [304, 1, 499, 374],
                "image": "data\\images\\2364730.jpg"
            },
            {
                "VG_image_id": "2361628",
                "VG_object_id": "1729406",
                "bbox": [312, 40, 372, 191],
                "image": "data\\images\\2361628.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["where is the photo taken", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", -1],
            ["where is the photo taken", 1],
            ["when was the picture taken", -1],
            ["what is on the man's head", -1],
            ["what is the persion on the left wearing", 1]
        ],
        "context": [
            "a man standing on a sidewalk looking at his cell phone.",
            "a little girl writing on a piece of paper."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2410379",
                "VG_object_id": "219021",
                "bbox": [204, 6, 307, 374],
                "image": "data\\images\\2410379.jpg"
            },
            {
                "VG_image_id": "2367200",
                "VG_object_id": "2838717",
                "bbox": [96, 96, 218, 423],
                "image": "data\\images\\2367200.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the player wearing on head", 2],
            ["what color is the player's shirt", 1],
            ["what is on the player's head", 1],
            ["what is the persion on the right holding", 1]
        ],
        "org_questions": [
            ["what is the player doing", -1],
            ["what color is the player's shirt", 1],
            ["what is on the player's head", 1],
            ["how many people are there", 2],
            ["what is the gender of the player", -1],
            ["what is the player wearing", -1],
            ["what is the player wearing on head", 2],
            ["what color is the ground", -1],
            ["when was the photo taken", -1],
            ["what is the man standing on", -1],
            ["what kind of shoes is the man wearing", -1],
            ["what is the persion on the right holding", 1]
        ],
        "context": [
            "a man in a red shirt and white shorts holding a tennis racket.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2322782",
                "VG_object_id": "990690",
                "bbox": [97, 82, 352, 314],
                "image": "data\\images\\2322782.jpg"
            },
            {
                "VG_image_id": "2334979",
                "VG_object_id": "2254217",
                "bbox": [43, 16, 434, 341],
                "image": "data\\images\\2334979.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many giraffes are there", 2],
            ["what color is the ground the giraffe standing on", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["how many giraffes are there", 2],
            ["what color is the ground the giraffe standing on", 1],
            ["what is the ground the giraffe standing on made of", -1],
            ["what is next to the giraffe", -1],
            ["what is the giraffe on", -1],
            ["what is the ground covered with", 1],
            ["what color is the background", -1],
            ["when was the photo taken", -1],
            ["what kind of animals are these", -1],
            ["where was this photo taken", -1],
            ["what are the giraffes doing", -1]
        ],
        "context": [
            "a giraffe is eating grass in a field.",
            "a group of giraffes standing around a tree branch."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2320144",
                "VG_object_id": "995790",
                "bbox": [284, 206, 337, 276],
                "image": "data\\images\\2320144.jpg"
            },
            {
                "VG_image_id": "2345987",
                "VG_object_id": "2466182",
                "bbox": [107, 85, 168, 178],
                "image": "data\\images\\2345987.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the person doing", 1],
            ["what color is the person's shirt", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what color is the person's shirt", 1],
            ["what color is the ground", 2],
            ["how many people are there", 1],
            ["what is in the background", -1],
            ["where is the person", 1],
            ["what is the man wearing", 1],
            ["when was this photo taken", -1],
            ["what is on the man's face", -1],
            ["what is the persion on the left wearing on the head", -1]
        ],
        "context": [
            "a man sitting in a hammock on the beach.",
            "a man riding a horse while holding a polo stick."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2385473",
                "VG_object_id": "1296409",
                "bbox": [3, 221, 498, 331],
                "image": "data\\images\\2385473.jpg"
            },
            {
                "VG_image_id": "2406139",
                "VG_object_id": "3189312",
                "bbox": [1, 183, 497, 329],
                "image": "data\\images\\2406139.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there on the land", 2],
            ["what color is the land", 1],
            ["what is in the background", 1],
            ["where is this photo taken", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["how many zebras are there on the land", 2],
            ["what is in the background", 1],
            ["what kind of animal is on the land", -1],
            ["where is this photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is on the ground", -1],
            ["how is the weather", -1],
            ["what is the zebra standing on", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a group of zebras standing next to each other.",
            "a zebra standing in a field next to a fence."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2417418",
                "VG_object_id": "3493921",
                "bbox": [285, 81, 415, 308],
                "image": "data\\images\\2417418.jpg"
            },
            {
                "VG_image_id": "2353575",
                "VG_object_id": "844778",
                "bbox": [406, 35, 484, 274],
                "image": "data\\images\\2353575.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his head", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what time is it", -1],
            ["what is the man wearing on his head", 1],
            ["when was this photo taken", -1],
            ["what is the man standing on", -1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a painting of a baseball player swinging a bat.",
            "two children playing with a kite in a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2407514",
                "VG_object_id": "278249",
                "bbox": [84, 249, 236, 358],
                "image": "data\\images\\2407514.jpg"
            },
            {
                "VG_image_id": "2383530",
                "VG_object_id": "532058",
                "bbox": [260, 217, 331, 355],
                "image": "data\\images\\2383530.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["who is wearing the trousers", 2],
            ["what is on the child's head", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["who is wearing the trousers", 2],
            ["what is on the child's head", 1],
            ["what are the people doing in the picture", -1],
            ["How many people are there", -1],
            ["what is the ground behind covered with", -1],
            ["what is the person holding", -1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a young girl swinging a bat at a ball",
            "a young boy swinging a bat at a ball."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2337369",
                "VG_object_id": "3231800",
                "bbox": [41, 9, 369, 236],
                "image": "data\\images\\2337369.jpg"
            },
            {
                "VG_image_id": "2383592",
                "VG_object_id": "693902",
                "bbox": [13, 1, 447, 203],
                "image": "data\\images\\2383592.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in front of the tv", 2],
            ["how many people are in the picture", 2]
        ],
        "org_questions": [
            ["what is in the tv", -1],
            ["what is in front of the tv", 2],
            ["what color is the table", -1],
            ["where is the tv", -1],
            ["what is on the television", -1],
            ["what color is the screen of the television", -1],
            ["what color is the background", -1],
            ["how many people are in the picture", 2],
            ["what is the tv on", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a boy holding a wii remote in front of a television.",
            "a cat sitting in front of a tv playing a video game."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2319402",
                "VG_object_id": "2919816",
                "bbox": [366, 162, 446, 239],
                "image": "data\\images\\2319402.jpg"
            },
            {
                "VG_image_id": "2396426",
                "VG_object_id": "444458",
                "bbox": [86, 53, 184, 116],
                "image": "data\\images\\2396426.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man doing", 2],
            ["where is the picture taken", 2],
            ["Where is the man", 2],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is the persion riding", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["What is man doing", 2],
            ["where is the picture taken", 2],
            ["what color is the man's shirt", 1],
            ["how many people are there", -1],
            ["what is on the man's faces", -1],
            ["where is the man", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the gender of the person", -1],
            ["what is the persion riding", 1],
            ["what is the man wearing", 1],
            ["Where is the man", 2]
        ],
        "context": [
            "a man is sitting on the sidewalk next to a yellow building.",
            "a man riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2320337",
                "VG_object_id": "994119",
                "bbox": [1, 1, 498, 500],
                "image": "data\\images\\2320337.jpg"
            },
            {
                "VG_image_id": "2340888",
                "VG_object_id": "3713944",
                "bbox": [2, 1, 499, 372],
                "image": "data\\images\\2340888.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are there on the table", 2]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", -1],
            ["how many people are there", -1],
            ["how many plates are there on the table", 2],
            ["what color is the plate", -1],
            ["where is the picture taken", -1],
            ["what is the table sitting on", -1],
            ["where are the plates", -1]
        ],
        "context": [
            "a piece of chocolate cake on a plate.",
            "a plate of food with a fried egg on top of it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2380257",
                "VG_object_id": "1348818",
                "bbox": [18, 8, 214, 298],
                "image": "data\\images\\2380257.jpg"
            },
            {
                "VG_image_id": "2372390",
                "VG_object_id": "2587161",
                "bbox": [179, 39, 367, 297],
                "image": "data\\images\\2372390.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's hat", 1],
            ["how many people are there in the picture", 1],
            ["where is the photo taken", 1],
            ["What is the background of image", 1],
            ["where is the man", 1],
            ["What is man doing", 1],
            ["what are the people doing", 1],
            ["what is the man wearing on the head", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's hat", 1],
            ["how many people are there in the picture", 1],
            ["where is the photo taken", 1],
            ["What is the background of image", 1],
            ["where is the man", 1],
            ["What is man doing", 1],
            ["what are the people doing", 1],
            ["what is the man wearing on the head", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", 1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a couple sitting on a bench in a park."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2378212",
                "VG_object_id": "561310",
                "bbox": [3, 235, 499, 372],
                "image": "data\\images\\2378212.jpg"
            },
            {
                "VG_image_id": "2393057",
                "VG_object_id": "1223400",
                "bbox": [0, 200, 500, 279],
                "image": "data\\images\\2393057.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what is on the land", 1],
            ["what is in the background", 1],
            ["where is the picture taken", 1],
            ["what is the ground covered with", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is on the land", 1],
            ["what is in the background", 1],
            ["how many cars are there in the picture", -1],
            ["where is the picture taken", 1],
            ["what is the weather like", -1],
            ["what is the ground covered with", 1],
            ["when was the picture taken", -1],
            ["what is covering the ground", 1],
            ["how is the weather", -1]
        ],
        "context": [
            "a woman walking her dog on a leash next to a river.",
            "a person jumping a snowboard on a snowy hill"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2372944",
                "VG_object_id": "2035389",
                "bbox": [1, 355, 499, 498],
                "image": "data\\images\\2372944.jpg"
            },
            {
                "VG_image_id": "2406208",
                "VG_object_id": "325800",
                "bbox": [15, 327, 488, 498],
                "image": "data\\images\\2406208.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what is on the table", 1],
            ["how many people are there in the picture", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is on the table", 1],
            ["what color is the table", 2],
            ["How many cups are there", -1],
            ["what shape is the table", -1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["how many people are there in the picture", 1],
            ["what is behind the table", -1],
            ["what is under the table", -1],
            ["where is the picture taken", -1],
            ["what is in the background", 1],
            ["what is covering the table", -1]
        ],
        "context": [
            "a little girl eating food from a spoon.",
            "a small vase with a red flower in it"
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2351016",
                "VG_object_id": "3591065",
                "bbox": [56, 6, 494, 328],
                "image": "data\\images\\2351016.jpg"
            },
            {
                "VG_image_id": "2330634",
                "VG_object_id": "3136523",
                "bbox": [211, 61, 399, 228],
                "image": "data\\images\\2330634.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many boats are there", 1],
            ["what time is it", 1],
            ["when was the photo taken", 1],
            ["what time of day is it", 1]
        ],
        "org_questions": [
            ["how many boats are there", 1],
            ["what color are the boats", -1],
            ["what time is it", 1],
            ["what is in the background", -1],
            ["where is the boat", -1],
            ["what is the boat doing", -1],
            ["when was the photo taken", 1],
            ["what type of boat is shown", -1],
            ["where was this picture taken", -1],
            ["what time of day is it", 1]
        ],
        "context": [
            "a large boat with people standing on the dock.",
            "a couple of large boats floating on top of a lake."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2353575",
                "VG_object_id": "844779",
                "bbox": [431, 75, 482, 169],
                "image": "data\\images\\2353575.jpg"
            },
            {
                "VG_image_id": "2372813",
                "VG_object_id": "1866213",
                "bbox": [71, 96, 188, 223],
                "image": "data\\images\\2372813.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 1],
            ["what color is the ground", 1],
            ["how many people are there", 1],
            ["Where is the person", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 1],
            ["what color is the ground", 1],
            ["how many people are there", 1],
            ["Where is the person", 1],
            ["what is the man wearing around his neck", -1],
            ["what is the man doing", -1],
            ["when was the photo taken", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "two children playing with a kite in a field.",
            "a man holding a baseball bat on a field."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2357069",
                "VG_object_id": "815299",
                "bbox": [144, 39, 226, 204],
                "image": "data\\images\\2357069.jpg"
            },
            {
                "VG_image_id": "2401598",
                "VG_object_id": "1146216",
                "bbox": [175, 90, 234, 194],
                "image": "data\\images\\2401598.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the shirt", 2],
            ["what color are the boy's clothes", 2],
            ["what is the child doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what are the people doing", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the color of the shirt", 2],
            ["what is the child doing", 1],
            ["what is the color of the pant", -1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the boy holding", -1],
            ["what are the people doing", 1],
            ["what color are the boy's clothes", 2],
            ["when was the photo taken", -1],
            ["what is the man wearing", 1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a boy on a skateboard doing a trick.",
            "a group of people riding skis on a snow covered slope."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2342112",
                "VG_object_id": "2885675",
                "bbox": [6, 36, 400, 325],
                "image": "data\\images\\2342112.jpg"
            },
            {
                "VG_image_id": "2408481",
                "VG_object_id": "260917",
                "bbox": [1, 0, 331, 499],
                "image": "data\\images\\2408481.jpg"
            }
        ],
        "questions_with_scores": [["what color is the bench", 2]],
        "org_questions": [
            ["what color is the bench", 2],
            ["how many people are on the benches", -1],
            ["when is the picture taken", -1],
            ["what is the bench made of", -1],
            ["where is the bench", -1],
            ["how is the weather", -1],
            ["what is the bench sitting on", -1],
            ["what is on the bench", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a bench that is sitting in the dirt.",
            "a red bench with a plant on it"
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2401058",
                "VG_object_id": "1151995",
                "bbox": [119, 115, 358, 228],
                "image": "data\\images\\2401058.jpg"
            },
            {
                "VG_image_id": "2375891",
                "VG_object_id": "578325",
                "bbox": [256, 19, 386, 181],
                "image": "data\\images\\2375891.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["what gesture is the woman", 2],
            ["What color is woman's hair", 1],
            ["What is woman doing", 1],
            ["where is the woman", 1],
            ["What is woman holding", 1],
            ["what color is the shirt of the woman", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is woman's hair", 1],
            ["What is woman doing", 1],
            ["where is the woman", 1],
            ["what is the woman wearing", -1],
            ["what gesture is the woman", 2],
            ["What is woman holding", 1],
            ["what color is the shirt of the woman", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what gender is the person", -1]
        ],
        "context": [
            "a man and a woman taking a picture in a mirror.",
            "a woman holding a plate of food with a plate of cake."
        ]
    },
    {
        "object_category": "skier",
        "images": [
            {
                "VG_image_id": "2397630",
                "VG_object_id": "1188177",
                "bbox": [189, 17, 495, 245],
                "image": "data\\images\\2397630.jpg"
            },
            {
                "VG_image_id": "2369530",
                "VG_object_id": "1749241",
                "bbox": [135, 149, 262, 313],
                "image": "data\\images\\2369530.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the man wearing", 2],
            ["what are the people doing", 1],
            ["what color is the background", 1],
            ["what color are the man's clothes", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what are the people doing", 1],
            ["what color is the background", 1],
            ["what time is it", -1],
            ["where is the person", -1],
            ["what is the gender of the person", -1],
            ["what color are the man's clothes", 1],
            ["who is in the photo", -1],
            ["what is the man wearing", 2],
            ["what is on the person's feet", -1],
            ["when was the photo taken", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a man riding a snowboard on top of a snow covered slope.",
            "a man water skiing on a lake with trees in the background."
        ]
    },
    {
        "object_category": "computer",
        "images": [
            {
                "VG_image_id": "2413602",
                "VG_object_id": "170090",
                "bbox": [234, 212, 407, 309],
                "image": "data\\images\\2413602.jpg"
            },
            {
                "VG_image_id": "2368739",
                "VG_object_id": "1900031",
                "bbox": [60, 51, 483, 348],
                "image": "data\\images\\2368739.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the computer", 1],
            ["where is the computer", 1],
            ["what is the main color of the screen of the computer", 1],
            ["what is the color under the computer", 1],
            ["how is the laptop", 1]
        ],
        "org_questions": [
            ["what color is the computer", 1],
            ["where is the computer", 1],
            ["how many computers are there in the photo", -1],
            ["how many people are there in the picture", 2],
            ["what is the main color of the screen of the computer", 1],
            ["what is the color under the computer", 1],
            ["what shape is the laptop", -1],
            ["what is the laptop made of", -1],
            ["how is the laptop", 1],
            ["what kind of laptop is this", -1]
        ],
        "context": [
            "three people sitting on a couch looking at a laptop.",
            "a laptop computer sitting on top of a desk."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2327476",
                "VG_object_id": "2789345",
                "bbox": [63, 0, 435, 498],
                "image": "data\\images\\2327476.jpg"
            },
            {
                "VG_image_id": "2336705",
                "VG_object_id": "2874474",
                "bbox": [1, 364, 357, 498],
                "image": "data\\images\\2336705.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is the table made of", -1],
            ["how many keyboards are there on the table", -1],
            ["where is the table", -1],
            ["what is on the table", 1],
            ["where was this picture taken", -1],
            ["what is next to the table", -1],
            ["what is the table color", -1]
        ],
        "context": [
            "a man holding a coffee cup and a pair of glasses.",
            "a blue bowl filled with bananas and peaches."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2353075",
                "VG_object_id": "2307214",
                "bbox": [416, 201, 498, 290],
                "image": "data\\images\\2353075.jpg"
            },
            {
                "VG_image_id": "2416663",
                "VG_object_id": "2879806",
                "bbox": [3, 264, 126, 347],
                "image": "data\\images\\2416663.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 1],
            ["what is in the background", 1],
            ["how many vehicles are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the car", 1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the pattern on the car", -1],
            ["how is the weather", -1],
            ["what is the weather like", -1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1],
            ["what is in front of the car", -1],
            ["how many vehicles are in the picture", 1]
        ],
        "context": [
            "a semi truck is driving down the street.",
            "a car is driving down the street in front of a house."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2327284",
                "VG_object_id": "3523409",
                "bbox": [3, 240, 186, 305],
                "image": "data\\images\\2327284.jpg"
            },
            {
                "VG_image_id": "2400061",
                "VG_object_id": "1163490",
                "bbox": [1, 83, 499, 369],
                "image": "data\\images\\2400061.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the train", 2],
            ["what is the ground covered with", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["What color is the train", 2],
            ["Where is the train", -1],
            ["how many trains are there", -1],
            ["what is the ground covered with", 1],
            ["What is the weather like", -1],
            ["what is in the background", 1],
            ["when was the photo taken", -1],
            ["what is the train doing", -1],
            ["what type of train is shown", -1],
            ["what is the train sitting on", -1]
        ],
        "context": [
            "a train is parked in a parking lot.",
            "a train that is sitting on the tracks."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2409129",
                "VG_object_id": "248405",
                "bbox": [7, 114, 222, 394],
                "image": "data\\images\\2409129.jpg"
            },
            {
                "VG_image_id": "2350722",
                "VG_object_id": "1918431",
                "bbox": [310, 268, 372, 369],
                "image": "data\\images\\2350722.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["what color is the shirt", 1],
            ["what is the person in the shirt holding", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the person in the shirt holding", 1],
            ["how many people are there in the photo", 2],
            ["where is the picture taken", 1],
            ["what is the gender of the person", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man cutting a pizza on top of a stove.",
            "a woman and a boy are looking at a display of donuts."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2364135",
                "VG_object_id": "2349086",
                "bbox": [0, 12, 353, 495],
                "image": "data\\images\\2364135.jpg"
            },
            {
                "VG_image_id": "2341241",
                "VG_object_id": "3004904",
                "bbox": [189, 25, 275, 265],
                "image": "data\\images\\2341241.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's shirt", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what is the boy wearing", -1],
            ["what color is the boy's shirt", 1],
            ["what is the persion doing", 1],
            ["how many people are there", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man with a tie and a shirt on",
            "a young boy riding on the back of a brown horse."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2316535",
                "VG_object_id": "2776669",
                "bbox": [326, 35, 443, 242],
                "image": "data\\images\\2316535.jpg"
            },
            {
                "VG_image_id": "2406943",
                "VG_object_id": "287805",
                "bbox": [151, 147, 228, 280],
                "image": "data\\images\\2406943.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["how many people are there", 2],
            ["what is the woman holding", 1],
            ["what is the woman doing", 1],
            ["what gesture is the woman", 1],
            ["what is the lady holding", 1],
            ["what is the woman wearing", 1],
            ["what is the woman playing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what is the woman holding", 1],
            ["what is the woman doing", 1],
            ["how many people are there", 2],
            ["where is the lady", -1],
            ["what gesture is the woman", 1],
            ["what color is the woman's hair", -1],
            ["what is the lady holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman wearing", 1],
            ["what is the woman playing", 1]
        ],
        "context": [
            "a woman swinging a tennis racket at a ball.",
            "a woman is throwing a frisbee on the beach."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2339702",
                "VG_object_id": "2650369",
                "bbox": [359, 81, 455, 280],
                "image": "data\\images\\2339702.jpg"
            },
            {
                "VG_image_id": "2375063",
                "VG_object_id": "2106228",
                "bbox": [148, 78, 316, 238],
                "image": "data\\images\\2375063.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is on the man's head", 2],
            ["what is the person doing", 1],
            ["what is the person holding", 1],
            ["what is the color of the shirt", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", 1],
            ["how many persons are there", -1],
            ["where is the person", -1],
            ["how is the weather", -1],
            ["what is the person wearing", -1],
            ["what is the person holding", 1],
            ["what is the color of the shirt", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what is on the man's head", 2]
        ],
        "context": [
            "a man riding a wave on top of a surfboard.",
            "a man riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2327517",
                "VG_object_id": "2920148",
                "bbox": [4, 96, 216, 278],
                "image": "data\\images\\2327517.jpg"
            },
            {
                "VG_image_id": "713733",
                "VG_object_id": "1076945",
                "bbox": [473, 26, 1016, 680],
                "image": "data\\images\\713733.jpg"
            }
        ],
        "questions_with_scores": [
            ["WHat is man doing", 2],
            ["How many people are there", 2],
            ["What is the background of image", 1],
            ["where is the photo taken", 1],
            ["what is the man holding", 1],
            ["what is the man looking at", 1]
        ],
        "org_questions": [
            ["WHat is man doing", 2],
            ["How many people are there", 2],
            ["What is the background of image", 1],
            ["what is the man wearing on his face", -1],
            ["where is the photo taken", 1],
            ["what is the weather like", -1],
            ["when was the picture taken", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["what is the man looking at", 1]
        ],
        "context": [
            "a person flying a kite in a field.",
            "a group of people sitting on the grass."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2371723",
                "VG_object_id": "596153",
                "bbox": [2, 224, 499, 373],
                "image": "data\\images\\2371723.jpg"
            },
            {
                "VG_image_id": "2323662",
                "VG_object_id": "989121",
                "bbox": [112, 254, 498, 346],
                "image": "data\\images\\2323662.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the floor", 1],
            ["What is the pattern of the floor", 1]
        ],
        "org_questions": [
            ["what is the floor made of", -1],
            ["what is on the floor", 1],
            ["how many plants are there in the picture", -1],
            ["what color are the walls", -1],
            ["where is the floor", -1],
            ["What is the pattern of the floor", 1],
            ["where was this picture taken", -1],
            ["what is covering the floor", -1],
            ["how many people are in the picture", -1],
            ["what is sitting on the floor", -1]
        ],
        "context": [
            "a living room with a table and chairs and a window.",
            "a large black statue is on a cart in a room."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2360817",
                "VG_object_id": "2791934",
                "bbox": [2, 310, 277, 498],
                "image": "data\\images\\2360817.jpg"
            },
            {
                "VG_image_id": "2351383",
                "VG_object_id": "2006137",
                "bbox": [22, 270, 348, 497],
                "image": "data\\images\\2351383.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["what color is the wall", 1],
            ["Where is the photo taken", 1],
            ["what pattern is the floor", 1],
            ["What is the floor made of", 1],
            ["what kind of floor is this", 1],
            ["what is covering the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["what color is the wall", 1],
            ["how many people are there", -1],
            ["Where is the photo taken", 1],
            ["what pattern is the floor", 1],
            ["What is the floor made of", 1],
            ["what kind of floor is this", 1],
            ["what is covering the floor", 1]
        ],
        "context": [
            "a cat sitting on a step in a bathroom.",
            "a dirty bathroom with a toilet and a broken cabinet."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2355665",
                "VG_object_id": "827563",
                "bbox": [117, 244, 179, 333],
                "image": "data\\images\\2355665.jpg"
            },
            {
                "VG_image_id": "2390564",
                "VG_object_id": "496122",
                "bbox": [252, 213, 437, 374],
                "image": "data\\images\\2390564.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's trousers", 1],
            ["what is the man holding", 1],
            ["what is the man doing", 1],
            ["what is the persion holding", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what color is the man's trousers", 1],
            ["how many people are there", -1],
            ["what is the man holding", 1],
            ["what is the man doing", 1],
            ["what is the persion holding", 1],
            ["what type of pants is the man wearing", -1],
            ["what is behind the man", 1]
        ],
        "context": [
            "a man leaning against a wall with a sign saying",
            "a man sitting down with a dog and a man looking at his phone."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2379826",
                "VG_object_id": "711536",
                "bbox": [137, 192, 499, 372],
                "image": "data\\images\\2379826.jpg"
            },
            {
                "VG_image_id": "2373168",
                "VG_object_id": "734008",
                "bbox": [2, 210, 81, 373],
                "image": "data\\images\\2373168.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the chair", 2],
            ["What is on the chair", 2],
            ["who is sitting on the chair", 1],
            ["what is in the background", 1],
            ["what is in the chair", 1],
            ["what is the color of the table", 1]
        ],
        "org_questions": [
            ["What color is the chair", 2],
            ["What is on the chair", 2],
            ["what shape is the back of the chair", -1],
            ["how many chairs are there", -1],
            ["where is the chair", -1],
            ["who is sitting on the chair", 1],
            ["what is in the background", 1],
            ["what is next to the chair", -1],
            ["what is in the chair", 1],
            ["what is the color of the table", 1]
        ],
        "context": [
            "a bird perched on a metal bar in a patio.",
            "a girl is eating a large sandwich"
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2370214",
                "VG_object_id": "605393",
                "bbox": [7, 83, 158, 419],
                "image": "data\\images\\2370214.jpg"
            },
            {
                "VG_image_id": "2341454",
                "VG_object_id": "3116741",
                "bbox": [173, 235, 482, 488],
                "image": "data\\images\\2341454.jpg"
            }
        ],
        "questions_with_scores": [["what is horse doing", 2]],
        "org_questions": [
            ["what color is the horse", -1],
            ["what is horse doing", 2],
            ["what is in the distance", -1],
            ["how many horses are there", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", -1],
            ["where is  the horse", -1],
            ["what kind of animal is in the picture", -1],
            ["when was the picture taken", -1],
            ["what is the horse standing on", -1],
            ["what is behind the horse", -1]
        ],
        "context": [
            "a man dressed in a traditional costume standing next to a horse.",
            "a horse and a sheep grazing in a field."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2346947",
                "VG_object_id": "1672033",
                "bbox": [1, 147, 498, 239],
                "image": "data\\images\\2346947.jpg"
            },
            {
                "VG_image_id": "2356812",
                "VG_object_id": "2374606",
                "bbox": [3, 119, 495, 332],
                "image": "data\\images\\2356812.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is on the land", 1],
            ["what is the ground covered with", 1],
            ["what is in the middle of the picture", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what is on the land", 1],
            ["how many people are there in the picture", 2],
            ["what color is the land", -1],
            ["what is the ground covered with", 1],
            ["what is in the middle of the picture", 1],
            ["how is the weather", 1],
            ["what is the condition of the ground", -1],
            ["where was this photo taken", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a red vase sitting on top of a stone step.",
            "a man holding an umbrella in front of a fruit stand."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2384584",
                "VG_object_id": "525792",
                "bbox": [2, 428, 304, 499],
                "image": "data\\images\\2384584.jpg"
            },
            {
                "VG_image_id": "2356768",
                "VG_object_id": "1696727",
                "bbox": [97, 182, 498, 372],
                "image": "data\\images\\2356768.jpg"
            }
        ],
        "questions_with_scores": [
            ["What  color is the floor", 2],
            ["what room is this", 2],
            ["What is on the table", 1],
            ["what room is the floor in", 1],
            ["where is the picture taken", 1],
            ["what kind of flooring is this", 1]
        ],
        "org_questions": [
            ["What  color is the floor", 2],
            ["What is on the table", 1],
            ["how many people are there", -1],
            ["what room is the floor in", 1],
            ["what is the floor made of", -1],
            ["how many chairs are there on the floor", -1],
            ["what is on the floor", -1],
            ["where is the picture taken", 1],
            ["what is the flooring", -1],
            ["what kind of flooring is this", 1],
            ["what is in the room", -1],
            ["what room is this", 2]
        ],
        "context": [
            "a computer desk with a computer monitor and keyboard.",
            "a kitchen with a tile floor and white cabinets."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2335982",
                "VG_object_id": "3924682",
                "bbox": [14, 29, 289, 475],
                "image": "data\\images\\2335982.jpg"
            },
            {
                "VG_image_id": "2402729",
                "VG_object_id": "657888",
                "bbox": [169, 0, 374, 419],
                "image": "data\\images\\2402729.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many ties are man wearing ", 2],
            ["What is man holding", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["How many ties are man wearing ", 2],
            ["What is man holding", 1],
            ["what color is the wall", -1],
            ["where is the man", -1],
            ["what is the man doing", -1],
            ["what is the man wearing", -1],
            ["what gesture is the man", -1],
            ["who is in the photo", -1],
            ["what is on the man's face", -1],
            ["where was this photo taken", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a man with a tie on and a man in a white shirt.",
            "a man and woman celebrating their birthday with a cake."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2405467",
                "VG_object_id": "373081",
                "bbox": [138, 5, 259, 330],
                "image": "data\\images\\2405467.jpg"
            },
            {
                "VG_image_id": "2336787",
                "VG_object_id": "2575654",
                "bbox": [42, 46, 202, 331],
                "image": "data\\images\\2336787.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's hair", 2],
            ["where is the woman", 1],
            ["when was the photo taken", 1],
            ["what is on the woman's head", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", -1],
            ["what color is the woman's hair", 2],
            ["How many people are there", -1],
            ["What season is it", -1],
            ["where is the woman", 1],
            ["what is the lady doing", -1],
            ["what is on the lady's face", -1],
            ["when was the photo taken", 1],
            ["what is the woman looking at", -1],
            ["what is in the woman's hand", -1],
            ["what is on the woman's head", 1],
            ["what is the girl holding", -1]
        ],
        "context": [
            "a woman in a skirt talking on a cell phone.",
            "a woman is standing on a sidewalk with a cell phone."
        ]
    },
    {
        "object_category": "basket",
        "images": [
            {
                "VG_image_id": "2410144",
                "VG_object_id": "225016",
                "bbox": [40, 299, 144, 352],
                "image": "data\\images\\2410144.jpg"
            },
            {
                "VG_image_id": "2354990",
                "VG_object_id": "3505373",
                "bbox": [111, 263, 329, 490],
                "image": "data\\images\\2354990.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 2],
            ["how many people are there", 1],
            ["Where is the basket", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what shape is the basket", -1],
            ["Where is the basket", 1],
            ["what is in the background", 2],
            ["when was the photo taken", -1],
            ["what is in the photo", -1]
        ],
        "context": [
            "a man in a kitchen preparing lobsters.",
            "a young man carrying a basket of bananas"
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2373220",
                "VG_object_id": "733762",
                "bbox": [161, 39, 333, 147],
                "image": "data\\images\\2373220.jpg"
            },
            {
                "VG_image_id": "2411288",
                "VG_object_id": "359850",
                "bbox": [334, 8, 498, 275],
                "image": "data\\images\\2411288.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the weather like", 2],
            ["How many people are there", 2],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["What is the weather like", 2],
            ["What is person wearing on his head", -1],
            ["How many people are there", 2],
            ["where is the photo taken", 1],
            ["what time is it", -1],
            ["what is in front of the building", -1],
            ["WHat color is the sky", -1],
            ["how many floors does the building have", -1],
            ["what is the building made of", -1],
            ["when was the photo taken", -1],
            ["what is the building in the background", -1]
        ],
        "context": [
            "a man standing on skis in the snow.",
            "a little girl with a colorful neck tie and a brown and orange tie."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2361982",
                "VG_object_id": "778103",
                "bbox": [153, 92, 433, 336],
                "image": "data\\images\\2361982.jpg"
            },
            {
                "VG_image_id": "2357709",
                "VG_object_id": "809477",
                "bbox": [2, 275, 110, 390],
                "image": "data\\images\\2357709.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bag", 2],
            ["what is the bag leaning on", 1],
            ["how many people are there", 1],
            ["what is the main color of the backpack", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the bag", 2],
            ["where is the bag", -1],
            ["what is the bag leaning on", 1],
            ["how many people are there", 1],
            ["how is the weather", -1],
            ["what is the bag made of", -1],
            ["what is on the bag", -1],
            ["what is on the ground", -1],
            ["what is the main color of the backpack", 1],
            ["what is on the floor", -1],
            ["how many people are in the photo", 1]
        ],
        "context": [
            "a green duffle bag sitting on top of a bed.",
            "a woman standing next to a pile of luggage."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2322219",
                "VG_object_id": "3297308",
                "bbox": [113, 15, 323, 280],
                "image": "data\\images\\2322219.jpg"
            },
            {
                "VG_image_id": "2372781",
                "VG_object_id": "1827069",
                "bbox": [90, 15, 175, 145],
                "image": "data\\images\\2372781.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the boy holding", 2],
            ["what color is the child's clothes", 1],
            ["where is the child", 1]
        ],
        "org_questions": [
            ["what is the child doing", -1],
            ["how many people are there in the picture", -1],
            ["what color is the child's clothes", 1],
            ["what is on the child's head", -1],
            ["When is photo taken", -1],
            ["what is the boy holding", 2],
            ["where is the child", 1],
            ["what is the child wearing", -1],
            ["how many kids are there", -1],
            ["what is the little boy doing", -1]
        ],
        "context": [
            "a little boy sitting on a couch with a cell phone.",
            "a birthday cake with candles on it"
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2358594",
                "VG_object_id": "2060861",
                "bbox": [77, 34, 212, 119],
                "image": "data\\images\\2358594.jpg"
            },
            {
                "VG_image_id": "2388900",
                "VG_object_id": "669899",
                "bbox": [360, 1, 500, 96],
                "image": "data\\images\\2388900.jpg"
            }
        ],
        "questions_with_scores": [["how many people are there", 1]],
        "org_questions": [
            ["how many people are there", 1],
            ["who is in the picture", -1],
            ["how is the photo", -1],
            ["where was this picture taken", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a library with a lot of laptops on the table",
            "two men sitting on a couch playing a video game."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2366617",
                "VG_object_id": "2155319",
                "bbox": [291, 272, 476, 372],
                "image": "data\\images\\2366617.jpg"
            },
            {
                "VG_image_id": "2367293",
                "VG_object_id": "752136",
                "bbox": [122, 295, 206, 381],
                "image": "data\\images\\2367293.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 2],
            ["where is the dog", 2],
            ["what is on the dog's neck", 1],
            ["what is the dog doing ", 1],
            ["what is behind the dog", 1],
            ["what is the dog's color", 1],
            ["what is around the dog's neck", 1],
            ["what is in front of the dog", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what color is the dog", 2],
            ["where is the dog", 2],
            ["what is on the dog's neck", 1],
            ["what is the dog doing ", 1],
            ["what is the dog on", -1],
            ["What is dog looking", -1],
            ["what type of animal is shown", -1],
            ["what is behind the dog", 1],
            ["what is the dog's color", 1],
            ["what is around the dog's neck", 1],
            ["what is in front of the dog", 1]
        ],
        "context": [
            "two men standing in a living room with a dog.",
            "a woman sitting on a small electric scooter with a dog."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2414003",
                "VG_object_id": "161649",
                "bbox": [4, 5, 495, 497],
                "image": "data\\images\\2414003.jpg"
            },
            {
                "VG_image_id": "2359668",
                "VG_object_id": "2327009",
                "bbox": [0, 25, 373, 498],
                "image": "data\\images\\2359668.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of food in on the table", 1],
            ["what kind of food is it", 1],
            ["what is the food", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what kind of food in on the table", 1],
            ["how many cups are there", -1],
            ["what is the table made of", -1],
            ["What is on the table", -1],
            ["what kind of food is it", 1],
            ["what color is the food", -1],
            ["where was the photo taken", -1],
            ["what is the table sitting on", -1],
            ["where is the plate", -1],
            ["what is on the table", -1],
            ["how many plates are there", -1],
            ["what is the food", 1],
            ["how many glasses are there on the table", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a bowl of soup with meat and vegetables.",
            "a plate of donuts and a glass of beer."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2410534",
                "VG_object_id": "215178",
                "bbox": [261, 58, 392, 296],
                "image": "data\\images\\2410534.jpg"
            },
            {
                "VG_image_id": "2332294",
                "VG_object_id": "3506735",
                "bbox": [14, 34, 221, 278],
                "image": "data\\images\\2332294.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color are the man's pants", 2],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is in the background", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["what color are the man's pants", 2],
            ["when is this picture taken", -1],
            ["where is the man", 1],
            ["what is the man wearing", -1],
            ["what is in the background", 1],
            ["what is on the man's head", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a man in a plaid shirt and khaki shorts playing frisbee.",
            "a man doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2316194",
                "VG_object_id": "3261671",
                "bbox": [173, 85, 488, 331],
                "image": "data\\images\\2316194.jpg"
            },
            {
                "VG_image_id": "2350983",
                "VG_object_id": "2116183",
                "bbox": [116, 17, 332, 329],
                "image": "data\\images\\2350983.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many tennis balls are there", 2],
            ["what color is the player's shirt", 2]
        ],
        "org_questions": [
            ["how many tennis balls are there", 2],
            ["what color is the player's shirt", 2],
            ["what is the gender of the player", -1],
            ["what is the player wearing on head", -1],
            ["what is the player holding", -1],
            ["what is the player wearing", -1],
            ["when was the picture taken", -1],
            ["why is the man holding the racket", -1],
            ["who is playing tennis", -1],
            ["where is the man", -1],
            ["what sport is the man playing", -1]
        ],
        "context": [
            "a man holding a tennis racket in his right hand.",
            "a man holding a tennis racket and ball."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2335379",
                "VG_object_id": "2125476",
                "bbox": [3, 275, 330, 499],
                "image": "data\\images\\2335379.jpg"
            },
            {
                "VG_image_id": "2318484",
                "VG_object_id": "2807158",
                "bbox": [227, 266, 364, 333],
                "image": "data\\images\\2318484.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the ground covered with", 1],
            ["what color is the ground", 1],
            ["what is the giraffe doing", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["how many giraffes are there", -1],
            ["what animal is standing on the ground", -1],
            ["what color is the ground", 1],
            ["what is in the distance", -1],
            ["what is on the ground", -1],
            ["how is the weather", -1],
            ["what is the giraffe doing", 1],
            ["what is the giraffe standing on", -1]
        ],
        "context": [
            "a giraffe laying down in the dirt.",
            "a giraffe standing in a field next to a forest."
        ]
    },
    {
        "object_category": "vegetable",
        "images": [
            {
                "VG_image_id": "2343096",
                "VG_object_id": "3072939",
                "bbox": [1, 2, 496, 329],
                "image": "data\\images\\2343096.jpg"
            },
            {
                "VG_image_id": "2342135",
                "VG_object_id": "936862",
                "bbox": [210, 156, 317, 254],
                "image": "data\\images\\2342135.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the green vegetable", 1],
            ["what is the food on", 1],
            ["What food are there on the plate", 1],
            ["what is green", 1],
            ["what type of food is shown", 1]
        ],
        "org_questions": [
            ["what is the green vegetable", 1],
            ["what is the food on", 1],
            ["how many kinds of vegetables are there", -1],
            ["What food are there on the plate", 1],
            ["what kind of vegetable is it", -1],
            ["what is on the top of the plate", -1],
            ["what is green", 1],
            ["what type of food is shown", 1]
        ],
        "context": [
            "a plate of food with mushrooms and broccoli.",
            "a table with two plates of food and a bowl of meat."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2413969",
                "VG_object_id": "162449",
                "bbox": [0, 199, 68, 310],
                "image": "data\\images\\2413969.jpg"
            },
            {
                "VG_image_id": "2370346",
                "VG_object_id": "604841",
                "bbox": [215, 167, 292, 240],
                "image": "data\\images\\2370346.jpg"
            }
        ],
        "questions_with_scores": [["what color is the man's pant", 1]],
        "org_questions": [
            ["what color is the man's pant", 1],
            ["what are the people doing in the picture", -1],
            ["who is wearing the trousers", -1],
            ["Where is the man", -1],
            ["What is man holding", -1],
            ["when was the photo taken", -1],
            ["what is the batter wearing", -1],
            ["how many people are in the photo", -1],
            ["when was this picture taken", -1],
            ["what color is the dirt", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2363990",
                "VG_object_id": "760390",
                "bbox": [88, 133, 177, 265],
                "image": "data\\images\\2363990.jpg"
            },
            {
                "VG_image_id": "2379349",
                "VG_object_id": "553500",
                "bbox": [32, 324, 331, 495],
                "image": "data\\images\\2379349.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the chair", 2],
            ["how many chairs are there", 1],
            ["what color is the ground", 1],
            ["what is the floor made of", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["where is the chair", 2],
            ["how many chairs are there", 1],
            ["what color is the ground", 1],
            ["what is the floor made of", 1],
            ["how many people are in  the picture", -1],
            ["how many pillows are there on the chair", -1],
            ["what is on the chair", -1],
            ["where was this photo taken", 1],
            ["what are the chairs made of", -1]
        ],
        "context": [
            "a bathroom with a sink, a rug and a rug.",
            "a table with a umbrella and chairs outside"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2317756",
                "VG_object_id": "1017651",
                "bbox": [203, 174, 275, 281],
                "image": "data\\images\\2317756.jpg"
            },
            {
                "VG_image_id": "2377954",
                "VG_object_id": "1774093",
                "bbox": [142, 309, 222, 373],
                "image": "data\\images\\2377954.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the gesture of the man", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the gesture of the man", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is the man doing", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is the man wearing", -1],
            ["where is the man", 1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a man and woman sitting on a bench in a park.",
            "a group of people standing around a building."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2352170",
                "VG_object_id": "1966037",
                "bbox": [0, 153, 182, 263],
                "image": "data\\images\\2352170.jpg"
            },
            {
                "VG_image_id": "2345592",
                "VG_object_id": "2055190",
                "bbox": [105, 90, 497, 319],
                "image": "data\\images\\2345592.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sofa", 1],
            ["what color is the pillow", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 1],
            ["what is on the sofa", -1],
            ["what color is the pillow", 1],
            ["how many people are there", -1],
            ["what is in front of the couch", -1],
            ["how many cats are there on the sofa", -1],
            ["what room is this", -1],
            ["what is the table made of", -1],
            ["who is in the room", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a living room with a tv, fireplace, and a television.",
            "a living room with a couch, coffee table, and a coffee table."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2370367",
                "VG_object_id": "741961",
                "bbox": [3, 412, 342, 481],
                "image": "data\\images\\2370367.jpg"
            },
            {
                "VG_image_id": "150497",
                "VG_object_id": "1075398",
                "bbox": [5, 299, 1022, 681],
                "image": "data\\images\\150497.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["what color is the wall", 1],
            ["what is on the ground", 1],
            ["where was this picture taken", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["what color is the wall", 1],
            ["where is the floor", -1],
            ["what is the floor made of", -1],
            ["what is on the ground", 1],
            ["where was this picture taken", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a door with a plaque on it",
            "a red fire truck parked in a garage."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2349366",
                "VG_object_id": "875306",
                "bbox": [157, 165, 339, 444],
                "image": "data\\images\\2349366.jpg"
            },
            {
                "VG_image_id": "2415042",
                "VG_object_id": "147182",
                "bbox": [149, 195, 246, 301],
                "image": "data\\images\\2415042.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", 1],
            ["how many people are in the picture", 1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", 1],
            ["how many people are in the picture", 1],
            ["where is the person", 1],
            ["how is the weather", -1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a woman holding a tennis racket on a court.",
            "a group of people standing on top of a van."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2334154",
                "VG_object_id": "3048942",
                "bbox": [163, 117, 245, 257],
                "image": "data\\images\\2334154.jpg"
            },
            {
                "VG_image_id": "2410340",
                "VG_object_id": "220119",
                "bbox": [113, 117, 267, 250],
                "image": "data\\images\\2410340.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's trousers", 2],
            ["what is the man wearing on his head", 1],
            ["what color is the ground", 1],
            ["what sport is the man doing", 1],
            ["where is the man", 1],
            ["what is the boy wearing", 1]
        ],
        "org_questions": [
            ["what color are the man's trousers", 2],
            ["what is the man wearing on his head", 1],
            ["what color is the ground", 1],
            ["what gender is the person in the trousers", -1],
            ["how many people are there", -1],
            ["what sport is the man doing", 1],
            ["where is the man", 1],
            ["what is the persion holding", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is on the person's feet", -1],
            ["what is the boy wearing", 1]
        ],
        "context": [
            "a man skiing on a course",
            "a man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2372645",
                "VG_object_id": "3732467",
                "bbox": [1, 151, 499, 331],
                "image": "data\\images\\2372645.jpg"
            },
            {
                "VG_image_id": "2373400",
                "VG_object_id": "1869547",
                "bbox": [1, 263, 498, 332],
                "image": "data\\images\\2373400.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many trucks are there on the land", 2],
            ["what main color is the land", 1],
            ["how many people are there on the land", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["what is on the ground", 1],
            ["where was the photo taken", 1],
            ["what is on the side of the road", 1]
        ],
        "org_questions": [
            ["what main color is the land", 1],
            ["how many people are there on the land", 1],
            ["how many trucks are there on the land", 2],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["what is on the ground", 1],
            ["how is the weather", -1],
            ["where was the photo taken", 1],
            ["what is on the side of the road", 1]
        ],
        "context": [
            "a red and white food truck parked on gravel.",
            "a stop sign on the side of a road."
        ]
    },
    {
        "object_category": "keyboard",
        "images": [
            {
                "VG_image_id": "2339271",
                "VG_object_id": "951905",
                "bbox": [135, 263, 360, 362],
                "image": "data\\images\\2339271.jpg"
            },
            {
                "VG_image_id": "2348000",
                "VG_object_id": "885551",
                "bbox": [173, 382, 360, 435],
                "image": "data\\images\\2348000.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the keyboard", 2],
            ["what type of computer is shown", 2],
            ["what color is the table", 1],
            ["how many laptops are in the picture", 1],
            ["what color is the computer screen", 1]
        ],
        "org_questions": [
            ["what color is the keyboard", 2],
            ["what color is the table", 1],
            ["what is the keyboard on", -1],
            ["how many laptops are in the picture", 1],
            ["where is the computer", -1],
            ["what is on the keyboard", -1],
            ["how many people are there in the picture", -1],
            ["what is on the table", -1],
            ["what type of computer is shown", 2],
            ["what color is the computer screen", 1]
        ],
        "context": [
            "a computer monitor sitting on top of a glass desk.",
            "a laptop computer sitting on top of a wooden table."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2384604",
                "VG_object_id": "525657",
                "bbox": [0, 169, 500, 410],
                "image": "data\\images\\2384604.jpg"
            },
            {
                "VG_image_id": "2392941",
                "VG_object_id": "476098",
                "bbox": [10, 358, 328, 494],
                "image": "data\\images\\2392941.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are on the land", 1],
            ["what shape is the land", 1],
            ["what is the man doing", 1],
            ["how is the weather", 1],
            ["what is in the background", 1],
            ["what is the weather like", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["how many people are on the land", 1],
            ["what shape is the land", 1],
            ["where is the picture taken", -1],
            ["What is on the ground", -1],
            ["what time is it", -1],
            ["what is the land made of", -1],
            ["what is the man doing", 1],
            ["how is the weather", 1],
            ["what is in the background", 1],
            ["what is the weather like", 1]
        ],
        "context": [
            "a man walking down a street with an umbrella.",
            "a man riding a skateboard on top of a cement wall."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2346133",
                "VG_object_id": "2610739",
                "bbox": [3, 151, 153, 238],
                "image": "data\\images\\2346133.jpg"
            },
            {
                "VG_image_id": "2386109",
                "VG_object_id": "519794",
                "bbox": [19, 59, 152, 167],
                "image": "data\\images\\2386109.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the bowl", 2],
            ["What is in the bowl", 1],
            ["What  color is the table", 1],
            ["what is the bowl made of", 1]
        ],
        "org_questions": [
            ["What is in the bowl", 1],
            ["What  color is the table", 1],
            ["What color is the bowl", 2],
            ["how many plates are there on the table", -1],
            ["what is the bowl made of", 1],
            ["what the bowl is on", -1],
            ["what is beside the bowl", -1],
            ["what is the bowl placed on", -1],
            ["where is the bowl", -1],
            ["what is on the table", -1],
            ["what is on the plate", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a plate of food with a cup of coffee and a plate of food.",
            "a plate of food on a table"
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2360876",
                "VG_object_id": "2684584",
                "bbox": [60, 69, 247, 322],
                "image": "data\\images\\2360876.jpg"
            },
            {
                "VG_image_id": "2323310",
                "VG_object_id": "3423243",
                "bbox": [105, 27, 465, 331],
                "image": "data\\images\\2323310.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what color are the man's clothes", 1],
            ["what color are the man's trousers", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what color are the man's clothes", 1],
            ["what color are the man's trousers", 1],
            ["how many people are there", -1],
            ["what is the person holding", -1],
            ["where is  the horse", -1],
            ["what is the ground covered with", 1],
            ["what is on the horse", -1],
            ["when was the photo taken", -1],
            ["who is riding the horse", -1],
            ["what are the horses doing", -1],
            ["what is on the horse's head", -1]
        ],
        "context": [
            "a group of people riding horses on top of a grass covered field.",
            "a man riding a horse on a dirt field."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2412851",
                "VG_object_id": "3395342",
                "bbox": [0, 240, 229, 372],
                "image": "data\\images\\2412851.jpg"
            },
            {
                "VG_image_id": "2366941",
                "VG_object_id": "625227",
                "bbox": [144, 371, 340, 483],
                "image": "data\\images\\2366941.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bench", 2],
            ["how many people are there", 2],
            ["what color is the ground", 1],
            ["what is the bench made of", 1],
            ["what is in the distance", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the bench", -1],
            ["where is the bench", 2],
            ["what color is the ground", 1],
            ["who is sitting on the bench", -1],
            ["when is this photo taken", -1],
            ["what is the bench made of", 1],
            ["what is in the distance", 1],
            ["what are the people doing", -1],
            ["how many people are there", 2],
            ["what are the people sitting on", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a group of people waiting for their luggage.",
            "an elderly woman sitting on a bench in front of a house."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2329690",
                "VG_object_id": "3485452",
                "bbox": [203, 155, 402, 260],
                "image": "data\\images\\2329690.jpg"
            },
            {
                "VG_image_id": "2374030",
                "VG_object_id": "587472",
                "bbox": [325, 203, 445, 332],
                "image": "data\\images\\2374030.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's shirt", 2],
            ["what is the boy doing", 1],
            ["what is the boy sitting on", 1]
        ],
        "org_questions": [
            ["what color is the boy's shirt", 2],
            ["what is the boy doing", 1],
            ["what is the boy sitting on", 1],
            ["how many people are there", -1],
            ["what is the pattern of the man's shirt", -1],
            ["who is wearing the shirt", -1],
            ["where is the person", -1],
            ["what color is the background", -1]
        ],
        "context": [
            "two boys laying on a bed with painted faces.",
            "a man and a woman sitting at a table with computers."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2362357",
                "VG_object_id": "2392786",
                "bbox": [306, 305, 481, 496],
                "image": "data\\images\\2362357.jpg"
            },
            {
                "VG_image_id": "2352913",
                "VG_object_id": "2338473",
                "bbox": [176, 171, 481, 332],
                "image": "data\\images\\2352913.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of bag is it", 2],
            ["what color is the bag", 2]
        ],
        "org_questions": [
            ["what kind of bag is it", 2],
            ["what color is the bag", 2],
            ["how many bags are there in the picture", -1],
            ["where is the bag", -1],
            ["how is the weather", -1],
            ["who is carrying the bag", -1],
            ["what is the bag made of", -1],
            ["what is on the ground", -1],
            ["what is black", -1]
        ],
        "context": [
            "a man holding a sign with a picture of a woman sitting on a step.",
            "a pile of luggage sitting on a train."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2409864",
                "VG_object_id": "1087809",
                "bbox": [62, 161, 122, 294],
                "image": "data\\images\\2409864.jpg"
            },
            {
                "VG_image_id": "2334460",
                "VG_object_id": "3252968",
                "bbox": [143, 24, 268, 271],
                "image": "data\\images\\2334460.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's coat", 2],
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["what is the persion riding on", 1],
            ["what is the man holding", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's coat", 2],
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["what is the man wearing on his head", -1],
            ["what is the ground covered with", -1],
            ["what is the weather like", -1],
            ["where is the man", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion riding on", 1],
            ["what is the man holding", 1],
            ["what is the person wearing", 1]
        ],
        "context": [
            "a group of people dressed in santa costumes.",
            "a man riding a skateboard across a street."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2368662",
                "VG_object_id": "2200135",
                "bbox": [88, 112, 374, 331],
                "image": "data\\images\\2368662.jpg"
            },
            {
                "VG_image_id": "2350345",
                "VG_object_id": "2420365",
                "bbox": [153, 41, 350, 374],
                "image": "data\\images\\2350345.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the elephant", 1],
            ["what are the elephant doing", 1],
            ["How many elephants are there", 1],
            ["where is the elephant", 1],
            ["where was the picture taken", 1]
        ],
        "org_questions": [
            ["what is on the elephant", 1],
            ["what color is the elephant", -1],
            ["what are the elephant doing", 1],
            ["How many elephants are there", 1],
            ["where is the elephant", 1],
            ["what is the ground the elephants standing on made of", -1],
            ["what is in front of the elephant", -1],
            ["what is in the distance", -1],
            ["when was the picture taken", -1],
            ["what animal is in the picture", -1],
            ["where was the picture taken", 1],
            ["what is on the elephant's head", -1]
        ],
        "context": [
            "a group of people riding on the back of an elephant.",
            "a large elephant standing next to a tall fence."
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2336619",
                "VG_object_id": "959914",
                "bbox": [289, 229, 355, 269],
                "image": "data\\images\\2336619.jpg"
            },
            {
                "VG_image_id": "2393025",
                "VG_object_id": "475504",
                "bbox": [1, 240, 500, 306],
                "image": "data\\images\\2393025.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the desk", 1],
            ["what is on the desk", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the desk", 1],
            ["what is on the desk", 1],
            ["how many tables are there", -1],
            ["what is the shape of the desk", -1],
            ["how many phones are there on the table", -1],
            ["what is on the table", 1],
            ["what material is the table made of", -1],
            ["where is the table", -1],
            ["what is made of wood", -1],
            ["what is the desk made out of", -1]
        ],
        "context": [
            "a cat sitting on a desk in front of a computer monitor.",
            "a laptop computer sitting on top of a wooden table."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2358225",
                "VG_object_id": "2000479",
                "bbox": [296, 44, 373, 176],
                "image": "data\\images\\2358225.jpg"
            },
            {
                "VG_image_id": "2330561",
                "VG_object_id": "2811599",
                "bbox": [296, 129, 425, 301],
                "image": "data\\images\\2330561.jpg"
            }
        ],
        "questions_with_scores": [
            ["what room is the cabinet in", 1],
            ["what is hanging on the wall", 1],
            ["what room is this", 1],
            ["what is next to the wall", 1]
        ],
        "org_questions": [
            ["what is on the cabinet", -1],
            ["how many people are there", -1],
            ["what room is the cabinet in", 1],
            ["what is hanging on the wall", 1],
            ["what room is this", 1],
            ["what is the cabinet made of", -1],
            ["what is next to the wall", 1]
        ],
        "context": [
            "a kitchen with a refrigerator, microwave and a microwave.",
            "a living room with a table, chairs and a table."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2359809",
                "VG_object_id": "2344014",
                "bbox": [222, 0, 499, 280],
                "image": "data\\images\\2359809.jpg"
            },
            {
                "VG_image_id": "2396209",
                "VG_object_id": "1199658",
                "bbox": [32, 378, 204, 485],
                "image": "data\\images\\2396209.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the dog biting", 2],
            ["how many people are there", 2],
            ["how many people are there in the picture", 2],
            ["what color is the dog", 1],
            ["what is in front of the dog", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what color is the dog", 1],
            ["what is the dog biting", 2],
            ["what color is the background", -1],
            ["how many people are there", 2],
            ["where is the dog", -1],
            ["What is the dog doing", -1],
            ["what is in front of the dog", 1],
            ["what is in the distance", 1],
            ["what kind of animal is in the picture", -1],
            ["what is the dog sitting on", -1],
            ["what is on the dog", -1],
            ["what is under the dog", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a dog eating a piece of pizza from a pizza box.",
            "a man is looking at a car engine."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2373742",
                "VG_object_id": "2481051",
                "bbox": [377, 300, 481, 368],
                "image": "data\\images\\2373742.jpg"
            },
            {
                "VG_image_id": "2370610",
                "VG_object_id": "2354555",
                "bbox": [3, 127, 143, 199],
                "image": "data\\images\\2370610.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many floor does the building have", 2],
            ["what color is the building's top", 1],
            ["what is in the distance", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the building's top", 1],
            ["what is in the distance", 1],
            ["how many floor does the building have", 2],
            ["what shape is the roof of the building", -1],
            ["when is this picture taken", -1],
            ["what are the people doing", -1],
            ["what is the weather like", -1],
            ["what is on the ground", -1],
            ["where is the picture taken", 1],
            ["how many people are there", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a beach with people flying kites on it.",
            "a young boy playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2366606",
                "VG_object_id": "627221",
                "bbox": [260, 39, 484, 391],
                "image": "data\\images\\2366606.jpg"
            },
            {
                "VG_image_id": "2411272",
                "VG_object_id": "1081274",
                "bbox": [188, 14, 318, 167],
                "image": "data\\images\\2411272.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child holding", 2],
            ["what is the child doing", 1],
            ["where is the child", 1],
            ["How many people are there", 1],
            ["What color is girl's shirt", 1],
            ["what is in front of the girl", 1],
            ["what is behind the girl", 1]
        ],
        "org_questions": [
            ["what is the child holding", 2],
            ["what is the child doing", 1],
            ["where is the child", 1],
            ["How many people are there", 1],
            ["What color is girl's shirt", 1],
            ["what is on the girl's head", -1],
            ["what is in front of the girl", 1],
            ["who is in the photo", -1],
            ["what is the child wearing", -1],
            ["what is behind the girl", 1],
            ["what is the little girl wearing", -1]
        ],
        "context": [
            "two children holding stuffed animals and one is holding a teddy bear.",
            "a little girl drinking milk from a glass."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2366417",
                "VG_object_id": "3879268",
                "bbox": [177, 51, 347, 331],
                "image": "data\\images\\2366417.jpg"
            },
            {
                "VG_image_id": "2316550",
                "VG_object_id": "3196779",
                "bbox": [41, 91, 191, 331],
                "image": "data\\images\\2316550.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the gesture of the man", 1],
            ["what is the man wearing", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the man doing", 1],
            ["what is the man's posture", 1],
            ["when was the photo taken", 1],
            ["what is the man standing on", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the gesture of the man", 1],
            ["what is the man wearing", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the man doing", 1],
            ["what is the man's posture", 1],
            ["who is in the photo", -1],
            ["when was the photo taken", 1],
            ["what is the man standing on", 1],
            ["what is on the man's face", 1]
        ],
        "context": [
            "a man riding a bike across a street.",
            "a group of people waiting for a train."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2379427",
                "VG_object_id": "552838",
                "bbox": [69, 198, 253, 286],
                "image": "data\\images\\2379427.jpg"
            },
            {
                "VG_image_id": "2317977",
                "VG_object_id": "3461741",
                "bbox": [0, 235, 277, 484],
                "image": "data\\images\\2317977.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bed", 1],
            ["how many pillows are on the bed", 1]
        ],
        "org_questions": [
            ["what color is the bed", 1],
            ["how many pillows are on the bed", 1],
            ["what color is the wall", -1],
            ["where is the photo taken", -1],
            ["what is on the bed", -1],
            ["what is under the blanket", -1],
            ["how many cats are on the blanket", -1],
            ["What is on the blanket", -1],
            ["what room is this", -1],
            ["what is the pattern of the bed", -1],
            ["where are the pillows", -1]
        ],
        "context": [
            "a bedroom with two beds and a closet.",
            "a bedroom with a bed, nightstand, lamp and nightstand."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2319198",
                "VG_object_id": "2708199",
                "bbox": [0, 221, 332, 498],
                "image": "data\\images\\2319198.jpg"
            },
            {
                "VG_image_id": "2402117",
                "VG_object_id": "392550",
                "bbox": [271, 175, 497, 304],
                "image": "data\\images\\2402117.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the plate", 1],
            ["How many plates are there", 1]
        ],
        "org_questions": [
            ["What color is the plate", 1],
            ["How many plates are there", 1],
            ["what is in the distance", -1],
            ["where is the plate", -1],
            ["what is on the plate", -1],
            ["what color is the table the plate on", -1],
            ["what color is the table", -1],
            ["what shape is the plate", -1],
            ["what is the plate made of", -1],
            ["what kind of food is this", -1],
            ["what is the food on", -1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a plate with some bread and some bananas",
            "a table with two plates of food and a cup of coffee."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2359333",
                "VG_object_id": "794213",
                "bbox": [291, 22, 464, 200],
                "image": "data\\images\\2359333.jpg"
            },
            {
                "VG_image_id": "2370701",
                "VG_object_id": "2485065",
                "bbox": [230, 57, 492, 317],
                "image": "data\\images\\2370701.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the background", 1],
            ["What is next to the laptop", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the color of the laptop", -1],
            ["where is the laptop", -1],
            ["what color is the background", 1],
            ["How many people are there", -1],
            ["what is on the computer screen", -1],
            ["What is next to the laptop", 1],
            ["What is above the laptop", -1],
            ["how many laptops are there", -1],
            ["what type of computer is shown", -1],
            ["where was the photo taken", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a laptop, camera, and other electronics on a couch.",
            "a laptop computer sitting on top of a table."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2382761",
                "VG_object_id": "537343",
                "bbox": [2, 248, 500, 333],
                "image": "data\\images\\2382761.jpg"
            },
            {
                "VG_image_id": "2357468",
                "VG_object_id": "3553491",
                "bbox": [1, 190, 499, 373],
                "image": "data\\images\\2357468.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["what kind of animal is on the land", 1],
            ["What is the object on the ground", 1],
            ["What kind of animal is there", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what is on the ground", 1],
            ["what is the ground covered with", -1],
            ["how many people are there", -1],
            ["what kind of animal is on the land", 1],
            ["What is the object on the ground", 1],
            ["What kind of animal is there", 1],
            ["what is the land made of", -1],
            ["when was the photo taken", -1],
            ["where was this photo taken", 1],
            ["how is the weather", -1],
            ["where is the grass", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a man riding a motorcycle past a large tree.",
            "a zebra standing next to a zebra laying down."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2354185",
                "VG_object_id": "3574382",
                "bbox": [282, 84, 490, 172],
                "image": "data\\images\\2354185.jpg"
            },
            {
                "VG_image_id": "2409897",
                "VG_object_id": "230831",
                "bbox": [280, 148, 380, 183],
                "image": "data\\images\\2409897.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the seat", 2],
            ["what is in the background", 1],
            ["what is the chair made of", 1]
        ],
        "org_questions": [
            ["what color is the seat", 2],
            ["what is in the background", 1],
            ["how many chairs are in the picture", -1],
            ["what time is it", -1],
            ["how many people are there sitting on the seat", -1],
            ["where is the chair", -1],
            ["what is the chair made of", 1]
        ],
        "context": [
            "a motorcycle is parked in a garage with people standing around.",
            "a woman with a teddy bear in a basket."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2388666",
                "VG_object_id": "671742",
                "bbox": [172, 121, 499, 271],
                "image": "data\\images\\2388666.jpg"
            },
            {
                "VG_image_id": "2345763",
                "VG_object_id": "904908",
                "bbox": [50, 209, 428, 484],
                "image": "data\\images\\2345763.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bowl", 1],
            ["what color is the table", 1],
            ["what kind of food is on the plate", 1],
            ["what is the food on", 1]
        ],
        "org_questions": [
            ["what color is the bowl", 1],
            ["what is on the bowl", -1],
            ["what color is the table", 1],
            ["where is the bowl", -1],
            ["what is in the bowl", -1],
            ["what is the bowl made of", -1],
            ["what is inside the bowl", -1],
            ["how many bowls are there", -1],
            ["what is the shape of the plate", -1],
            ["what kind of food is on the plate", 1],
            ["what is the food on", 1]
        ],
        "context": [
            "a chocolate cake on a plate with a cup of coffee.",
            "a bowl of apples and oranges on a table."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2333993",
                "VG_object_id": "3304747",
                "bbox": [51, 68, 336, 288],
                "image": "data\\images\\2333993.jpg"
            },
            {
                "VG_image_id": "2408211",
                "VG_object_id": "265955",
                "bbox": [169, 159, 376, 285],
                "image": "data\\images\\2408211.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many layers does the bus have", 2],
            ["what color is the road under the bus", 2],
            ["what is the main color of the bus", 2],
            ["what color of the road does the bus park on", 1]
        ],
        "org_questions": [
            ["how many deckers does the bus has", -1],
            ["what color of the road does the bus park on", 1],
            ["what is on the side of the bus", -1],
            ["where is the bus", -1],
            ["what is the weather like", -1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["who is driving the bus", -1],
            ["what is the bus doing", -1],
            ["what kind of bus is this", -1],
            ["where was the photo taken", -1],
            ["how many layers does the bus have", 2],
            ["what color is the road under the bus", 2],
            ["what is the main color of the bus", 2]
        ],
        "context": [
            "two red double decker buses are parked on the side of the road.",
            "a bus parked on the side of the road."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2403273",
                "VG_object_id": "1126346",
                "bbox": [176, 158, 363, 374],
                "image": "data\\images\\2403273.jpg"
            },
            {
                "VG_image_id": "2374035",
                "VG_object_id": "3722945",
                "bbox": [11, 41, 198, 313],
                "image": "data\\images\\2374035.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shelf", 2],
            ["what is in front of the shelf", 1],
            ["where is the shelf", 1],
            ["what is on the shelf", 1],
            ["where was this picture taken", 1],
            ["where is the man standing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what color is the shelf", 2],
            ["how many people are in the picture", -1],
            ["what is in front of the shelf", 1],
            ["where is the shelf", 1],
            ["what is on the shelf", 1],
            ["what is in the background", -1],
            ["how many books are there on the shelf", -1],
            ["what is the man doing", -1],
            ["who is in the photo", -1],
            ["where was this picture taken", 1],
            ["where is the man standing", 1],
            ["what is behind the man", 1]
        ],
        "context": [
            "a man in a white shirt",
            "a man standing next to a bike in a room."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2376326",
                "VG_object_id": "573897",
                "bbox": [0, 0, 499, 312],
                "image": "data\\images\\2376326.jpg"
            },
            {
                "VG_image_id": "2370492",
                "VG_object_id": "2269064",
                "bbox": [31, 183, 481, 363],
                "image": "data\\images\\2370492.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["what is the desk sitting on", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 1],
            ["how many plates are there on the table", -1],
            ["How many tables are there", -1],
            ["what is the table made of", -1],
            ["what is the desk sitting on", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a white computer mouse sitting on top of a wooden table.",
            "a cat is sitting on a desk with papers."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2395696",
                "VG_object_id": "1203712",
                "bbox": [18, 63, 160, 370],
                "image": "data\\images\\2395696.jpg"
            },
            {
                "VG_image_id": "2385781",
                "VG_object_id": "685804",
                "bbox": [111, 0, 230, 209],
                "image": "data\\images\\2385781.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["Where is the woman", 1],
            ["what is the ground covered with", 1],
            ["What is woman doing", 1],
            ["what gesture is the woman", 1],
            ["what is on the woman's head", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 2],
            ["how many people are there", -1],
            ["Where is the woman", 1],
            ["what is the ground covered with", 1],
            ["What is woman doing", 1],
            ["what gesture is the woman", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's head", 1],
            ["what is the persion wearing", 1],
            ["what is the persion wearing on the right", -1]
        ],
        "context": [
            "a young girl holding a sign in a store.",
            "a woman sitting on a bench next to a fire hydrant."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2341652",
                "VG_object_id": "940877",
                "bbox": [160, 180, 231, 422],
                "image": "data\\images\\2341652.jpg"
            },
            {
                "VG_image_id": "2393462",
                "VG_object_id": "1219677",
                "bbox": [133, 153, 237, 287],
                "image": "data\\images\\2393462.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on the head", 2],
            ["what is the man on", 2],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what is the man wearing on the head", 2],
            ["what is the color of the man", -1],
            ["what is the man on", 2],
            ["how many people are there", -1],
            ["how old is the man", -1],
            ["where is the man", 1],
            ["what is the ground covered with", -1],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", 1]
        ],
        "context": [
            "a man on a motorcycle with a dog on the back.",
            "a man jumping in the air on a skateboard."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2330762",
                "VG_object_id": "3270482",
                "bbox": [440, 123, 499, 324],
                "image": "data\\images\\2330762.jpg"
            },
            {
                "VG_image_id": "2349084",
                "VG_object_id": "2132121",
                "bbox": [181, 150, 303, 204],
                "image": "data\\images\\2349084.jpg"
            }
        ],
        "questions_with_scores": [["what is on the table", 1]],
        "org_questions": [
            ["what is on the table", 1],
            ["how many people are there", -1],
            ["what is the shape of the table", -1],
            ["What color is the table", -1],
            ["what is the table made of", -1],
            ["how many bottles are there on the table", -1],
            ["where was the photo taken", -1],
            ["where is the table", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a man sitting on a couch with a dog on his lap.",
            "a living room with a fireplace and a chair."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2369769",
                "VG_object_id": "2028884",
                "bbox": [239, 2, 353, 324],
                "image": "data\\images\\2369769.jpg"
            },
            {
                "VG_image_id": "2350608",
                "VG_object_id": "2503125",
                "bbox": [319, 87, 419, 264],
                "image": "data\\images\\2350608.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 2],
            ["what is the man holding", 1],
            ["what kind of skis is the man wearing", -1],
            ["where is the man", -1],
            ["what is the color of the floor", -1],
            ["what is in the background", -1],
            ["what is the person doing", -1],
            ["what is the man wearing on head", -1],
            ["when was the photo taken", -1],
            ["who is skiing", -1],
            ["what is on the person's feet", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a couple of people skiing down a snow covered slope.",
            "a person riding a snowboard down a snow covered slope."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2355592",
                "VG_object_id": "828288",
                "bbox": [12, 10, 372, 496],
                "image": "data\\images\\2355592.jpg"
            },
            {
                "VG_image_id": "2342237",
                "VG_object_id": "3919630",
                "bbox": [1, 2, 482, 372],
                "image": "data\\images\\2342237.jpg"
            }
        ],
        "questions_with_scores": [["How many pizzas are there", 2]],
        "org_questions": [
            ["How many pizzas are there", 2],
            ["what is the piazza on", -1],
            ["what is the table made of", -1],
            ["where is the table", -1],
            ["what is on the table", -1],
            ["what kind of food is this", -1],
            ["what is the pizza sitting on", -1],
            ["where is the pizza sitting", -1],
            ["where is the pizza", -1]
        ],
        "context": [
            "a table with several pizzas on it and a bottle of beer.",
            "a pizza with cheese and olives on a wooden board."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2324335",
                "VG_object_id": "2893158",
                "bbox": [413, 93, 498, 308],
                "image": "data\\images\\2324335.jpg"
            },
            {
                "VG_image_id": "2372659",
                "VG_object_id": "735917",
                "bbox": [250, 62, 428, 311],
                "image": "data\\images\\2372659.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman doing", 2],
            ["what is the woman holding", 1],
            ["what is the woman wearing", 1],
            ["what is the gesture of the woman", 1],
            ["where is the woman", 1],
            ["what color is the background", 1],
            ["what is the color of the woman's shirt", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 1],
            ["what is the woman wearing", 1],
            ["what is the gesture of the woman", 1],
            ["how many people are there", -1],
            ["where is the woman", 1],
            ["what color is the background", 1],
            ["What is woman doing", 2],
            ["what is the color of the woman's shirt", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a horse is walking in a field near a lake.",
            "a woman holding a baby in a barn."
        ]
    },
    {
        "object_category": "lamp",
        "images": [
            {
                "VG_image_id": "2366769",
                "VG_object_id": "1850688",
                "bbox": [193, 9, 364, 84],
                "image": "data\\images\\2366769.jpg"
            },
            {
                "VG_image_id": "2341352",
                "VG_object_id": "3217919",
                "bbox": [207, 69, 269, 159],
                "image": "data\\images\\2341352.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the lamp", 1],
            ["How many lamps are there", 1],
            ["what is the lamp putting on", 1],
            ["Where is the lamp standing on", 1],
            ["what is under the lamp", 1],
            ["how many lamps are there in the picture", 1],
            ["what is hanging on the wall", 1]
        ],
        "org_questions": [
            ["Where is the lamp", 1],
            ["How many lamps are there", 1],
            ["what is the lamp putting on", 1],
            ["Where is the lamp standing on", 1],
            ["what is under the lamp", 1],
            ["how many lamps are there in the picture", 1],
            ["where was the photo taken", -1],
            ["what is turned on", -1],
            ["what is hanging on the wall", 1]
        ],
        "context": [
            "a man taking a picture of himself in a bathroom mirror.",
            "a bed with a book on it and a lamp on the nightstand."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2410111",
                "VG_object_id": "225621",
                "bbox": [200, 307, 271, 360],
                "image": "data\\images\\2410111.jpg"
            },
            {
                "VG_image_id": "2371724",
                "VG_object_id": "3519231",
                "bbox": [441, 219, 498, 261],
                "image": "data\\images\\2371724.jpg"
            }
        ],
        "questions_with_scores": [
            ["how is the weather", 1],
            ["where is the car", 1],
            ["what is in the background", 1],
            ["how many cars are there", 1],
            ["what type of vehicle is shown", 1],
            ["what is on the side of the truck", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["how is the weather", 1],
            ["where is the car", 1],
            ["what is in the background", 1],
            ["how many cars are there", 1],
            ["what time is it", -1],
            ["what shape is the car", -1],
            ["what color is the car", -1],
            ["what is the ground covered with", -1],
            ["what type of vehicle is shown", 1],
            ["when was the photo taken", -1],
            ["what is on the side of the truck", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a city street with cars and people walking on the sidewalk.",
            "a large jetliner sitting on top of an airport tarmac."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2354893",
                "VG_object_id": "1686671",
                "bbox": [393, 224, 452, 301],
                "image": "data\\images\\2354893.jpg"
            },
            {
                "VG_image_id": "2361095",
                "VG_object_id": "1752046",
                "bbox": [77, 92, 141, 147],
                "image": "data\\images\\2361095.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color is the background", 2],
            ["what is the pattern of the man's shirt", 1],
            ["how many people are there in the picture", 1],
            ["Where is the man", 1],
            ["what is the man in the middle holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the background", 2],
            ["what is the pattern of the man's shirt", 1],
            ["how many people are there in the picture", 1],
            ["Where is the man", 1],
            ["what is the man in the middle holding", 1],
            ["when was the photo taken", -1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a large display of bananas",
            "a man riding a skateboard down a sidewalk."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2327870",
                "VG_object_id": "2722387",
                "bbox": [278, 26, 453, 129],
                "image": "data\\images\\2327870.jpg"
            },
            {
                "VG_image_id": "2380544",
                "VG_object_id": "1345347",
                "bbox": [16, 0, 488, 128],
                "image": "data\\images\\2380544.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["what is the pattern on the umbrella", 1],
            ["how many people are there", 1],
            ["what pattern is on the umbrella", 1],
            ["what is holding the umbrella", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", -1],
            ["what is the main color of the umbrella", -1],
            ["what is in the background", 1],
            ["what is the pattern on the umbrella", 1],
            ["how many umbrellas are there", -1],
            ["where is the umbrella", -1],
            ["what is the umbrella made of", -1],
            ["what is the posture of the person", -1],
            ["what kind of umbrella is this", -1],
            ["how is the umbrella", -1],
            ["how many people are there", 1],
            ["what pattern is on the umbrella", 1],
            ["How many umbrellas are there", -1],
            ["who is holding the umbrella", -1],
            ["what is holding the umbrella", 1]
        ],
        "context": [
            "a woman standing in front of a store with an umbrella.",
            "a woman holding a blue umbrella"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2392327",
                "VG_object_id": "1231382",
                "bbox": [117, 133, 192, 300],
                "image": "data\\images\\2392327.jpg"
            },
            {
                "VG_image_id": "2407815",
                "VG_object_id": "273126",
                "bbox": [92, 51, 190, 300],
                "image": "data\\images\\2407815.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man standing on", 1],
            ["how many people are there", 1],
            ["what is on the head of the man", 1],
            ["Where is the man", 1],
            ["what is the man wearing", 1],
            ["who is in the photo", 1],
            ["where is the picture taken", 1],
            ["what is the man staying on", 1],
            ["how many people are there in the picture", 1],
            ["What is man doing", 1],
            ["What is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", -1],
            ["what is the man doing", 1],
            ["what is the man standing on", 1],
            ["how many people are there", 1],
            ["what is on the head of the man", 1],
            ["what is the weather like", -1],
            ["Where is the man", 1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["where is the picture taken", 1],
            ["what is the man staying on", 1],
            ["how many people are there in the picture", 1],
            ["What is man doing", 1],
            ["What is the man holding", 1]
        ],
        "context": [
            "a man holding a stop sign and a car",
            "a man and a woman riding a motorcycle with a dog in the side car."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2395805",
                "VG_object_id": "450718",
                "bbox": [294, 154, 351, 204],
                "image": "data\\images\\2395805.jpg"
            },
            {
                "VG_image_id": "2382018",
                "VG_object_id": "699888",
                "bbox": [90, 57, 155, 128],
                "image": "data\\images\\2382018.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what sport are the people playing", 2],
            ["how many people are there in the photo", 1],
            ["what is the ground covered with", 1],
            ["what is the persion holding", 1],
            ["what color is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what sport are the people playing", 2],
            ["what gender is the person in the shirt", -1],
            ["how many people are there in the photo", 1],
            ["what is the ground covered with", 1],
            ["what is the persion holding", 1],
            ["when was the photo taken", -1],
            ["what is the man wearing", -1],
            ["what color is the man wearing", 1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "two men playing frisbee on a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2387540",
                "VG_object_id": "512653",
                "bbox": [186, 141, 350, 313],
                "image": "data\\images\\2387540.jpg"
            },
            {
                "VG_image_id": "2369468",
                "VG_object_id": "611180",
                "bbox": [181, 167, 287, 218],
                "image": "data\\images\\2369468.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the person holding", 2],
            ["What is the man wearing on his head", 2]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person holding", 2],
            ["how many people are there in the picture", -1],
            ["What is the man wearing on his head", 2],
            ["where is the person", -1],
            ["What is the man doing", -1],
            ["what is the color of the trousers", -1],
            ["how many people are there", -1],
            ["what is the man wearing", -1],
            ["what color are the shoes", -1],
            ["what color is the dirt", -1]
        ],
        "context": [
            "a baseball player swinging a bat on a field.",
            "a baseball player pitching a ball on a field."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2385503",
                "VG_object_id": "1296103",
                "bbox": [298, 162, 452, 295],
                "image": "data\\images\\2385503.jpg"
            },
            {
                "VG_image_id": "2400335",
                "VG_object_id": "3818409",
                "bbox": [6, 133, 496, 338],
                "image": "data\\images\\2400335.jpg"
            }
        ],
        "questions_with_scores": [["what is on the sofa", 1]],
        "org_questions": [
            ["what color is the sofa", -1],
            ["what is on the sofa", 1],
            ["How many people are there", -1],
            ["how many cats are there on the sofa", -1],
            ["how many people are there sitting on the sofa", -1],
            ["where is this scene", -1],
            ["what room is this", -1],
            ["what is next to the couch", -1]
        ],
        "context": [
            "a living room with a couch, coffee table, and television.",
            "a living room with a couch, coffee table and a television."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2400252",
                "VG_object_id": "1161183",
                "bbox": [204, 1, 499, 372],
                "image": "data\\images\\2400252.jpg"
            },
            {
                "VG_image_id": "2373985",
                "VG_object_id": "1904389",
                "bbox": [151, 87, 317, 368],
                "image": "data\\images\\2373985.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["what is the girl wearing", 1],
            ["what is in front of the girl", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1],
            ["what is the little girl holding", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what is the girl wearing", 1],
            ["what is in front of the girl", 1],
            ["what is the color of the hair", -1],
            ["how many people are there", 1],
            ["what is on the girl's head", -1],
            ["what is the woman holding", 1],
            ["what is the little girl holding", 1]
        ],
        "context": [
            "a small child sleeping with a stuffed animal.",
            "two young girls playing a video game in a living room."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2355036",
                "VG_object_id": "833408",
                "bbox": [1, 369, 373, 495],
                "image": "data\\images\\2355036.jpg"
            },
            {
                "VG_image_id": "2385984",
                "VG_object_id": "684563",
                "bbox": [0, 350, 499, 419],
                "image": "data\\images\\2385984.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["what is the table sitting on", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is the table made of", -1],
            ["what is on the table", 1],
            ["how many plates are there", -1],
            ["where was the photo taken", -1],
            ["what is the table sitting on", 1],
            ["what material is the table made of", -1]
        ],
        "context": [
            "a man and a woman sitting at a table with a bottle of wine.",
            "a vase with a plant in it next to a vase."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2403273",
                "VG_object_id": "1126333",
                "bbox": [78, 152, 240, 466],
                "image": "data\\images\\2403273.jpg"
            },
            {
                "VG_image_id": "2323067",
                "VG_object_id": "3479079",
                "bbox": [62, 150, 191, 395],
                "image": "data\\images\\2323067.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["what gender is the person", 2],
            ["what color is the person's shirt", 1],
            ["how many people are there", 1],
            ["what is on the person's head", 1],
            ["who is wearing a white shirt", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the person doing", 2],
            ["what gender is the person", 2],
            ["what color is the person's shirt", 1],
            ["how many people are there", 1],
            ["what is on the person's head", 1],
            ["where is the person", -1],
            ["how is the weather", -1],
            ["who is wearing a white shirt", 1],
            ["what is the persion holding", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a man in a white shirt",
            "two women sitting in chairs in a room."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2413141",
                "VG_object_id": "179442",
                "bbox": [0, 0, 499, 331],
                "image": "data\\images\\2413141.jpg"
            },
            {
                "VG_image_id": "2407849",
                "VG_object_id": "272550",
                "bbox": [40, 31, 359, 141],
                "image": "data\\images\\2407849.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is the food on", 1],
            ["what food is it", 1],
            ["what is the table color", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is  on the table", -1],
            ["what is the food on", 1],
            ["How many cups are there", -1],
            ["what shape is the table", -1],
            ["what food is it", 1],
            ["where is the picture taken", -1],
            ["what is the table color", 1]
        ],
        "context": [
            "a table topped with plates of food and a bowl of chips.",
            "a pizza with onions, tomatoes, onions and cheese."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2403695",
                "VG_object_id": "350584",
                "bbox": [157, 1, 209, 114],
                "image": "data\\images\\2403695.jpg"
            },
            {
                "VG_image_id": "2330113",
                "VG_object_id": "3461891",
                "bbox": [104, 172, 159, 369],
                "image": "data\\images\\2330113.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["where is the woman", 2],
            ["what color is the woman's clothes", 1],
            ["what is the gesture of the woman", 1],
            ["What is woman doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", 1],
            ["what is the woman holding", 2],
            ["what is the gesture of the woman", 1],
            ["how many people are there", -1],
            ["where is the woman", 2],
            ["What is woman doing", 1],
            ["what is the woman wearing", -1]
        ],
        "context": [
            "a dog laying on the ground with a suitcase.",
            "a group of people standing on a sidewalk holding umbrellas."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2316114",
                "VG_object_id": "3058518",
                "bbox": [223, 132, 292, 236],
                "image": "data\\images\\2316114.jpg"
            },
            {
                "VG_image_id": "2326347",
                "VG_object_id": "3315183",
                "bbox": [82, 208, 168, 462],
                "image": "data\\images\\2326347.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the male child doing", 1],
            ["where is the male child standing", 1],
            ["how many people are there", 1],
            ["what is the boy holding", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what is the male child wearing on his upper body", -1],
            ["what is the male child doing", 1],
            ["where is the male child standing", 1],
            ["how many people are there", 1],
            ["what color are the persons' trousers", -1],
            ["what is the boy holding", 1],
            ["What is the background of image", -1],
            ["what is the child wearing", -1],
            ["when was the photo taken", -1],
            ["what is the man standing on", 1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a man riding a wake board on a lake.",
            "a man and woman standing on a beach holding surfboards."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2379129",
                "VG_object_id": "554800",
                "bbox": [2, 178, 451, 499],
                "image": "data\\images\\2379129.jpg"
            },
            {
                "VG_image_id": "2358215",
                "VG_object_id": "3547065",
                "bbox": [152, 230, 404, 331],
                "image": "data\\images\\2358215.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["What food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["how many people are there", 1],
            ["how many plates are there on the table", -1],
            ["what shape is the plate", -1],
            ["What food is on the plate", 1],
            ["where is the plate", -1],
            ["what is the table under the plate made of", -1],
            ["what is in the background", -1],
            ["what is the plate on", -1],
            ["what is on the table", -1],
            ["what is the table color", -1]
        ],
        "context": [
            "a plate of food with a sandwich and some fries.",
            "a family sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2328653",
                "VG_object_id": "3058420",
                "bbox": [5, 291, 120, 357],
                "image": "data\\images\\2328653.jpg"
            },
            {
                "VG_image_id": "2355457",
                "VG_object_id": "2682642",
                "bbox": [77, 39, 419, 205],
                "image": "data\\images\\2355457.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 1],
            ["what color is the ground", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["where is the chair", -1],
            ["what color is the ground", 1],
            ["how many seats are there", -1],
            ["what is the ground covered with", 1],
            ["what is in the background", -1],
            ["where was the photo taken", -1],
            ["what is next to the chair", -1],
            ["what is the chair made of", -1]
        ],
        "context": [
            "a kitchen with a stove, oven, stove and sink.",
            "a red seat on a train."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2380656",
                "VG_object_id": "544233",
                "bbox": [46, 330, 227, 424],
                "image": "data\\images\\2380656.jpg"
            },
            {
                "VG_image_id": "2354548",
                "VG_object_id": "837628",
                "bbox": [1, 175, 500, 332],
                "image": "data\\images\\2354548.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the bench", 1],
            ["when is the picture taken", 1],
            ["what gender is the person around the bench", 1],
            ["what color is the chair", 1],
            ["what is the persion doing", 1],
            ["what is the bench made of", 1]
        ],
        "org_questions": [
            ["what is on the bench", 1],
            ["when is the picture taken", 1],
            ["how many people are in the picture", -1],
            ["what shape is the bench", -1],
            ["what gender is the person around the bench", 1],
            ["what color is the chair", 1],
            ["where is the bench", -1],
            ["what season is the photo taken in", -1],
            ["how is the weather", -1],
            ["what is the persion doing", 1],
            ["what is the bench made of", 1]
        ],
        "context": [
            "a man riding a skateboard on top of a bench.",
            "a woman sitting on a bench looking at her cell phone."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2405304",
                "VG_object_id": "373806",
                "bbox": [318, 140, 474, 352],
                "image": "data\\images\\2405304.jpg"
            },
            {
                "VG_image_id": "2355413",
                "VG_object_id": "1818710",
                "bbox": [11, 209, 246, 374],
                "image": "data\\images\\2355413.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["how many persons are there", 2],
            ["what is the color of the shirt", 1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["where is the picture taken", 1],
            ["how many people are there in the picture", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what is the person doing", 2],
            ["what is the color of the shirt", 1],
            ["where is the person", 1],
            ["how many persons are there", 2],
            ["what is the person holding", 1],
            ["where is the picture taken", 1],
            ["how many people are there in the picture", 1],
            ["when was the picture taken", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a group of people sitting around a table eating food.",
            "a giraffe sticking its tongue out at a zoo."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2316990",
                "VG_object_id": "2903624",
                "bbox": [25, 63, 475, 199],
                "image": "data\\images\\2316990.jpg"
            },
            {
                "VG_image_id": "2389053",
                "VG_object_id": "507086",
                "bbox": [287, 189, 445, 295],
                "image": "data\\images\\2389053.jpg"
            }
        ],
        "questions_with_scores": [["what is in the background", 1]],
        "org_questions": [
            ["what is on the truck", -1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["when is this photo taken", -1],
            ["what color is the ground", -1],
            ["What is the weather like", -1],
            ["what is the floor made of", -1],
            ["where is the truck", -1],
            ["what type of vehicle is shown", -1],
            ["what is the truck doing", -1],
            ["what is the truck on", -1],
            ["what kind of truck is this", -1]
        ],
        "context": [
            "a truck is parked next to a building.",
            "a truck carrying a load of plant on the road."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2417442",
                "VG_object_id": "3066653",
                "bbox": [4, 237, 379, 332],
                "image": "data\\images\\2417442.jpg"
            },
            {
                "VG_image_id": "2320361",
                "VG_object_id": "993872",
                "bbox": [66, 150, 476, 373],
                "image": "data\\images\\2320361.jpg"
            }
        ],
        "questions_with_scores": [["how many urinals are there", 1]],
        "org_questions": [
            ["what color is the floor", -1],
            ["how many urinals are there", 1],
            ["what is on the ground", -1],
            ["what room is the floor in", -1],
            ["What is the pattern of the floor", -1],
            ["what is the floor made of", -1],
            ["How many people are there in the picture", -1],
            ["where are the tiles", -1],
            ["what kind of floor is this", -1],
            ["what room is this", -1],
            ["what is the floor color", -1],
            ["How many people are there", -1],
            ["how many dogs are there on the floor", -1],
            ["what is the color of the wall", -1],
            ["what shape is the floor", -1]
        ],
        "context": [
            "a public restroom with urinals and sinks.",
            "a urinal in a bathroom stall with a toilet paper dispenser."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2326822",
                "VG_object_id": "3144438",
                "bbox": [290, 5, 498, 294],
                "image": "data\\images\\2326822.jpg"
            },
            {
                "VG_image_id": "2356825",
                "VG_object_id": "3246461",
                "bbox": [134, 123, 189, 224],
                "image": "data\\images\\2356825.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["what is the man holding", 2],
            ["what animal is in the photo", 1],
            ["what color is the person's shirt", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 2],
            ["what animal is in the photo", 1],
            ["What is the man wearing on his head", -1],
            ["how many people are there", -1],
            ["what color is the person's shirt", 1],
            ["when was the photo taken", -1],
            ["what is the man holding", 2],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a police officer is standing next to a dog.",
            "a group of people riding on the back of an elephant."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2319452",
                "VG_object_id": "2936363",
                "bbox": [317, 87, 413, 165],
                "image": "data\\images\\2319452.jpg"
            },
            {
                "VG_image_id": "2354149",
                "VG_object_id": "2118046",
                "bbox": [183, 0, 317, 122],
                "image": "data\\images\\2354149.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person playing", 2],
            ["who is wearing the shirt", 1],
            ["how many mans are in the picture", 1],
            ["what is the player wearing", 1],
            ["how many people are in the picture", 1],
            ["what color is the player's shorts", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", 1],
            ["what is the person playing", 2],
            ["how many mans are in the picture", 1],
            ["where is the person", -1],
            ["What is in the background of image", -1],
            ["when was the photo taken", -1],
            ["what is the player wearing", 1],
            ["how many people are in the picture", 1],
            ["what color is the player's shorts", 1]
        ],
        "context": [
            "a woman jumping up to hit a tennis ball with a racket.",
            "a pair of black shorts"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2337914",
                "VG_object_id": "2448067",
                "bbox": [54, 105, 497, 496],
                "image": "data\\images\\2337914.jpg"
            },
            {
                "VG_image_id": "2324936",
                "VG_object_id": "2770374",
                "bbox": [4, 5, 495, 331],
                "image": "data\\images\\2324936.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 1],
            ["what color is the table", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["what kind of food is on the plate", -1],
            ["how many vases are there on the table", -1],
            ["what is the table made of", -1],
            ["what is on the table", -1],
            ["what is the table sitting on", -1],
            ["where was the photo taken", -1],
            ["what is next to the table", -1],
            ["what is in the glass", -1]
        ],
        "context": [
            "a glass of juice and a glass of iced tea.",
            "a plate of food with a glass of wine."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2361225",
                "VG_object_id": "1911084",
                "bbox": [113, 92, 328, 330],
                "image": "data\\images\\2361225.jpg"
            },
            {
                "VG_image_id": "2344664",
                "VG_object_id": "2847626",
                "bbox": [171, 238, 301, 487],
                "image": "data\\images\\2344664.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["how many people are there in the picture", 1],
            ["what sport is the man playing", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is the man wearing", 1],
            ["what gesture is the man", 1],
            ["who is in the photo", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["how many people are there in the picture", 1],
            ["what sport is the man playing", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is the man wearing", 1],
            ["what gesture is the man", 1],
            ["when was the photo taken", -1],
            ["what is behind the man", -1],
            ["who is in the photo", 1],
            ["what is on the man's face", 1]
        ],
        "context": [
            "a man in a blue shirt catching a frisbee.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2330991",
                "VG_object_id": "3264481",
                "bbox": [113, 155, 204, 370],
                "image": "data\\images\\2330991.jpg"
            },
            {
                "VG_image_id": "2414432",
                "VG_object_id": "3503384",
                "bbox": [168, 332, 255, 498],
                "image": "data\\images\\2414432.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the necktie", 2],
            ["what color is the shirt", 2],
            ["who is wearing the necktie", 1],
            ["what pattern is on the tie", 1]
        ],
        "org_questions": [
            ["what color is the necktie", 2],
            ["who is wearing the necktie", 1],
            ["what color is the shirt", 2],
            ["what is the persion wearing on his head", -1],
            ["what is the persion doing", -1],
            ["what pattern is on the tie", 1],
            ["where is the tie", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a woman in a suit and tie posing for a picture.",
            "a man wearing a green shirt and tie smiling."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2392244",
                "VG_object_id": "1232304",
                "bbox": [44, 8, 238, 191],
                "image": "data\\images\\2392244.jpg"
            },
            {
                "VG_image_id": "2378821",
                "VG_object_id": "1364205",
                "bbox": [107, 69, 272, 422],
                "image": "data\\images\\2378821.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child holding", 1],
            ["what color is the child's hair", 1],
            ["what color is the child's shirt", 1]
        ],
        "org_questions": [
            ["what is the child holding", 1],
            ["what color is the child's hair", 1],
            ["what color is the child's shirt", 1],
            ["how many people are there", -1],
            ["What is child doing", -1],
            ["what is the child sitting on", -1],
            ["what are the children doing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a child at a table with a plate of food.",
            "a young boy sitting on a bench eating a hot dog."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2359146",
                "VG_object_id": "3539881",
                "bbox": [280, 63, 401, 329],
                "image": "data\\images\\2359146.jpg"
            },
            {
                "VG_image_id": "2382896",
                "VG_object_id": "2049308",
                "bbox": [305, 55, 470, 315],
                "image": "data\\images\\2382896.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many girls are there", 2],
            ["what are the girls doing", 2],
            ["where are the girls", 1],
            ["what is the girl holding", 1],
            ["when was the photo taken", 1],
            ["what is on the woman's head", 1]
        ],
        "org_questions": [
            ["how many girls are there", 2],
            ["what are the girls doing", 2],
            ["where are the girls", 1],
            ["what is the ground covered with", -1],
            ["what is the girl holding", 1],
            ["when was the photo taken", 1],
            ["what is on the woman's head", 1],
            ["what is the girl wearing", -1]
        ],
        "context": [
            "a woman in a costume is standing next to a horse.",
            "a group of people sitting on a bench looking at their cell phones."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2370585",
                "VG_object_id": "1804114",
                "bbox": [278, 1, 485, 223],
                "image": "data\\images\\2370585.jpg"
            },
            {
                "VG_image_id": "2376987",
                "VG_object_id": "718916",
                "bbox": [42, 82, 109, 225],
                "image": "data\\images\\2376987.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["what color is the ground", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", 1],
            ["what is the persion riding", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["what color is the ground", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", 1],
            ["how is the weather", -1],
            ["what is the persion riding", 1],
            ["when was the picture taken", -1],
            ["where is the bike", -1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "person and his motorcycle are racing.",
            "a group of men walking down a sidewalk."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2354619",
                "VG_object_id": "3266170",
                "bbox": [76, 97, 350, 382],
                "image": "data\\images\\2354619.jpg"
            },
            {
                "VG_image_id": "2370921",
                "VG_object_id": "2135817",
                "bbox": [2, 113, 263, 400],
                "image": "data\\images\\2370921.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is on the front of the train", 1],
            ["what is behind the train", 1]
        ],
        "org_questions": [
            ["what color is the train", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["where is the train", -1],
            ["What is the weather like", -1],
            ["what is the train doing", -1],
            ["when was the picture taken", -1],
            ["what is on the front of the train", 1],
            ["what type of train is this", -1],
            ["what is behind the train", 1]
        ],
        "context": [
            "a steam train on the tracks with a building in the background.",
            "a green train engine"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2415912",
                "VG_object_id": "3487710",
                "bbox": [166, 50, 323, 243],
                "image": "data\\images\\2415912.jpg"
            },
            {
                "VG_image_id": "2373416",
                "VG_object_id": "3322948",
                "bbox": [232, 182, 362, 439],
                "image": "data\\images\\2373416.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's hair", 1],
            ["how many people are there", 1],
            ["what gesture is the man", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man wearing", -1],
            ["what color is the man's hair", 1],
            ["where is the man", -1],
            ["how many people are there", 1],
            ["what is the man doing", 2],
            ["what gesture is the man", 1],
            ["what is the man holding", 1],
            ["what is the man wearing on the head", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a man is standing in front of a glass of wine.",
            "two men sitting on a couch"
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2406055",
                "VG_object_id": "326772",
                "bbox": [3, 306, 327, 497],
                "image": "data\\images\\2406055.jpg"
            },
            {
                "VG_image_id": "2363293",
                "VG_object_id": "1701619",
                "bbox": [0, 277, 496, 332],
                "image": "data\\images\\2363293.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what are the people doing on the beach", 2],
            ["what color is the sand", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["how many birds are there", -1],
            ["what is standing on the beach", -1],
            ["what are the people doing on the beach", 2],
            ["what is in the distance", -1],
            ["where was this photo taken", -1],
            ["where are the waves", -1],
            ["what color is the sand", 1],
            ["what is on the beach", -1]
        ],
        "context": [
            "a man walking on the beach with a surfboard.",
            "a person is parasailing on the beach."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2414257",
                "VG_object_id": "297886",
                "bbox": [183, 25, 297, 370],
                "image": "data\\images\\2414257.jpg"
            },
            {
                "VG_image_id": "2324115",
                "VG_object_id": "988507",
                "bbox": [28, 67, 152, 394],
                "image": "data\\images\\2324115.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 2],
            ["what color are the woman's trousers", 1],
            ["where is the girl staying", 1],
            ["what is the persion holding", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what color are the woman's trousers", 1],
            ["what is the woman doing", 2],
            ["how many girls are there in the picture", -1],
            ["where is the girl staying", 1],
            ["what is the persion holding", 1],
            ["how many people are there standing around the girl", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is the woman wearing", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a woman standing in a kitchen with a stove top oven.",
            "a woman sitting on a bench looking at the water."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2393002",
                "VG_object_id": "1223803",
                "bbox": [315, 261, 499, 342],
                "image": "data\\images\\2393002.jpg"
            },
            {
                "VG_image_id": "2343966",
                "VG_object_id": "3444012",
                "bbox": [3, 38, 499, 352],
                "image": "data\\images\\2343966.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["how many potatoes are there on the table", 1],
            ["How many people are there", 1],
            ["what color are the plates on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["how many potatoes are there on the table", 1],
            ["what shape are the containers under the food", -1],
            ["Where is the table", -1],
            ["what is the table made of", -1],
            ["How many people are there", 1],
            ["what color are the plates on the table", 1],
            ["what is the food sitting on", -1],
            ["where was the picture taken", -1],
            ["what is next to the table", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man and a woman eating food at a restaurant.",
            "a yellow bowl of potatoes and a bowl of potatoes."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2380975",
                "VG_object_id": "1340618",
                "bbox": [0, 224, 500, 374],
                "image": "data\\images\\2380975.jpg"
            },
            {
                "VG_image_id": "2363277",
                "VG_object_id": "2698106",
                "bbox": [38, 189, 494, 242],
                "image": "data\\images\\2363277.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what kind of animal is on the ground", 1],
            ["how many people are there on the ground", 1]
        ],
        "org_questions": [
            ["what kind of animal is on the ground", 1],
            ["how many people are there on the ground", 1],
            ["how many birds are there on the ground", -1],
            ["what is the floor made of", -1],
            ["where is the land", -1],
            ["what is on the land", -1],
            ["What is land made of", -1],
            ["when was the picture taken", -1],
            ["what is the weather like", -1],
            ["what are the animals standing on", -1],
            ["what is covering the ground", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man and two horses are in a field.",
            "a group of zebras walking across a field."
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2368384",
                "VG_object_id": "3482564",
                "bbox": [134, 90, 332, 125],
                "image": "data\\images\\2368384.jpg"
            },
            {
                "VG_image_id": "2353437",
                "VG_object_id": "845950",
                "bbox": [2, 82, 375, 499],
                "image": "data\\images\\2353437.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many vehicles are there on the road", 2],
            ["how many cars are there", 2],
            ["what color are the vehicles", 1],
            ["what is on the vehicle", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many vehicles are there on the road", 2],
            ["what color are the vehicles", 1],
            ["what is on the vehicle", 1],
            ["where is the vehicle", -1],
            ["how is the weather", -1],
            ["what is on the side of the car", -1],
            ["how many people are there in front of the vehicle", -1],
            ["how many cars are there", 2],
            ["when was this picture taken", -1],
            ["where was this photo taken", -1],
            ["what is in the background", 1],
            ["what is in front of the car", -1]
        ],
        "context": [
            "a stuffed bear is on the sidewalk.",
            "a black and white cat laying on top of a jeep."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2355395",
                "VG_object_id": "829992",
                "bbox": [117, 115, 186, 276],
                "image": "data\\images\\2355395.jpg"
            },
            {
                "VG_image_id": "2336092",
                "VG_object_id": "2299507",
                "bbox": [1, 204, 88, 374],
                "image": "data\\images\\2336092.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["when is the photo taken", -1],
            ["what is the person standing on", -1],
            ["what color is the person", -1],
            ["what is the man wearing on his head", -1],
            ["where was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man holding", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man riding a skateboard down a sidewalk.",
            "a group of people riding bikes down a street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2359876",
                "VG_object_id": "3536937",
                "bbox": [13, 272, 497, 371],
                "image": "data\\images\\2359876.jpg"
            },
            {
                "VG_image_id": "2404485",
                "VG_object_id": "1115274",
                "bbox": [2, 222, 499, 308],
                "image": "data\\images\\2404485.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the land", 2],
            ["what is in the distance", 2],
            ["what is the weather like", 1],
            ["what color is the vehicle", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is on the land", 2],
            ["what is the weather like", 1],
            ["what is in the distance", 2],
            ["how many motorcycles are there on the ground", -1],
            ["what pattern is the land", -1],
            ["what color is the vehicle", 1],
            ["when was the picture taken", -1],
            ["what is the ground covered with", 1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a car parked in the snow next to a stop sign.",
            "a green and white bus is driving down the street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2367573",
                "VG_object_id": "2048436",
                "bbox": [294, 50, 379, 156],
                "image": "data\\images\\2367573.jpg"
            },
            {
                "VG_image_id": "2411780",
                "VG_object_id": "358047",
                "bbox": [59, 363, 108, 473],
                "image": "data\\images\\2411780.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 2],
            ["what is the persion standing on", 1],
            ["where is the man", 1],
            ["what is in the background", 1],
            ["what is the man holding", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 2],
            ["what is the persion standing on", 1],
            ["how many people are there", -1],
            ["what is the man wearing on the head", -1],
            ["When is photo taken", -1],
            ["where is the man", 1],
            ["what is in the background", 1],
            ["who is in the picture", -1],
            ["what is the man holding", 1],
            ["what is on the ground", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a boy sits on a tractor in the street.",
            "a man flying a kite on the beach."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2380767",
                "VG_object_id": "3833329",
                "bbox": [148, 95, 313, 331],
                "image": "data\\images\\2380767.jpg"
            },
            {
                "VG_image_id": "2395695",
                "VG_object_id": "1203734",
                "bbox": [0, 21, 86, 162],
                "image": "data\\images\\2395695.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what sport is the man doing", 1],
            ["what is the man holding", 1],
            ["how many people are there in the picture", 1],
            ["what is in the distance", 1],
            ["what gesture is the man", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", -1],
            ["what is the man wearing", -1],
            ["what sport is the man doing", 1],
            ["what color is the background", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man holding", 1],
            ["how many people are there in the picture", 1],
            ["what is in the distance", 1],
            ["what gesture is the man", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a man in a red shirt and a yellow frisbee.",
            "a man holding a tennis racket and ball."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2390514",
                "VG_object_id": "496589",
                "bbox": [227, 226, 393, 327],
                "image": "data\\images\\2390514.jpg"
            },
            {
                "VG_image_id": "2381120",
                "VG_object_id": "707193",
                "bbox": [13, 227, 425, 330],
                "image": "data\\images\\2381120.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the players' shirts", 1],
            ["what color are the players' hats", 1],
            ["what are the people doing on the field", 1],
            ["what is in the background", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what color are the players' shirts", 1],
            ["what color are the players' hats", 1],
            ["what are the people doing on the field", 1],
            ["how many people are there in the picture", -1],
            ["what is in the background", 1],
            ["what plant is in the field", -1],
            ["where was this photo taken", -1],
            ["what is the ground covered with", -1],
            ["what sport is being played", 1],
            ["what is on the ground", -1],
            ["what is green", -1]
        ],
        "context": [
            "a football game is being played on the field.",
            "a group of men standing on top of a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2366870",
                "VG_object_id": "625590",
                "bbox": [259, 182, 331, 273],
                "image": "data\\images\\2366870.jpg"
            },
            {
                "VG_image_id": "2347248",
                "VG_object_id": "892298",
                "bbox": [231, 137, 312, 193],
                "image": "data\\images\\2347248.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many trousers are there in the photo", 1],
            ["where is the trouser", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what color is the trouser", -1],
            ["how many trousers are there in the photo", 1],
            ["where is the trouser", 1],
            ["what time is it", -1],
            ["what is the man doing", -1],
            ["How many people are there", 1],
            ["what color are the man's trousers", -1],
            ["what is on the man's face", -1],
            ["what kind of pants is the man wearing", -1],
            ["when was this photo taken", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "two people on skis posing for a picture.",
            "a man flying through the air while riding a snowboard."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2390462",
                "VG_object_id": "1250018",
                "bbox": [2, 160, 497, 498],
                "image": "data\\images\\2390462.jpg"
            },
            {
                "VG_image_id": "2340451",
                "VG_object_id": "947553",
                "bbox": [191, 100, 461, 228],
                "image": "data\\images\\2340451.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are in the picture", 2],
            ["how many people are there in the picture", 2],
            ["what is the horse doing", 1],
            ["what color are the horses", 1],
            ["what is the color of the horse", 1]
        ],
        "org_questions": [
            ["how many horses are in the picture", 2],
            ["what is the horse doing", 1],
            ["how many people are there in the picture", 2],
            ["what is the ground covered with", -1],
            ["where is the horse", -1],
            ["what color are the horses", 1],
            ["what is the color of the horse", 1],
            ["what type of animal is shown", -1],
            ["who is in the picture", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a horse standing next to a fence with a saddle on its back.",
            "a person riding a horse jumping over a post."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2318586",
                "VG_object_id": "1009350",
                "bbox": [85, 92, 286, 364],
                "image": "data\\images\\2318586.jpg"
            },
            {
                "VG_image_id": "2370884",
                "VG_object_id": "601005",
                "bbox": [53, 137, 251, 230],
                "image": "data\\images\\2370884.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many dogs are there", 1],
            ["What is the dog doing", 1],
            ["What color is the ground", 1],
            ["where is the dog", 1],
            ["what is in the distance", 1],
            ["what is the dog standing on", 1]
        ],
        "org_questions": [
            ["How many dogs are there", 1],
            ["What is the dog doing", 1],
            ["What color is the ground", 1],
            ["what is on the dog's neck", -1],
            ["where is the dog", 1],
            ["what is in the distance", 1],
            ["what is the dog wearing", -1],
            ["What is dog looking", -1],
            ["when was the photo taken", -1],
            ["what animal is in the picture", -1],
            ["what is the dog standing on", 1],
            ["what is the dog looking at", -1]
        ],
        "context": [
            "a woman holding a frisbee while standing next to a dog.",
            "two dogs laying next to a bus"
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2371985",
                "VG_object_id": "1697719",
                "bbox": [282, 95, 359, 239],
                "image": "data\\images\\2371985.jpg"
            },
            {
                "VG_image_id": "2341628",
                "VG_object_id": "2070486",
                "bbox": [101, 74, 219, 244],
                "image": "data\\images\\2341628.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["how many people are there", 2],
            ["what is the man riding", 1],
            ["where is the photo taken", 1],
            ["when was the picture taken", 1],
            ["what are the men doing", 1],
            ["what is on the man's head", 1],
            ["what is the man sitting on", 1]
        ],
        "org_questions": [
            ["what is the man riding", 1],
            ["what color is the ground", 2],
            ["how many people are there", 2],
            ["where is the photo taken", 1],
            ["what is the man wearing", -1],
            ["when was the picture taken", 1],
            ["who is in the picture", -1],
            ["what are the men doing", 1],
            ["what is on the man's head", 1],
            ["what is the man sitting on", 1]
        ],
        "context": [
            "a man standing on the sidewalk holding a stop sign.",
            "a man riding a horse in a field with people watching."
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2375312",
                "VG_object_id": "3693417",
                "bbox": [1, 196, 499, 372],
                "image": "data\\images\\2375312.jpg"
            },
            {
                "VG_image_id": "2415366",
                "VG_object_id": "3308202",
                "bbox": [0, 90, 498, 370],
                "image": "data\\images\\2415366.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the desk", 2],
            ["what color is the table", 2],
            ["what color is the screen", 2]
        ],
        "org_questions": [
            ["what is the color of the desk", 2],
            ["what is the desk made of", -1],
            ["what is the color of the screen on the desk", -1],
            ["how many monitors are there", -1],
            ["How many screens are there on the desk", -1],
            ["what color is the table", 2],
            ["where was this photo taken", -1],
            ["what is sitting on the desk", -1],
            ["what is the computer sitting on", -1],
            ["where is the computer", -1],
            ["what color is the screen", 2]
        ],
        "context": [
            "a computer monitor and keyboard on a desk.",
            "a computer monitor with snowflakes on it."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2390818",
                "VG_object_id": "493860",
                "bbox": [9, 52, 493, 264],
                "image": "data\\images\\2390818.jpg"
            },
            {
                "VG_image_id": "2360275",
                "VG_object_id": "1955911",
                "bbox": [12, 37, 267, 495],
                "image": "data\\images\\2360275.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many zebras are there", 2],
            ["What is zebra doing", 1],
            ["what is the main color of the grass", 1]
        ],
        "org_questions": [
            ["How many zebras are there", 2],
            ["What color is photo", -1],
            ["where is the zebras", -1],
            ["What is zebra doing", 1],
            ["what is in the distance", -1],
            ["what is the main color of the grass", 1],
            ["when was this picture taken", -1],
            ["what type of animal is shown", -1],
            ["where was this picture taken", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "three zebras are eating hay in a field.",
            "a close up of a zebra's head with a tree in the background."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2324913",
                "VG_object_id": "3373247",
                "bbox": [213, 70, 494, 301],
                "image": "data\\images\\2324913.jpg"
            },
            {
                "VG_image_id": "2344070",
                "VG_object_id": "2337775",
                "bbox": [36, 178, 191, 322],
                "image": "data\\images\\2344070.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what color is the bowl", 2],
            ["what color is the food", 1],
            ["what is on the left side of the plate", 1]
        ],
        "org_questions": [
            ["what color is the food", 1],
            ["what color is the table", 2],
            ["what color is the bowl", 2],
            ["what shape is plate the food placed on", -1],
            ["How many plates are there", -1],
            ["where is the food on", -1],
            ["what is the table made of", -1],
            ["what is the food in the middle of the plate", -1],
            ["what is on the left side of the plate", 1],
            ["what type of food is on the plate", -1],
            ["what is the food on", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a plate of food with meat and vegetables.",
            "a plate of food on a table"
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2333917",
                "VG_object_id": "3371174",
                "bbox": [238, 26, 279, 91],
                "image": "data\\images\\2333917.jpg"
            },
            {
                "VG_image_id": "2323224",
                "VG_object_id": "3155963",
                "bbox": [254, 54, 396, 312],
                "image": "data\\images\\2323224.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is photo taken", 2],
            ["what color is the person's shirt", 2],
            ["What is person holding", 1],
            ["what gesture is the person", 1],
            ["what is the person wearing", 1],
            ["who is in the photo", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What is person holding", 1],
            ["Where is photo taken", 2],
            ["what animal is next to the person", -1],
            ["what color is the person's shirt", 2],
            ["what gesture is the person", 1],
            ["what is the person wearing", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", 1],
            ["what is the persion doing", 1]
        ],
        "context": [
            "two children are sitting next to a tree and holding tennis rackets.",
            "a man riding on the back of a motorcycle in a field."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2338234",
                "VG_object_id": "954861",
                "bbox": [104, 0, 500, 342],
                "image": "data\\images\\2338234.jpg"
            },
            {
                "VG_image_id": "2375506",
                "VG_object_id": "722034",
                "bbox": [2, 85, 87, 193],
                "image": "data\\images\\2375506.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 2],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what color is the car", 2],
            ["what is the ground covered with", 1],
            ["how many buses are there in the picture", -1],
            ["what is the weather like", -1],
            ["what is in the distance", 1],
            ["when was this picture taken", -1],
            ["what type of vehicle is shown", -1],
            ["what is on the car", -1]
        ],
        "context": [
            "a young boy poses with a car in front of the race car.",
            "a group of cows sitting in a dirt field."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2399964",
                "VG_object_id": "1164310",
                "bbox": [53, 176, 278, 311],
                "image": "data\\images\\2399964.jpg"
            },
            {
                "VG_image_id": "2354597",
                "VG_object_id": "2081552",
                "bbox": [7, 114, 286, 499],
                "image": "data\\images\\2354597.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is next to the bench", 1],
            ["what is behind the bench", 1],
            ["what is in front of the bench", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 1],
            ["what color is the ground", 2],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["what is in the distance", -1],
            ["what  is on the bench", -1],
            ["Where is the bench", -1],
            ["what is the bench made of", -1],
            ["when was the photo taken", -1],
            ["what is next to the bench", 1],
            ["what is behind the bench", 1],
            ["what is in front of the bench", 1]
        ],
        "context": [
            "a bench on a sidewalk near a body of water.",
            "a bench in a garden of a house."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2394285",
                "VG_object_id": "465548",
                "bbox": [8, 6, 331, 438],
                "image": "data\\images\\2394285.jpg"
            },
            {
                "VG_image_id": "2404606",
                "VG_object_id": "340411",
                "bbox": [50, 36, 313, 465],
                "image": "data\\images\\2404606.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants are in the picture", 1],
            ["what is the elephant doing", 1],
            ["when was this photo taken", 1]
        ],
        "org_questions": [
            ["how many elephants are in the picture", 1],
            ["where is the elephant", -1],
            ["what is the elephant doing", 1],
            ["what color is the elephants", -1],
            ["what is on the elephant", -1],
            ["what is in front of the elephant", -1],
            ["what are the elephants standing on", -1],
            ["when was this photo taken", 1],
            ["what kind of animal is this", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a baby elephant standing next to an adult elephant.",
            "a man sitting on a chair next to an elephant."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2344150",
                "VG_object_id": "917523",
                "bbox": [233, 154, 374, 196],
                "image": "data\\images\\2344150.jpg"
            },
            {
                "VG_image_id": "2410307",
                "VG_object_id": "220878",
                "bbox": [230, 291, 302, 331],
                "image": "data\\images\\2410307.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bag", 1],
            ["where is the bag", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the bag", 1],
            ["where is the bag", 1],
            ["how many bags are in the picture", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a man standing in a bathroom next to a sink.",
            "a room with a desk, chair, and a television."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2407832",
                "VG_object_id": "272803",
                "bbox": [59, 1, 452, 330],
                "image": "data\\images\\2407832.jpg"
            },
            {
                "VG_image_id": "2396774",
                "VG_object_id": "1195218",
                "bbox": [181, 124, 398, 334],
                "image": "data\\images\\2396774.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what is the food", 1],
            ["what kind of food is it", 1],
            ["what color is the food in the plate", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what is the food", 1],
            ["how many plates are there", -1],
            ["when is the photo taken", -1],
            ["what kind of food is it", 1],
            ["What is on the plate", -1],
            ["what color is the food in the plate", 1],
            ["what shape is the plate", -1],
            ["where is the plate", -1],
            ["what is the plate sitting on", -1],
            ["what is next to the plate", 1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a plate of food with a carrot and rice.",
            "a banana and nuts on a plate with a cup of coffee."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2370434",
                "VG_object_id": "604184",
                "bbox": [296, 138, 407, 241],
                "image": "data\\images\\2370434.jpg"
            },
            {
                "VG_image_id": "2417452",
                "VG_object_id": "3106470",
                "bbox": [134, 31, 398, 238],
                "image": "data\\images\\2417452.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["What is man doing", 2],
            ["where is the man", 1],
            ["what is the man riding on", 1],
            ["what is the man holding", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", 2],
            ["What is man doing", 2],
            ["how many people are there", -1],
            ["what is the floor made of", -1],
            ["where is the man", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the man riding on", 1],
            ["what is the man holding", 1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "two elephants carrying people on their backs.",
            "a man riding a skateboard up the side of a ramp."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2358542",
                "VG_object_id": "801542",
                "bbox": [248, 317, 498, 496],
                "image": "data\\images\\2358542.jpg"
            },
            {
                "VG_image_id": "2409709",
                "VG_object_id": "235568",
                "bbox": [19, 38, 498, 298],
                "image": "data\\images\\2409709.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cats are there on the bed", 2],
            ["what color is the blanket", 1],
            ["what is under the blanket", 1],
            ["how many people are in the photo", 1],
            ["what is the main color of the blanket", 1]
        ],
        "org_questions": [
            ["what color is the blanket", 1],
            ["what is under the blanket", 1],
            ["how many cats are there on the bed", 2],
            ["what is on the blanket", -1],
            ["how many people are in the photo", 1],
            ["what is the main color of the blanket", 1],
            ["where is the blanket", -1]
        ],
        "context": [
            "a child sleeping on a bed with stuffed animals.",
            "a black cat laying on a couch with a knitted hat on it."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2360821",
                "VG_object_id": "785735",
                "bbox": [272, 40, 378, 132],
                "image": "data\\images\\2360821.jpg"
            },
            {
                "VG_image_id": "2335461",
                "VG_object_id": "962835",
                "bbox": [267, 218, 442, 332],
                "image": "data\\images\\2335461.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 1],
            ["what is the person wearing on the head", 1],
            ["what is the persion doing", 1],
            ["What is ground made of", 1],
            ["what color is the ground", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["where is the photo taken", -1],
            ["what is the person wearing on the head", 1],
            ["how many people are there", -1],
            ["what is the persion doing", 1],
            ["What is ground made of", 1],
            ["what is the persion holding", -1],
            ["when was the picture taken", -1],
            ["what is the persion wearing", -1],
            ["what color is the ground", 1]
        ],
        "context": [
            "a man doing a trick on a skateboard.",
            "a woman is feeding a giraffe at the zoo."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2403397",
                "VG_object_id": "353525",
                "bbox": [317, 89, 432, 332],
                "image": "data\\images\\2403397.jpg"
            },
            {
                "VG_image_id": "2385223",
                "VG_object_id": "1299410",
                "bbox": [282, 139, 347, 312],
                "image": "data\\images\\2385223.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing", 2],
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["how many people are there", 1],
            ["where are the people", 1],
            ["what are the people wearing", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", 2],
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["how many people are there", 1],
            ["what color is the background", -1],
            ["what gesture is the woman", -1],
            ["what is the woman holding", -1],
            ["what is on the woman's head", -1],
            ["when was the photo taken", -1],
            ["where are the people", 1],
            ["what are the people wearing", 1]
        ],
        "context": [
            "a man in a tie is holding a group of people.",
            "three people standing on a ski slope with mountains in the background."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "713424",
                "VG_object_id": "1583726",
                "bbox": [704, 576, 824, 740],
                "image": "data\\images\\713424.jpg"
            },
            {
                "VG_image_id": "2416781",
                "VG_object_id": "1057176",
                "bbox": [2, 323, 96, 499],
                "image": "data\\images\\2416781.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the photo taken", 2],
            ["What color is bag", 1],
            ["what is the persion holding", 1],
            ["where is the bag putting", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["What color is bag", 1],
            ["Where is the photo taken", 2],
            ["how many people are there", -1],
            ["what is the persion holding", 1],
            ["what is in front of the bag", -1],
            ["where is the bag putting", 1],
            ["who is in the photo", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a man standing on a sidewalk in front of a store.",
            "a cat is standing on a kitchen counter."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2343626",
                "VG_object_id": "2347076",
                "bbox": [38, 9, 482, 473],
                "image": "data\\images\\2343626.jpg"
            },
            {
                "VG_image_id": "2345638",
                "VG_object_id": "906180",
                "bbox": [129, 100, 402, 336],
                "image": "data\\images\\2345638.jpg"
            }
        ],
        "questions_with_scores": [["How many giraffes are there", 1]],
        "org_questions": [
            ["what color is the grass", -1],
            ["what color is the background", -1],
            ["How many giraffes are there", 1],
            ["what is the giraffe standing on", -1],
            ["what is in the distance", -1],
            ["what is the ground covered with", -1],
            ["what is behind the giraffe", -1],
            ["when was the photo taken", -1],
            ["what type of animal is shown", -1],
            ["where was this photo taken", -1],
            ["what are the giraffes doing", -1],
            ["where are the giraffes standing", -1]
        ],
        "context": [
            "a group of giraffes standing in a field.",
            "two giraffes are standing on the grass together."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2358933",
                "VG_object_id": "797717",
                "bbox": [43, 323, 401, 498],
                "image": "data\\images\\2358933.jpg"
            },
            {
                "VG_image_id": "2360375",
                "VG_object_id": "788321",
                "bbox": [3, 147, 499, 361],
                "image": "data\\images\\2360375.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many animals are there", 1],
            ["what kind of animal is it", 1],
            ["what animals are there", 1],
            ["what animal is on the land", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["how many animals are there", 1],
            ["what kind of animal is it", 1],
            ["Whatis the background of image", -1],
            ["What is on the land", -1],
            ["what is the weather like", -1],
            ["what animals are there", 1],
            ["what animal is on the land", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a zebra standing in a fenced in area.",
            "a herd of cattle grazing in a field of tall grass."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2343903",
                "VG_object_id": "2361519",
                "bbox": [34, 289, 80, 442],
                "image": "data\\images\\2343903.jpg"
            },
            {
                "VG_image_id": "2348680",
                "VG_object_id": "880624",
                "bbox": [346, 96, 438, 329],
                "image": "data\\images\\2348680.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what color is the background", -1],
            ["what time is it", -1],
            ["what is the man wearing on his head", -1],
            ["what is the man doing", -1],
            ["What is weather like", -1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["when was this photo taken", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man and woman flying a kite on a beach.",
            "a woman with paint on her body holding an umbrella."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2404646",
                "VG_object_id": "339991",
                "bbox": [119, 121, 374, 442],
                "image": "data\\images\\2404646.jpg"
            },
            {
                "VG_image_id": "2340673",
                "VG_object_id": "3482639",
                "bbox": [194, 0, 361, 326],
                "image": "data\\images\\2340673.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["who is playing", 1]
        ],
        "org_questions": [
            ["what color is the player's shirt", -1],
            ["what color is the ground the player standing on", -1],
            ["what is the player wearing", -1],
            ["how many people are there", 1],
            ["what is the gender of the person", -1],
            ["what is the man doing", -1],
            ["what is the player holding", -1],
            ["what kind of sports is the player doing", -1],
            ["when was the photo taken", -1],
            ["who is playing", 1],
            ["where is the man", -1],
            ["what sport is being played", -1],
            ["what color is the player's pants", -1],
            ["What is the person holding", -1],
            ["which two color is the field", -1]
        ],
        "context": [
            "a young boy hitting a tennis ball with a racquet.",
            "a man jumping up to hit a tennis ball."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2355736",
                "VG_object_id": "826949",
                "bbox": [67, 19, 139, 276],
                "image": "data\\images\\2355736.jpg"
            },
            {
                "VG_image_id": "2320530",
                "VG_object_id": "3582941",
                "bbox": [132, 55, 282, 191],
                "image": "data\\images\\2320530.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["where is the man", 1],
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["where is the man", 1],
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", 1],
            ["how long is the man's hair", -1],
            ["what is the ground covered with", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what is the persion holding", -1],
            ["what is the man wearing on the head", -1]
        ],
        "context": [
            "a young boy is playing baseball on a field.",
            "a man riding a skateboard up the side of a metal fence."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2317850",
                "VG_object_id": "3180396",
                "bbox": [212, 47, 392, 270],
                "image": "data\\images\\2317850.jpg"
            },
            {
                "VG_image_id": "2380398",
                "VG_object_id": "545685",
                "bbox": [263, 104, 359, 294],
                "image": "data\\images\\2380398.jpg"
            }
        ],
        "questions_with_scores": [["what color is the player's shirt", 1]],
        "org_questions": [
            ["what color is the player's shirt", 1],
            ["what is the player doing", -1],
            ["what is the player wearing", -1],
            ["how many people are there", -1],
            ["how many players are there in the photo", -1],
            ["What color is the ground", -1],
            ["what color is the court", -1],
            ["what color is the man's shirt", -1],
            ["when was the photo taken", -1],
            ["what game is being played", -1],
            ["who is in the picture", -1],
            ["what is the number of people", -1]
        ],
        "context": [
            "a player jumps to get the ball in the air.",
            "two baseball players are shaking hands on the field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2350012",
                "VG_object_id": "2142453",
                "bbox": [132, 174, 270, 484],
                "image": "data\\images\\2350012.jpg"
            },
            {
                "VG_image_id": "2353788",
                "VG_object_id": "3576887",
                "bbox": [104, 188, 211, 376],
                "image": "data\\images\\2353788.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["what color is the woman's clothes", 1],
            ["what is in the background", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what color is the woman's clothes", 1],
            ["where is the woman", -1],
            ["how many people are there", -1],
            ["what is the woman wearing on head", -1],
            ["what is in the background", 1],
            ["what is the woman holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman standing on", -1],
            ["what is the persion on the left wearing", -1]
        ],
        "context": [
            "a woman standing next to a giant teddy bear.",
            "a girl walking in a line of buses."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2331009",
                "VG_object_id": "2830636",
                "bbox": [96, 62, 353, 327],
                "image": "data\\images\\2331009.jpg"
            },
            {
                "VG_image_id": "2375449",
                "VG_object_id": "2523476",
                "bbox": [157, 129, 301, 315],
                "image": "data\\images\\2375449.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the baby sitting on", 2],
            ["Where is the girl", 1],
            ["what is the girl holding", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what is the baby sitting on", 2],
            ["what is the girl doing", -1],
            ["Where is the girl", 1],
            ["what is the girl holding", 1],
            ["who is in the photo", -1],
            ["how many children are in the photo", -1],
            ["what are the children doing", -1],
            ["what is the little girl doing", -1]
        ],
        "context": [
            "a child sitting in a high chair eating cake.",
            "a man sitting on a bed holding a baby and playing a video game."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2365434",
                "VG_object_id": "634149",
                "bbox": [1, 234, 370, 499],
                "image": "data\\images\\2365434.jpg"
            },
            {
                "VG_image_id": "2358067",
                "VG_object_id": "1870430",
                "bbox": [20, 246, 392, 372],
                "image": "data\\images\\2358067.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the food on", 2],
            ["how many people are in the picture", 2],
            ["what color is the food", 1],
            ["what type of food is this", 1],
            ["what is white", 1],
            ["what is covering the table", 1]
        ],
        "org_questions": [
            ["How many forks are there", -1],
            ["what color is the food", 1],
            ["what type of food is this", 1],
            ["what is the food on", 2],
            ["what is white", 1],
            ["what is covering the table", 1],
            ["how many people are in the picture", 2]
        ],
        "context": [
            "a woman sitting at a table with a plate of food.",
            "a bunch of vegetables sitting on top of a table."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2384458",
                "VG_object_id": "691690",
                "bbox": [8, 175, 498, 372],
                "image": "data\\images\\2384458.jpg"
            },
            {
                "VG_image_id": "2387437",
                "VG_object_id": "513321",
                "bbox": [47, 320, 500, 371],
                "image": "data\\images\\2387437.jpg"
            }
        ],
        "questions_with_scores": [
            ["When is photo taken", 2],
            ["What is the weather like", 2],
            ["WHat is on the street", 1],
            ["what color is the road", 1],
            ["when was this picture taken", 1]
        ],
        "org_questions": [
            ["When is photo taken", 2],
            ["What is the weather like", 2],
            ["WHat is on the street", 1],
            ["what color is the road", 1],
            ["what is in the background", -1],
            ["how many people are on the street", -1],
            ["where was this picture taken", -1],
            ["what is the road made of", -1],
            ["where is the road", -1],
            ["when was this picture taken", 1]
        ],
        "context": [
            "a person walking down a wet street at night.",
            "a parking meter with a beer on it."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2390386",
                "VG_object_id": "498006",
                "bbox": [115, 65, 210, 173],
                "image": "data\\images\\2390386.jpg"
            },
            {
                "VG_image_id": "2386907",
                "VG_object_id": "680176",
                "bbox": [151, 61, 203, 101],
                "image": "data\\images\\2386907.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the color of the shirt", 1],
            ["what is the man on", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is behind the person", 1]
        ],
        "org_questions": [
            ["what is the color of the shirt", 1],
            ["what is the man doing", 2],
            ["what is the man on", 1],
            ["how many people are there", 1],
            ["What is the person wearing on the head", -1],
            ["where is the man", 1],
            ["what is behind the person", 1],
            ["when was the photo taken", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man sitting on the ground next to a bunch of bananas.",
            "a man herding sheep with a dog."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2402537",
                "VG_object_id": "388765",
                "bbox": [367, 79, 432, 150],
                "image": "data\\images\\2402537.jpg"
            },
            {
                "VG_image_id": "2368399",
                "VG_object_id": "2624268",
                "bbox": [196, 163, 258, 225],
                "image": "data\\images\\2368399.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the clock dial", 1],
            ["what time is it on the clock", 1],
            ["where is the picture taken", 1],
            ["what kind of symbol is showed on the clock", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the clock dial", 1],
            ["what time is it on the clock", 1],
            ["where is the picture taken", 1],
            ["how many clocks are in the picture", -1],
            ["what shape is the clock", -1],
            ["what is the clock made of", -1],
            ["what kind of symbol is showed on the clock", 1],
            ["when is this picture taken", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a desk with two monitors and a mouse",
            "a building with a clock on the side of it"
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2341886",
                "VG_object_id": "2311525",
                "bbox": [210, 77, 292, 281],
                "image": "data\\images\\2341886.jpg"
            },
            {
                "VG_image_id": "2369377",
                "VG_object_id": "2220025",
                "bbox": [418, 13, 492, 127],
                "image": "data\\images\\2369377.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport are the people playing", 2],
            ["what color is the boy's clothes", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what is the boy wearing on his head", 1],
            ["What is man doing", 1],
            ["what is the boy holding", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what color is the boy's clothes", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what is the boy wearing on his head", 1],
            ["What is man doing", 1],
            ["what is the weather like", -1],
            ["what is the boy holding", 1],
            ["when was the photo taken", -1],
            ["what kind of pants is the person wearing", -1],
            ["what is the person wearing", 1],
            ["what sport are the people playing", 2]
        ],
        "context": [
            "a family on skis posing for a picture.",
            "a person is skateboarding on the steps."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2389383",
                "VG_object_id": "1261435",
                "bbox": [235, 171, 321, 222],
                "image": "data\\images\\2389383.jpg"
            },
            {
                "VG_image_id": "2319965",
                "VG_object_id": "3233977",
                "bbox": [381, 243, 436, 311],
                "image": "data\\images\\2319965.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trouser", 2],
            ["where is the man ", 2],
            ["what color is the man's jacket", 1],
            ["how many people are there", 1],
            ["what is on the man's head", 1],
            ["what is the person holding", 1],
            ["what kind of pants is the man wearing", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 2],
            ["where is the man ", 2],
            ["what color is the man's jacket", 1],
            ["how many people are there", 1],
            ["what is on the man's head", 1],
            ["who is wearing the trousers", -1],
            ["what is the person holding", 1],
            ["when was the photo taken", -1],
            ["what kind of pants is the man wearing", 1],
            ["what is the person wearing", -1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a person skiing down a snowy hill with trees in the background.",
            "a group of men walking across a tarmac."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2417689",
                "VG_object_id": "3457650",
                "bbox": [202, 130, 309, 226],
                "image": "data\\images\\2417689.jpg"
            },
            {
                "VG_image_id": "2388228",
                "VG_object_id": "674707",
                "bbox": [86, 217, 197, 289],
                "image": "data\\images\\2388228.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's trouser", 2],
            ["What color is the ground", 2],
            ["How many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["what color are the trousers", 1],
            ["where is the man", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is man's trouser", 2],
            ["What color is the ground", 2],
            ["what is behind the trousers", -1],
            ["who is wearing the trousers", -1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["what color are the trousers", 1],
            ["when was the picture taken", -1],
            ["what is on the man's feet", -1],
            ["where is the man", 1],
            ["what is the person wearing", 1]
        ],
        "context": [
            "a baseball player sliding into a base while another player tries to tag him out.",
            "a man jumping in the air on a skateboard."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2359950",
                "VG_object_id": "2636428",
                "bbox": [4, 245, 113, 366],
                "image": "data\\images\\2359950.jpg"
            },
            {
                "VG_image_id": "2332212",
                "VG_object_id": "970150",
                "bbox": [28, 208, 259, 329],
                "image": "data\\images\\2332212.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bicycles are there", 2],
            ["what is on the bicycle", 1],
            ["what is the bicycle doing", 1],
            ["What is the background of image", 1],
            ["where was the photo taken", 1],
            ["what is next to the bike", 1]
        ],
        "org_questions": [
            ["how many bicycles are there", 2],
            ["what is on the bicycle", 1],
            ["what is the ground the bicycle on made of", -1],
            ["when is this picture taken", -1],
            ["what color is the bicycle", -1],
            ["where is the bicycle", -1],
            ["what is the bicycle doing", 1],
            ["What is the background of image", 1],
            ["where was the photo taken", 1],
            ["what is next to the bike", 1],
            ["what is the bike color", -1],
            ["what color is the grass", -1]
        ],
        "context": [
            "a man riding a bike down a road next to a lake.",
            "a row of bikes parked next to a park."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2403262",
                "VG_object_id": "3815921",
                "bbox": [77, 1, 198, 313],
                "image": "data\\images\\2403262.jpg"
            },
            {
                "VG_image_id": "2372854",
                "VG_object_id": "3730870",
                "bbox": [186, 335, 236, 468],
                "image": "data\\images\\2372854.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's pants", 1],
            ["what is the boy doing", 1],
            ["what is the boy holding", 1],
            ["Where is the man", 1],
            ["what is in the background", 1],
            ["What is man doing", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the boy's pants", 1],
            ["what is the boy doing", 1],
            ["what is the boy holding", 1],
            ["What is the boy wearing on his head", -1],
            ["Where is the man", 1],
            ["what is in the background", 1],
            ["What is man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", -1],
            ["how many people are there", 1]
        ],
        "context": [
            "a young man riding a skateboard down a street.",
            "a man and a child holding a kite in front of flags."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2323736",
                "VG_object_id": "3473040",
                "bbox": [16, 0, 496, 332],
                "image": "data\\images\\2323736.jpg"
            },
            {
                "VG_image_id": "2361830",
                "VG_object_id": "779086",
                "bbox": [343, 61, 499, 185],
                "image": "data\\images\\2361830.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many giraffes are there in the picture", 1],
            ["what is in front of the building", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["What color is the building", -1],
            ["How many giraffes are there in the picture", 1],
            ["what time is it", -1],
            ["What is behind the building", -1],
            ["What is the weather like", -1],
            ["what is in front of the building", 1],
            ["how many people are there", -1],
            ["where was the photo taken", 1],
            ["what is the building made of", -1],
            ["where are the trees", -1],
            ["when was the photo taken", -1],
            ["what is on the building", -1]
        ],
        "context": [
            "two giraffes standing next to each other in front of a building.",
            "a street sign in a garden with a bench in the background."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2407798",
                "VG_object_id": "1095932",
                "bbox": [98, 32, 222, 282],
                "image": "data\\images\\2407798.jpg"
            },
            {
                "VG_image_id": "2412196",
                "VG_object_id": "200774",
                "bbox": [39, 183, 117, 301],
                "image": "data\\images\\2412196.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what is the woman doing", 1],
            ["how many women are there", 1],
            ["how many people are there in the photo", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what is the woman holding", 2],
            ["how many women are there", 1],
            ["what color is the background", -1],
            ["what gesture is the woman", -1],
            ["what is the woman wearing", -1],
            ["how many people are there in the photo", 1],
            ["What color is the background of image", -1],
            ["when was the picture taken", -1],
            ["what is the woman walking on", -1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a woman standing on a sidewalk while using her cell phone.",
            "a woman walking down a street with a blue umbrella."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2394588",
                "VG_object_id": "1211363",
                "bbox": [4, 217, 324, 499],
                "image": "data\\images\\2394588.jpg"
            },
            {
                "VG_image_id": "2375941",
                "VG_object_id": "1983658",
                "bbox": [4, 241, 499, 321],
                "image": "data\\images\\2375941.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["what animal is on the ground", 1],
            ["what is the floor made of", 1],
            ["What kind of animals are they", 1],
            ["where was the photo taken", 1],
            ["what is in the foreground", 1]
        ],
        "org_questions": [
            ["what is the color of the ground", -1],
            ["what is on the ground", 1],
            ["how many people are there in the picture", -1],
            ["what animal is on the ground", 1],
            ["what is the floor made of", 1],
            ["What kind of animals are they", 1],
            ["where was the photo taken", 1],
            ["how is the weather", -1],
            ["what is the weather like", -1],
            ["what is in the foreground", 1]
        ],
        "context": [
            "a polar bear walking on a concrete surface.",
            "two sheep are sitting in a field of grass."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2409090",
                "VG_object_id": "249197",
                "bbox": [0, 133, 79, 375],
                "image": "data\\images\\2409090.jpg"
            },
            {
                "VG_image_id": "2414519",
                "VG_object_id": "156536",
                "bbox": [99, 35, 258, 490],
                "image": "data\\images\\2414519.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["what is the woman doing", 1],
            ["how many people are there", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's coat", -1],
            ["what is the woman's posture", -1],
            ["what is the woman holding", 1],
            ["where is the photo taken", -1],
            ["what is the weather like", -1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", -1],
            ["who is in the picture", -1],
            ["when was the picture taken", -1],
            ["how many people are there", 1],
            ["what color is the woman's hair", -1],
            ["How many people are there", 1],
            ["where is the woman", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man in a black suit and tie is standing on the sidewalk.",
            "a woman in a black coat talking on a cell phone."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2328955",
                "VG_object_id": "2819544",
                "bbox": [30, 48, 472, 285],
                "image": "data\\images\\2328955.jpg"
            },
            {
                "VG_image_id": "2353788",
                "VG_object_id": "2036903",
                "bbox": [2, 1, 322, 374],
                "image": "data\\images\\2353788.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many buses are there", 2],
            ["how many people are there", 2],
            ["what color is the bus", 1]
        ],
        "org_questions": [
            ["how many buses are there", 2],
            ["how many people are there", 2],
            ["what color is the bus", 1],
            ["When is photo taken", -1],
            ["what is on the ground", -1],
            ["where is the bus", -1],
            ["how is the weather", -1],
            ["what are the people doing", -1],
            ["what kind of bus is this", -1],
            ["what is behind the bus", -1],
            ["where was this photo taken", -1],
            ["what is on the side of the bus", -1]
        ],
        "context": [
            "a bus is parked on the side of the road.",
            "a girl walking in a line of buses."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2341421",
                "VG_object_id": "2836875",
                "bbox": [20, 132, 493, 330],
                "image": "data\\images\\2341421.jpg"
            },
            {
                "VG_image_id": "2396949",
                "VG_object_id": "438995",
                "bbox": [251, 290, 494, 375],
                "image": "data\\images\\2396949.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many sinks are in the picture", 1],
            ["what is placed on the sink", 1]
        ],
        "org_questions": [
            ["how many sinks are in the picture", 1],
            ["what is on the counter", -1],
            ["what color is the wall", -1],
            ["what is the sink made of", -1],
            ["what is the sink on", -1],
            ["how many people are in the picture", -1],
            ["what is placed on the sink", 1],
            ["what color is the wall behind the sink", -1],
            ["what shape is the sink", -1],
            ["what room is this", -1],
            ["where is the sink", -1],
            ["what is next to the sink", -1]
        ],
        "context": [
            "a bathroom with two sinks and a mirror.",
            "a bathroom with a sink, mirror and toilet."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2354894",
                "VG_object_id": "2332557",
                "bbox": [60, 107, 184, 356],
                "image": "data\\images\\2354894.jpg"
            },
            {
                "VG_image_id": "2360855",
                "VG_object_id": "1862529",
                "bbox": [111, 34, 405, 269],
                "image": "data\\images\\2360855.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["where is the photo taken", -1],
            ["what is the girl holding", -1],
            ["where is the girl", -1],
            ["who is in the photo", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a little girl jumping on a bed with a white sheet.",
            "two women jumping on a bed"
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2364733",
                "VG_object_id": "2482739",
                "bbox": [310, 200, 499, 365],
                "image": "data\\images\\2364733.jpg"
            },
            {
                "VG_image_id": "2405843",
                "VG_object_id": "371582",
                "bbox": [402, 273, 453, 373],
                "image": "data\\images\\2405843.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 1],
            ["what is the floor under the chair made of", 1],
            ["what color is the floor under the chair", 1],
            ["what is the floor made of", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["what is the floor under the chair made of", 1],
            ["what color is the floor under the chair", 1],
            ["how many sofa are in the picture", -1],
            ["what is the floor made of", 1],
            ["where is the chair", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a young boy holding a frisbee in a toy store.",
            "a woman playing a video game with a remote."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2359078",
                "VG_object_id": "796350",
                "bbox": [188, 257, 252, 357],
                "image": "data\\images\\2359078.jpg"
            },
            {
                "VG_image_id": "2350972",
                "VG_object_id": "863948",
                "bbox": [118, 166, 237, 280],
                "image": "data\\images\\2350972.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many towels are there", 2],
            ["what color is the background", 1],
            ["what color is the wall", 1]
        ],
        "org_questions": [
            ["how many towels are there", 2],
            ["where is the towel", -1],
            ["what color is the background", 1],
            ["what color is the wall", 1],
            ["what room is this", -1],
            ["who is in the photo", -1],
            ["what is on the wall", -1],
            ["what is hanging on the wall", -1]
        ],
        "context": [
            "a bathroom with a glass shower and a sink.",
            "a bathroom with a sink, mirror, and a mirror."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2369146",
                "VG_object_id": "1876172",
                "bbox": [3, 184, 498, 372],
                "image": "data\\images\\2369146.jpg"
            },
            {
                "VG_image_id": "2397453",
                "VG_object_id": "434032",
                "bbox": [7, 215, 500, 370],
                "image": "data\\images\\2397453.jpg"
            }
        ],
        "questions_with_scores": [
            ["When is photo taken", 2],
            ["What is the weather like", 2],
            ["what time is it", 1],
            ["when is the picture taken", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["When is photo taken", 2],
            ["What is the weather like", 2],
            ["how many cars are there", -1],
            ["what time is it", 1],
            ["when is the picture taken", 1],
            ["how many people are there in the picture", -1],
            ["what is the road made of", -1],
            ["what is on the sidewalk", -1],
            ["where was this picture taken", -1],
            ["how is the weather", 1]
        ],
        "context": [
            "a city street with a green traffic light.",
            "a wet city street with a traffic light and a traffic light."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2394621",
                "VG_object_id": "1211132",
                "bbox": [58, 34, 271, 375],
                "image": "data\\images\\2394621.jpg"
            },
            {
                "VG_image_id": "2340456",
                "VG_object_id": "2744977",
                "bbox": [124, 54, 256, 292],
                "image": "data\\images\\2340456.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["what is the man holding", 1],
            ["what is the man wearing", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["what is on the man head", -1],
            ["where is the man", -1],
            ["what is the man holding", 1],
            ["what is the man's posture", -1],
            ["how many people are shown", -1],
            ["what is the man wearing", 1],
            ["what is on the man's face", 1],
            ["what is the persion on the right wearing", -1]
        ],
        "context": [
            "two people playing a video game with nintendo wii controllers.",
            "a man and woman standing next to a table with a cake."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2383640",
                "VG_object_id": "1318891",
                "bbox": [101, 166, 365, 260],
                "image": "data\\images\\2383640.jpg"
            },
            {
                "VG_image_id": "2357302",
                "VG_object_id": "2142259",
                "bbox": [113, 94, 449, 291],
                "image": "data\\images\\2357302.jpg"
            }
        ],
        "questions_with_scores": [["How many zebras are there", 2]],
        "org_questions": [
            ["How many zebras are there", 2],
            ["What is zebra doing", -1],
            ["what color is the grass", -1],
            ["where is the zebra", -1],
            ["what is in the distance", -1],
            ["what main color is the background", -1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["where was the photo taken", -1],
            ["what is the zebra eating", -1],
            ["what animal is in the picture", -1]
        ],
        "context": [
            "a group of zebras eating from a trough in a field.",
            "a zebra walking in a grassy field next to trees."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2327992",
                "VG_object_id": "3503536",
                "bbox": [0, 0, 333, 498],
                "image": "data\\images\\2327992.jpg"
            },
            {
                "VG_image_id": "2407902",
                "VG_object_id": "271711",
                "bbox": [0, 0, 499, 332],
                "image": "data\\images\\2407902.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 1],
            ["what color is the floor", 1]
        ],
        "org_questions": [
            ["what color is the wall", 1],
            ["what color is the floor", 1],
            ["what is on the floor", -1],
            ["how many people are there", -1],
            ["Where is the toilet", -1],
            ["what is beside the toilet", -1],
            ["how many toy ducks are there", -1],
            ["what room is this", -1],
            ["what is the wall made of", -1],
            ["where was this picture taken", -1],
            ["what is in the bathroom", -1]
        ],
        "context": [
            "a bathroom with a blue door and toilet paper on the floor.",
            "a bathroom with a toilet and a trash can."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2417765",
                "VG_object_id": "3124581",
                "bbox": [8, 350, 345, 489],
                "image": "data\\images\\2417765.jpg"
            },
            {
                "VG_image_id": "2415183",
                "VG_object_id": "144323",
                "bbox": [2, 309, 203, 499],
                "image": "data\\images\\2415183.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["how many people are in the background", 1],
            ["what food is it", 1],
            ["what is the food on the table", 1],
            ["when was the photo taken", 1],
            ["who is in the photo", 1],
            ["what is in the background", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", -1],
            ["how many people are in the background", 1],
            ["what shape is the table", -1],
            ["where is the table", -1],
            ["what food is it", 1],
            ["what is the food on the table", 1],
            ["when was the photo taken", 1],
            ["who is in the photo", 1],
            ["what is in the background", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a vase of yellow flowers sitting on a table.",
            "a bride and groom cutting a cake together."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2322174",
                "VG_object_id": "3023791",
                "bbox": [186, 262, 277, 434],
                "image": "data\\images\\2322174.jpg"
            },
            {
                "VG_image_id": "2366870",
                "VG_object_id": "625570",
                "bbox": [250, 72, 333, 310],
                "image": "data\\images\\2366870.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["what color is the person's coat", 1],
            ["what color is the person's cap", 1]
        ],
        "org_questions": [
            ["what color is the person's coat", 1],
            ["what color is the person's cap", 1],
            ["how many people are there in the photo", 2],
            ["what is the person holding", -1],
            ["when is the photo taken", -1],
            ["where is the person", -1],
            ["what is the weather like", -1],
            ["what color is the background", -1],
            ["what are the people doing", -1],
            ["who is in the picture", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a skier is posing for a picture on a mountain.",
            "two people on skis posing for a picture."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2349614",
                "VG_object_id": "1714184",
                "bbox": [52, 27, 440, 314],
                "image": "data\\images\\2349614.jpg"
            },
            {
                "VG_image_id": "2399783",
                "VG_object_id": "1166372",
                "bbox": [187, 42, 408, 227],
                "image": "data\\images\\2399783.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of TV is it", 1],
            ["what is the TV screen showing", 1]
        ],
        "org_questions": [
            ["what kind of TV is it", 1],
            ["what is the TV screen showing", 1],
            ["how many people are there in the picture", -1],
            ["where is the tv", -1],
            ["which room is the television placing in", -1],
            ["what is on the television", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is behind the tv", -1]
        ],
        "context": [
            "a television that is on in a hallway.",
            "a television that is sitting on a stand."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2396641",
                "VG_object_id": "442016",
                "bbox": [1, 88, 499, 374],
                "image": "data\\images\\2396641.jpg"
            },
            {
                "VG_image_id": "2334727",
                "VG_object_id": "2253866",
                "bbox": [73, 167, 463, 332],
                "image": "data\\images\\2334727.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is the plate made of", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is the plate made of", 1],
            ["what is on the plate", -1],
            ["how many people are there", 1],
            ["what shape is the plate on the table", -1],
            ["what kind of food is it", -1],
            ["how many plates are there on the table", -1],
            ["what is the table made of", -1],
            ["where was this photo taken", -1],
            ["what is the pizza sitting on", -1],
            ["where is the pizza", -1],
            ["what shape is the table", -1]
        ],
        "context": [
            "a pizza on a pan on a table with a glass of wine.",
            "a woman eating a plate of food"
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2323902",
                "VG_object_id": "3372834",
                "bbox": [3, 0, 362, 331],
                "image": "data\\images\\2323902.jpg"
            },
            {
                "VG_image_id": "2372111",
                "VG_object_id": "2116660",
                "bbox": [75, 64, 391, 277],
                "image": "data\\images\\2372111.jpg"
            }
        ],
        "questions_with_scores": [["What color is train", 1]],
        "org_questions": [
            ["what main color is the train", -1],
            ["where is the train", -1],
            ["how many trains are there", -1],
            ["what is in the distance", -1],
            ["where is the photo taken", -1],
            ["What color is train", 1],
            ["when was the picture taken", -1],
            ["what is the train doing", -1],
            ["what is on the side of the train", -1],
            ["what is above the train", -1],
            ["what is on the train", -1]
        ],
        "context": [
            "a train car with a red house in the background.",
            "a train is pulling into a train station."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2400377",
                "VG_object_id": "1159683",
                "bbox": [372, 196, 500, 312],
                "image": "data\\images\\2400377.jpg"
            },
            {
                "VG_image_id": "2405566",
                "VG_object_id": "330983",
                "bbox": [242, 303, 497, 373],
                "image": "data\\images\\2405566.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the rug", 2],
            ["what is on the rug", 2],
            ["How many tables are there", 1],
            ["what is next to the floor", 1],
            ["what is in the floor", 1]
        ],
        "org_questions": [
            ["what is the color of the rug", 2],
            ["what is on the rug", 2],
            ["where is the rug", -1],
            ["How many tables are there", 1],
            ["what shape is the rug", -1],
            ["what is the ground covered with", -1],
            ["how many people are there in the photo", -1],
            ["what is next to the floor", 1],
            ["what is covering the floor", -1],
            ["what material is the floor made of", -1],
            ["what is in the floor", 1]
        ],
        "context": [
            "a table with a bunch of bananas on it",
            "a bunk bed with a desk and a desk."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316902",
                "VG_object_id": "2794786",
                "bbox": [133, 21, 289, 331],
                "image": "data\\images\\2316902.jpg"
            },
            {
                "VG_image_id": "2354357",
                "VG_object_id": "839193",
                "bbox": [63, 66, 179, 428],
                "image": "data\\images\\2354357.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's jacket", 2],
            ["What is man wearing in the head", 1],
            ["where is the man", 1],
            ["what is on the man's feet", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's jacket", 2],
            ["how many people are there", -1],
            ["What is man wearing in the head", 1],
            ["where is the man", 1],
            ["what is the man holding in hand", -1],
            ["what color is the background", -1],
            ["when was the photo taken", -1],
            ["what is on the man's feet", 1],
            ["what is the man standing on", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a man walking a dog on a leash next to a tree."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2368896",
                "VG_object_id": "2423314",
                "bbox": [255, 74, 445, 228],
                "image": "data\\images\\2368896.jpg"
            },
            {
                "VG_image_id": "2412207",
                "VG_object_id": "200450",
                "bbox": [373, 177, 482, 256],
                "image": "data\\images\\2412207.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the computer", 2],
            ["What color is the table", 2]
        ],
        "org_questions": [
            ["What color is the computer", 2],
            ["What color is the table", 2],
            ["How many screens are there", -1],
            ["what is laptop sitting on", -1],
            ["what device does the screen belong to", -1],
            ["where is the screen", -1],
            ["what is on the screen", -1],
            ["what type of computer is shown", -1],
            ["what is in the background", -1],
            ["what is next to the laptop", -1]
        ],
        "context": [
            "a small dog sitting on a table next to a laptop.",
            "a computer monitor and keyboard on a desk."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2359373",
                "VG_object_id": "2318039",
                "bbox": [1, 373, 374, 497],
                "image": "data\\images\\2359373.jpg"
            },
            {
                "VG_image_id": "2345004",
                "VG_object_id": "3628020",
                "bbox": [142, 21, 495, 359],
                "image": "data\\images\\2345004.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is photo taken", 1],
            ["what is on the ground", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["Where is photo taken", 1],
            ["What color is the land", -1],
            ["How many people are there", -1],
            ["what is on the ground", 1],
            ["what is the ground covered with", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a parking meter on the sidewalk next to a building.",
            "a view of a city from an airplane window."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2345837",
                "VG_object_id": "904453",
                "bbox": [92, 130, 498, 243],
                "image": "data\\images\\2345837.jpg"
            },
            {
                "VG_image_id": "2354771",
                "VG_object_id": "1041946",
                "bbox": [139, 79, 418, 317],
                "image": "data\\images\\2354771.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the head of the train", 2],
            ["how many trains are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the head of the train", 2],
            ["how many trains are there in the picture", 1],
            ["where is the photo taken", -1],
            ["what is on the side of the train", -1],
            ["what is the train doing", -1],
            ["what is the land made of", -1],
            ["where is the train", -1],
            ["when was the picture taken", -1],
            ["what is the train on", -1],
            ["what is on the tracks", -1],
            ["what is next to the train", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a couple of trains that are sitting on some tracks",
            "a bullet train is parked at a station."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2348421",
                "VG_object_id": "882224",
                "bbox": [2, 225, 498, 312],
                "image": "data\\images\\2348421.jpg"
            },
            {
                "VG_image_id": "2366198",
                "VG_object_id": "3880454",
                "bbox": [5, 220, 491, 357],
                "image": "data\\images\\2366198.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the object on the ground", 1],
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["where was this photo taken", 1],
            ["how is the weather", 1],
            ["what is the weather like", 1],
            ["what is in front of the ground", 1]
        ],
        "org_questions": [
            ["What is the object on the ground", 1],
            ["How many people are there", -1],
            ["what is the land made of", 1],
            ["what is in the distance", -1],
            ["what is on the land", 1],
            ["where was this photo taken", 1],
            ["how is the weather", 1],
            ["what is the weather like", 1],
            ["what is in front of the ground", 1]
        ],
        "context": [
            "a yellow fire hydrant sitting on the side of a road.",
            "two airplanes are parked on the runway at an airport."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2344385",
                "VG_object_id": "2935041",
                "bbox": [1, 216, 491, 371],
                "image": "data\\images\\2344385.jpg"
            },
            {
                "VG_image_id": "2402141",
                "VG_object_id": "1139910",
                "bbox": [200, 300, 498, 464],
                "image": "data\\images\\2402141.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many tables are there in the picture", 1],
            ["where is this photo taken", 1],
            ["what room is it", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is the floor made of", -1],
            ["How many people are there", -1],
            ["where is the photo taken", -1],
            ["What is on the floor", -1],
            ["how many tables are there in the picture", 1],
            ["what kind of flooring is in the room", -1],
            ["what is covering the floor", -1],
            ["what material is the floor made of", -1],
            ["what is the table made of", -1],
            ["what is on the floor", -1],
            ["where is this photo taken", 1],
            ["what room is it", 1],
            ["what pattern is the floor", -1],
            ["where is the floor", -1]
        ],
        "context": [
            "a living room with a couch, table, and a television.",
            "a kitchen with a stove, sink, and refrigerator."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2363729",
                "VG_object_id": "3744079",
                "bbox": [137, 305, 202, 364],
                "image": "data\\images\\2363729.jpg"
            },
            {
                "VG_image_id": "2348892",
                "VG_object_id": "878985",
                "bbox": [285, 63, 337, 117],
                "image": "data\\images\\2348892.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is person's shirt", 2],
            ["who is in the picture", 2],
            ["What is person doing", 1],
            ["where is the person", 1],
            ["what gender is the person in the shirt", 1]
        ],
        "org_questions": [
            ["What color is person's shirt", 2],
            ["What is person doing", 1],
            ["where is the person", 1],
            ["what gender is the person in the shirt", 1],
            ["who is in the picture", 2],
            ["when was this photo taken", -1],
            ["how many people are there", -1],
            ["who is wearing a white shirt", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a girl running down a sidewalk next to a parking meter.",
            "a man standing in a boat filled with bags of food."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2398677",
                "VG_object_id": "1177349",
                "bbox": [97, 209, 344, 450],
                "image": "data\\images\\2398677.jpg"
            },
            {
                "VG_image_id": "2412283",
                "VG_object_id": "198244",
                "bbox": [218, 130, 372, 272],
                "image": "data\\images\\2412283.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what color is the chair", 1],
            ["what is on the chair", 1],
            ["What is above the chair", 1],
            ["how many chairs are there", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["what is on the chair", 1],
            ["how many people are in the picture", 2],
            ["where is the chair", -1],
            ["what is the ground covered with", -1],
            ["What is above the chair", 1],
            ["how many chairs are there in the photo", -1],
            ["how many chairs are there", 1],
            ["what is the floor made of", -1],
            ["what is on the table", 1]
        ],
        "context": [
            "a woman sitting in a chair reading a book.",
            "a desk with a laptop and a book shelf"
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2374495",
                "VG_object_id": "2153918",
                "bbox": [17, 351, 124, 498],
                "image": "data\\images\\2374495.jpg"
            },
            {
                "VG_image_id": "2336705",
                "VG_object_id": "2874473",
                "bbox": [1, 244, 371, 497],
                "image": "data\\images\\2336705.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the table", 1],
            ["what is in the bowl", 1],
            ["what is sitting on the table", 1],
            ["what is next to the table", 1]
        ],
        "org_questions": [
            ["what color is the bowl", -1],
            ["what color is the table", 1],
            ["how many bowls are in the picture", -1],
            ["where is the bowl", -1],
            ["what is in the bowl", 1],
            ["what color is bowl", -1],
            ["what is the shape of the table", -1],
            ["where was the photo taken", -1],
            ["what is sitting on the table", 1],
            ["what is next to the table", 1],
            ["how many people are there", 2]
        ],
        "context": [
            "a group of young women standing around a table filled with food.",
            "a blue bowl filled with bananas and peaches."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2382600",
                "VG_object_id": "1327626",
                "bbox": [251, 34, 499, 184],
                "image": "data\\images\\2382600.jpg"
            },
            {
                "VG_image_id": "2403158",
                "VG_object_id": "356225",
                "bbox": [1, 3, 499, 93],
                "image": "data\\images\\2403158.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the building", 1],
            ["What is on the foreground", 1],
            ["what color is the roof of the building", 1],
            ["what is in front of the building", 1]
        ],
        "org_questions": [
            ["What color is the building", 1],
            ["What is on the foreground", 1],
            ["How many cars are there", -1],
            ["when is this picture taken", -1],
            ["what is the building made of", -1],
            ["Where is the photo taken", -1],
            ["what color is the roof of the building", 1],
            ["what is on the side of the building", -1],
            ["what is in front of the building", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a large airplane sitting on top of a tarmac.",
            "a white car with a yellow light on top of it."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2400841",
                "VG_object_id": "404966",
                "bbox": [0, 105, 231, 374],
                "image": "data\\images\\2400841.jpg"
            },
            {
                "VG_image_id": "2386536",
                "VG_object_id": "681676",
                "bbox": [1, 1, 499, 371],
                "image": "data\\images\\2386536.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 1],
            ["what color is the clock", 1]
        ],
        "org_questions": [
            ["what color is the building", 1],
            ["what color is the clock", 1],
            ["how many people are there", -1],
            ["where is the building", -1],
            ["what is the building made of", -1],
            ["how is the weather", -1],
            ["what kind of building is this", -1]
        ],
        "context": [
            "a tall building with a clock on the top of it.",
            "a large clock on a building"
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2317454",
                "VG_object_id": "3184362",
                "bbox": [264, 165, 409, 296],
                "image": "data\\images\\2317454.jpg"
            },
            {
                "VG_image_id": "2417910",
                "VG_object_id": "3561045",
                "bbox": [69, 72, 231, 367],
                "image": "data\\images\\2417910.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many people are there in the picture", 2],
            ["what color is the child's shirt", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", 1],
            ["what is the boy doing", -1],
            ["where is the girl", -1],
            ["what is the child holding", -1],
            ["What is child doing", -1],
            ["how many people are there", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1],
            ["what is the little girl doing", -1],
            ["what color is the child's hair", -1],
            ["what color is the table", 2],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man and a child sitting at a table with pizza.",
            "a little girl sitting on a counter eating a piece of food."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2416628",
                "VG_object_id": "3517282",
                "bbox": [182, 17, 342, 278],
                "image": "data\\images\\2416628.jpg"
            },
            {
                "VG_image_id": "1591973",
                "VG_object_id": "1605602",
                "bbox": [402, 189, 673, 867],
                "image": "data\\images\\1591973.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's clothes", 1],
            ["what is the persion holding", 1],
            ["where is the girl", 1],
            ["what is the child wearing", 1]
        ],
        "org_questions": [
            ["what is the girl doing", -1],
            ["what color is the girl's clothes", 1],
            ["what is the persion holding", 1],
            ["how many people are there", -1],
            ["where is the girl", 1],
            ["what is the gender of the person", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is on the girl's feet", -1],
            ["what is the child wearing", 1]
        ],
        "context": [
            "a little girl sitting on a bench with a bottle of beer.",
            "a young girl holding an umbrella in the rain."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2405457",
                "VG_object_id": "3812621",
                "bbox": [73, 113, 233, 402],
                "image": "data\\images\\2405457.jpg"
            },
            {
                "VG_image_id": "2359852",
                "VG_object_id": "2014357",
                "bbox": [270, 42, 426, 295],
                "image": "data\\images\\2359852.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color is the court", 1],
            ["what color is the ground", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the court", 1],
            ["what sport is the man playing", -1],
            ["how many people are there", -1],
            ["what is the player holding", -1],
            ["what sport is it", -1],
            ["how many players are there", -1],
            ["what color is the ground", 1],
            ["when was the picture taken", -1],
            ["what is on the man's head", -1],
            ["what is the player doing", -1],
            ["what is the boy wearing", -1]
        ],
        "context": [
            "a baseball player is getting ready to hit a ball.",
            "a person playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2319418",
                "VG_object_id": "1002136",
                "bbox": [0, 2, 499, 152],
                "image": "data\\images\\2319418.jpg"
            },
            {
                "VG_image_id": "2345154",
                "VG_object_id": "3626752",
                "bbox": [0, 1, 483, 116],
                "image": "data\\images\\2345154.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many windows are on the wall", 1],
            ["who is in the picture", 1],
            ["how many people are there in front of the building", 1],
            ["what is in front of the building", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["how many windows are on the wall", 1],
            ["what is the building made of", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", 1],
            ["how many people are there in front of the building", 1],
            ["what is in front of the building", 1],
            ["where is the building", -1],
            ["what is on the ground", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a man riding a skateboard down a sidewalk.",
            "a woman is talking on a cell phone."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2408377",
                "VG_object_id": "262946",
                "bbox": [150, 94, 242, 147],
                "image": "data\\images\\2408377.jpg"
            },
            {
                "VG_image_id": "2331160",
                "VG_object_id": "2770545",
                "bbox": [147, 81, 319, 321],
                "image": "data\\images\\2331160.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the court", 2],
            ["what color is the shirt", 1],
            ["what is on the man's head", 1],
            ["what is the shirt color", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what color is the court", 2],
            ["what is on the man's head", 1],
            ["what is the man doing", -1],
            ["what is the ground covered with", -1],
            ["where is the person ", -1],
            ["what is in the man's hands", -1],
            ["what is the persion holding", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1],
            ["what is the shirt color", 1]
        ],
        "context": [
            "a man is playing tennis on a tennis court.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2348830",
                "VG_object_id": "879478",
                "bbox": [45, 37, 339, 498],
                "image": "data\\images\\2348830.jpg"
            },
            {
                "VG_image_id": "2333044",
                "VG_object_id": "2721955",
                "bbox": [180, 39, 272, 196],
                "image": "data\\images\\2333044.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 1],
            ["where is the photo taken", 1],
            ["when is this picture taken", 1],
            ["how many people are there", 1],
            ["who is in the photo", 1],
            ["what is the persion wearing", 1],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["what time is it", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", -1],
            ["what is the person holding", 1],
            ["where is the photo taken", 1],
            ["What is the weather like", -1],
            ["when is this picture taken", 1],
            ["what is the background of the person", -1],
            ["how many people are there", 1],
            ["who is in the photo", 1],
            ["what are the people doing", -1],
            ["what is the persion wearing", 1],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["what is the persion doing", -1],
            ["what time is it", 1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "a woman holding a cell phone in her hand.",
            "a man standing in a kitchen preparing bread."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2316301",
                "VG_object_id": "3514560",
                "bbox": [170, 107, 320, 202],
                "image": "data\\images\\2316301.jpg"
            },
            {
                "VG_image_id": "2387918",
                "VG_object_id": "676550",
                "bbox": [344, 0, 489, 70],
                "image": "data\\images\\2387918.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 1],
            ["who is in the photo", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["What time is it", -1],
            ["where is the person", 1],
            ["what is in the background", -1],
            ["who is in the photo", 1],
            ["what is the persion sitting on", 1],
            ["what type of shirt is the girl wearing", -1]
        ],
        "context": [
            "a toddler sitting on the floor playing with toys.",
            "a table with plates and glasses of wine."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2411051",
                "VG_object_id": "360802",
                "bbox": [3, 205, 497, 328],
                "image": "data\\images\\2411051.jpg"
            },
            {
                "VG_image_id": "2378588",
                "VG_object_id": "2124323",
                "bbox": [1, 182, 498, 372],
                "image": "data\\images\\2378588.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["what is beside the table", 1],
            ["what shape is the table", 1],
            ["what kind of food is on the table", 1],
            ["what is the table color", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["How many people are there", 2],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["what is beside the table", 1],
            ["what shape is the table", 1],
            ["what kind of food is on the table", 1],
            ["where is the picture taken", -1],
            ["what is the table color", 1]
        ],
        "context": [
            "a boy eating pizza on a table",
            "a slice of pie and a cup of coffee on a table."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2378266",
                "VG_object_id": "560775",
                "bbox": [75, 259, 292, 459],
                "image": "data\\images\\2378266.jpg"
            },
            {
                "VG_image_id": "2328077",
                "VG_object_id": "2839291",
                "bbox": [113, 60, 380, 287],
                "image": "data\\images\\2328077.jpg"
            }
        ],
        "questions_with_scores": [
            ["what time is it", 1],
            ["when is this picture taken", 1],
            ["what color is the sky", 1],
            ["what is behind the clock", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the clock made of", -1],
            ["what time is it", 1],
            ["when is this picture taken", 1],
            ["what color is the sky", 1],
            ["where is the clock", -1],
            ["what shape is the clock", -1],
            ["what is on the clock", -1],
            ["how many clocks are there", -1],
            ["what is behind the clock", 1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "a large clock tower with a bell on top of it.",
            "a clock with a neon sign and a baseball sign."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2392525",
                "VG_object_id": "479317",
                "bbox": [128, 254, 185, 414],
                "image": "data\\images\\2392525.jpg"
            },
            {
                "VG_image_id": "2326852",
                "VG_object_id": "3013649",
                "bbox": [115, 62, 252, 292],
                "image": "data\\images\\2326852.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["when is this picture taken", 1],
            ["what is the persion doing", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["when is this picture taken", 1],
            ["what color are the person's shoes", -1],
            ["what is the weather like", -1],
            ["what kind of trousers is the woman wearing", -1],
            ["what is the persion doing", 1],
            ["how many people are there", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", -1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "a person walking down a narrow alley holding an umbrella.",
            "a woman sitting on a bench in a park."
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2320263",
                "VG_object_id": "994665",
                "bbox": [141, 68, 392, 299],
                "image": "data\\images\\2320263.jpg"
            },
            {
                "VG_image_id": "2334905",
                "VG_object_id": "2394292",
                "bbox": [18, 69, 383, 313],
                "image": "data\\images\\2334905.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building behind the motorcycle", 1]
        ],
        "org_questions": [
            ["what color is the building behind the motorcycle", 1],
            ["what color is the motorcycle", -1],
            ["How many motorcycles are there", -1],
            ["what is the ground covered with", -1],
            ["where are the motor ", -1],
            ["what is in the distance", -1],
            ["how many people are there in the picture", -1],
            ["when was the picture taken", -1],
            ["what type of vehicle is shown", -1],
            ["where is this scene", -1],
            ["what is parked on the motorcycle", -1],
            ["what is the motorcycle parked on", -1]
        ],
        "context": [
            "a motorcycle parked on the side of a street.",
            "a black motorcycle parked in front of a house."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2372873",
                "VG_object_id": "735056",
                "bbox": [46, 8, 164, 264],
                "image": "data\\images\\2372873.jpg"
            },
            {
                "VG_image_id": "2341652",
                "VG_object_id": "940877",
                "bbox": [160, 180, 231, 422],
                "image": "data\\images\\2341652.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["Where is man", 1],
            ["where is the picture taken", 1],
            ["what type of pants is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["How many people are there", -1],
            ["Where is man", 1],
            ["where is the picture taken", 1],
            ["what type of pants is the man wearing", 1],
            ["who is wearing a black shirt", -1],
            ["when was the photo taken", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man is standing by a grill with a hot dog on it.",
            "a man on a motorcycle with a dog on the back."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2371766",
                "VG_object_id": "1703707",
                "bbox": [6, 192, 493, 339],
                "image": "data\\images\\2371766.jpg"
            },
            {
                "VG_image_id": "2384739",
                "VG_object_id": "1305669",
                "bbox": [1, 111, 498, 374],
                "image": "data\\images\\2384739.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the ground", 2],
            ["what color is the ground", 1],
            ["what is on the ground", 1],
            ["what animal is on the ground", 1],
            ["where was this picture taken", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is on the ground", 1],
            ["how many people are there on the ground", 2],
            ["what time is it", -1],
            ["what animal is on the ground", 1],
            ["what is the weather like", -1],
            ["where was this picture taken", 1],
            ["when was the photo taken", -1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a soldier stands guard in front of a military truck.",
            "a dead bird laying on the ground with a cat laying on the ground."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2323579",
                "VG_object_id": "3408812",
                "bbox": [136, 15, 423, 248],
                "image": "data\\images\\2323579.jpg"
            },
            {
                "VG_image_id": "2375587",
                "VG_object_id": "1906142",
                "bbox": [162, 285, 275, 479],
                "image": "data\\images\\2375587.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the boy's shirt", 1],
            ["What is the boy doing", 1],
            ["What color is the boy's hair", 1]
        ],
        "org_questions": [
            ["What color is the boy's shirt", 1],
            ["What is the boy doing", 1],
            ["What color is the boy's hair", 1],
            ["how many children are in the picture", -1],
            ["what is the boy wearing on his head", -1],
            ["what is the child wearing on head", -1],
            ["where is the boy", -1],
            ["what is the gender of the person", -1],
            ["who is in the photo", -1],
            ["what is the boy wearing", -1]
        ],
        "context": [
            "a baby sitting in a high chair with a piece of cake on it.",
            "a man and a boy playing a video game."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2336308",
                "VG_object_id": "2605907",
                "bbox": [8, 3, 496, 373],
                "image": "data\\images\\2336308.jpg"
            },
            {
                "VG_image_id": "2354850",
                "VG_object_id": "2342475",
                "bbox": [0, 183, 495, 330],
                "image": "data\\images\\2354850.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is on the ground", 2],
            ["what is the ground covered with", 1],
            ["what is on the land", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["what color is the land", -1],
            ["how many people are in the picture", 2],
            ["what is the weather like", -1],
            ["what is on the land", 1],
            ["what is the land made of", -1],
            ["where was the photo taken", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["what is the weather", -1],
            ["what is on the ground", 2]
        ],
        "context": [
            "a shoe in a blue suitcase on the ground.",
            "two women sitting on a bench talking to each other."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2317578",
                "VG_object_id": "3790944",
                "bbox": [172, 192, 235, 324],
                "image": "data\\images\\2317578.jpg"
            },
            {
                "VG_image_id": "2316781",
                "VG_object_id": "3032673",
                "bbox": [3, 201, 302, 497],
                "image": "data\\images\\2316781.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 1],
            ["what gender is the person in the shirt", 1],
            ["what is the person wearing on neck", 1],
            ["what is the persion wearing around his neck", 1],
            ["what is in the background", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what gender is the person in the shirt", 1],
            ["what is the person wearing on neck", 1],
            ["how many people are there", -1],
            ["what is the pattern of the shirt", -1],
            ["what is the persion wearing around his neck", 1],
            ["what is in the background", 1],
            ["what is the persion doing", 1]
        ],
        "context": [
            "a woman drinking from a glass while standing next to a table.",
            "a man wearing a white shirt and sunglasses."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2387646",
                "VG_object_id": "511952",
                "bbox": [0, 143, 280, 213],
                "image": "data\\images\\2387646.jpg"
            },
            {
                "VG_image_id": "2397152",
                "VG_object_id": "436769",
                "bbox": [3, 240, 500, 371],
                "image": "data\\images\\2397152.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bottles are there on the counter", 2],
            ["What is on the counter", 1],
            ["What is next to the counter", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["What is on the counter", 1],
            ["What is next to the counter", 1],
            ["What color is the wall", -1],
            ["how many bottles are there on the counter", 2],
            ["where is the counter", -1],
            ["where is the picture taken", -1],
            ["what is the counter color", -1],
            ["how many people are in the photo", 1],
            ["what is the wall made of", -1]
        ],
        "context": [
            "a young boy standing in a kitchen with an oven.",
            "a bathroom with a toilet, mirror, and flowers in vases."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2347010",
                "VG_object_id": "3613809",
                "bbox": [62, 0, 496, 372],
                "image": "data\\images\\2347010.jpg"
            },
            {
                "VG_image_id": "2336825",
                "VG_object_id": "2388903",
                "bbox": [2, 418, 331, 498],
                "image": "data\\images\\2336825.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many birds are there on the floor", 2],
            ["how many birds are there in the picture", 2],
            ["how many chairs are there in the picture", 2],
            ["what color is the floor", 1],
            ["how many chairs are there on the floor", 1],
            ["what kind of animal is on the floor", 1],
            ["what is on the ground", 1],
            ["What is the floor made of", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["how many birds are there on the floor", 2],
            ["how many chairs are there on the floor", 1],
            ["what kind of animal is on the floor", 1],
            ["what is on the ground", 1],
            ["What is the floor made of", 1],
            ["how many birds are there in the picture", 2],
            ["how many chairs are there in the picture", 2]
        ],
        "context": [
            "three pigeons are standing on the sidewalk.",
            "a blue chair with a colorful blanket on it"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2415183",
                "VG_object_id": "144325",
                "bbox": [50, 85, 175, 290],
                "image": "data\\images\\2415183.jpg"
            },
            {
                "VG_image_id": "2315858",
                "VG_object_id": "2868027",
                "bbox": [53, 136, 291, 498],
                "image": "data\\images\\2315858.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is on the man's face", 1],
            ["how many men are there", 1],
            ["where is the picture taken", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's hair", -1],
            ["what is the man wearing", -1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["What is man doing", -1],
            ["what gesture is the man", -1],
            ["when was the photo taken", -1],
            ["what is behind the man", -1],
            ["what is on the man's face", 1],
            ["what color is the man's shirt", -1],
            ["how many men are there", 1],
            ["where is the picture taken", 1],
            ["what is the man holding", 1],
            ["who is in the photo", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a bride and groom cutting a cake together.",
            "a man in a suit and tie holding a wii remote."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2374185",
                "VG_object_id": "1927867",
                "bbox": [107, 123, 244, 261],
                "image": "data\\images\\2374185.jpg"
            },
            {
                "VG_image_id": "2358706",
                "VG_object_id": "1746219",
                "bbox": [149, 136, 230, 416],
                "image": "data\\images\\2358706.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is the person standing on", 1],
            ["what is in the background", 1],
            ["what color is the man's shirt", 1],
            ["where is the person", 1],
            ["what is the persion wearing", 1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the person standing on", 1],
            ["how many people are there in the picture", 2],
            ["what is in the background", 1],
            ["how is the weather", -1],
            ["what time is it", -1],
            ["what color is the man's shirt", 1],
            ["where is the person", 1],
            ["what is the persion wearing", 1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1],
            ["when was this picture taken", -1],
            ["what is the person holding", -1]
        ],
        "context": [
            "a snowboarder is in the air doing a trick.",
            "a man and woman standing next to motorcycles."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2385603",
                "VG_object_id": "1294735",
                "bbox": [209, 155, 312, 292],
                "image": "data\\images\\2385603.jpg"
            },
            {
                "VG_image_id": "2344100",
                "VG_object_id": "2218351",
                "bbox": [199, 69, 330, 266],
                "image": "data\\images\\2344100.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's clothes", 2],
            ["What color is man's pant", 1],
            ["What is person holding", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["What color is man's clothes", 2],
            ["How many people are there", -1],
            ["What color is man's pant", 1],
            ["What is person holding", 1],
            ["what is in the background", -1],
            ["what is the man wearing", -1],
            ["what is the person holding", 1],
            ["who is in the photo", -1],
            ["what is the skier doing", -1],
            ["what is on the skier's feet", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a person riding a snowboard down a snow covered slope.",
            "a woman riding skis down a snow covered slope."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2375019",
                "VG_object_id": "3089760",
                "bbox": [1, 132, 496, 332],
                "image": "data\\images\\2375019.jpg"
            },
            {
                "VG_image_id": "2373660",
                "VG_object_id": "588237",
                "bbox": [44, 268, 411, 373],
                "image": "data\\images\\2373660.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bed", 1],
            ["what is next to the bed", 1]
        ],
        "org_questions": [
            ["what color is the bed", 1],
            ["what color is the pillow on the right", -1],
            ["how many cats are on the blanket", -1],
            ["when is this photo taken", -1],
            ["what is on the bed", -1],
            ["what kind of bed is this", -1],
            ["what is next to the bed", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a bed with a book and a book on it",
            "a bed with a lamp hanging from the ceiling."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2350640",
                "VG_object_id": "3592846",
                "bbox": [45, 2, 212, 154],
                "image": "data\\images\\2350640.jpg"
            },
            {
                "VG_image_id": "2398075",
                "VG_object_id": "428519",
                "bbox": [409, 13, 496, 240],
                "image": "data\\images\\2398075.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the plant sitting on", 1],
            ["what color is the table", 1],
            ["what is on the left", 1]
        ],
        "org_questions": [
            ["Where is plant", -1],
            ["What color are leaves", -1],
            ["how many people are there", -1],
            ["what is the plant sitting on", 1],
            ["what color is the table", 1],
            ["what is hanging on the wall", -1],
            ["what is on the left", 1],
            ["what is in the background", -1],
            ["where are the trees", -1]
        ],
        "context": [
            "a piano with a sheet music on top of it.",
            "a cat sitting on a desk next to a computer."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2329818",
                "VG_object_id": "3036790",
                "bbox": [3, 34, 333, 498],
                "image": "data\\images\\2329818.jpg"
            },
            {
                "VG_image_id": "2362367",
                "VG_object_id": "2154635",
                "bbox": [2, 114, 331, 498],
                "image": "data\\images\\2362367.jpg"
            }
        ],
        "questions_with_scores": [["what color is the building", 1]],
        "org_questions": [
            ["what is the building made of", -1],
            ["what color is the building", 1],
            ["when is this picture taken", -1],
            ["Where is the photo taken", -1],
            ["What is on the foreground", -1],
            ["what is in front of the building", -1],
            ["what is the shape of the clock", -1],
            ["who is in the photo", -1],
            ["what time is it", -1],
            ["where is the building", -1],
            ["what is behind the clock", -1]
        ],
        "context": [
            "a clock on the side of a building.",
            "a clock on a brick building with a sky background"
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2319491",
                "VG_object_id": "3345156",
                "bbox": [142, 273, 323, 402],
                "image": "data\\images\\2319491.jpg"
            },
            {
                "VG_image_id": "2324913",
                "VG_object_id": "3373247",
                "bbox": [213, 70, 494, 301],
                "image": "data\\images\\2324913.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what color is the food", 2],
            ["how many people are there", 1],
            ["what type of food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what color is the food", 2],
            ["what shape of plate is the food put on", -1],
            ["how many people are there", 1],
            ["what is the food placing on", -1],
            ["what is in the bowl", -1],
            ["what is the shape of the plate", -1],
            ["where is the plate", -1],
            ["what is the table made of", -1],
            ["what type of food is on the plate", 1],
            ["what is on the table", -1],
            ["what is next to the plate", -1]
        ],
        "context": [
            "a woman is eating a sandwich at a table.",
            "a plate of food with meat and vegetables."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2332992",
                "VG_object_id": "3346459",
                "bbox": [0, 0, 499, 372],
                "image": "data\\images\\2332992.jpg"
            },
            {
                "VG_image_id": "2391398",
                "VG_object_id": "3827089",
                "bbox": [27, 243, 317, 373],
                "image": "data\\images\\2391398.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["how many people are there", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what time is it", -1],
            ["how many motorcycles are there on the ground", -1],
            ["where is the photo taken", -1],
            ["what is covering the ground", -1],
            ["what is the weather like", -1],
            ["what is the land made of", -1],
            ["what is in the background", -1],
            ["how is the weather", -1],
            ["how many people are in the picture", 1],
            ["what is on the ground", -1],
            ["how many people are there", 1],
            ["what is in the distance", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a fire hydrant sitting next to a fire hydrant.",
            "a bench sitting in the grass near a body of water."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2357205",
                "VG_object_id": "1667863",
                "bbox": [137, 22, 281, 250],
                "image": "data\\images\\2357205.jpg"
            },
            {
                "VG_image_id": "2362621",
                "VG_object_id": "3465146",
                "bbox": [0, 43, 80, 354],
                "image": "data\\images\\2362621.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["what is the man holding", 2],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what color is the background", 1],
            ["what color is the man's shirt", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is on the man's head", 2],
            ["how many people are there", -1],
            ["what color is the background", 1],
            ["where is the man", -1],
            ["what is the man standing on", -1],
            ["what color is the man's shirt", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man holding", 2]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "two police officers on motorcycles on a city street."
        ]
    },
    {
        "object_category": "ocean",
        "images": [
            {
                "VG_image_id": "2345325",
                "VG_object_id": "2133875",
                "bbox": [2, 124, 496, 296],
                "image": "data\\images\\2345325.jpg"
            },
            {
                "VG_image_id": "2328330",
                "VG_object_id": "3515728",
                "bbox": [40, 129, 498, 194],
                "image": "data\\images\\2328330.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the water", 1],
            ["What are the people doing in the water", 1],
            ["how many people are there in the ocean", 1],
            ["what is in the water", 1],
            ["what is the beach of the ocean made of", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["What color is the water", 1],
            ["What are the people doing in the water", 1],
            ["how many people are there in the ocean", 1],
            ["what is the gender of the person", -1],
            ["what is in the water", 1],
            ["what is the beach of the ocean made of", 1],
            ["what is the person holding", 1],
            ["where was this photo taken", -1],
            ["when was the picture taken", -1],
            ["what is the weather like", -1],
            ["where is the water", -1]
        ],
        "context": [
            "a group of people riding surfboards in the ocean.",
            "a man is playing frisbee on the beach."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2402192",
                "VG_object_id": "658546",
                "bbox": [368, 112, 474, 220],
                "image": "data\\images\\2402192.jpg"
            },
            {
                "VG_image_id": "2410754",
                "VG_object_id": "321491",
                "bbox": [163, 71, 307, 373],
                "image": "data\\images\\2410754.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing", 2],
            ["where is the woman", 2],
            ["what is the woman holding", 1],
            ["what is the ground covered with", 1],
            ["what is the woman doing", 1],
            ["what gesture is the woman", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", 2],
            ["what is the woman holding", 1],
            ["what is the ground covered with", 1],
            ["where is the woman", 2],
            ["what is the woman doing", 1],
            ["what gesture is the woman", 1],
            ["What is the woman wearing on her head", -1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a man and woman sitting on a towel on the beach.",
            "a woman holding a kite in a park."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2412848",
                "VG_object_id": "185558",
                "bbox": [258, 312, 344, 375],
                "image": "data\\images\\2412848.jpg"
            },
            {
                "VG_image_id": "2380218",
                "VG_object_id": "546917",
                "bbox": [73, 68, 135, 154],
                "image": "data\\images\\2380218.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["How many people are there", 1],
            ["what is the man in the blue shirt doing", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", -1],
            ["How many people are there", 1],
            ["where is the photo taken", 2],
            ["what is the man wearing on the head", -1],
            ["what is the ground covered with", -1],
            ["what is the gender of the person", -1],
            ["when was the picture taken", -1],
            ["what is the man in the blue shirt wearing", -1],
            ["who is in the photo", -1],
            ["what is the man in the blue shirt doing", 1]
        ],
        "context": [
            "a group of people playing baseball on a field.",
            "a young boy throwing a frisbee in a park."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2393144",
                "VG_object_id": "474544",
                "bbox": [8, 88, 320, 440],
                "image": "data\\images\\2393144.jpg"
            },
            {
                "VG_image_id": "2341844",
                "VG_object_id": "3208842",
                "bbox": [322, 141, 397, 287],
                "image": "data\\images\\2341844.jpg"
            }
        ],
        "questions_with_scores": [
            ["How old is the girl", 2],
            ["what is the woman's posture", 2],
            ["What is girl holding", 1],
            ["where is the woman", 1],
            ["what is the girl doing", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["How old is the girl", 2],
            ["How many people are there", -1],
            ["What is girl holding", 1],
            ["where is the woman", 1],
            ["what is the girl doing", 1],
            ["What is woman wearing in her face", -1],
            ["what is the woman's posture", 2],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1],
            ["what is on the woman's feet", -1]
        ],
        "context": [
            "a beautiful girl sitting on a bench in a park.",
            "a man riding a scooter with a little girl on it."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2401814",
                "VG_object_id": "395695",
                "bbox": [3, 3, 487, 263],
                "image": "data\\images\\2401814.jpg"
            },
            {
                "VG_image_id": "2386693",
                "VG_object_id": "1283497",
                "bbox": [152, 6, 359, 316],
                "image": "data\\images\\2386693.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants are there", 2],
            ["what is in front of the elephants", 1]
        ],
        "org_questions": [
            ["what are the elephants doing", -1],
            ["how many elephants are there", 2],
            ["what is in front of the elephants", 1],
            ["where is the nose of the elephant", -1],
            ["what is on the elephant", -1],
            ["what is in the distance", -1],
            ["where are the elephants", -1],
            ["where is the elephant", -1],
            ["when was the photo taken", -1],
            ["what type of animal is shown", -1],
            ["what color is the elephant", -1]
        ],
        "context": [
            "a baby elephant drinking water from a pond.",
            "an elephant walking down a road with trees in the background."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2382486",
                "VG_object_id": "1328345",
                "bbox": [37, 284, 498, 439],
                "image": "data\\images\\2382486.jpg"
            },
            {
                "VG_image_id": "2333993",
                "VG_object_id": "3926940",
                "bbox": [0, 228, 497, 372],
                "image": "data\\images\\2333993.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many buses are there on the street", 2],
            ["what color is the bus on the street", 1],
            ["what color is the sky", 1]
        ],
        "org_questions": [
            ["what color is the street", -1],
            ["how many buses are there on the street", 2],
            ["what color is the bus on the street", 1],
            ["what is the ground covered with", -1],
            ["how is the weather", -1],
            ["what is in the distance", -1],
            ["how many people are in the photo", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["what is on the ground", -1],
            ["what color is the sky", 1]
        ],
        "context": [
            "a red and white bus driving down a street.",
            "two red double decker buses are parked on the side of the road."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2386114",
                "VG_object_id": "1289236",
                "bbox": [0, 44, 410, 241],
                "image": "data\\images\\2386114.jpg"
            },
            {
                "VG_image_id": "2320583",
                "VG_object_id": "3102437",
                "bbox": [4, 202, 330, 440],
                "image": "data\\images\\2320583.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sky", 1],
            ["what is in front of the cars", 1],
            ["what is parked on the ground", 1]
        ],
        "org_questions": [
            ["where is the car", -1],
            ["what color is the sky", 1],
            ["what is on the side of the car", -1],
            ["how many cars are there", -1],
            ["what time is it", -1],
            ["what is the weather like", -1],
            ["which part of the car can we see in the picture", -1],
            ["what is in front of the cars", 1],
            ["when was the photo taken", -1],
            ["what is parked on the ground", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a motorcycle parked in a parking lot next to a car.",
            "a car with luggage on top of it"
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2345933",
                "VG_object_id": "3620786",
                "bbox": [134, 133, 453, 298],
                "image": "data\\images\\2345933.jpg"
            },
            {
                "VG_image_id": "2370718",
                "VG_object_id": "3857083",
                "bbox": [41, 158, 421, 324],
                "image": "data\\images\\2370718.jpg"
            }
        ],
        "questions_with_scores": [
            ["What object is in the center of image", 2],
            ["what is the color of the bed", 2],
            ["How many people are there in the image", 1],
            ["what is on the bed", 1],
            ["what is covering the bed", 1]
        ],
        "org_questions": [
            ["What object is in the center of image", 2],
            ["How many people are there in the image", 1],
            ["what is on the bed", 1],
            ["what is the color of the bed", 2],
            ["where was the photo taken", -1],
            ["where is the blanket", -1],
            ["what is in the room", -1],
            ["what is covering the bed", 1]
        ],
        "context": [
            "a man in an orange shirt holding a bag of luggage.",
            "a tv sitting on top of a stand with a cartoon character on it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2374345",
                "VG_object_id": "727457",
                "bbox": [290, 31, 356, 201],
                "image": "data\\images\\2374345.jpg"
            },
            {
                "VG_image_id": "2356875",
                "VG_object_id": "2104823",
                "bbox": [117, 96, 340, 498],
                "image": "data\\images\\2356875.jpg"
            }
        ],
        "questions_with_scores": [
            ["where are the people", 2],
            ["where is the picture taken", 1],
            ["what color are the man's trousers", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["where are the people", 2],
            ["what color is the background", -1],
            ["What is man doing", -1],
            ["where is the picture taken", 1],
            ["what color are the man's trousers", 1],
            ["what is the man doing", -1],
            ["what is the man wearing on the head", -1],
            ["when was the photo taken", -1],
            ["what is in the background", 1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man standing in a boat filled with vegetables.",
            "a man in a pink shirt holding a frisbee."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2404930",
                "VG_object_id": "336481",
                "bbox": [387, 230, 442, 281],
                "image": "data\\images\\2404930.jpg"
            },
            {
                "VG_image_id": "2376987",
                "VG_object_id": "718924",
                "bbox": [174, 94, 277, 145],
                "image": "data\\images\\2376987.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the car", 1],
            ["How many cars are there", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["What is on the street", -1],
            ["What color is the car", 1],
            ["How many cars are there", 1],
            ["where is the car", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1],
            ["what is the car used for", -1],
            ["when was this taken", -1],
            ["what is in the background", -1],
            ["when was the photo taken", -1],
            ["where are the cars parked", -1],
            ["how many people are in the photo", 1]
        ],
        "context": [
            "a double decker bus driving down a street.",
            "a group of men walking down a sidewalk."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2317366",
                "VG_object_id": "3222965",
                "bbox": [378, 11, 462, 81],
                "image": "data\\images\\2317366.jpg"
            },
            {
                "VG_image_id": "2394609",
                "VG_object_id": "1211220",
                "bbox": [224, 53, 275, 112],
                "image": "data\\images\\2394609.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["where is the photo taken", -1],
            ["how many people are there", -1],
            ["what is the man wearing on his head", -1],
            ["who is wearing the shirt", -1],
            ["when was the picture taken", -1],
            ["what is the person wearing", -1],
            ["what is the man doing", 1],
            ["where is the man", -1]
        ],
        "context": [
            "a motorcycle on display at a museum.",
            "a man riding a skateboard on top of a wooden ramp."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316823",
                "VG_object_id": "3108153",
                "bbox": [225, 72, 302, 252],
                "image": "data\\images\\2316823.jpg"
            },
            {
                "VG_image_id": "2315937",
                "VG_object_id": "3231914",
                "bbox": [96, 99, 323, 265],
                "image": "data\\images\\2315937.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is on the man's head", 2],
            ["what is the color of the man's pants", 1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["what is the man wearing", 1],
            ["where is the picture taken", 1],
            ["what is the persion riding", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is the color of the man's pants", 1],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["what is the man standing on", 1],
            ["what is the man wearing", 1],
            ["where is the picture taken", 1],
            ["who is in the picture", -1],
            ["what is on the man's head", 2],
            ["what is the persion riding", 1],
            ["what is the man on", 1]
        ],
        "context": [
            "a man riding a horse on the beach at sunset.",
            "a man surfing in the ocean"
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2382567",
                "VG_object_id": "1327818",
                "bbox": [2, 101, 253, 222],
                "image": "data\\images\\2382567.jpg"
            },
            {
                "VG_image_id": "2350676",
                "VG_object_id": "2263752",
                "bbox": [5, 103, 414, 281],
                "image": "data\\images\\2350676.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the plane in the middle", 1],
            ["how many planes are there on the ground", 1]
        ],
        "org_questions": [
            ["what color is the plane in the middle", 1],
            ["how many planes are there on the ground", 1],
            ["what color is the background", -1],
            ["what is behind the planes", -1],
            ["where is the airplane", -1],
            ["what is in the distance", -1],
            ["what color is the sky", -1],
            ["what color are the planes", -1],
            ["when was the photo taken", -1],
            ["what is on the ground", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a large airplane is parked on the runway.",
            "a group of airplanes are on the runway."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2360811",
                "VG_object_id": "2379525",
                "bbox": [1, 204, 370, 337],
                "image": "data\\images\\2360811.jpg"
            },
            {
                "VG_image_id": "2413814",
                "VG_object_id": "165730",
                "bbox": [17, 101, 473, 284],
                "image": "data\\images\\2413814.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the vehicle", 1],
            ["how many people are there", 1],
            ["where is the truck", 1]
        ],
        "org_questions": [
            ["what kind of vehicle is it", -1],
            ["what color is the ground", -1],
            ["what color is the vehicle", 1],
            ["how many people are there", 1],
            ["what time is it", -1],
            ["where is the truck", 1],
            ["what is at the back of the truck", -1],
            ["what is the persion holding", -1],
            ["what is the truck doing", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man in a truck driving down a street.",
            "a truck with a large mural of bicyclists on the side of it."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2350236",
                "VG_object_id": "1687163",
                "bbox": [0, 40, 432, 134],
                "image": "data\\images\\2350236.jpg"
            },
            {
                "VG_image_id": "2374055",
                "VG_object_id": "2873929",
                "bbox": [0, 64, 331, 498],
                "image": "data\\images\\2374055.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["what is the main color of the building", 1]
        ],
        "org_questions": [
            ["what color is the building", 2],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["what is the weather like", -1],
            ["What is in the center of image", -1],
            ["how many trees are there", -1],
            ["what is the main color of the building", 1],
            ["when was this picture taken", -1],
            ["where is the building", -1],
            ["what is the building made of", -1],
            ["what time is it", -1],
            ["what type of building is this", -1]
        ],
        "context": [
            "a group of sheep grazing in a field near a castle.",
            "a clock tower on top of a building."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2387436",
                "VG_object_id": "513350",
                "bbox": [21, 153, 499, 327],
                "image": "data\\images\\2387436.jpg"
            },
            {
                "VG_image_id": "2379948",
                "VG_object_id": "1352419",
                "bbox": [180, 160, 496, 367],
                "image": "data\\images\\2379948.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["What is on the land", 1],
            ["what is the weather like", 1],
            ["what color is the grass", 1]
        ],
        "org_questions": [
            ["What is on the land", 1],
            ["how many people are there", 2],
            ["where is this photo taken", -1],
            ["what is the weather like", 1],
            ["what pattern is the land", -1],
            ["what color is the grass", 1],
            ["what is the ground covered with", -1],
            ["what is the condition of the ground", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a woman sitting on a bench with an umbrella.",
            "two black bears sitting on the ground next to a tree."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2374435",
                "VG_object_id": "2441637",
                "bbox": [3, 160, 183, 317],
                "image": "data\\images\\2374435.jpg"
            },
            {
                "VG_image_id": "2359094",
                "VG_object_id": "796196",
                "bbox": [58, 240, 200, 344],
                "image": "data\\images\\2359094.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the couch", 2],
            ["how many pillows are there on the couch", 1],
            ["what is in front of the couch", 1],
            ["What is in the back of sofa", 1]
        ],
        "org_questions": [
            ["what color is the couch", 2],
            ["how many pillows are there on the couch", 1],
            ["what is in front of the couch", 1],
            ["what is the floor under the sofa made of", -1],
            ["how many people are there", -1],
            ["What is in the back of sofa", 1],
            ["where is the couch", -1],
            ["what is in the room", -1],
            ["what is next to the couch", -1],
            ["how many chairs are there", -1]
        ],
        "context": [
            "a living room with a couch, chair, and table.",
            "the living room of the villa"
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2326560",
                "VG_object_id": "982047",
                "bbox": [29, 342, 223, 430],
                "image": "data\\images\\2326560.jpg"
            },
            {
                "VG_image_id": "2343196",
                "VG_object_id": "2064634",
                "bbox": [7, 86, 320, 398],
                "image": "data\\images\\2343196.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["what is on the plate", 1],
            ["How many people are there", 1],
            ["what is the plate", 1],
            ["what is the plate made of", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["what is on the plate", 1],
            ["How many people are there", 1],
            ["what is the table made of", -1],
            ["what is the plate", 1],
            ["how many plates are on the table", -1],
            ["what is the plate made of", 1],
            ["where is the food", -1],
            ["what shape is the plate", -1],
            ["what is the plate sitting on", -1],
            ["what is next to the plate", 1]
        ],
        "context": [
            "a young boy is eating a piece of pizza.",
            "a banana on a plate with a floral pattern."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2367016",
                "VG_object_id": "2727618",
                "bbox": [0, 0, 499, 332],
                "image": "data\\images\\2367016.jpg"
            },
            {
                "VG_image_id": "2392957",
                "VG_object_id": "3825011",
                "bbox": [0, 0, 499, 368],
                "image": "data\\images\\2392957.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what is on the table", 1],
            ["how many people are there", -1],
            ["how many baskets are on the table", -1],
            ["How many tables are there", -1],
            ["when was the photo taken", -1],
            ["what is the table made out of", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a pair of scissors, a ruler, and a ruler.",
            "a cup of hot chocolate sitting on a plate next to a piece of bread."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2407742",
                "VG_object_id": "323424",
                "bbox": [99, 30, 389, 372],
                "image": "data\\images\\2407742.jpg"
            },
            {
                "VG_image_id": "2412133",
                "VG_object_id": "202148",
                "bbox": [1, 77, 181, 292],
                "image": "data\\images\\2412133.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what is in the woman's hand", 1],
            ["WHat color is woman's hair", 1],
            ["What is woman holding", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what is in the woman's hand", 1],
            ["WHat color is woman's hair", 1],
            ["what is on the woman's head", -1],
            ["what is the woman's posture", -1],
            ["What is woman holding", 1],
            ["where was this photo taken", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what are the people doing", 1],
            ["where is the woman sitting", -1]
        ],
        "context": [
            "a woman is sitting at a table with a drink and a drink.",
            "two women sitting at a table with a pizza on it."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2377429",
                "VG_object_id": "565647",
                "bbox": [208, 0, 303, 239],
                "image": "data\\images\\2377429.jpg"
            },
            {
                "VG_image_id": "2392636",
                "VG_object_id": "3825640",
                "bbox": [117, 59, 215, 171],
                "image": "data\\images\\2392636.jpg"
            }
        ],
        "questions_with_scores": [["what color is the curtain", 1]],
        "org_questions": [
            ["what color is the curtain", 1],
            ["what color is the wall", -1],
            ["when is this photo taken", -1],
            ["where is the curtain", -1],
            ["what is in the background", -1],
            ["where is the photo taken", -1],
            ["what room is this", -1]
        ],
        "context": [
            "a living room with a couch and a mirror",
            "a living room with a couch, a television and a fireplace."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2315626",
                "VG_object_id": "3624799",
                "bbox": [69, 79, 484, 486],
                "image": "data\\images\\2315626.jpg"
            },
            {
                "VG_image_id": "2417601",
                "VG_object_id": "2962186",
                "bbox": [90, 67, 391, 452],
                "image": "data\\images\\2417601.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are in the photo", 2],
            ["what is the child doing", 1],
            ["where is the child", 1],
            ["What color is child's shirt", 1],
            ["What is child holding", 1],
            ["what is the child holding", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the child doing", 1],
            ["where is the child", 1],
            ["how many children are in the photo", 2],
            ["What color is child's shirt", 1],
            ["What is child holding", 1],
            ["what is the child holding", 1],
            ["what color is the background", -1],
            ["who is in the picture", -1],
            ["what is in the background", 1],
            ["what is the boy wearing", -1]
        ],
        "context": [
            "a young girl holding a frisbee in her hand.",
            "a young boy brushing his teeth with a toothbrush."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2316066",
                "VG_object_id": "3364722",
                "bbox": [32, 24, 265, 272],
                "image": "data\\images\\2316066.jpg"
            },
            {
                "VG_image_id": "2378647",
                "VG_object_id": "2519688",
                "bbox": [7, 56, 310, 346],
                "image": "data\\images\\2378647.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the guy's shirt", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the guy's shirt", 1],
            ["how many people are there", 1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["what is the man wearing", -1],
            ["what is the person holding", -1],
            ["What is guy doing", -1],
            ["when was the photo taken", -1],
            ["who is skateboarding", -1],
            ["what is in the background", -1],
            ["who is in the air", -1],
            ["what is the persion riding", -1]
        ],
        "context": [
            "a man riding a skateboard on a ramp.",
            "a man riding a skateboard down a ramp."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2366964",
                "VG_object_id": "625029",
                "bbox": [104, 116, 451, 278],
                "image": "data\\images\\2366964.jpg"
            },
            {
                "VG_image_id": "2368283",
                "VG_object_id": "747081",
                "bbox": [50, 64, 298, 251],
                "image": "data\\images\\2368283.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the elephants", 2],
            ["What is on the elephant", 2],
            ["what is behind the elephants", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the elephants", 2],
            ["what is behind the elephants", 1],
            ["how many people are there", 1],
            ["What is on the elephant's head", -1],
            ["What is elephant doing", -1],
            ["what is the ground covered with", 1],
            ["What is on the elephant", 2],
            ["when was the picture taken", -1],
            ["what animal is in the picture", -1],
            ["what are the elephants standing on", -1],
            ["what kind of animal is this", -1]
        ],
        "context": [
            "two elephants standing next to each other in a zoo.",
            "two elephants are carrying a man on their backs."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2394878",
                "VG_object_id": "460110",
                "bbox": [208, 45, 406, 234],
                "image": "data\\images\\2394878.jpg"
            },
            {
                "VG_image_id": "2336374",
                "VG_object_id": "2543589",
                "bbox": [210, 89, 463, 301],
                "image": "data\\images\\2336374.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the buses", 2],
            ["WHat color is the sky", 2],
            ["how many buses are there", 1],
            ["what is the number of the bus", 1]
        ],
        "org_questions": [
            ["how many buses are there", 1],
            ["what color are the buses", 2],
            ["what is in front of the bus", -1],
            ["Where is the bus", -1],
            ["what is the bus doing", -1],
            ["what is the number of the bus", 1],
            ["WHat color is the sky", 2],
            ["when was the photo taken", -1],
            ["who is driving the bus", -1],
            ["what kind of vehicle is this", -1],
            ["what type of bus is this", -1],
            ["what is on the bus", -1]
        ],
        "context": [
            "a couple of double decker buses parked in a parking lot.",
            "a bus is driving down the street in a city."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2333518",
                "VG_object_id": "2768575",
                "bbox": [151, 26, 486, 366],
                "image": "data\\images\\2333518.jpg"
            },
            {
                "VG_image_id": "2328221",
                "VG_object_id": "2756837",
                "bbox": [178, 2, 355, 331],
                "image": "data\\images\\2328221.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["where was this photo taken", 1],
            ["what is the girl wearing", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["how many people are there", -1],
            ["what is the woman wearing on her head", -1],
            ["what is the woman's posture", -1],
            ["who is in the photo", -1],
            ["where was this photo taken", 1],
            ["what is the girl wearing", 1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "a woman holding a pan with an egg in it.",
            "a woman standing in front of a display of cupcakes"
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2394609",
                "VG_object_id": "1211237",
                "bbox": [107, 4, 158, 57],
                "image": "data\\images\\2394609.jpg"
            },
            {
                "VG_image_id": "2409465",
                "VG_object_id": "241158",
                "bbox": [211, 171, 361, 262],
                "image": "data\\images\\2409465.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the plant", 1],
            ["what is in the background", 1],
            ["what is the plant sitting on", 1],
            ["what is on the wall", 1],
            ["where is the photo taken", 1],
            ["what is next to the plant", 1]
        ],
        "org_questions": [
            ["where is the plant", 1],
            ["how many people are there", -1],
            ["what is in the background", 1],
            ["what color is the wall", -1],
            ["what is the plant sitting on", 1],
            ["what kind of plant is this", -1],
            ["what is on the wall", 1],
            ["where is the photo taken", 1],
            ["what is next to the plant", 1],
            ["how many plants are there", -1],
            ["what color are the leaves", -1],
            ["where are the leaves", -1]
        ],
        "context": [
            "a man riding a skateboard on top of a wooden ramp.",
            "a bathroom with a toilet, sink, and a plant."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2372412",
                "VG_object_id": "3734238",
                "bbox": [284, 42, 484, 245],
                "image": "data\\images\\2372412.jpg"
            },
            {
                "VG_image_id": "2335874",
                "VG_object_id": "2229338",
                "bbox": [316, 17, 498, 331],
                "image": "data\\images\\2335874.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man doing", 2],
            ["what is the facial expression of the man", 2],
            ["How many people are there", 1],
            ["What color is man's shirt", 1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is the persion on the right holding", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What is man doing", 2],
            ["What color is man's shirt", 1],
            ["where is the man", 1],
            ["What is the man wearing", -1],
            ["what is the facial expression of the man", 2],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion on the right holding", 1]
        ],
        "context": [
            "two men standing in a living room holding wii controllers.",
            "an elephant eats greens from a cage."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2345637",
                "VG_object_id": "2348460",
                "bbox": [214, 3, 494, 330],
                "image": "data\\images\\2345637.jpg"
            },
            {
                "VG_image_id": "2346588",
                "VG_object_id": "2073561",
                "bbox": [168, 125, 228, 355],
                "image": "data\\images\\2346588.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the lady wearing on face", 2],
            ["what is beside the lady", 1],
            ["what color is the lady's clothes", 1],
            ["what is the woman holding", 1],
            ["what is on the woman's face", 1],
            ["what is the woman looking at", 1]
        ],
        "org_questions": [
            ["what is the lady wearing on face", 2],
            ["what is beside the lady", 1],
            ["what color is the lady's clothes", 1],
            ["How many people are there", -1],
            ["where is the woman", -1],
            ["what is the woman holding", 1],
            ["who is in the photo", -1],
            ["what is on the woman's face", 1],
            ["what is the woman looking at", 1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a woman with a cat on her shoulder.",
            "a woman in a pink dress standing in front of a bed."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2409021",
                "VG_object_id": "250547",
                "bbox": [14, 90, 150, 499],
                "image": "data\\images\\2409021.jpg"
            },
            {
                "VG_image_id": "2416476",
                "VG_object_id": "2765061",
                "bbox": [24, 45, 336, 407],
                "image": "data\\images\\2416476.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["what color is the lady's clothes", 1],
            ["where is the woman", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["what is the woman doing", -1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what color is the lady's clothes", 1],
            ["where is the woman", 1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is on the woman's head", -1],
            ["where is the woman looking", -1]
        ],
        "context": [
            "a woman standing in a kitchen with a red can.",
            "a woman sitting at a table with a sandwich in her hands."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2368960",
                "VG_object_id": "1708754",
                "bbox": [313, 116, 472, 279],
                "image": "data\\images\\2368960.jpg"
            },
            {
                "VG_image_id": "2335223",
                "VG_object_id": "3489862",
                "bbox": [126, 47, 208, 144],
                "image": "data\\images\\2335223.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what time is it", 1],
            ["when was this photo taken", 1],
            ["what is in the distance", 1],
            ["what is the ground covered with", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his head", -1],
            ["what time is it", 1],
            ["who is wearing the shirt", -1],
            ["What is the gender of the person", -1],
            ["when was this photo taken", 1],
            ["what is the person wearing", -1],
            ["who is in the photo", -1],
            ["what is in the distance", 1],
            ["what gender is the person in the shirt", -1],
            ["what is the ground covered with", 1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a man riding a boat on a lake while holding a rope.",
            "a man riding a bike through a flooded street."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2375126",
                "VG_object_id": "2028048",
                "bbox": [147, 25, 388, 332],
                "image": "data\\images\\2375126.jpg"
            },
            {
                "VG_image_id": "2412512",
                "VG_object_id": "192817",
                "bbox": [251, 37, 397, 320],
                "image": "data\\images\\2412512.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what is the man wearing on his head", 1],
            ["what color is the player's pants", 1],
            ["what color is the players's shirt", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man wearing on his head", 1],
            ["what is the player doing", -1],
            ["what is the player holding", -1],
            ["what is the player wearing", -1],
            ["what color is the player's pants", 1],
            ["what color is the players's shirt", 1],
            ["how many people are there", -1],
            ["when was the picture taken", -1],
            ["who is playing tennis", -1],
            ["what sport is being played", -1],
            ["where is the man", -1]
        ],
        "context": [
            "a man swinging a tennis racket at a ball.",
            "a man playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2359181",
                "VG_object_id": "3490490",
                "bbox": [302, 219, 499, 499],
                "image": "data\\images\\2359181.jpg"
            },
            {
                "VG_image_id": "2403652",
                "VG_object_id": "1122765",
                "bbox": [266, 9, 358, 212],
                "image": "data\\images\\2403652.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the person", 1],
            ["what is in the distance", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["how many people are there", -1],
            ["what is the man doing", 1],
            ["where is the person", 1],
            ["what is in the distance", 1],
            ["what is the man wearing", 1],
            ["What is gender of person", -1],
            ["where is the man", 1],
            ["when was the photo taken", -1],
            ["who is wearing a hat", -1],
            ["what is the man holding", -1],
            ["what is on the man's head", 1],
            ["how many people are in the picture", -1],
            ["what is the persion holding", -1]
        ],
        "context": [
            "a person standing next to a bird on a pole.",
            "a man riding a skateboard down a street next to a bus."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2363869",
                "VG_object_id": "3743076",
                "bbox": [4, 22, 491, 330],
                "image": "data\\images\\2363869.jpg"
            },
            {
                "VG_image_id": "2416834",
                "VG_object_id": "3193823",
                "bbox": [42, 192, 497, 323],
                "image": "data\\images\\2416834.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of animal is on the land", 2],
            ["what color is the animal", 2],
            ["what is the land covered with", 1],
            ["What is land made of", 1],
            ["what is on the floor", 1]
        ],
        "org_questions": [
            ["what kind of animal is on the land", 2],
            ["what color is the animal", 2],
            ["what is the land covered with", 1],
            ["how many people are there on the land", -1],
            ["what is the weather like", -1],
            ["What is land made of", 1],
            ["what is on the floor", 1],
            ["when was the picture taken", -1],
            ["what are the animals doing", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a cow is walking down a dirt road.",
            "a baby elephant standing next to an adult elephant."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2413597",
                "VG_object_id": "170194",
                "bbox": [120, 28, 337, 332],
                "image": "data\\images\\2413597.jpg"
            },
            {
                "VG_image_id": "2345799",
                "VG_object_id": "904682",
                "bbox": [61, 102, 374, 500],
                "image": "data\\images\\2345799.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["what is the man wearing on his neck", -1],
            ["what is the person holding", 1],
            ["how many people are there in the picture", 2],
            ["what color is his shirt", -1],
            ["what is in the background", -1],
            ["what is the man doing", -1],
            ["where is the man standing", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man in a suit and tie holding a wine glass.",
            "a man in a suit holding two bottles of wine."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2384312",
                "VG_object_id": "3847020",
                "bbox": [0, 70, 351, 498],
                "image": "data\\images\\2384312.jpg"
            },
            {
                "VG_image_id": "2407196",
                "VG_object_id": "3810856",
                "bbox": [59, 69, 363, 331],
                "image": "data\\images\\2407196.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 1],
            ["what color is the man's hair", 1],
            ["what color is the man's tie", 1],
            ["where is the man", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the man wearing", 1],
            ["what color is the man's hair", 1],
            ["what color is the man's tie", 1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["what is the persion riding", -1],
            ["what is the persion wearing on his face", -1],
            ["who is in the photo", -1],
            ["what is in the background", 1],
            ["what is around the man's neck", -1]
        ],
        "context": [
            "a man with a tie and a tie.",
            "a man in a suit and tie talks on a cell phone."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2362938",
                "VG_object_id": "1832479",
                "bbox": [134, 1, 293, 369],
                "image": "data\\images\\2362938.jpg"
            },
            {
                "VG_image_id": "2329475",
                "VG_object_id": "3111652",
                "bbox": [113, 25, 217, 206],
                "image": "data\\images\\2329475.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's shirt", 2],
            ["what color is the child's trousers", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", 2],
            ["how many people are there in the picture", -1],
            ["what color is the child's trousers", 1],
            ["what is on the child's head", -1],
            ["where is the person", -1],
            ["what is the boy holding", -1],
            ["what is the child on", -1],
            ["what color is the background", -1],
            ["when was the photo taken", -1],
            ["why is the man in the air", -1],
            ["who is skateboarding", -1],
            ["what is the man wearing", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a skateboarder doing a trick on a rail.",
            "a boy doing a trick on a skateboard at a skate park."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2408996",
                "VG_object_id": "364026",
                "bbox": [1, 262, 332, 500],
                "image": "data\\images\\2408996.jpg"
            },
            {
                "VG_image_id": "2369664",
                "VG_object_id": "2147131",
                "bbox": [2, 67, 495, 332],
                "image": "data\\images\\2369664.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cows are there", 2],
            ["what color is the grass", 2]
        ],
        "org_questions": [
            ["how many cows are there", 2],
            ["what color is the grass", 2],
            ["what animal is on the field", -1],
            ["what is on the animal's neck", -1],
            ["what is in the background", -1],
            ["What is the background of image", -1],
            ["where was this photo taken", -1],
            ["where is the grass", -1],
            ["what are the cows standing on", -1],
            ["what is the cow doing", -1],
            ["where are the cows standing", -1]
        ],
        "context": [
            "a large brown cow standing in a field.",
            "a herd of cows grazing in a field."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2349721",
                "VG_object_id": "2454724",
                "bbox": [109, 128, 425, 178],
                "image": "data\\images\\2349721.jpg"
            },
            {
                "VG_image_id": "2337831",
                "VG_object_id": "2607680",
                "bbox": [188, 22, 291, 473],
                "image": "data\\images\\2337831.jpg"
            }
        ],
        "questions_with_scores": [
            ["what direction is the board face", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the board", -1],
            ["what direction is the board face", 1],
            ["what is in the distance", -1],
            ["how many people are there", 1],
            ["where is the board", -1],
            ["what kind of sport is the board used for", -1],
            ["when was the photo taken", -1],
            ["what is under the surfboard", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a surfboard is sitting on the beach.",
            "a man standing next to a surfboard on a beach."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2330531",
                "VG_object_id": "3134012",
                "bbox": [15, 158, 155, 346],
                "image": "data\\images\\2330531.jpg"
            },
            {
                "VG_image_id": "2399236",
                "VG_object_id": "1171037",
                "bbox": [246, 95, 350, 401],
                "image": "data\\images\\2399236.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the dog doing", 1],
            ["how many people are there", 1],
            ["what is the dog looking at", 1]
        ],
        "org_questions": [
            ["what is the dog doing", 1],
            ["where is the dog", -1],
            ["how many people are there", 1],
            ["what is on the dog's neck", -1],
            ["what type of animal is shown", -1],
            ["what is the dog looking at", 1]
        ],
        "context": [
            "a man holding a dog while looking at a cake with candles.",
            "a dog standing on its hind legs looking at a television."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2371548",
                "VG_object_id": "597042",
                "bbox": [35, 175, 384, 374],
                "image": "data\\images\\2371548.jpg"
            },
            {
                "VG_image_id": "2343780",
                "VG_object_id": "2677568",
                "bbox": [239, 0, 374, 263],
                "image": "data\\images\\2343780.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shelf", 1],
            ["what is on the left of the shelf", 1],
            ["what is next to the shelf", 1]
        ],
        "org_questions": [
            ["what color is the shelf", 1],
            ["what is on the left of the shelf", 1],
            ["Where is the photo taken", -1],
            ["what is the pattern of the wall", -1],
            ["what is the shelf made of", -1],
            ["What is in the shelf", -1],
            ["where is the shelf", -1],
            ["how many books are there", -1],
            ["what is on the floor", -1],
            ["what is next to the shelf", 1],
            ["what is on top of the bookshelf", -1]
        ],
        "context": [
            "a book shelf with books and a clock.",
            "a cat is watching a television on a couch."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2365226",
                "VG_object_id": "3885362",
                "bbox": [89, 42, 485, 298],
                "image": "data\\images\\2365226.jpg"
            },
            {
                "VG_image_id": "2324306",
                "VG_object_id": "3412337",
                "bbox": [130, 309, 385, 498],
                "image": "data\\images\\2324306.jpg"
            }
        ],
        "questions_with_scores": [["How many beds are there", 1]],
        "org_questions": [
            ["what color is the bed", -1],
            ["what color is the floor", -1],
            ["what color is the wall", -1],
            ["How many beds are there", 1],
            ["where is this bed", -1],
            ["what is on the bed", -1],
            ["how many people are there on the bed", -1],
            ["what room is this", -1],
            ["what is the floor made of", -1],
            ["what is next to the bed", -1],
            ["what is under the bed", -1]
        ],
        "context": [
            "a bedroom with bunk beds and a ladder.",
            "a bed with a baby's bed in a room."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2379129",
                "VG_object_id": "554807",
                "bbox": [54, 178, 390, 457],
                "image": "data\\images\\2379129.jpg"
            },
            {
                "VG_image_id": "2387343",
                "VG_object_id": "1276832",
                "bbox": [43, 55, 496, 365],
                "image": "data\\images\\2387343.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the food in the middle of the plate", 1],
            ["what color is the table", 1],
            ["how many glasses are there on the table", 1]
        ],
        "org_questions": [
            ["what is the food in the middle of the plate", 1],
            ["what color is the table", 1],
            ["how many glasses are there on the table", 1],
            ["what shape of plate is the food put on", -1],
            ["where is the food on", -1],
            ["What is the plate placed on", -1],
            ["what is the table made of", -1],
            ["what is next to the plate", -1],
            ["what shape is the plate", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a plate of food with a sandwich and some fries.",
            "a plate of food with a cup of coffee."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2378735",
                "VG_object_id": "1365565",
                "bbox": [4, 318, 374, 425],
                "image": "data\\images\\2378735.jpg"
            },
            {
                "VG_image_id": "2404957",
                "VG_object_id": "336124",
                "bbox": [13, 199, 500, 327],
                "image": "data\\images\\2404957.jpg"
            }
        ],
        "questions_with_scores": [
            ["What animals are they", 2],
            ["What color is the ground", 1]
        ],
        "org_questions": [
            ["What color is the ground", 1],
            ["What animals are they", 2],
            ["How many people are there", -1],
            ["what is the land made of", -1],
            ["What is the weather like", -1],
            ["What are on the ground", -1],
            ["where are the rocks", -1],
            ["how is the ground", -1],
            ["what is around the ground", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "two giraffes standing next to a white building.",
            "two zebras standing in a fenced in area."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2365841",
                "VG_object_id": "3882247",
                "bbox": [1, 7, 329, 498],
                "image": "data\\images\\2365841.jpg"
            },
            {
                "VG_image_id": "2356030",
                "VG_object_id": "824301",
                "bbox": [185, 26, 500, 259],
                "image": "data\\images\\2356030.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the roof of the building", 1],
            ["how many floors does the building have", 1],
            ["what is in front of the building", 1],
            ["what shape is the roof of the building", 1],
            ["How many people are there", 1],
            ["what color is the wall", 1],
            ["who is in the photo", 1],
            ["what is on the top of the building", 1]
        ],
        "org_questions": [
            ["what color is the roof of the building", 1],
            ["how many floors does the building have", 1],
            ["what is in front of the building", 1],
            ["what shape is the roof of the building", 1],
            ["where is the photo taken", -1],
            ["what is the building made of", -1],
            ["How many people are there", 1],
            ["what color is the wall", 1],
            ["who is in the photo", 1],
            ["when was the picture taken", -1],
            ["what is on the top of the building", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a large building with a clock tower on top.",
            "a couple of people sitting on a fountain."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2386029",
                "VG_object_id": "1290222",
                "bbox": [419, 62, 475, 217],
                "image": "data\\images\\2386029.jpg"
            },
            {
                "VG_image_id": "2350626",
                "VG_object_id": "866599",
                "bbox": [108, 86, 219, 198],
                "image": "data\\images\\2350626.jpg"
            }
        ],
        "questions_with_scores": [["what color is the curtain", 2]],
        "org_questions": [
            ["what color is the curtain", 2],
            ["where is the photo taken", -1],
            ["how many people are there", -1],
            ["What is in front of the curtain", -1],
            ["what room is it", -1],
            ["where is the curtain", -1],
            ["when was the picture taken", -1],
            ["what is covering the window", -1],
            ["what is hanging from the window", -1],
            ["what is above the window", -1],
            ["what is behind the window", -1]
        ],
        "context": [
            "a bed with a brown headboard and pillows.",
            "a living room with a couch and a table."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2324946",
                "VG_object_id": "3278644",
                "bbox": [201, 37, 295, 324],
                "image": "data\\images\\2324946.jpg"
            },
            {
                "VG_image_id": "2323633",
                "VG_object_id": "3113238",
                "bbox": [239, 181, 324, 290],
                "image": "data\\images\\2323633.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color are the man's trousers", 2],
            ["how many dogs are there in the picture", 1],
            ["what is the man doing", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color are the man's trousers", 2],
            ["how many dogs are there in the picture", 1],
            ["what is the man doing", 1],
            ["where is the person", -1],
            ["what is the weather like", -1],
            ["what color is the ground", -1],
            ["when was the picture taken", -1],
            ["what is on the man's head", 1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a dog jumping up to catch a frisbee.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2403591",
                "VG_object_id": "1123327",
                "bbox": [4, 0, 95, 246],
                "image": "data\\images\\2403591.jpg"
            },
            {
                "VG_image_id": "2358069",
                "VG_object_id": "2509560",
                "bbox": [72, 182, 252, 248],
                "image": "data\\images\\2358069.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the side of the car", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the car", -1],
            ["where is the car", -1],
            ["what is on the side of the car", 1],
            ["how many cars are there", -1],
            ["what time is it", -1],
            ["what is the vehicle on", -1],
            ["what type of vehicle is shown", -1],
            ["when was the photo taken", -1],
            ["where was the picture taken", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a police officer riding a motorcycle down a street.",
            "a white car is parked in a field"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2351181",
                "VG_object_id": "862540",
                "bbox": [101, 61, 317, 292],
                "image": "data\\images\\2351181.jpg"
            },
            {
                "VG_image_id": "2369478",
                "VG_object_id": "2474662",
                "bbox": [2, 2, 448, 374],
                "image": "data\\images\\2369478.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what is the man doing", 2],
            ["what is the man wearing on his face", 1],
            ["where is the photo taken", 1],
            ["What color is man's shirt", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what is the man holding", 2],
            ["what is the man wearing on his face", 1],
            ["what is the man doing", 2],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["What color is man's shirt", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["who is in the photo", -1],
            ["what is behind the man", 1],
            ["what is the man standing on", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man in a striped suit taking a selfie in a mirror.",
            "a man eating a donut in a store."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2329009",
                "VG_object_id": "2948947",
                "bbox": [2, 2, 499, 252],
                "image": "data\\images\\2329009.jpg"
            },
            {
                "VG_image_id": "2368030",
                "VG_object_id": "748734",
                "bbox": [3, 4, 497, 330],
                "image": "data\\images\\2368030.jpg"
            }
        ],
        "questions_with_scores": [["how many people are on the field", 2]],
        "org_questions": [
            ["how many people are on the field", 2],
            ["what is the man doing", -1],
            ["what is the color of the grass", -1],
            ["what are the people doing on the field", -1],
            ["what color are the shoes of the person on the right", -1],
            ["where was this taken", -1],
            ["when was the photo taken", -1],
            ["what game is being played", -1],
            ["when was this picture taken", -1],
            ["what sport is this", -1]
        ],
        "context": [
            "a group of baseball players walking on a field.",
            "a baseball pitcher throwing a ball on a field."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2319200",
                "VG_object_id": "1003822",
                "bbox": [6, 157, 493, 373],
                "image": "data\\images\\2319200.jpg"
            },
            {
                "VG_image_id": "2381709",
                "VG_object_id": "702568",
                "bbox": [8, 324, 331, 499],
                "image": "data\\images\\2381709.jpg"
            }
        ],
        "questions_with_scores": [
            ["What animal are there", 2],
            ["where is the photo taken", 1],
            ["what color is the ground", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 1],
            ["what color is the ground", 1],
            ["What animal are there", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["when is this picture taken", -1],
            ["how is the weather", -1],
            ["what is in the background", 1],
            ["what is on the ground", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a horse standing on a beach with a man standing near the water.",
            "a giraffe standing in a field next to a tree."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2348093",
                "VG_object_id": "884887",
                "bbox": [74, 161, 144, 321],
                "image": "data\\images\\2348093.jpg"
            },
            {
                "VG_image_id": "2357159",
                "VG_object_id": "2018109",
                "bbox": [255, 376, 294, 454],
                "image": "data\\images\\2357159.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the man doing", 1],
            ["What is the man holding", 1],
            ["what is on the person's head", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["What color is the man's coat", -1],
            ["What is the man doing", 1],
            ["What is the man holding", 1],
            ["where is the person", -1],
            ["what is the person wearing on the head", -1],
            ["what is in the distance", -1],
            ["what is on the person's head", 1],
            ["how many people are there", 1],
            ["who is in the picture", -1],
            ["what is the man walking on", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man walking down a street holding an umbrella.",
            "a clock tower in a city square with people walking around."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2398893",
                "VG_object_id": "1174554",
                "bbox": [309, 159, 383, 200],
                "image": "data\\images\\2398893.jpg"
            },
            {
                "VG_image_id": "2333286",
                "VG_object_id": "3686085",
                "bbox": [330, 204, 485, 254],
                "image": "data\\images\\2333286.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the motorcycle", 1],
            ["How many people are sitting on the motorcycle", 1],
            ["What is next to the seat", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["What color is the motorcycle", 1],
            ["How many people are sitting on the motorcycle", 1],
            ["What is next to the seat", 1],
            ["how many motorcycles are there", -1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["what color is the seat of the motorcycle", -1]
        ],
        "context": [
            "a blue and silver motorcycle is parked in a building.",
            "a woman sitting on a motorcycle with a kite on the ground."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2349366",
                "VG_object_id": "875304",
                "bbox": [115, 43, 342, 499],
                "image": "data\\images\\2349366.jpg"
            },
            {
                "VG_image_id": "2344534",
                "VG_object_id": "915051",
                "bbox": [28, 39, 227, 496],
                "image": "data\\images\\2344534.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what color is the woman's shirt", 1],
            ["what is woman doing", 1],
            ["what is the persion sitting on", 1],
            ["how many people are there", 1],
            ["what is in the woman's hand", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what is woman doing", 1],
            ["what is the woman wearing on the head", -1],
            ["what is the persion sitting on", 1],
            ["what is the woman holding", 2],
            ["what is the woman wearing", -1],
            ["how many people are there", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["where is the woman", -1],
            ["what is in the woman's hand", 1]
        ],
        "context": [
            "a woman holding a tennis racket on a court.",
            "a woman holding two tennis balls on a tennis court."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2395860",
                "VG_object_id": "1202601",
                "bbox": [2, 225, 497, 375],
                "image": "data\\images\\2395860.jpg"
            },
            {
                "VG_image_id": "2376394",
                "VG_object_id": "573251",
                "bbox": [310, 214, 497, 331],
                "image": "data\\images\\2376394.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what color is the horse", 1],
            ["what is behind the horse", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what color is the horse", 1],
            ["what kind of animal is on the land", -1],
            ["how many sheep are there on the land", -1],
            ["where is the land", -1],
            ["what is on the floor", -1],
            ["how many people are there in the picture", -1],
            ["what is the horse doing", -1],
            ["what is the ground covered with", -1],
            ["what is behind the horse", 1],
            ["what is the horse standing on", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a horse standing in a field next to some trees.",
            "a horse is standing in a fenced in area."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2324739",
                "VG_object_id": "2783789",
                "bbox": [63, 42, 366, 244],
                "image": "data\\images\\2324739.jpg"
            },
            {
                "VG_image_id": "2379553",
                "VG_object_id": "711939",
                "bbox": [151, 151, 234, 306],
                "image": "data\\images\\2379553.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2]
        ],
        "org_questions": [
            ["what is the man wearing", -1],
            ["what is the gesture of the man", -1],
            ["how many people are there in the photo", 2],
            ["what is the ground covered with", -1],
            ["Where is the man", -1],
            ["what is in the background", -1],
            ["What is man doing", -1],
            ["when was the photo taken", -1],
            ["who is surfing", -1],
            ["what is the man on", -1]
        ],
        "context": [
            "a man riding a surfboard on top of a wave.",
            "two surfers riding the waves in the ocean."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2345381",
                "VG_object_id": "908584",
                "bbox": [54, 112, 120, 298],
                "image": "data\\images\\2345381.jpg"
            },
            {
                "VG_image_id": "2370802",
                "VG_object_id": "1868380",
                "bbox": [133, 313, 245, 498],
                "image": "data\\images\\2370802.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the background", 2],
            ["when was the photo taken", 2],
            ["what is the man on", 1],
            ["who is wearing a black shirt", 1],
            ["what is the persion wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the background", 2],
            ["what are the people doing", -1],
            ["what animal is the man with", -1],
            ["what is on the man's head", -1],
            ["where is the man", -1],
            ["what is the man on", 1],
            ["how many people are in the photo", -1],
            ["when was the photo taken", 2],
            ["who is wearing a black shirt", 1],
            ["what is the persion wearing", 1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a group of people standing outside of a food truck.",
            "a protester holds a cross and a cross during a protest."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2396839",
                "VG_object_id": "1194618",
                "bbox": [180, 74, 350, 350],
                "image": "data\\images\\2396839.jpg"
            },
            {
                "VG_image_id": "2355027",
                "VG_object_id": "1753084",
                "bbox": [25, 38, 110, 258],
                "image": "data\\images\\2355027.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is child doing", 2],
            ["What color is child shirt", 1],
            ["what are the child playing", 1],
            ["what is the girl doing", 1],
            ["how many people are there", 1],
            ["who is on the water", 1]
        ],
        "org_questions": [
            ["what is child doing", 2],
            ["What color is child shirt", 1],
            ["Where is child", -1],
            ["what are the child playing", 1],
            ["where is the photo taken", -1],
            ["what is the girl doing", 1],
            ["how many people are there", 1],
            ["who is on the water", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a young boy surfing in the ocean.",
            "a boat is docked at a pier with a flag on the front."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "713143",
                "VG_object_id": "1579540",
                "bbox": [463, 637, 695, 765],
                "image": "data\\images\\713143.jpg"
            },
            {
                "VG_image_id": "2413467",
                "VG_object_id": "173043",
                "bbox": [5, 196, 139, 477],
                "image": "data\\images\\2413467.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cabinet", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", 1],
            ["where is the cabinet", -1],
            ["what is hanging on the cabinet", -1],
            ["what room is this", 1],
            ["what are the cabinets made of", -1],
            ["where was this picture taken", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a kitchen with a sink and a refrigerator.",
            "a white cabinet with a silver handle and a white cabinet"
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2348487",
                "VG_object_id": "3605259",
                "bbox": [20, 200, 78, 261],
                "image": "data\\images\\2348487.jpg"
            },
            {
                "VG_image_id": "2404353",
                "VG_object_id": "1116443",
                "bbox": [282, 173, 343, 222],
                "image": "data\\images\\2404353.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bag", 2],
            ["what is the bag placed on", 1],
            ["what is the ground covered with", 1],
            ["where is the photo taken", 1],
            ["what are the people doing", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the bag", 2],
            ["what is the bag placed on", 1],
            ["what is the ground covered with", 1],
            ["where is the photo taken", 1],
            ["what are the people doing", 1],
            ["what is the weather like", -1],
            ["what is in the background", 1],
            ["where is the bag", -1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a man standing on a picnic table eating a sandwich.",
            "a woman riding a bike down a street."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2363566",
                "VG_object_id": "763820",
                "bbox": [282, 37, 439, 219],
                "image": "data\\images\\2363566.jpg"
            },
            {
                "VG_image_id": "2374091",
                "VG_object_id": "2255994",
                "bbox": [96, 17, 186, 212],
                "image": "data\\images\\2374091.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 2],
            ["what is the girl holding", 2],
            ["what color is the girl's shirt", 1],
            ["how many people are there", 1],
            ["Where is the girl", 1],
            ["where is the picture taken", 1],
            ["what is the main color of the woman's hair", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 2],
            ["what color is the girl's shirt", 1],
            ["what is the girl holding", 2],
            ["how many people are there", 1],
            ["Where is the girl", 1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1],
            ["where is the picture taken", 1],
            ["what is the main color of the woman's hair", 1]
        ],
        "context": [
            "a boy and girl eating pizza at a table.",
            "a bathroom sink with a large mirror and a girl taking a picture."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2335073",
                "VG_object_id": "3211043",
                "bbox": [2, 300, 494, 342],
                "image": "data\\images\\2335073.jpg"
            },
            {
                "VG_image_id": "2323661",
                "VG_object_id": "2959411",
                "bbox": [0, 4, 499, 372],
                "image": "data\\images\\2323661.jpg"
            }
        ],
        "questions_with_scores": [["what color is the table", 1]],
        "org_questions": [
            ["what color is the table", 1],
            ["what is the table made of", -1],
            ["what is placed on the table", -1],
            ["how many people are there", -1],
            ["where is the table", -1],
            ["how many bottles are there on the table", -1],
            ["what is on the table", -1],
            ["who is in the photo", -1],
            ["what kind of table is this", -1]
        ],
        "context": [
            "a sandwich with meat, cheese, and vegetables on a plate.",
            "a chocolate donut and a coffee cup on a table."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2321773",
                "VG_object_id": "3194432",
                "bbox": [209, 173, 324, 313],
                "image": "data\\images\\2321773.jpg"
            },
            {
                "VG_image_id": "2319385",
                "VG_object_id": "3512428",
                "bbox": [279, 73, 422, 260],
                "image": "data\\images\\2319385.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["where is the picture taken", 2],
            ["what color is the girl's clothes", 1],
            ["what is the girl doing", 1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what color is the girl's clothes", 1],
            ["what color is the ground", 2],
            ["what is the girl doing", 1],
            ["how many people are there", -1],
            ["what is the person wearing on his head", -1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["who is in the photo", -1],
            ["what sport is being played", 1],
            ["what is the persion wearing", -1],
            ["where is the picture taken", 2]
        ],
        "context": [
            "a group of young people playing a game of soccer.",
            "a man riding skis down a snow covered slope."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2333797",
                "VG_object_id": "2870157",
                "bbox": [0, 261, 280, 370],
                "image": "data\\images\\2333797.jpg"
            },
            {
                "VG_image_id": "2346985",
                "VG_object_id": "3613899",
                "bbox": [273, 208, 483, 330],
                "image": "data\\images\\2346985.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is on the land", -1],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["where is the land", -1],
            ["what is the ground covered with", 1],
            ["where was this picture taken", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man is doing a trick on a skateboard.",
            "a person on a motorcycle on a race track."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2368860",
                "VG_object_id": "2050530",
                "bbox": [205, 125, 336, 307],
                "image": "data\\images\\2368860.jpg"
            },
            {
                "VG_image_id": "2410379",
                "VG_object_id": "219021",
                "bbox": [204, 6, 307, 374],
                "image": "data\\images\\2410379.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what is on the man's head", 2],
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what is the player doing", 1],
            ["what is the player wearing", 1],
            ["how many players are there in the photo", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what is the man holding", 2],
            ["what is on the man's head", 2],
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what is the player doing", 1],
            ["what is the player wearing", 1],
            ["how many players are there in the photo", 1],
            ["when was the photo taken", -1],
            ["what sport is being played", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a man in a red shirt and white shorts holding a tennis racket."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2337414",
                "VG_object_id": "957803",
                "bbox": [402, 134, 499, 243],
                "image": "data\\images\\2337414.jpg"
            },
            {
                "VG_image_id": "2407488",
                "VG_object_id": "278597",
                "bbox": [276, 202, 375, 288],
                "image": "data\\images\\2407488.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the pillow", 2],
            ["What is the pillow on", 1],
            ["How many people are there", 1],
            ["where is the pillow placed on", 1],
            ["where is the pillow", 1],
            ["where are the pillows", 1]
        ],
        "org_questions": [
            ["What color is the pillow", 2],
            ["What is the pillow on", 1],
            ["How many people are there", 1],
            ["where is the photo taken", -1],
            ["where is the pillow placed on", 1],
            ["where is the pillow", 1],
            ["how many pillow are there", -1],
            ["where are the pillows", 1]
        ],
        "context": [
            "a man laying in bed with two dogs.",
            "a living room with a green couch and a green couch."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2385270",
                "VG_object_id": "688921",
                "bbox": [157, 28, 343, 333],
                "image": "data\\images\\2385270.jpg"
            },
            {
                "VG_image_id": "2366894",
                "VG_object_id": "2016037",
                "bbox": [265, 3, 372, 238],
                "image": "data\\images\\2366894.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman doing", 2],
            ["where is the woman", 2],
            ["what color is woman's shirt", 1],
            ["what kind of animal is in the picture", 1],
            ["what color is the background", 1],
            ["what is the woman holding", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["what color is woman's shirt", 1],
            ["What is woman doing", 2],
            ["what kind of animal is in the picture", 1],
            ["what is the woman wearing on her face", -1],
            ["where is the woman", 2],
            ["what color is the background", 1],
            ["what is the woman holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", 1]
        ],
        "context": [
            "a woman walking down a path with a camera.",
            "a woman riding a horse on the beach."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2391398",
                "VG_object_id": "3827089",
                "bbox": [27, 243, 317, 373],
                "image": "data\\images\\2391398.jpg"
            },
            {
                "VG_image_id": "2354723",
                "VG_object_id": "3776371",
                "bbox": [0, 183, 496, 330],
                "image": "data\\images\\2354723.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["Where is the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["how many people are there", 1],
            ["what is the persion doing", -1],
            ["what is in the distance", -1],
            ["where is the picture taken", -1],
            ["what is the land made of", -1],
            ["what is on the land", -1],
            ["Where is the photo taken", 1],
            ["how many giraffes are in the picture", -1],
            ["where is the land", -1],
            ["how many people are on the floor", -1],
            ["how is the weather", -1],
            ["when was the picture taken", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a bench sitting in the grass near a body of water.",
            "a herd of cows laying on top of a lush green field."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2377944",
                "VG_object_id": "715697",
                "bbox": [17, 30, 340, 296],
                "image": "data\\images\\2377944.jpg"
            },
            {
                "VG_image_id": "2386449",
                "VG_object_id": "682101",
                "bbox": [34, 51, 227, 257],
                "image": "data\\images\\2386449.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 1],
            ["what is the dog wearing", 1],
            ["where is the dog", 1],
            ["what is the ground the dog standing on made of", 1],
            ["what is the dog doing", 1],
            ["What is the background of image", 1],
            ["What is dog looking", 1],
            ["what is behind the dog", 1],
            ["what is the dog on", 1]
        ],
        "org_questions": [
            ["what color is the dog", 1],
            ["what is the dog wearing", 1],
            ["where is the dog", 1],
            ["how many people are there", -1],
            ["what is the ground the dog standing on made of", 1],
            ["what is the dog doing", 1],
            ["What is the background of image", 1],
            ["What is dog looking", 1],
            ["what type of animal is shown", -1],
            ["what is on the dog's face", -1],
            ["what is behind the dog", 1],
            ["what is the dog on", 1]
        ],
        "context": [
            "a dog wearing a life jacket on a surfboard.",
            "a dog laying on the floor next to a teddy bear."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2342764",
                "VG_object_id": "931005",
                "bbox": [94, 40, 357, 497],
                "image": "data\\images\\2342764.jpg"
            },
            {
                "VG_image_id": "2366899",
                "VG_object_id": "2494659",
                "bbox": [265, 115, 497, 317],
                "image": "data\\images\\2366899.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man wearing", 2],
            ["where is the photo taken", 1],
            ["what is on the wall", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is the man wearing", 2],
            ["what is the man wearing on his face", -1],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["what is the man holding ", -1],
            ["what is the man wearing on head", -1],
            ["who is in the picture", -1],
            ["what is in the background", -1],
            ["what is on the wall", 1],
            ["where are the people", 1]
        ],
        "context": [
            "a man and woman shaking hands.",
            "person and artist perform at festival."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2343973",
                "VG_object_id": "2469592",
                "bbox": [56, 168, 257, 449],
                "image": "data\\images\\2343973.jpg"
            },
            {
                "VG_image_id": "2351556",
                "VG_object_id": "2326585",
                "bbox": [216, 193, 295, 254],
                "image": "data\\images\\2351556.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["what is the girl holding", 1],
            ["what is on the girl's head", 1],
            ["what sport is being played", 1],
            ["what is the girl playing", 1],
            ["what color is the girl's shirt", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["where is the girl", -1],
            ["what is the girl holding", 1],
            ["how many people are there", -1],
            ["who has a long hair", -1],
            ["what color is the floor", -1],
            ["what is the weather like", -1],
            ["what is on the girl's head", 1],
            ["when was the photo taken", -1],
            ["what sport is being played", 1],
            ["what is the girl playing", 1],
            ["what color is the girl's shirt", 1]
        ],
        "context": [
            "a woman swinging a tennis racket on a tennis court.",
            "a group of people playing frisbee in a park."
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2341175",
                "VG_object_id": "3656690",
                "bbox": [5, 2, 498, 182],
                "image": "data\\images\\2341175.jpg"
            },
            {
                "VG_image_id": "2367424",
                "VG_object_id": "751666",
                "bbox": [2, 1, 499, 375],
                "image": "data\\images\\2367424.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 2],
            ["how many people are there", 2],
            ["what is on the wall", 1],
            ["how many sofas are there", 1],
            ["what is in the room", 1]
        ],
        "org_questions": [
            ["what color is the wall", 2],
            ["what is on the wall", 1],
            ["what color is the table", -1],
            ["how many people are there", 2],
            ["where is the picture taken", -1],
            ["what is the floor made of", -1],
            ["where are the windows", -1],
            ["how many sofas are there", 1],
            ["when was the photo taken", -1],
            ["what room is this", -1],
            ["what is in the room", 1]
        ],
        "context": [
            "a living room with a couch, chair, table and mirror.",
            "a group of people standing around a table with glasses."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2342764",
                "VG_object_id": "931010",
                "bbox": [4, 320, 141, 500],
                "image": "data\\images\\2342764.jpg"
            },
            {
                "VG_image_id": "2412132",
                "VG_object_id": "202200",
                "bbox": [71, 174, 183, 407],
                "image": "data\\images\\2412132.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the gender of the person in the trousers", 1],
            ["how many people are there", 1],
            ["what is the persion doing", 1],
            ["who is wearing the trousers", 1],
            ["what is the man doing", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the trousers", -1],
            ["what is the gender of the person in the trousers", 1],
            ["how many people are there", 1],
            ["what is the persion wearing on the head", -1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", -1],
            ["who is wearing the trousers", 1],
            ["where is the trousers", -1],
            ["what is the man doing", 1],
            ["What is man holding", -1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a man and woman shaking hands.",
            "two women walking down the street in the sun"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2417133",
                "VG_object_id": "2996508",
                "bbox": [18, 44, 153, 263],
                "image": "data\\images\\2417133.jpg"
            },
            {
                "VG_image_id": "2323214",
                "VG_object_id": "3105878",
                "bbox": [146, 71, 300, 499],
                "image": "data\\images\\2323214.jpg"
            }
        ],
        "questions_with_scores": [["what color is the wall", 2]],
        "org_questions": [
            ["what color is the wall", 2],
            ["how many people are there", -1],
            ["what is the man wearing", -1],
            ["What is man doing", -1],
            ["who is in the photo", -1],
            ["what is behind the man", -1],
            ["what is the man holding", -1],
            ["where was this photo taken", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "two men standing in a living room.",
            "a man and a woman playing a video game."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2369527",
                "VG_object_id": "3862981",
                "bbox": [108, 111, 248, 311],
                "image": "data\\images\\2369527.jpg"
            },
            {
                "VG_image_id": "2360638",
                "VG_object_id": "1965141",
                "bbox": [82, 30, 304, 432],
                "image": "data\\images\\2360638.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is person's shirt", 2],
            ["Where are people", 1],
            ["what is on the person's head", 1],
            ["what is the man doing", 1],
            ["what  is the person holding", 1],
            ["where is the person", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is person's shirt", 2],
            ["Where are people", 1],
            ["what is on the person's head", 1],
            ["what is the man doing", 1],
            ["what  is the person holding", 1],
            ["what is the man wearing", -1],
            ["where is the person", 1],
            ["what is the gender of the person in the picture", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what kind of shoes is the man wearing", -1]
        ],
        "context": [
            "a group of people playing frisbee in a field.",
            "a man swinging a tennis racket on a court."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2404699",
                "VG_object_id": "339221",
                "bbox": [3, 257, 498, 375],
                "image": "data\\images\\2404699.jpg"
            },
            {
                "VG_image_id": "2326030",
                "VG_object_id": "3033718",
                "bbox": [2, 232, 365, 372],
                "image": "data\\images\\2326030.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 2],
            ["what is on the floor", 1],
            ["What is the pattern of the floor", 1],
            ["what is covering the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is on the floor", 1],
            ["what is on the table", 2],
            ["how many luggages are there in the picture", -1],
            ["Where is the picture taken", -1],
            ["What is the pattern of the floor", 1],
            ["what is covering the floor", 1]
        ],
        "context": [
            "a desk with three computers and a monitor on it.",
            "a living room with a couch, coffee table, and a lamp."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2354160",
                "VG_object_id": "840613",
                "bbox": [4, 88, 319, 238],
                "image": "data\\images\\2354160.jpg"
            },
            {
                "VG_image_id": "2386861",
                "VG_object_id": "1281717",
                "bbox": [1, 157, 499, 231],
                "image": "data\\images\\2386861.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["what color is the land", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is on the ground", 1],
            ["How many people are there", 2],
            ["what is the land made of", -1],
            ["WHat is on the background of image", -1],
            ["where was this photo taken", -1],
            ["how is the weather", -1],
            ["what is next to the road", -1]
        ],
        "context": [
            "a truck driving down a road next to a field.",
            "a young man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2331164",
                "VG_object_id": "3120796",
                "bbox": [1, 245, 498, 327],
                "image": "data\\images\\2331164.jpg"
            },
            {
                "VG_image_id": "2323392",
                "VG_object_id": "3349210",
                "bbox": [3, 227, 371, 308],
                "image": "data\\images\\2323392.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cows are on the street", 2],
            ["how many people are on the street", 2],
            ["what is the weather like", 1]
        ],
        "org_questions": [
            ["how many cows are on the street", 2],
            ["how many people are on the street", 2],
            ["what color is the street", -1],
            ["what is the road made of", -1],
            ["what is the weather like", 1],
            ["what is on the ground", -1],
            ["where was this picture taken", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a couple of cows walking down a street.",
            "a woman holding a sign on a city street."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2380894",
                "VG_object_id": "543171",
                "bbox": [1, 95, 112, 368],
                "image": "data\\images\\2380894.jpg"
            },
            {
                "VG_image_id": "2399796",
                "VG_object_id": "1166194",
                "bbox": [284, 118, 373, 208],
                "image": "data\\images\\2399796.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["how many people are there in the picture", 2],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["what color is the shirt", 1],
            ["who is wearing a black shirt", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["how many faces are there in the picture", -1],
            ["what color is the shirt", 1],
            ["who is wearing a black shirt", 1],
            ["what kind of pants is the man wearing", -1],
            ["what is on the man's face", 1],
            ["what type of shirt is the man wearing", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man is preparing a meal at a buffet.",
            "a group of men standing around a machine."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "713674",
                "VG_object_id": "1587457",
                "bbox": [396, 420, 495, 531],
                "image": "data\\images\\713674.jpg"
            },
            {
                "VG_image_id": "2362502",
                "VG_object_id": "2184323",
                "bbox": [1, 0, 498, 238],
                "image": "data\\images\\2362502.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there in the picture", 2],
            ["where is the blanket", 1],
            ["what is the pattern of the blanket", 1],
            ["what is in the picture", 1]
        ],
        "org_questions": [
            ["where is the blanket", 1],
            ["what color is the blanket", -1],
            ["how many children are there in the picture", 2],
            ["when is this photo taken", -1],
            ["what is the pattern of the blanket", 1],
            ["where was the photo taken", -1],
            ["what is in the picture", 1]
        ],
        "context": [
            "a living room with a couch, refrigerator, and a couch.",
            "two babies laying on a blanket"
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2378124",
                "VG_object_id": "3672043",
                "bbox": [4, 210, 494, 325],
                "image": "data\\images\\2378124.jpg"
            },
            {
                "VG_image_id": "2353560",
                "VG_object_id": "3292982",
                "bbox": [0, 171, 499, 373],
                "image": "data\\images\\2353560.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people on the beach doing", 1],
            ["what is in the distance", 1],
            ["how many people are there", 1],
            ["what is the person doing on the beach", 1],
            ["who is on the beach", 1]
        ],
        "org_questions": [
            ["what is on the beach", -1],
            ["what are the people on the beach doing", 1],
            ["what is in the distance", 1],
            ["how many people are there", 1],
            ["what color is the beach", -1],
            ["what is the person doing on the beach", 1],
            ["when was the picture taken", -1],
            ["where is this scene", -1],
            ["who is on the beach", 1],
            ["where is the beach", -1]
        ],
        "context": [
            "two people walking on the beach with surfboards.",
            "a group of people flying kites on a beach."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2319226",
                "VG_object_id": "1003654",
                "bbox": [0, 1, 231, 301],
                "image": "data\\images\\2319226.jpg"
            },
            {
                "VG_image_id": "2383557",
                "VG_object_id": "693982",
                "bbox": [236, 82, 367, 372],
                "image": "data\\images\\2383557.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's clothes", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["What is man doing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's clothes", 1],
            ["how many people are there in the picture", -1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["What is man doing", 1],
            ["who is in the photo", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man looking at a laptop computer with a drawing of a bird on it.",
            "a man and woman on a subway train talking on their cell phones."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2404421",
                "VG_object_id": "342849",
                "bbox": [249, 248, 305, 282],
                "image": "data\\images\\2404421.jpg"
            },
            {
                "VG_image_id": "2395054",
                "VG_object_id": "458204",
                "bbox": [14, 144, 150, 308],
                "image": "data\\images\\2395054.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many seats are there", 2],
            ["what color is the seat", 1],
            ["what is on the seat", 1],
            ["what is next to the toilet", 1],
            ["what is above the toilet", 1]
        ],
        "org_questions": [
            ["what color is the seat", 1],
            ["how many seats are there", 2],
            ["what color is the floor", -1],
            ["what is on the seat", 1],
            ["what is the ground covered with", -1],
            ["how many people are there in the photo", -1],
            ["how many motorcycles are in the picture", -1],
            ["what shape is the toilet", -1],
            ["how is the toilet", -1],
            ["what is next to the toilet", 1],
            ["what is above the toilet", 1]
        ],
        "context": [
            "a bathroom with a toilet, sink, and a shower.",
            "two toilets in a bathroom with black and white walls."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2346996",
                "VG_object_id": "2525964",
                "bbox": [214, 60, 380, 243],
                "image": "data\\images\\2346996.jpg"
            },
            {
                "VG_image_id": "2408377",
                "VG_object_id": "262946",
                "bbox": [150, 94, 242, 147],
                "image": "data\\images\\2408377.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color of the shirt", 1],
            ["what is the man wearing on his wrists", 1],
            ["what color is the man's pants", 1]
        ],
        "org_questions": [
            ["what is the main color of the shirt", 1],
            ["what is the man wearing on his wrists", 1],
            ["what color is the man's pants", 1],
            ["what gender is the person in the shirt", -1],
            ["how many people are there", -1],
            ["where is the person", -1],
            ["how is the weather", -1],
            ["what is the person doing", -1],
            ["when was the photo taken", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man swinging a tennis racket at a ball.",
            "a man is playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2341040",
                "VG_object_id": "2457605",
                "bbox": [88, 89, 474, 317],
                "image": "data\\images\\2341040.jpg"
            },
            {
                "VG_image_id": "2373271",
                "VG_object_id": "733412",
                "bbox": [2, 284, 184, 396],
                "image": "data\\images\\2373271.jpg"
            }
        ],
        "questions_with_scores": [["what is the plate made of", 1]],
        "org_questions": [
            ["what is in the hand", -1],
            ["what is the plate made of", 1],
            ["how many plates are there", -1],
            ["what shape is the plate", -1],
            ["what color is the table", -1],
            ["where is the plate", -1],
            ["what is the man doing", -1],
            ["what is the food on", -1],
            ["what is on the table", -1],
            ["what is under the table", -1],
            ["what is the table color", -1],
            ["what is the table made out of", -1]
        ],
        "context": [
            "a person is eating a piece of cake on a plate.",
            "a young child sitting at a table with a spoon."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2315991",
                "VG_object_id": "3464780",
                "bbox": [4, 154, 499, 300],
                "image": "data\\images\\2315991.jpg"
            },
            {
                "VG_image_id": "2350311",
                "VG_object_id": "3594249",
                "bbox": [1, 188, 498, 371],
                "image": "data\\images\\2350311.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the floor", 1],
            ["what is the floor made of", 1],
            ["where is the photo taken", 1],
            ["where is the floor", 1],
            ["What is on the floor", 1],
            ["what is in the room", 1],
            ["what is covering the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is the floor made of", 1],
            ["how many people are there", 2],
            ["where is the photo taken", 1],
            ["where is the floor", 1],
            ["What is on the floor", 1],
            ["what is in the room", 1],
            ["what is covering the floor", 1]
        ],
        "context": [
            "a dog sitting in front of a television.",
            "a man sitting on a toilet in a room."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2315858",
                "VG_object_id": "2868027",
                "bbox": [53, 136, 291, 498],
                "image": "data\\images\\2315858.jpg"
            },
            {
                "VG_image_id": "2390215",
                "VG_object_id": "499501",
                "bbox": [165, 56, 340, 393],
                "image": "data\\images\\2390215.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's jacket", 2],
            ["where is the photo taken", 2],
            ["what is the man doing", 1],
            ["what gesture is the man", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what color is the man's jacket", 2],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["where is the photo taken", 2],
            ["what is on the man head", -1],
            ["what gesture is the man", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is behind the man", 1],
            ["what is the man wearing on his face", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man in a suit and tie holding a wii remote.",
            "a man in a suit and tie standing on a street."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2377743",
                "VG_object_id": "1897299",
                "bbox": [0, 285, 499, 372],
                "image": "data\\images\\2377743.jpg"
            },
            {
                "VG_image_id": "2399528",
                "VG_object_id": "415652",
                "bbox": [15, 257, 499, 327],
                "image": "data\\images\\2399528.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 1],
            ["how many people are there", 1],
            ["what kind of food is it", 1],
            ["how many plates are there on the table", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["What is on the table", -1],
            ["how many people are there", 1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["what kind of food is it", 1],
            ["how many plates are there on the table", 1],
            ["where was the picture taken", -1],
            ["what is in the table", -1]
        ],
        "context": [
            "a group of young girls eating pizza at a table.",
            "a glass vase filled with oranges and a plate of oranges."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2389975",
                "VG_object_id": "1255285",
                "bbox": [339, 225, 397, 274],
                "image": "data\\images\\2389975.jpg"
            },
            {
                "VG_image_id": "2332345",
                "VG_object_id": "3443382",
                "bbox": [360, 242, 435, 287],
                "image": "data\\images\\2332345.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bowl", 2],
            ["where was the photo taken", 2],
            ["what is in the bowl", 1],
            ["where is the bowl", 1],
            ["how many people are there", 1],
            ["what is the bowl made of", 1],
            ["What is next to the bowl", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the bowl", 2],
            ["what is in the bowl", 1],
            ["where is the bowl", 1],
            ["how many people are there", 1],
            ["what is the bowl made of", 1],
            ["What is next to the bowl", 1],
            ["How many bowl are there in the image", -1],
            ["where was the photo taken", 2],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a man is holding a cow",
            "three women are smiling and smiling in a kitchen."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2361684",
                "VG_object_id": "2023855",
                "bbox": [78, 50, 197, 142],
                "image": "data\\images\\2361684.jpg"
            },
            {
                "VG_image_id": "2387231",
                "VG_object_id": "1277820",
                "bbox": [329, 127, 380, 270],
                "image": "data\\images\\2387231.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's coat", 1],
            ["what is the gesture of the person", 1],
            ["what is the person doing", 1],
            ["how many persons are there", 1],
            ["what is the persion riding", 1]
        ],
        "org_questions": [
            ["what color is the person's coat", 1],
            ["what is the gesture of the person", 1],
            ["what is the person doing", 1],
            ["how many persons are there", 1],
            ["where is the man", -1],
            ["how is the weather", -1],
            ["what is the person wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", -1],
            ["what is the persion riding", 1]
        ],
        "context": [
            "a snowboarder is in mid air after a jump.",
            "a group of people walking in the snow."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2410628",
                "VG_object_id": "213188",
                "bbox": [3, 440, 375, 500],
                "image": "data\\images\\2410628.jpg"
            },
            {
                "VG_image_id": "2382615",
                "VG_object_id": "1327516",
                "bbox": [0, 261, 499, 330],
                "image": "data\\images\\2382615.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 2],
            ["what is the ground covered with", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["what color is the ground", -1],
            ["what is on the ground", 1],
            ["how many people are there in the picture", -1],
            ["when was the photo taken", -1],
            ["what is in the background", -1],
            ["where was the picture taken", -1],
            ["where is the picture taken", 2]
        ],
        "context": [
            "a cat is standing in an open doorway.",
            "a motorcycle parked in front of a house."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2411731",
                "VG_object_id": "307558",
                "bbox": [2, 44, 468, 332],
                "image": "data\\images\\2411731.jpg"
            },
            {
                "VG_image_id": "2385375",
                "VG_object_id": "1297694",
                "bbox": [0, 0, 500, 332],
                "image": "data\\images\\2385375.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of food is on the plate", 2],
            ["what is on the plate", 2],
            ["what color is the table", 1],
            ["how many plates are there on the table", 1],
            ["who color is the table", 1],
            ["what food is on the table", 1],
            ["what is next to the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["how many plates are there on the table", 1],
            ["what kind of food is on the plate", 2],
            ["where is the photo taken", -1],
            ["what is the table made of", -1],
            ["who color is the table", 1],
            ["what is on the plate", 2],
            ["what food is on the table", 1],
            ["what is the food sitting on", -1],
            ["what is next to the table", 1],
            ["where is the food", -1],
            ["what is under the table", -1]
        ],
        "context": [
            "a table with a display of apples on it.",
            "a hot dog on a piece of bread on a plate."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2367763",
                "VG_object_id": "620647",
                "bbox": [10, 299, 122, 496],
                "image": "data\\images\\2367763.jpg"
            },
            {
                "VG_image_id": "2369889",
                "VG_object_id": "2678482",
                "bbox": [116, 52, 201, 170],
                "image": "data\\images\\2369889.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 1],
            ["what is the woman's posture", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["where is the woman", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 1],
            ["what is the woman's posture", 1],
            ["how many people are there", 1],
            ["how is the weather", -1],
            ["what is the ground covered with", 1],
            ["where is the woman", 1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["what is the woman holding", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a giraffe is sticking its tongue out at a zoo.",
            "a woman sitting on a bed using a laptop."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2366153",
                "VG_object_id": "2489864",
                "bbox": [240, 162, 356, 282],
                "image": "data\\images\\2366153.jpg"
            },
            {
                "VG_image_id": "2377806",
                "VG_object_id": "2073892",
                "bbox": [213, 114, 360, 289],
                "image": "data\\images\\2377806.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the horse", 2],
            ["how many horses are there in the picture", 1],
            ["what is the ground the horse standing on made of", 1],
            ["what is in the background", 1],
            ["what are the horses doing", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["how many horses are there in the picture", 1],
            ["what is the ground the horse standing on made of", 1],
            ["where is the horse", 2],
            ["what color are the horses", -1],
            ["what is in the background", 1],
            ["what is the horse carrying", -1],
            ["what is behind the horse", -1],
            ["what type of animal is shown", -1],
            ["who is on the horse", -1],
            ["what is on the horses", -1],
            ["what are the horses doing", 1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a horse drawn carriage is standing in the middle of a city.",
            "two horses pulling a cart with a man on it."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2373171",
                "VG_object_id": "1722414",
                "bbox": [309, 286, 373, 353],
                "image": "data\\images\\2373171.jpg"
            },
            {
                "VG_image_id": "2324026",
                "VG_object_id": "3124586",
                "bbox": [276, 164, 344, 222],
                "image": "data\\images\\2324026.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the pillow", 2],
            ["what color is the pillow", 1],
            ["how many pillows are there", 1],
            ["what  are the pillows placed on", 1],
            ["what is the pillow on", 1],
            ["where are the pillows", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the pillow", 1],
            ["where is the pillow", 2],
            ["how many pillows are there", 1],
            ["what  are the pillows placed on", 1],
            ["what is the pillow on", 1],
            ["where are the pillows", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a woman standing in a living room holding a wii remote.",
            "a bed with a white comforter and a lamp"
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2399018",
                "VG_object_id": "1173141",
                "bbox": [245, 184, 322, 268],
                "image": "data\\images\\2399018.jpg"
            },
            {
                "VG_image_id": "2359196",
                "VG_object_id": "1740287",
                "bbox": [114, 6, 338, 480],
                "image": "data\\images\\2359196.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the bear", 1],
            ["Where is the bear sitting near to", 1],
            ["where is the animal", 1],
            ["where is the stuffed bear", 1]
        ],
        "org_questions": [
            ["What color is the bear", 1],
            ["Where is the bear sitting near to", 1],
            ["how many animals are there", -1],
            ["where is the animal", 1],
            ["what is the stuffed animal", -1],
            ["where is the stuffed bear", 1]
        ],
        "context": [
            "a young boy playing a piano with stuffed animals.",
            "a teddy bear sitting on a wooden fence."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2374290",
                "VG_object_id": "727770",
                "bbox": [133, 200, 207, 283],
                "image": "data\\images\\2374290.jpg"
            },
            {
                "VG_image_id": "2414143",
                "VG_object_id": "159134",
                "bbox": [305, 135, 383, 201],
                "image": "data\\images\\2414143.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the gender of the person in the shirt", 2],
            ["who is in the photo", 2],
            ["who is wearing the shirt", 1]
        ],
        "org_questions": [
            ["what is the gender of the person in the shirt", 2],
            ["what color is the hair of the person in the shirt", -1],
            ["How many people are there", -1],
            ["what is the persion wearing on the head", -1],
            ["who is wearing the shirt", 1],
            ["where is the person", -1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", 2]
        ],
        "context": [
            "a woman in a white dress playing tennis.",
            "a man playing tennis on a clay court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2318026",
                "VG_object_id": "3180255",
                "bbox": [351, 2, 469, 171],
                "image": "data\\images\\2318026.jpg"
            },
            {
                "VG_image_id": "2353772",
                "VG_object_id": "843279",
                "bbox": [156, 214, 252, 485],
                "image": "data\\images\\2353772.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["where is the man", 1],
            ["how many men are there", 1],
            ["what color is the person's shirt", 1],
            ["what clothes are the man wearing", 1],
            ["what is the man wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["where is the man", 1],
            ["how many men are there", 1],
            ["What is the man wearing on his head", -1],
            ["what color is the person's shirt", 1],
            ["what clothes are the man wearing", 1],
            ["what is the man wearing", 1],
            ["who is wearing a hat", -1],
            ["when was the photo taken", -1],
            ["what is the man holding", 1],
            ["who is in the picture", -1]
        ],
        "context": [
            "two jockeys riding horses on a beach.",
            "a man holding an umbrella standing in the water."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2316618",
                "VG_object_id": "3306564",
                "bbox": [2, 84, 373, 497],
                "image": "data\\images\\2316618.jpg"
            },
            {
                "VG_image_id": "2355848",
                "VG_object_id": "825921",
                "bbox": [129, 105, 402, 276],
                "image": "data\\images\\2355848.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table under the plate", 1],
            ["what pattern does the table under the plate have", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["what color is the table under the plate", 1],
            ["what pattern does the table under the plate have", 1],
            ["how many people are there", -1],
            ["where is the plate", -1],
            ["what shape is the plate", -1],
            ["what is in the distance", -1],
            ["what kind of food is on the plate", -1],
            ["what is the food on", -1],
            ["what is on the plate", -1],
            ["what is next to the plate", 1]
        ],
        "context": [
            "a plate of food with a sandwich and a bowl of soup.",
            "a plate of food with a sandwich and salad."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2378736",
                "VG_object_id": "1365547",
                "bbox": [0, 242, 417, 499],
                "image": "data\\images\\2378736.jpg"
            },
            {
                "VG_image_id": "2392123",
                "VG_object_id": "482909",
                "bbox": [3, 272, 498, 371],
                "image": "data\\images\\2392123.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's clothes", 2],
            ["what is the man doing on the field", 1]
        ],
        "org_questions": [
            ["what is the man doing on the field", 1],
            ["how many people are on the field", -1],
            ["what color is the background", -1],
            ["what is the gender of the person", -1],
            ["what is on the ground", -1],
            ["what is the weather like", -1],
            ["where was this photo taken", -1],
            ["where is the green grass", -1],
            ["what is the ground covered with", -1],
            ["what is the man standing on", -1],
            ["what is green", -1],
            ["what color is the man's clothes", 2]
        ],
        "context": [
            "a man holding a kite in a field.",
            "a man catching a frisbee in a park."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2323034",
                "VG_object_id": "3142009",
                "bbox": [155, 234, 287, 394],
                "image": "data\\images\\2323034.jpg"
            },
            {
                "VG_image_id": "2353246",
                "VG_object_id": "1808583",
                "bbox": [87, 287, 207, 444],
                "image": "data\\images\\2353246.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["what is in the background", -1],
            ["how many people are there in the picture", 1],
            ["what gender is the person in the trouser", -1],
            ["what time is it", -1],
            ["who is wearing the trousers", -1],
            ["what kind of pants is the man wearing", -1],
            ["when was this photo taken", -1],
            ["what is the man doing", -1]
        ],
        "context": [
            "a young child on skis in the snow.",
            "two people on skis on a snowy slope."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2343277",
                "VG_object_id": "925511",
                "bbox": [173, 190, 297, 491],
                "image": "data\\images\\2343277.jpg"
            },
            {
                "VG_image_id": "2363406",
                "VG_object_id": "2384013",
                "bbox": [261, 83, 345, 226],
                "image": "data\\images\\2363406.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["where is the picture taken", 1],
            ["what is on the man's head", 1],
            ["what is the color of the shirt", 1],
            ["how is the weather", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["where is the picture taken", 1],
            ["what is on the man's head", 1],
            ["what is the color of the shirt", 1],
            ["when is the photo taken", -1],
            ["how is the weather", 1],
            ["what is the person wearing", -1],
            ["who is in the photo", -1],
            ["what is the man holding", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a man and woman under an umbrella in the rain.",
            "a man riding on the back of a brown horse."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2407879",
                "VG_object_id": "271995",
                "bbox": [178, 25, 489, 357],
                "image": "data\\images\\2407879.jpg"
            },
            {
                "VG_image_id": "2372038",
                "VG_object_id": "2623436",
                "bbox": [9, 82, 200, 197],
                "image": "data\\images\\2372038.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the truck", 2],
            ["what time is it", 2],
            ["where is the truck", 1],
            ["what is in the distance", 1],
            ["when was the photo taken", 1],
            ["what is on the side of the truck", 1]
        ],
        "org_questions": [
            ["what color is the truck", 2],
            ["where is the truck", 1],
            ["what time is it", 2],
            ["how many people are there", -1],
            ["what is in the distance", 1],
            ["what is on the truck", -1],
            ["what color is the ground", -1],
            ["what type of vehicle is shown", -1],
            ["when was the photo taken", 1],
            ["what is on the side of the truck", 1],
            ["what is in front of the truck", -1]
        ],
        "context": [
            "a large green truck driving down a road.",
            "a large air plane on a run way"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2319353",
                "VG_object_id": "2811754",
                "bbox": [49, 302, 95, 392],
                "image": "data\\images\\2319353.jpg"
            },
            {
                "VG_image_id": "2380197",
                "VG_object_id": "547083",
                "bbox": [272, 103, 345, 178],
                "image": "data\\images\\2380197.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is woman's shirt", 2],
            ["What is woman doing", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is woman's shirt", 2],
            ["What is woman doing", 1],
            ["what gender is the person in the shirt", -1],
            ["what is the pattern of the person's shirt", -1],
            ["how is the weather", -1],
            ["what is hanging on the shirt", -1],
            ["when was the photo taken", -1],
            ["what is the woman wearing", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a group of young people playing basketball on a court.",
            "a woman riding a bike down a road next to a body of water."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2410849",
                "VG_object_id": "3814499",
                "bbox": [4, 364, 329, 497],
                "image": "data\\images\\2410849.jpg"
            },
            {
                "VG_image_id": "2355986",
                "VG_object_id": "1999448",
                "bbox": [0, 95, 498, 331],
                "image": "data\\images\\2355986.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["what color is the person's shirt", 1]
        ],
        "org_questions": [
            ["how many people are in the picture", 1],
            ["what color is the person's shirt", 1],
            ["what is the persion doing", -1],
            ["what is the ground covered with", -1],
            ["when is the picture taken", -1],
            ["what is on the sky", -1],
            ["where was this photo taken", -1],
            ["how is the weather", -1],
            ["where is the grass", -1],
            ["what is in the background", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "two men jumping up to catch a frisbee.",
            "a woman throwing a frisbee in a park."
        ]
    },
    {
        "object_category": "book",
        "images": [
            {
                "VG_image_id": "2393025",
                "VG_object_id": "475501",
                "bbox": [418, 64, 500, 115],
                "image": "data\\images\\2393025.jpg"
            },
            {
                "VG_image_id": "2329925",
                "VG_object_id": "2871285",
                "bbox": [26, 179, 123, 229],
                "image": "data\\images\\2329925.jpg"
            }
        ],
        "questions_with_scores": [["what color is the table", 2]],
        "org_questions": [
            ["what color is the book", -1],
            ["how many keyboards are there", -1],
            ["where is the book", -1],
            ["what is beside the books", -1],
            ["how many people are there", -1],
            ["what color is the table", 2],
            ["what is the main color of the book", -1],
            ["how many people are in the picture", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a wooden table.",
            "a laptop computer sitting on a desk next to a cup of coffee."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2379246",
                "VG_object_id": "554173",
                "bbox": [283, 253, 458, 368],
                "image": "data\\images\\2379246.jpg"
            },
            {
                "VG_image_id": "2367073",
                "VG_object_id": "624581",
                "bbox": [0, 288, 464, 499],
                "image": "data\\images\\2367073.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["what is the pattern of the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is the pattern of the floor", 1],
            ["what color is the wall", -1],
            ["How many people are there", -1],
            ["where is the floor", -1],
            ["what is the floor made of", -1],
            ["what room is this", -1],
            ["where was the photo taken", -1],
            ["what is next to the toilet", -1],
            ["what is in the bathroom", -1]
        ],
        "context": [
            "a bathroom with a sink, toilet and shower.",
            "a yellow toilet sitting next to a trash can."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2317361",
                "VG_object_id": "3337982",
                "bbox": [0, 352, 175, 497],
                "image": "data\\images\\2317361.jpg"
            },
            {
                "VG_image_id": "2326623",
                "VG_object_id": "3063184",
                "bbox": [270, 132, 497, 373],
                "image": "data\\images\\2326623.jpg"
            }
        ],
        "questions_with_scores": [["how many people are there", 1]],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", -1],
            ["how many people are there", 1],
            ["Where is the table", -1],
            ["what is the table made of", -1],
            ["how many plates are there on the table", -1],
            ["where was the photo taken", -1],
            ["what is next to the desk", -1],
            ["what is sitting on the desk", -1],
            ["where is the picture taken", -1],
            ["what is in front of the desk", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a living room with a couch, table, and chairs.",
            "a man and a dog laying on a couch."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2362149",
                "VG_object_id": "1970541",
                "bbox": [204, 187, 259, 297],
                "image": "data\\images\\2362149.jpg"
            },
            {
                "VG_image_id": "2392444",
                "VG_object_id": "668052",
                "bbox": [210, 392, 260, 495],
                "image": "data\\images\\2392444.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["where is this photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["what are the people wearing", 1]
        ],
        "org_questions": [
            ["What is the weather like", -1],
            ["What color is the street", -1],
            ["how many people are there", 1],
            ["where is this photo taken", 1],
            ["what is the person holding", -1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what are the people doing", -1],
            ["what are the people wearing", 1],
            ["where are the people", -1]
        ],
        "context": [
            "a woman walking down a snow covered street.",
            "a clock on a pole in a city."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2363566",
                "VG_object_id": "763812",
                "bbox": [65, 23, 228, 225],
                "image": "data\\images\\2363566.jpg"
            },
            {
                "VG_image_id": "2341247",
                "VG_object_id": "3168577",
                "bbox": [4, 26, 311, 491],
                "image": "data\\images\\2341247.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child doing", 1],
            ["what color is the child's clothes", 1],
            ["what is the boy holding", 1]
        ],
        "org_questions": [
            ["what is the child doing", 1],
            ["what color is the child's clothes", 1],
            ["how many children are there", -1],
            ["what is the child wearing on his head", -1],
            ["what is the boy holding", 1],
            ["what is the boy wearing", -1],
            ["what is on the child's head", -1],
            ["where was the photo taken", -1],
            ["who is in the photo", -1],
            ["where is the child", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a boy and girl eating pizza at a table.",
            "a baby standing in front of a refrigerator."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2398075",
                "VG_object_id": "428513",
                "bbox": [208, 65, 365, 296],
                "image": "data\\images\\2398075.jpg"
            },
            {
                "VG_image_id": "2400251",
                "VG_object_id": "660812",
                "bbox": [33, 89, 468, 327],
                "image": "data\\images\\2400251.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is the cat doing", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is the cat doing", 1],
            ["how many cats together", -1],
            ["where is the cat", -1],
            ["what is the cat sitting on", -1],
            ["what gesture is the cat", -1],
            ["how many animals are there", -1],
            ["what type of animal is shown", -1],
            ["what is on the cat's head", -1],
            ["what is sitting on the desk", -1],
            ["what is next to the cat", -1],
            ["what is under the cat", -1]
        ],
        "context": [
            "a cat sitting on a desk next to a computer.",
            "a cat laying on a laptop computer on a desk."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2351391",
                "VG_object_id": "3589374",
                "bbox": [3, 4, 483, 373],
                "image": "data\\images\\2351391.jpg"
            },
            {
                "VG_image_id": "2347065",
                "VG_object_id": "893978",
                "bbox": [1, 305, 264, 499],
                "image": "data\\images\\2347065.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the court", 2],
            ["how many people are on the court", 2],
            ["how many players are there on the court", 1],
            ["what color is the person's shirt", 1],
            ["How many person are there", 1]
        ],
        "org_questions": [
            ["what color is the court", 2],
            ["how many people are on the court", 2],
            ["what is the man doing", -1],
            ["What sports are people playing", -1],
            ["what sport is it", -1],
            ["how many players are there on the court", 1],
            ["what color is the person's shirt", 1],
            ["How many person are there", 1],
            ["where was this photo taken", -1],
            ["what is on the ground", -1],
            ["where are the white lines", -1],
            ["what is the court made of", -1]
        ],
        "context": [
            "a tennis player is on the court with a camera.",
            "a man swinging a tennis racket on a tennis court."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2359727",
                "VG_object_id": "3764157",
                "bbox": [104, 279, 271, 405],
                "image": "data\\images\\2359727.jpg"
            },
            {
                "VG_image_id": "2412733",
                "VG_object_id": "1618069",
                "bbox": [365, 33, 440, 143],
                "image": "data\\images\\2412733.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the animal's head", 2],
            ["where is the bear", 1],
            ["what is on the left", 1],
            ["what is next to the bear", 1]
        ],
        "org_questions": [
            ["what animal is it", -1],
            ["what color is the animal's head", 2],
            ["where is the bear", 1],
            ["what is on the left", 1],
            ["what is next to the bear", 1],
            ["what kind of animal is this", -1]
        ],
        "context": [
            "a stuffed animal hanging from a wall",
            "a group of people laying on a couch."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2351815",
                "VG_object_id": "2259458",
                "bbox": [217, 99, 350, 206],
                "image": "data\\images\\2351815.jpg"
            },
            {
                "VG_image_id": "2330067",
                "VG_object_id": "2933001",
                "bbox": [193, 201, 276, 338],
                "image": "data\\images\\2330067.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color of the dog", 1],
            ["where is the dog", 1],
            ["what is in the background", 1],
            ["what is the ground covered with", 1],
            ["what is the dog sitting on", 1],
            ["what is next to the dog", 1]
        ],
        "org_questions": [
            ["what is the main color of the dog", 1],
            ["where is the dog", 1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what is the dog doing", -1],
            ["what is the ground covered with", 1],
            ["what is the dog wearing", -1],
            ["what type of animal is in the picture", -1],
            ["when was the picture taken", -1],
            ["what is the dog sitting on", 1],
            ["what is next to the dog", 1]
        ],
        "context": [
            "a dog in the back of a truck with a truck bed.",
            "a dog sitting on a boat in the water."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2381085",
                "VG_object_id": "707460",
                "bbox": [114, 257, 456, 369],
                "image": "data\\images\\2381085.jpg"
            },
            {
                "VG_image_id": "2373291",
                "VG_object_id": "1827977",
                "bbox": [227, 356, 341, 492],
                "image": "data\\images\\2373291.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the ground", 2],
            ["how many giraffes are there on the ground", 2],
            ["what color is the ground", 2],
            ["what is the ground covered with", 1],
            ["what is standing on the land", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["how many people are there on the ground", 2],
            ["how many giraffes are there on the ground", 2],
            ["What is the color of image", -1],
            ["what is standing on the land", 1],
            ["what is the weather like", -1],
            ["What is the background of image", -1],
            ["where was this picture taken", -1],
            ["what is covering the ground", 1],
            ["what is in the foreground", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a giraffe sitting on the ground next to a tree.",
            "two people in a field flying a kite."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2333901",
                "VG_object_id": "2854633",
                "bbox": [164, 133, 371, 285],
                "image": "data\\images\\2333901.jpg"
            },
            {
                "VG_image_id": "2340888",
                "VG_object_id": "3467468",
                "bbox": [21, 69, 244, 311],
                "image": "data\\images\\2340888.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 1],
            ["what food is on the plate", 1],
            ["what color is the food on the plate", 1],
            ["how many plates are there", 1],
            ["what color is the table", 1],
            ["what is in the bowl", 1],
            ["what kind of food is this", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what food is on the plate", 1],
            ["what color is the food on the plate", 1],
            ["how many plates are there", 1],
            ["what shape is the plate", -1],
            ["where is the food placed on", -1],
            ["what color is the table", 1],
            ["what is in the bowl", 1],
            ["what kind of food is this", 1]
        ],
        "context": [
            "a plate with a piece of bread and a spoon on it.",
            "a plate of food with a fried egg on top of it."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2395479",
                "VG_object_id": "454123",
                "bbox": [71, 222, 442, 373],
                "image": "data\\images\\2395479.jpg"
            },
            {
                "VG_image_id": "2326415",
                "VG_object_id": "3939625",
                "bbox": [2, 347, 291, 498],
                "image": "data\\images\\2326415.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["what color is the table", 1],
            ["what is the woman doing", 1],
            ["How many people are there", 1],
            ["What is the background of photo", 1],
            ["how many plates are there on the table", 1],
            ["what color is the food on the plate", 1],
            ["where was the photo taken", 1],
            ["where are the people", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is on the table", 1],
            ["what color is the table", 1],
            ["what is the woman doing", 1],
            ["How many people are there", 1],
            ["What is the background of photo", 1],
            ["how many plates are there on the table", 1],
            ["what color is the food on the plate", 1],
            ["what is the table made of", -1],
            ["where was the photo taken", 1],
            ["where are the people", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a group of people in chef hats preparing food.",
            "a woman smiling at a table with a piece of cake."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2386861",
                "VG_object_id": "1281726",
                "bbox": [4, 174, 499, 231],
                "image": "data\\images\\2386861.jpg"
            },
            {
                "VG_image_id": "2322614",
                "VG_object_id": "2881714",
                "bbox": [1, 293, 499, 373],
                "image": "data\\images\\2322614.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are on the street", 2],
            ["what is on the street", 1]
        ],
        "org_questions": [
            ["what is on the street", 1],
            ["how many buses are there on the street", -1],
            ["what is the main color of the road", -1],
            ["how many people are on the street", 2],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["what is the road made of", -1],
            ["what is the weather like", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a young man riding a skateboard down a street.",
            "a car driving down a street next to a traffic light."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2385595",
                "VG_object_id": "1294884",
                "bbox": [248, 19, 499, 187],
                "image": "data\\images\\2385595.jpg"
            },
            {
                "VG_image_id": "2363054",
                "VG_object_id": "1713447",
                "bbox": [0, 75, 499, 331],
                "image": "data\\images\\2363054.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animal is on the bed", 2],
            ["how many pillows are there on the bed", 1],
            ["what is on the bed", 1]
        ],
        "org_questions": [
            ["what color is the bed", -1],
            ["what animal is on the bed", 2],
            ["how many pillows are there on the bed", 1],
            ["What is the pattern of bed", -1],
            ["what is on the bed", 1],
            ["how many people are there", -1],
            ["what color is the pet on the bed", -1],
            ["where is the picture taken", -1],
            ["where is the blanket", -1]
        ],
        "context": [
            "a woman and a dog laying on a bed.",
            "a cat laying on a bed with a stuffed duck."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2341127",
                "VG_object_id": "2483160",
                "bbox": [305, 176, 390, 285],
                "image": "data\\images\\2341127.jpg"
            },
            {
                "VG_image_id": "2354585",
                "VG_object_id": "837367",
                "bbox": [170, 126, 260, 206],
                "image": "data\\images\\2354585.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers of the person in the shirt", 2],
            ["what gender is the person in the shirt", 1],
            ["what is the pattern of the person's shirt", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the land under the person in the shirt", -1],
            ["what gender is the person in the shirt", 1],
            ["what color is the trousers of the person in the shirt", 2],
            ["how many people are there", -1],
            ["what is the pattern of the person's shirt", 1],
            ["what is the persion doing", 1],
            ["where is the person", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a woman and a child playing with a ball in a field.",
            "a baseball player throwing a ball on a field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "713012",
                "VG_object_id": "3699581",
                "bbox": [161, 39, 621, 813],
                "image": "data\\images\\713012.jpg"
            },
            {
                "VG_image_id": "2319897",
                "VG_object_id": "3314658",
                "bbox": [57, 5, 256, 488],
                "image": "data\\images\\2319897.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the woman's trousers", 2],
            ["what color is the woman's shirt", 2],
            ["what color is the ground", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["what color are the woman's trousers", 2],
            ["what color is the woman's shirt", 2],
            ["what color is the ground", 1],
            ["how many people are there", -1],
            ["what is the woman doing", -1],
            ["where is the woman", -1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["what is the persion sitting on", 1],
            ["who is in the picture", -1],
            ["what is the woman looking at", -1]
        ],
        "context": [
            "a woman is taking a picture of another woman.",
            "a woman standing on a sidewalk looking at her phone."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2358213",
                "VG_object_id": "2740187",
                "bbox": [287, 24, 490, 331],
                "image": "data\\images\\2358213.jpg"
            },
            {
                "VG_image_id": "2358520",
                "VG_object_id": "3768417",
                "bbox": [349, 0, 499, 107],
                "image": "data\\images\\2358520.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what color is the woman's shirt", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what color is the woman's shirt", 1],
            ["what time is it", -1],
            ["what is in the background", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a man and woman laughing and laughing at a party.",
            "two plates of food on a table with drinks."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2373923",
                "VG_object_id": "2169682",
                "bbox": [78, 298, 236, 340],
                "image": "data\\images\\2373923.jpg"
            },
            {
                "VG_image_id": "2392369",
                "VG_object_id": "1230857",
                "bbox": [101, 180, 462, 325],
                "image": "data\\images\\2392369.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the water", 1],
            ["what is on the board", 1]
        ],
        "org_questions": [
            ["How many boards are there", -1],
            ["What color is the water", 1],
            ["How many people are there", -1],
            ["Where is the person", -1],
            ["what is the board made of", -1],
            ["what is in the background", -1],
            ["what is on the board", 1],
            ["how is the water", -1],
            ["where was the photo taken", -1],
            ["what is white", -1]
        ],
        "context": [
            "a group of people on surfboards paddling in the water.",
            "a man in a wet suit holding a surfboard on the beach."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2348084",
                "VG_object_id": "1909549",
                "bbox": [37, 261, 215, 461],
                "image": "data\\images\\2348084.jpg"
            },
            {
                "VG_image_id": "2397075",
                "VG_object_id": "437632",
                "bbox": [121, 243, 309, 340],
                "image": "data\\images\\2397075.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 1],
            ["what is  on the plate", 1],
            ["how many people are there", 1],
            ["what is in the bowl", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what is  on the plate", 1],
            ["what is the table made of", -1],
            ["how many people are there", 1],
            ["where is the bowl", -1],
            ["what the bowl is on", -1],
            ["what is beside the bowl", -1],
            ["what is in the bowl", 1],
            ["what shape is the plate", -1],
            ["what kind of food is on the table", -1],
            ["what is the plate sitting on", -1],
            ["what is the food on", -1]
        ],
        "context": [
            "a cup of soup and a sandwich on a table.",
            "a man sitting at a table with a pizza and a drink."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2318386",
                "VG_object_id": "2801818",
                "bbox": [231, 171, 396, 334],
                "image": "data\\images\\2318386.jpg"
            },
            {
                "VG_image_id": "2373271",
                "VG_object_id": "733415",
                "bbox": [19, 131, 374, 343],
                "image": "data\\images\\2373271.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the person's shirt", 1],
            ["what is the person doing", 1],
            ["who is in the photo", 1],
            ["what color is the shirt", 1]
        ],
        "org_questions": [
            ["what is the color of the person's shirt", 1],
            ["what is the person doing", 1],
            ["How many people are there", -1],
            ["where is the person", -1],
            ["what is the gender of the person", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", 1],
            ["what is the persion sitting on", -1],
            ["what color is the shirt", 1]
        ],
        "context": [
            "a woman sitting at a table with an umbrella.",
            "a young child sitting at a table with a spoon."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2322008",
                "VG_object_id": "2871685",
                "bbox": [6, 18, 190, 325],
                "image": "data\\images\\2322008.jpg"
            },
            {
                "VG_image_id": "2360825",
                "VG_object_id": "2560303",
                "bbox": [239, 17, 487, 305],
                "image": "data\\images\\2360825.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the guy doing", 1],
            ["what color is the guy's shirt", 1],
            ["what is the man wearing on his face", 1],
            ["what is the man carrying", 1],
            ["What is guy doing", 1],
            ["what kind of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the guy doing", 1],
            ["what color is the guy's shirt", 1],
            ["how many people are there", -1],
            ["what is the man wearing on his face", 1],
            ["what is the man carrying", 1],
            ["What is guy doing", 1],
            ["what gender is the person", -1],
            ["who is in the photo", -1],
            ["what kind of shirt is the man wearing", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man sitting at a desk using a laptop.",
            "a man is standing in front of a van."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2393854",
                "VG_object_id": "469343",
                "bbox": [165, 17, 403, 281],
                "image": "data\\images\\2393854.jpg"
            },
            {
                "VG_image_id": "2393841",
                "VG_object_id": "1216316",
                "bbox": [128, 112, 254, 262],
                "image": "data\\images\\2393841.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the bus", 2],
            ["where is the bus", 1],
            ["what is in the background", 1],
            ["what is the bus doing", 1],
            ["what is on the side of the bus", 1],
            ["what is next to the bus", 1]
        ],
        "org_questions": [
            ["what is the color of the bus", 2],
            ["where is the bus", 1],
            ["how many buses are in the picture", -1],
            ["when is the picture taken", -1],
            ["what is in the background", 1],
            ["how many people are on the bus", -1],
            ["what is the bus doing", 1],
            ["what type of bus is shown", -1],
            ["what is on the side of the bus", 1],
            ["what is next to the bus", 1],
            ["what is on the bus", -1]
        ],
        "context": [
            "a black and white double decker bus on a city street",
            "a double decker bus parked in a parking lot."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2415143",
                "VG_object_id": "145135",
                "bbox": [125, 258, 199, 331],
                "image": "data\\images\\2415143.jpg"
            },
            {
                "VG_image_id": "2408623",
                "VG_object_id": "257989",
                "bbox": [22, 203, 131, 332],
                "image": "data\\images\\2408623.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there sitting on the chair", 1]
        ],
        "org_questions": [
            ["what color is the chair", -1],
            ["how many people are there sitting on the chair", 1],
            ["what color is the ground", -1],
            ["what is the floor made of", -1],
            ["how many pillows are there on the chair", -1],
            ["how many chairs are there", -1]
        ],
        "context": [
            "a large hangar with several old airplanes in it.",
            "two men sitting under an umbrella on a sidewalk."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2382883",
                "VG_object_id": "1325350",
                "bbox": [334, 36, 416, 130],
                "image": "data\\images\\2382883.jpg"
            },
            {
                "VG_image_id": "2354377",
                "VG_object_id": "838968",
                "bbox": [322, 8, 389, 146],
                "image": "data\\images\\2354377.jpg"
            }
        ],
        "questions_with_scores": [
            ["what gender is the person", 2],
            ["when was the picture taken", 2],
            ["what color is the shirt", 1],
            ["what is the persion doing", 1],
            ["what is the gender of the person", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what gender is the person", 2],
            ["how many people are in the picture", -1],
            ["what is the persion doing", 1],
            ["how many shirts are there", -1],
            ["what is the gender of the person", 1],
            ["when was the picture taken", 2],
            ["what type of shirt is the person wearing", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a woman holding a tennis racket on a tennis court.",
            "a woman on a bike talking to a man on a bike."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2342776",
                "VG_object_id": "930923",
                "bbox": [1, 3, 178, 232],
                "image": "data\\images\\2342776.jpg"
            },
            {
                "VG_image_id": "2352307",
                "VG_object_id": "2437186",
                "bbox": [45, 134, 291, 361],
                "image": "data\\images\\2352307.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there", 2],
            ["where are the children", 1],
            ["what is the child doing", 1],
            ["what is the child holding in hands", 1],
            ["What is child doing", 1],
            ["how many people are in the picture", 1],
            ["what is the boy sitting on", 1],
            ["what is the boy holding", 1]
        ],
        "org_questions": [
            ["how many children are there", 2],
            ["where are the children", 1],
            ["what is the child doing", 1],
            ["what is the child holding in hands", 1],
            ["What is child doing", 1],
            ["how many people are in the picture", 1],
            ["who is in the photo", -1],
            ["what is the boy sitting on", 1],
            ["what is the boy holding", 1]
        ],
        "context": [
            "a baby sitting at a table with a bowl of food.",
            "a group of children riding on top of a train."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2377308",
                "VG_object_id": "1709025",
                "bbox": [196, 267, 394, 374],
                "image": "data\\images\\2377308.jpg"
            },
            {
                "VG_image_id": "2408541",
                "VG_object_id": "259706",
                "bbox": [179, 211, 280, 270],
                "image": "data\\images\\2408541.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the seat", 2],
            ["what is the seat made of", 1],
            ["where is the seat", 1],
            ["what is in front of the chair", 1]
        ],
        "org_questions": [
            ["what color is the seat", 2],
            ["what is the seat made of", 1],
            ["where is the seat", 1],
            ["How many seats are there", -1],
            ["what time is it", -1],
            ["how many people are there in the photo", -1],
            ["what is on the chair", -1],
            ["what is in front of the chair", 1]
        ],
        "context": [
            "a bus with a tv on the top of it",
            "a desk with a pizza box and a tv on it"
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2389405",
                "VG_object_id": "3828654",
                "bbox": [47, 55, 296, 275],
                "image": "data\\images\\2389405.jpg"
            },
            {
                "VG_image_id": "2392778",
                "VG_object_id": "1226051",
                "bbox": [191, 13, 342, 312],
                "image": "data\\images\\2392778.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is girl's shirt", 2],
            ["What sports is girl doing", 1],
            ["what sport is the girl playing", 1],
            ["what is on the woman's head", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What sports is girl doing", 1],
            ["What color is girl's shirt", 2],
            ["where is the photo taken", -1],
            ["how old is the girl", -1],
            ["what sport is the girl playing", 1],
            ["Where is the girl", -1],
            ["What is girl doing", -1],
            ["what is on the woman's head", 1],
            ["who is playing", -1],
            ["when was the picture taken", -1],
            ["what is the woman holding", 1]
        ],
        "context": [
            "two people playing frisbee in a grassy field.",
            "a woman in a red shirt playing tennis."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2377259",
                "VG_object_id": "2629936",
                "bbox": [204, 304, 269, 349],
                "image": "data\\images\\2377259.jpg"
            },
            {
                "VG_image_id": "2352688",
                "VG_object_id": "3582215",
                "bbox": [192, 232, 292, 271],
                "image": "data\\images\\2352688.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what color is the table", 2],
            ["what shape is the plate", -1],
            ["What food is on the plate", -1],
            ["what is in the distance", -1],
            ["what is on the plate", -1],
            ["how many forks are there on the plate", -1],
            ["where was the picture taken", -1],
            ["where are the plates", -1],
            ["what kind of food is on the table", -1],
            ["what color are the plates on the table", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a man cutting a cake on a table",
            "students serve food to the community."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2328263",
                "VG_object_id": "978337",
                "bbox": [1, 4, 153, 375],
                "image": "data\\images\\2328263.jpg"
            },
            {
                "VG_image_id": "2331009",
                "VG_object_id": "2830636",
                "bbox": [96, 62, 353, 327],
                "image": "data\\images\\2331009.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["what  is the girl holding", 1],
            ["where is the girl", 1],
            ["what is the woman doing", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what  is the girl holding", 1],
            ["where is the girl", 1],
            ["what is the woman doing", 1],
            ["how many people are shown", -1],
            ["who is in the photo", -1],
            ["where was this photo taken", -1],
            ["what is the girl looking at", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a person holding a piece of paper in front of a refrigerator.",
            "a child sitting in a high chair eating cake."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2412885",
                "VG_object_id": "3802565",
                "bbox": [246, 101, 345, 199],
                "image": "data\\images\\2412885.jpg"
            },
            {
                "VG_image_id": "2359055",
                "VG_object_id": "2409287",
                "bbox": [194, 235, 230, 311],
                "image": "data\\images\\2359055.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the ground", 1],
            ["where is the picture taken", 1],
            ["what is the persion doing", 1],
            ["where is the person", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["What color is the ground", 1],
            ["what is the person on", -1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["what is the boy wearing", -1],
            ["when was this picture taken", -1],
            ["what kind of sport is the person doing", -1],
            ["what gender is the person", -1],
            ["where is the picture taken", 1],
            ["what is the persion doing", 1],
            ["where is the person", 1],
            ["what is the gender of the person", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a man riding a skateboard up the side of a ramp.",
            "a boy riding a skateboard down a street."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2343225",
                "VG_object_id": "2153881",
                "bbox": [24, 99, 218, 264],
                "image": "data\\images\\2343225.jpg"
            },
            {
                "VG_image_id": "2348790",
                "VG_object_id": "2527278",
                "bbox": [52, 298, 172, 374],
                "image": "data\\images\\2348790.jpg"
            }
        ],
        "questions_with_scores": [["What color is the table", 1]],
        "org_questions": [
            ["How many people are there", -1],
            ["What color is the table", 1],
            ["What is the food in the plate", -1],
            ["what is in the background", -1],
            ["where is the plate", -1],
            ["what type of food is shown", -1],
            ["what kind of meat is on the plate", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a large roast beef sandwich sitting on top of a plate.",
            "a man holding a sandwich in front of a window."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2363055",
                "VG_object_id": "768811",
                "bbox": [141, 8, 321, 238],
                "image": "data\\images\\2363055.jpg"
            },
            {
                "VG_image_id": "2376475",
                "VG_object_id": "3175299",
                "bbox": [285, 41, 416, 238],
                "image": "data\\images\\2376475.jpg"
            }
        ],
        "questions_with_scores": [
            ["WHat color is man's shirt", 2],
            ["What is man doing", 1],
            ["What is the background of image", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["WHat color is man's shirt", 2],
            ["What is man doing", 1],
            ["What is the background of image", 1],
            ["how many people are there", 1],
            ["where is the photo taken", -1],
            ["where is the man", -1],
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man wearing a purple tie and a white shirt.",
            "a man with a dog standing next to a pile of luggage."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2366351",
                "VG_object_id": "2098496",
                "bbox": [184, 163, 289, 279],
                "image": "data\\images\\2366351.jpg"
            },
            {
                "VG_image_id": "2412519",
                "VG_object_id": "192640",
                "bbox": [248, 58, 361, 187],
                "image": "data\\images\\2412519.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are there in the picture", 2],
            ["what is the man holding", 1],
            ["what color is the background", 1],
            ["where is the photo taken", 1],
            ["what is on the man's head", 1],
            ["what is the person doing", 1],
            ["where is the person", 1]
        ],
        "org_questions": [
            ["what is the man holding", 1],
            ["what color is the background", 1],
            ["what sport is the man playing", -1],
            ["where is the photo taken", 1],
            ["what is on the man's head", 1],
            ["what is the person doing", 1],
            ["what is in the distance", -1],
            ["where is the person", 1],
            ["who is wearing a white shirt", -1],
            ["what type of shirt is the man wearing", -1],
            ["when was the picture taken", -1],
            ["what is the man wearing", -1],
            ["how many dogs are there in the picture", 2]
        ],
        "context": [
            "a man playing tennis on a clay court.",
            "a man playing with a dog in a field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2371276",
                "VG_object_id": "2370172",
                "bbox": [120, 73, 252, 419],
                "image": "data\\images\\2371276.jpg"
            },
            {
                "VG_image_id": "2391901",
                "VG_object_id": "485106",
                "bbox": [333, 85, 457, 292],
                "image": "data\\images\\2391901.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["What color s the woman's dress", 2],
            ["what is on the woman's head", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what is the woman sitting on", -1],
            ["What color s the woman's dress", 2],
            ["what is on the woman's head", 1],
            ["what is the floor made of", -1],
            ["what is the woman doing", -1],
            ["what is the woman holding", -1],
            ["when was this photo taken", -1],
            ["who is on the bench", -1],
            ["where is the woman", -1],
            ["what is the girl wearing", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a woman in a pink dress is waiting on a bench.",
            "a woman and a child sitting on a bench."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2413310",
                "VG_object_id": "176059",
                "bbox": [221, 78, 321, 164],
                "image": "data\\images\\2413310.jpg"
            },
            {
                "VG_image_id": "2393661",
                "VG_object_id": "1217988",
                "bbox": [279, 203, 444, 363],
                "image": "data\\images\\2393661.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["how many chairs are in the room", 2],
            ["what is behind the chair", 1]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["How many people are there", -1],
            ["where is the chair", -1],
            ["what is the chair made of", -1],
            ["what is behind the chair", 1],
            ["What is above the chair", -1],
            ["what room is this", -1],
            ["what is in the room", -1],
            ["what is next to the couch", -1],
            ["how many chairs are in the room", 2]
        ],
        "context": [
            "a room with a computer and a computer monitor.",
            "a group of people sitting in a waiting room."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2387529",
                "VG_object_id": "512757",
                "bbox": [1, 0, 499, 330],
                "image": "data\\images\\2387529.jpg"
            },
            {
                "VG_image_id": "2347865",
                "VG_object_id": "2197646",
                "bbox": [3, 2, 496, 374],
                "image": "data\\images\\2347865.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what color are the plates on the table", 1],
            ["what kind of food is it", 1]
        ],
        "org_questions": [
            ["what is on the table", -1],
            ["what color is the table", 1],
            ["what color are the plates on the table", 1],
            ["how many cups are there", -1],
            ["what shape is the plate on the table", -1],
            ["where is the photo taken", -1],
            ["what is the table made of", -1],
            ["what kind of food is it", 1],
            ["when was this picture taken", -1],
            ["where are the plates", -1]
        ],
        "context": [
            "a plate of food with a cup of coffee and onion rings.",
            "a slice of pizza on a black plate"
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2388874",
                "VG_object_id": "1266090",
                "bbox": [81, 313, 277, 373],
                "image": "data\\images\\2388874.jpg"
            },
            {
                "VG_image_id": "2324126",
                "VG_object_id": "3426991",
                "bbox": [0, 302, 374, 498],
                "image": "data\\images\\2324126.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 1],
            ["what color is the floor", 1],
            ["what color is the refrigerator", 1],
            ["what is standing on the floor", 1],
            ["what is the ground covered with", 1],
            ["what type of flooring is shown", 1]
        ],
        "org_questions": [
            ["what is the floor made of", 1],
            ["what color is the floor", 1],
            ["what color is the refrigerator", 1],
            ["where is the picture taken", -1],
            ["what is standing on the floor", 1],
            ["what is on the floor", -1],
            ["what is the ground covered with", 1],
            ["What is floor made of", -1],
            ["what room is this", -1],
            ["what type of flooring is shown", 1],
            ["where is the floor", -1]
        ],
        "context": [
            "a man and a woman sitting in a kitchen.",
            "a black refrigerator in a room with a red floor."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2341778",
                "VG_object_id": "3068531",
                "bbox": [2, 15, 498, 372],
                "image": "data\\images\\2341778.jpg"
            },
            {
                "VG_image_id": "2317689",
                "VG_object_id": "1018415",
                "bbox": [146, 223, 443, 371],
                "image": "data\\images\\2317689.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is the table", 1],
            ["What is on the table", 1],
            ["What food is on the plate", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["What is on the table", 1],
            ["How many people are there", 2],
            ["what shape are the containers under the food", -1],
            ["What food is on the plate", 1],
            ["what is the food on", -1],
            ["what is covering the table", -1],
            ["how many plates are on the table", -1],
            ["where are the plates", -1]
        ],
        "context": [
            "a large cake with a slice cut out of it.",
            "a group of people sitting at a table with food."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2350101",
                "VG_object_id": "2138657",
                "bbox": [134, 73, 404, 299],
                "image": "data\\images\\2350101.jpg"
            },
            {
                "VG_image_id": "2398677",
                "VG_object_id": "1177350",
                "bbox": [104, 145, 358, 441],
                "image": "data\\images\\2398677.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what is the woman sitting on", 1],
            ["what color is the woman's clothes", 1],
            ["where is the woman sitting", 1]
        ],
        "org_questions": [
            ["what is the woman sitting on", 1],
            ["what color is the woman's clothes", 1],
            ["how many people are there", -1],
            ["where is the woman", -1],
            ["what is the woman holding", 2],
            ["what is the woman wearing", -1],
            ["who is in the photo", -1],
            ["what is the lady doing", -1],
            ["what is on the woman's head", -1],
            ["where was the photo taken", -1],
            ["where is the woman sitting", 1]
        ],
        "context": [
            "a woman sitting on a bed looking out a window.",
            "a woman sitting in a chair reading a book."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2380807",
                "VG_object_id": "543603",
                "bbox": [36, 2, 498, 131],
                "image": "data\\images\\2380807.jpg"
            },
            {
                "VG_image_id": "2394751",
                "VG_object_id": "461388",
                "bbox": [81, 47, 456, 160],
                "image": "data\\images\\2394751.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the cabinet", 2],
            ["what is white", 1]
        ],
        "org_questions": [
            ["What color is the cabinet", 2],
            ["What color is the counter", -1],
            ["how many people are there", -1],
            ["what is on the wall", -1],
            ["where is the cabinet", -1],
            ["what is under the cabinet", -1],
            ["What is on the cabinet", -1],
            ["what room is this", -1],
            ["what are the cabinets made of", -1],
            ["where was this picture taken", -1],
            ["what is white", 1],
            ["what is in the kitchen", -1]
        ],
        "context": [
            "a kitchen with a microwave, microwave, and a counter top.",
            "a kitchen with a stove, microwave, and a microwave."
        ]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2376026",
                "VG_object_id": "2344521",
                "bbox": [131, 182, 211, 339],
                "image": "data\\images\\2376026.jpg"
            },
            {
                "VG_image_id": "2373908",
                "VG_object_id": "2947446",
                "bbox": [193, 18, 251, 98],
                "image": "data\\images\\2373908.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are under the light", 2],
            ["how many lights are on", 2],
            ["what color of light is turned on", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many lights are there", -1],
            ["what color of light is turned on", 1],
            ["what is the shape of the lamp", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["what is in the background", 1],
            ["where is the light light", -1],
            ["what is on the street", -1],
            ["how many people are under the light", 2],
            ["how many lights are on", 2]
        ],
        "context": [
            "a traffic light with a green walk light and a red hand",
            "a group of construction workers standing around a traffic light."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2321705",
                "VG_object_id": "1047266",
                "bbox": [195, 96, 269, 323],
                "image": "data\\images\\2321705.jpg"
            },
            {
                "VG_image_id": "2355592",
                "VG_object_id": "828285",
                "bbox": [137, 227, 203, 411],
                "image": "data\\images\\2355592.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bottle", 2],
            ["how many people are there in the picture", 2],
            ["what color is the table", 1],
            ["what kind of drink does the bottle contain", 1],
            ["what is behind the bottle", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the bottle", 2],
            ["what color is the table", 1],
            ["what kind of drink does the bottle contain", 1],
            ["How many people are there", -1],
            ["Where is the photo taken", -1],
            ["what is behind the bottle", 1],
            ["what is the table under the bottle made of", -1],
            ["what is on top of the bottle", -1],
            ["where is the bottle of water", -1],
            ["what is on the table", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a bottle of water sitting on a table next to a remote control.",
            "a table with several pizzas on it and a bottle of beer."
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2403096",
                "VG_object_id": "356622",
                "bbox": [2, 122, 448, 374],
                "image": "data\\images\\2403096.jpg"
            },
            {
                "VG_image_id": "2414796",
                "VG_object_id": "151721",
                "bbox": [142, 163, 463, 329],
                "image": "data\\images\\2414796.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many screens are there on the desk", 2],
            ["What color is the wall", 1]
        ],
        "org_questions": [
            ["What color is the desk", -1],
            ["What is on the desk", -1],
            ["How many laptops are there", -1],
            ["How many screens are there on the desk", 2],
            ["What color is the wall", 1],
            ["how many dogs are there in the picture", -1],
            ["What color is the keyboard", -1],
            ["where is the computer", -1],
            ["what is the desk made of", -1],
            ["what is the desk sitting on", -1],
            ["what is behind the desk", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a wooden desk.",
            "a desk with a computer and a printer on it."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2355324",
                "VG_object_id": "830653",
                "bbox": [338, 10, 411, 265],
                "image": "data\\images\\2355324.jpg"
            },
            {
                "VG_image_id": "2328483",
                "VG_object_id": "3313493",
                "bbox": [200, 60, 260, 167],
                "image": "data\\images\\2328483.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's clothes", 2],
            ["what is the girl holding", 2],
            ["where is the girl", 1],
            ["what is on the girl's head", 1],
            ["What is girl doing", 1],
            ["what is the girl doing", 1],
            ["what is the woman standing on", 1]
        ],
        "org_questions": [
            ["what color is the girl's clothes", 2],
            ["what is the girl holding", 2],
            ["what is the weather like", -1],
            ["where is the girl", 1],
            ["what is on the girl's head", 1],
            ["What is girl doing", 1],
            ["what is the girl doing", 1],
            ["what color is the background", -1],
            ["how many people are there", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman standing on", 1]
        ],
        "context": [
            "a young girl holding a tennis racquet on a tennis court.",
            "a little girl holding an umbrella walking down a street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2363818",
                "VG_object_id": "3743459",
                "bbox": [5, 0, 484, 330],
                "image": "data\\images\\2363818.jpg"
            },
            {
                "VG_image_id": "2327909",
                "VG_object_id": "3518213",
                "bbox": [0, 319, 333, 499],
                "image": "data\\images\\2327909.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 2],
            ["what is on the land", 1],
            ["how many people are in the picture", 1],
            ["What is in the background of image", 1]
        ],
        "org_questions": [
            ["what color is the land", 2],
            ["what is on the land", 1],
            ["how many people are in the picture", 1],
            ["what is the weather like", -1],
            ["what is the ground covered with", -1],
            ["What is in the background of image", 1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["how is the weather", -1]
        ],
        "context": [
            "a dog laying on the ground with its tongue hanging out.",
            "three people sitting on a motorcycle in front of a building."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2339767",
                "VG_object_id": "2533157",
                "bbox": [102, 132, 246, 352],
                "image": "data\\images\\2339767.jpg"
            },
            {
                "VG_image_id": "2352805",
                "VG_object_id": "2044988",
                "bbox": [110, 50, 364, 252],
                "image": "data\\images\\2352805.jpg"
            }
        ],
        "questions_with_scores": [["where was the photo taken", 1]],
        "org_questions": [
            ["where is the zebra", -1],
            ["what is the color of the land", -1],
            ["how many zebras are there", -1],
            ["what is the ground covered with", -1],
            ["What is zebra doing", -1],
            ["what is in the distance", -1],
            ["What is the distance of the zebras", -1],
            ["when was the picture taken", -1],
            ["what kind of animal is in the picture", -1],
            ["who is in the photo", -1],
            ["what is the zebra standing on", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "two zebras standing in a field of grass with trees in the background.",
            "a mother zebra and her baby are standing in a field."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2344935",
                "VG_object_id": "2254742",
                "bbox": [0, 0, 499, 180],
                "image": "data\\images\\2344935.jpg"
            },
            {
                "VG_image_id": "2351369",
                "VG_object_id": "2215292",
                "bbox": [1, 139, 498, 332],
                "image": "data\\images\\2351369.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the keyboard", 1],
            ["how many plates are there on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["where is the cellphone", -1],
            ["what color is the keyboard", 1],
            ["how many people are there", -1],
            ["what is the table made of", -1],
            ["how many plates are there on the table", 1],
            ["what is sitting on the table", -1],
            ["what is the computer sitting on", -1],
            ["what is on the desk", -1]
        ],
        "context": [
            "a computer desk with a keyboard, mouse, and various electronic devices.",
            "a laptop computer sitting on top of a wooden table."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2321654",
                "VG_object_id": "2779153",
                "bbox": [204, 83, 275, 193],
                "image": "data\\images\\2321654.jpg"
            },
            {
                "VG_image_id": "2404823",
                "VG_object_id": "1112785",
                "bbox": [91, 376, 148, 496],
                "image": "data\\images\\2404823.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man doing", 2],
            ["Where is the photo taken", 1],
            ["What color is man's shirt", 1],
            ["where is the man", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["What is man doing", 2],
            ["How many people are there in the image", -1],
            ["Where is the photo taken", 1],
            ["What color is man's shirt", 1],
            ["what gesture is the man", -1],
            ["where is the man", 1],
            ["what are the people doing", 1],
            ["what is on the man's head", -1],
            ["when was the photo taken", -1],
            ["what kind of pants is the man wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man is leading two horses through a field.",
            "a man flying a kite on a beach."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2392314",
                "VG_object_id": "3826062",
                "bbox": [58, 60, 331, 321],
                "image": "data\\images\\2392314.jpg"
            },
            {
                "VG_image_id": "2330048",
                "VG_object_id": "3723373",
                "bbox": [348, 5, 499, 296],
                "image": "data\\images\\2330048.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 1],
            ["what is the cat sitting on", 1],
            ["where is the cat", 1]
        ],
        "org_questions": [
            ["what color is the cat", -1],
            ["what is on the cat's neck", -1],
            ["how many cats are there", -1],
            ["where is the picture taken", 1],
            ["what is the cat sitting on", 1],
            ["what is the cat doing", -1],
            ["where is the cat", 1],
            ["who is in the photo", -1],
            ["what is the cat looking at", -1],
            ["what is the cat wearing", -1],
            ["what is on the cat", -1]
        ],
        "context": [
            "a cat is sitting in a suitcase with clothes in it.",
            "a cat is laying on a bed next to a laptop."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2327095",
                "VG_object_id": "2898373",
                "bbox": [288, 50, 440, 232],
                "image": "data\\images\\2327095.jpg"
            },
            {
                "VG_image_id": "2396205",
                "VG_object_id": "446928",
                "bbox": [145, 50, 307, 399],
                "image": "data\\images\\2396205.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["how many people are there in the picture", 1],
            ["what gesture is the man", 1],
            ["what is the man holding", 1],
            ["when was the picture taken", 1],
            ["what is behind the man", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 2],
            ["how many people are there in the picture", 1],
            ["where is the photo taken", -1],
            ["what is the man wearing", -1],
            ["what gesture is the man", 1],
            ["what is the man holding", 1],
            ["when was the picture taken", 1],
            ["who is in the photo", -1],
            ["what is behind the man", 1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a motorcycle parked on the side of the road.",
            "a man sitting on a horse in a dirt field."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2367211",
                "VG_object_id": "1692696",
                "bbox": [119, 0, 494, 137],
                "image": "data\\images\\2367211.jpg"
            },
            {
                "VG_image_id": "2363388",
                "VG_object_id": "1889387",
                "bbox": [0, 1, 371, 140],
                "image": "data\\images\\2363388.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cabinet", 1],
            ["what is the main color of the wall", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", 1],
            ["what is the main color of the wall", 1],
            ["how many people are there", 1],
            ["where is the cabinet", -1],
            ["what is on the wall", -1],
            ["What is in the cabinet", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "a woman in a gray dress eating an apple.",
            "a pineapple with bananas on top of it"
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2411106",
                "VG_object_id": "316687",
                "bbox": [368, 244, 453, 306],
                "image": "data\\images\\2411106.jpg"
            },
            {
                "VG_image_id": "2368658",
                "VG_object_id": "616781",
                "bbox": [313, 182, 367, 251],
                "image": "data\\images\\2368658.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the pillows", 2],
            ["what color is the wall", 1]
        ],
        "org_questions": [
            ["what color are the pillows", 2],
            ["what color is the wall", 1],
            ["how many pillows are there", -1],
            ["where is the photo taken", -1],
            ["what is the pillow placed on", -1],
            ["where is the pillow placed on", -1],
            ["where is the pillow", -1],
            ["what is the bed made of", -1],
            ["what is on the bed", -1],
            ["what is next to the bed", -1],
            ["where are the pillows", -1],
            ["how many pillows are on the bed", -1]
        ],
        "context": [
            "a hotel room with a bed and a door",
            "a bedroom with a bed, dresser, and a window."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2325838",
                "VG_object_id": "984720",
                "bbox": [4, 88, 373, 498],
                "image": "data\\images\\2325838.jpg"
            },
            {
                "VG_image_id": "2334766",
                "VG_object_id": "3634823",
                "bbox": [8, 2, 493, 369],
                "image": "data\\images\\2334766.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animal is in the picture", 2],
            ["what is in the background", 1],
            ["what type of floor is this", 1]
        ],
        "org_questions": [
            ["what is on the floor", -1],
            ["what animal is in the picture", 2],
            ["what color is the floor", -1],
            ["How many people are there", -1],
            ["where is the floor", -1],
            ["what pattern is the floor", -1],
            ["what is the floor made of", -1],
            ["how many tables are on the floor", -1],
            ["who is in the picture", -1],
            ["where was the photo taken", -1],
            ["what is in the background", 1],
            ["what type of floor is this", 1]
        ],
        "context": [
            "a white dog laying on a wooden floor.",
            "a parrot perched on a pair of shoes."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2341931",
                "VG_object_id": "2908395",
                "bbox": [188, 87, 300, 214],
                "image": "data\\images\\2341931.jpg"
            },
            {
                "VG_image_id": "2391863",
                "VG_object_id": "485511",
                "bbox": [35, 124, 106, 211],
                "image": "data\\images\\2391863.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 2],
            ["how large is the screen", 1],
            ["what is behind the screen", 1]
        ],
        "org_questions": [
            ["what is the screen of", -1],
            ["how large is the screen", 1],
            ["what is behind the screen", 1],
            ["how many screens are there", -1],
            ["what is in front of the screen", -1],
            ["who is in the photo", -1],
            ["what is on the screen screen", -1],
            ["what is the person holding", -1],
            ["what is in the background", 2]
        ],
        "context": [
            "a person holding a cell phone in their hand.",
            "a man sitting at a desk using a laptop computer."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2318879",
                "VG_object_id": "3460485",
                "bbox": [3, 237, 497, 373],
                "image": "data\\images\\2318879.jpg"
            },
            {
                "VG_image_id": "2343933",
                "VG_object_id": "3099444",
                "bbox": [79, 321, 272, 374],
                "image": "data\\images\\2343933.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 2],
            ["what animal is standing on the ground", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what  is on the ground", -1],
            ["how many people are there", -1],
            ["what animal is standing on the ground", 1],
            ["what is the ground covered with", -1],
            ["where was the photo taken", 1],
            ["how is the weather", -1],
            ["what is in the background", 2],
            ["what is the weather like", -1]
        ],
        "context": [
            "a herd of cattle standing on a dirt field.",
            "a man riding on the back of a horse in a rodeo."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2341776",
                "VG_object_id": "3652924",
                "bbox": [18, 276, 413, 466],
                "image": "data\\images\\2341776.jpg"
            },
            {
                "VG_image_id": "2387551",
                "VG_object_id": "1275041",
                "bbox": [0, 134, 499, 372],
                "image": "data\\images\\2387551.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what kind of animal is it", 1],
            ["how many cows are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what kind of animal is it", 1],
            ["how many cows are there in the picture", 1],
            ["what is in the distance", -1],
            ["where are the trees", -1],
            ["what is the weather like", -1],
            ["where was the photo taken", -1],
            ["what is covering the ground", -1],
            ["what is green", -1]
        ],
        "context": [
            "a group of horses walking across a dirt field.",
            "a herd of cattle grazing on a lush green field."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2361167",
                "VG_object_id": "1713302",
                "bbox": [143, 52, 235, 332],
                "image": "data\\images\\2361167.jpg"
            },
            {
                "VG_image_id": "2397659",
                "VG_object_id": "1187866",
                "bbox": [62, 23, 292, 431],
                "image": "data\\images\\2397659.jpg"
            }
        ],
        "questions_with_scores": [["what color is the clock", 1]],
        "org_questions": [
            ["what color is the sky", -1],
            ["what color is the clock", 1],
            ["what color is the tower", -1],
            ["What time is it", -1],
            ["what is on the top of the tower", -1],
            ["how tall is the tower", -1],
            ["what is the tower made of", -1],
            ["what is clock  on", -1],
            ["where are the clocks", -1],
            ["how many clocks are there", -1],
            ["when was the photo taken", -1],
            ["what is on the building", -1]
        ],
        "context": [
            "a tall clock tower in a city.",
            "a clock tower with a gold and black clock."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2363054",
                "VG_object_id": "3747363",
                "bbox": [4, 137, 484, 320],
                "image": "data\\images\\2363054.jpg"
            },
            {
                "VG_image_id": "2354098",
                "VG_object_id": "841049",
                "bbox": [32, 252, 405, 360],
                "image": "data\\images\\2354098.jpg"
            }
        ],
        "questions_with_scores": [["what color is the bed", 1]],
        "org_questions": [
            ["what is on the bed", -1],
            ["what color is the bed", 1],
            ["how many cats are on the blanket", -1],
            ["what is next to the bed", -1],
            ["what is covering the bed", -1],
            ["what is laying on the bed", -1]
        ],
        "context": [
            "a cat laying on a bed with a stuffed duck.",
            "a dog laying on a bed looking out a window."
        ]
    },
    {
        "object_category": "book",
        "images": [
            {
                "VG_image_id": "2393025",
                "VG_object_id": "475501",
                "bbox": [418, 64, 500, 115],
                "image": "data\\images\\2393025.jpg"
            },
            {
                "VG_image_id": "2377259",
                "VG_object_id": "1780581",
                "bbox": [373, 201, 475, 241],
                "image": "data\\images\\2377259.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the book cover", 1],
            ["how many books are there", 1],
            ["what is in the background", 1],
            ["what is on the table", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the book cover", 1],
            ["how many books are there", 1],
            ["what color is the floor", -1],
            ["what is in the background", 1],
            ["where is the book", -1],
            ["what is the shelf made of", -1],
            ["what is the book on", -1],
            ["what is on the table", 1],
            ["where was the photo taken", -1],
            ["how many people are in the photo", 1],
            ["where are the books", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a wooden table.",
            "a man cutting a cake on a table"
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2379536",
                "VG_object_id": "552190",
                "bbox": [2, 132, 169, 500],
                "image": "data\\images\\2379536.jpg"
            },
            {
                "VG_image_id": "2377083",
                "VG_object_id": "567974",
                "bbox": [218, 48, 291, 163],
                "image": "data\\images\\2377083.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["how many girls are there", 1],
            ["what is the ground covered with", 1],
            ["where is the girl", 1],
            ["who is in the photo", 1],
            ["what is the little girl wearing", 1],
            ["how many people are in the photo", 1],
            ["how many people are there", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", -1],
            ["what is the girl doing", 1],
            ["how many girls are there", 1],
            ["what is the girl wearing on the head", -1],
            ["what is the ground covered with", 1],
            ["what time is it", -1],
            ["where is the girl", 1],
            ["what is the woman holding", -1],
            ["who is in the photo", 1],
            ["when was the photo taken", -1],
            ["what is the little girl wearing", 1],
            ["how many people are in the photo", 1],
            ["how many people are there", 1],
            ["what is the persion standing on", 1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a little girl feeding a cow in a barn.",
            "a man and two girls in a boat on the water."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2400525",
                "VG_object_id": "1157887",
                "bbox": [0, 193, 500, 374],
                "image": "data\\images\\2400525.jpg"
            },
            {
                "VG_image_id": "2382262",
                "VG_object_id": "1329862",
                "bbox": [4, 230, 370, 500],
                "image": "data\\images\\2382262.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["where was this photo taken", 2],
            ["what is the floor made of", 1],
            ["how many people are there", 1],
            ["what pattern is the land", 1],
            ["what color is the wall", 1],
            ["what is in the background", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is the floor made of", 1],
            ["where is the photo taken", 2],
            ["how many people are there", 1],
            ["what pattern is the land", 1],
            ["what color is the wall", 1],
            ["what is in the background", 1],
            ["what is the ground covered with", 1],
            ["where was this photo taken", 2]
        ],
        "context": [
            "a person standing in a room holding a remote.",
            "a sign on a pole"
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2327274",
                "VG_object_id": "2778285",
                "bbox": [0, 7, 204, 312],
                "image": "data\\images\\2327274.jpg"
            },
            {
                "VG_image_id": "2405467",
                "VG_object_id": "373081",
                "bbox": [138, 5, 259, 330],
                "image": "data\\images\\2405467.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman holding", 2],
            ["Where is the woman", 2],
            ["what is the lady doing", 1],
            ["what is in the background", 1],
            ["what is the lady wearing", 1]
        ],
        "org_questions": [
            ["What is woman holding", 2],
            ["Where is the woman", 2],
            ["how many people are there", -1],
            ["what is the lady doing", 1],
            ["what is in the background", 1],
            ["what is the lady wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["how is the woman's hair", -1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "a woman holding an umbrella in her hand.",
            "a woman in a skirt talking on a cell phone."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2370585",
                "VG_object_id": "2353365",
                "bbox": [251, 0, 456, 218],
                "image": "data\\images\\2370585.jpg"
            },
            {
                "VG_image_id": "2319216",
                "VG_object_id": "1003709",
                "bbox": [31, 18, 316, 463],
                "image": "data\\images\\2319216.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the people's helmet", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what color is the ground", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what color is the people's helmet", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["what is the person riding on", -1],
            ["What is the person holding", -1],
            ["what color is the ground", 1],
            ["when was the picture taken", -1],
            ["who is on the motorcycle", -1],
            ["what is on the man's head", -1],
            ["what is the person wearing", 1]
        ],
        "context": [
            "person and his motorcycle are racing.",
            "a man sitting on a motorcycle in front of a building."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2344518",
                "VG_object_id": "915203",
                "bbox": [311, 27, 436, 226],
                "image": "data\\images\\2344518.jpg"
            },
            {
                "VG_image_id": "2331742",
                "VG_object_id": "3655718",
                "bbox": [281, 5, 485, 340],
                "image": "data\\images\\2331742.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what is the man holding", 1],
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["what is in the background", -1],
            ["what sport is the man playing", -1],
            ["what is the man wearing", -1],
            ["where is the man looking", -1],
            ["who is in the photo", -1],
            ["what is on the man's face", -1]
        ],
        "context": [
            "a man and a woman eating food at a table.",
            "a man holding a cake with a knife."
        ]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2376294",
                "VG_object_id": "1913037",
                "bbox": [216, 26, 286, 96],
                "image": "data\\images\\2376294.jpg"
            },
            {
                "VG_image_id": "2390913",
                "VG_object_id": "1245928",
                "bbox": [412, 16, 476, 105],
                "image": "data\\images\\2390913.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the light", 2],
            ["where is the light", 1],
            ["what room is the light in", 1],
            ["what is on the wall", 1],
            ["what is hanging on the wall", 1]
        ],
        "org_questions": [
            ["what shape is the light", 2],
            ["where is the light", 1],
            ["how many people are there", -1],
            ["what room is the light in", 1],
            ["what is on the wall", 1],
            ["What color is the lamp", -1],
            ["where is the light coming from", -1],
            ["how many lights are there", -1],
            ["what is hanging on the wall", 1]
        ],
        "context": [
            "a living room with a couch, chairs, and a television.",
            "a bed with a black and white bed spread."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2365477",
                "VG_object_id": "3884091",
                "bbox": [26, 166, 460, 319],
                "image": "data\\images\\2365477.jpg"
            },
            {
                "VG_image_id": "2395860",
                "VG_object_id": "1202622",
                "bbox": [18, 243, 497, 367],
                "image": "data\\images\\2395860.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the animal", 2],
            ["how many animals are there", 2],
            ["what is the animal in the picture", 1]
        ],
        "org_questions": [
            ["what is the animal", 2],
            ["how many animals are there", 2],
            ["what is the ground covered with", -1],
            ["what is in the background", -1],
            ["what is on the ground", -1],
            ["what is the animal in the picture", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "two zebras standing next to each other on a dirt ground.",
            "a horse standing in a field next to some trees."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2362621",
                "VG_object_id": "3465145",
                "bbox": [39, 149, 499, 373],
                "image": "data\\images\\2362621.jpg"
            },
            {
                "VG_image_id": "2407109",
                "VG_object_id": "285369",
                "bbox": [0, 215, 498, 374],
                "image": "data\\images\\2407109.jpg"
            }
        ],
        "questions_with_scores": [
            ["What are people doing", 2],
            ["where was the photo taken", 2],
            ["what color is the floor", 1],
            ["what is on the ground", 1],
            ["how many people are there", 1],
            ["where is picture taken", 1],
            ["what is the land made of", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is on the ground", 1],
            ["how many people are there", 1],
            ["where is picture taken", 1],
            ["what kind of animal is on the land", -1],
            ["What are people doing", 2],
            ["what is the land made of", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["where are the shadows", -1],
            ["where was the photo taken", 2],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "two police officers on motorcycles on a city street.",
            "a man standing on top of a bus with a dog on top of it."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2366879",
                "VG_object_id": "1812333",
                "bbox": [300, 8, 487, 306],
                "image": "data\\images\\2366879.jpg"
            },
            {
                "VG_image_id": "2349505",
                "VG_object_id": "2852892",
                "bbox": [1, 0, 428, 132],
                "image": "data\\images\\2349505.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 2],
            ["how many people are there", 2],
            ["what is in the background", 1],
            ["where is the picture taken", 1],
            ["where is the curtain", 1],
            ["who is in the photo", 1],
            ["what is in the photo", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 2],
            ["what is in the background", 1],
            ["how many people are there", 2],
            ["where is the picture taken", 1],
            ["where is the curtain", 1],
            ["who is in the photo", 1],
            ["what is hanging on the wall", -1],
            ["what is in the photo", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a bedroom with a bed, mirror, and a mirror.",
            "two people sitting on a blue seat with their luggage."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2370610",
                "VG_object_id": "1688314",
                "bbox": [2, 179, 497, 331],
                "image": "data\\images\\2370610.jpg"
            },
            {
                "VG_image_id": "2377251",
                "VG_object_id": "3115901",
                "bbox": [0, 0, 499, 354],
                "image": "data\\images\\2377251.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is being played", 2],
            ["what are the people playing", 2],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what are the people doing", -1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["what is the color of the grass", -1],
            ["what kind of animals are there in the background", -1],
            ["what is in the distance", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["what sport is being played", 2],
            ["where are the people", -1],
            ["what are the people playing", 2]
        ],
        "context": [
            "a young boy playing tennis on a tennis court.",
            "a man walking across a field with a football."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2372889",
                "VG_object_id": "2383979",
                "bbox": [147, 72, 329, 294],
                "image": "data\\images\\2372889.jpg"
            },
            {
                "VG_image_id": "2417018",
                "VG_object_id": "3358134",
                "bbox": [339, 92, 499, 292],
                "image": "data\\images\\2417018.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["what color are the trees", 1]
        ],
        "org_questions": [
            ["what color is the building", 2],
            ["what is in the front of the picture", -1],
            ["how many building are there", -1],
            ["where is the building", -1],
            ["what is the weather like", -1],
            ["what is the building made of", -1],
            ["when was the picture taken", -1],
            ["what color is the sky", -1],
            ["what color are the trees", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a street sign that is on a pole.",
            "a man flying through the air while riding a skateboard."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2387231",
                "VG_object_id": "1277847",
                "bbox": [28, 127, 178, 201],
                "image": "data\\images\\2387231.jpg"
            },
            {
                "VG_image_id": "2416688",
                "VG_object_id": "3003869",
                "bbox": [1, 0, 329, 387],
                "image": "data\\images\\2416688.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what transportation is there", 1],
            ["What is in front of building", 1],
            ["where was the photo taken", 1],
            ["how many giraffes are there in front of the building", 1],
            ["how many people are there in front of the building", 1],
            ["how is the weather", 1],
            ["what is on the building", 1],
            ["where was this picture taken", 1],
            ["what is in front of the building", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["how many people are there", 1],
            ["what time is it", -1],
            ["what transportation is there", 1],
            ["What is in front of building", 1],
            ["what is the building made of", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", 1],
            ["how many giraffes are there in front of the building", 1],
            ["how many people are there in front of the building", 1],
            ["how is the weather", 1],
            ["what is on the building", 1],
            ["when was the picture taken", -1],
            ["where was this picture taken", 1],
            ["what is in front of the building", 1]
        ],
        "context": [
            "a group of people walking in the snow.",
            "a giraffe standing in a doorway next to a brick building."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2376678",
                "VG_object_id": "570689",
                "bbox": [39, 1, 453, 334],
                "image": "data\\images\\2376678.jpg"
            },
            {
                "VG_image_id": "2407129",
                "VG_object_id": "1924061",
                "bbox": [169, 64, 409, 373],
                "image": "data\\images\\2407129.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the man's trouser", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the man's trouser", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the man doing", -1],
            ["what is the man wearing", -1],
            ["what is the man holding", -1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what sport is being played", -1],
            ["what is in the background", -1],
            ["where is the man", -1]
        ],
        "context": [
            "a man hitting a tennis ball with a racquet.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2412818",
                "VG_object_id": "3495804",
                "bbox": [82, 65, 416, 301],
                "image": "data\\images\\2412818.jpg"
            },
            {
                "VG_image_id": "2407521",
                "VG_object_id": "366766",
                "bbox": [85, 90, 383, 295],
                "image": "data\\images\\2407521.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the zebra doing", 1],
            ["how many zebras are there", 1],
            ["What is zebra doing", 1]
        ],
        "org_questions": [
            ["what is the zebra doing", 1],
            ["what color is the grass", -1],
            ["how many zebras are there", 1],
            ["what is the ground covered with", -1],
            ["what main color is the background", -1],
            ["What is zebra doing", 1],
            ["how many trees are there in the picture", -1],
            ["when was the photo taken", -1],
            ["what kind of animals are these", -1],
            ["where are the zebras", -1],
            ["what are the zebras standing on", -1]
        ],
        "context": [
            "two zebras standing next to each other in a field.",
            "a group of zebras grazing in a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2346543",
                "VG_object_id": "898456",
                "bbox": [343, 147, 477, 306],
                "image": "data\\images\\2346543.jpg"
            },
            {
                "VG_image_id": "2406545",
                "VG_object_id": "293799",
                "bbox": [169, 84, 495, 177],
                "image": "data\\images\\2406545.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the surfboard", 2],
            ["what are the people doing", 1],
            ["What is the man doing", 1],
            ["where is the surfboard", 1]
        ],
        "org_questions": [
            ["what color is the surfboard", 2],
            ["what are the people doing", 1],
            ["how many people are there", -1],
            ["What is the man wearing", -1],
            ["what color are the man's clothes", -1],
            ["What is the man doing", 1],
            ["how many people are there in the picture", -1],
            ["where is the surfboard", 1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a man sitting on a concrete wall with a surfboard.",
            "a man riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2323352",
                "VG_object_id": "3345733",
                "bbox": [2, 245, 239, 387],
                "image": "data\\images\\2323352.jpg"
            },
            {
                "VG_image_id": "2345154",
                "VG_object_id": "2049444",
                "bbox": [0, 206, 84, 331],
                "image": "data\\images\\2345154.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bicycle", 1],
            ["what is the woman doing", 1],
            ["what color is the woman's jacket", 1],
            ["what is beside the bicycle", 1]
        ],
        "org_questions": [
            ["what color is the bicycle", 1],
            ["what is the woman doing", 1],
            ["what color is the woman's jacket", 1],
            ["how many bikes are there", -1],
            ["what is the ground covered with", -1],
            ["where is the bicycle", -1],
            ["what is beside the bicycle", 1],
            ["where was the picture taken", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a group of women with umbrellas walking down a road.",
            "a woman is talking on a cell phone."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2414668",
                "VG_object_id": "154308",
                "bbox": [17, 20, 230, 160],
                "image": "data\\images\\2414668.jpg"
            },
            {
                "VG_image_id": "2343784",
                "VG_object_id": "2120637",
                "bbox": [0, 164, 327, 311],
                "image": "data\\images\\2343784.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shelf", 2],
            ["what is in front of the shelf", 1],
            ["what is the shelf made of", 1]
        ],
        "org_questions": [
            ["what color is the shelf", 2],
            ["how many books are there on the shelf", -1],
            ["Where is the photo taken", -1],
            ["what is in front of the shelf", 1],
            ["what is the shelf made of", 1],
            ["what is on the shelf", -1],
            ["where are the books", -1],
            ["what is on the left side of the picture", -1],
            ["what is in the background", -1],
            ["what is hanging on the wall", -1],
            ["what is on top of the wall", -1]
        ],
        "context": [
            "a living room with a television and a bookcase.",
            "a person is watching tv with their feet on a glass table."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "1592772",
                "VG_object_id": "2677202",
                "bbox": [161, 73, 459, 283],
                "image": "data\\images\\1592772.jpg"
            },
            {
                "VG_image_id": "2368297",
                "VG_object_id": "1787980",
                "bbox": [108, 374, 239, 487],
                "image": "data\\images\\2368297.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the cabinet", 1],
            ["what is above the cabinet", 1],
            ["what is on the counter", 1],
            ["what is on the top of the cabinet", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", -1],
            ["what is the cabinet made of", -1],
            ["where is the cabinet", 1],
            ["how many people are there", -1],
            ["What is in the cabinet", -1],
            ["what is above the cabinet", 1],
            ["what is on the counter", 1],
            ["what is on the top of the cabinet", 1]
        ],
        "context": [
            "a room with a chair and a computer on a desk.",
            "a library with a lot of clocks on the wall"
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2319772",
                "VG_object_id": "998916",
                "bbox": [1, 327, 319, 374],
                "image": "data\\images\\2319772.jpg"
            },
            {
                "VG_image_id": "2368445",
                "VG_object_id": "2691293",
                "bbox": [1, 316, 329, 498],
                "image": "data\\images\\2368445.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are on the rug", 1],
            ["what is the person doing", 1],
            ["what room is the rug in", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["where is the rug", -1],
            ["how many people are on the rug", 1],
            ["what is the person doing", 1],
            ["what room is the rug in", 1],
            ["what is on the rug", -1],
            ["what is the ground covered with", 1],
            ["what is covering the floor", -1],
            ["what material is the floor made of", -1]
        ],
        "context": [
            "a little boy is standing in front of a toilet.",
            "two women playing a game with remote controllers."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2354304",
                "VG_object_id": "2667380",
                "bbox": [17, 297, 374, 427],
                "image": "data\\images\\2354304.jpg"
            },
            {
                "VG_image_id": "2332515",
                "VG_object_id": "969418",
                "bbox": [46, 230, 210, 303],
                "image": "data\\images\\2332515.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sink", 2],
            ["what color is the wall behind the sink", 2],
            ["what color is the wall", 1]
        ],
        "org_questions": [
            ["what color is the sink", 2],
            ["what is the sink made of", -1],
            ["what color is the wall", 1],
            ["how many sinks are in the picture", -1],
            ["where is the sink", -1],
            ["what is on the sink", -1],
            ["what is the sink on", -1],
            ["what color is the wall behind the sink", 2],
            ["what room is this", -1],
            ["what is above the sink", -1],
            ["what is under the sink", -1]
        ],
        "context": [
            "a bathroom with a large mirror and a sink.",
            "a bathroom with a sink, toilet and a cabinet."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2388894",
                "VG_object_id": "669933",
                "bbox": [27, 44, 208, 453],
                "image": "data\\images\\2388894.jpg"
            },
            {
                "VG_image_id": "2340228",
                "VG_object_id": "948464",
                "bbox": [52, 2, 500, 373],
                "image": "data\\images\\2340228.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["where was the photo taken", 2],
            ["what is the man doing", 1],
            ["Where is the man", 1],
            ["what is the man holding", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["Where is the man", 1],
            ["what is the man wearing", -1],
            ["what is the man holding", 1],
            ["what is the man's posture", -1],
            ["who is in the photo", -1],
            ["what is on the man's face", 1],
            ["what kind of shirt is the man wearing", -1],
            ["where was the photo taken", 2],
            ["what is the man looking at", -1]
        ],
        "context": [
            "a man making a clay pot on a pottery wheel",
            "a man with a beard and glasses wearing a tie"
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2372449",
                "VG_object_id": "3733952",
                "bbox": [4, 134, 266, 368],
                "image": "data\\images\\2372449.jpg"
            },
            {
                "VG_image_id": "2355609",
                "VG_object_id": "3567400",
                "bbox": [25, 103, 467, 348],
                "image": "data\\images\\2355609.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the plate made of", 1],
            ["how many pieces are there on the plate", 1],
            ["what shape is the tray", 1],
            ["what color is the table", 1],
            ["what is next to the pizza", 1]
        ],
        "org_questions": [
            ["what is the plate made of", 1],
            ["what color is the plate", -1],
            ["how many pieces are there on the plate", 1],
            ["what shape is the tray", 1],
            ["What food is on the tray", -1],
            ["what is in the tray", -1],
            ["what is on the tray", -1],
            ["what color is the table", 1],
            ["where is the pizza", -1],
            ["what is the pizza sitting on", -1],
            ["what kind of food is this", -1],
            ["what is next to the pizza", 1]
        ],
        "context": [
            "a man taking a picture of a slice of pizza and a slice of pizza.",
            "a pizza with toppings on a pan on a table."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2348758",
                "VG_object_id": "2239708",
                "bbox": [35, 132, 110, 362],
                "image": "data\\images\\2348758.jpg"
            },
            {
                "VG_image_id": "2358939",
                "VG_object_id": "797599",
                "bbox": [34, 13, 298, 333],
                "image": "data\\images\\2358939.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["how old is the man", 1],
            ["Where is the man", 1],
            ["how many people are there", 1],
            ["what is the man holding", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 2],
            ["how old is the man", 1],
            ["what is on the man's shirt", -1],
            ["Where is the man", 1],
            ["how many people are there", 1],
            ["what is the man holding", 1],
            ["what is the man wearing", 1],
            ["what is the gender of the person in the photo", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a group of people standing around a counter.",
            "a man smoking a cigarette"
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2360855",
                "VG_object_id": "1862529",
                "bbox": [111, 34, 405, 269],
                "image": "data\\images\\2360855.jpg"
            },
            {
                "VG_image_id": "2321274",
                "VG_object_id": "2933979",
                "bbox": [36, 26, 342, 343],
                "image": "data\\images\\2321274.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's clothes", 2],
            ["where is the girl", 2],
            ["what is the girl doing", 1],
            ["how many girls are there", 1],
            ["what is the woman holding", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what color is the girl's clothes", 2],
            ["where is the girl", 2],
            ["how many girls are there", 1],
            ["what is the woman holding", 1],
            ["where is the photo taken", 1],
            ["who is in the photo", -1],
            ["what is on the woman's head", -1],
            ["what is the woman wearing", -1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "two women jumping on a bed",
            "a woman is talking on a cell phone."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2380952",
                "VG_object_id": "542927",
                "bbox": [1, 189, 433, 275],
                "image": "data\\images\\2380952.jpg"
            },
            {
                "VG_image_id": "2343318",
                "VG_object_id": "2380510",
                "bbox": [66, 133, 259, 200],
                "image": "data\\images\\2343318.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there in the picture", 1]
        ],
        "org_questions": [
            ["how many cars are there in the picture", 1],
            ["what time is it", -1],
            ["where is the car", -1],
            ["how is the weather", -1],
            ["when is the photo taken", -1],
            ["what is the weather like", -1],
            ["what is on the ground", -1],
            ["what is behind the cars", -1],
            ["what is in front of the cars", -1],
            ["what color is the grass", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a boy doing a trick on a skateboard.",
            "a water main burst flooded street in a residential neighborhood."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2417910",
                "VG_object_id": "3561045",
                "bbox": [69, 72, 231, 367],
                "image": "data\\images\\2417910.jpg"
            },
            {
                "VG_image_id": "2404728",
                "VG_object_id": "3096471",
                "bbox": [279, 51, 459, 256],
                "image": "data\\images\\2404728.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 1],
            ["what is the child holding", 1],
            ["what is the little girl doing", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what are the people doing", 1],
            ["what is the persion wearing on his head", -1],
            ["what is the child holding", 1],
            ["who is in the photo", -1],
            ["what is the gender of the girl", -1],
            ["what is the little girl doing", 1]
        ],
        "context": [
            "a little girl sitting on a counter eating a piece of food.",
            "three children sitting at a table with plastic cups."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2354091",
                "VG_object_id": "3778292",
                "bbox": [121, 114, 348, 307],
                "image": "data\\images\\2354091.jpg"
            },
            {
                "VG_image_id": "2403695",
                "VG_object_id": "350573",
                "bbox": [217, 149, 499, 355],
                "image": "data\\images\\2403695.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the dog doing", 2],
            ["where is the dog", 2],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is the dog wearing", 1],
            ["what is the dog holding", 1]
        ],
        "org_questions": [
            ["what is the dog doing", 2],
            ["where is the dog", 2],
            ["how many people are there", 1],
            ["what color is the background", -1],
            ["what is the ground covered with", 1],
            ["what is the dog wearing", 1],
            ["What is in front of the dog", -1],
            ["what animal is in the picture", -1],
            ["what is on the dog's feet", -1],
            ["what is the dog holding", 1],
            ["what is the dog on", -1]
        ],
        "context": [
            "a dog catching a frisbee in its mouth.",
            "a dog laying on the ground with a suitcase."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2325479",
                "VG_object_id": "2801961",
                "bbox": [104, 18, 166, 191],
                "image": "data\\images\\2325479.jpg"
            },
            {
                "VG_image_id": "2355592",
                "VG_object_id": "828285",
                "bbox": [137, 227, 203, 411],
                "image": "data\\images\\2355592.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bottle", 2],
            ["what kind of drink is in the bottle", 1],
            ["what is he bottle made of", 1]
        ],
        "org_questions": [
            ["what color is the bottle", 2],
            ["what color is the table", -1],
            ["how many bottles are there", -1],
            ["what food is on the table", -1],
            ["What is the table made of", -1],
            ["what kind of drink is in the bottle", 1],
            ["What is in the bottle", -1],
            ["what is he bottle made of", 1],
            ["where was the photo taken", -1],
            ["where is the water", -1],
            ["what is on top of the bottle", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a pizza on a plate on a table with a drink and a drink.",
            "a table with several pizzas on it and a bottle of beer."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2330561",
                "VG_object_id": "2811599",
                "bbox": [296, 129, 425, 301],
                "image": "data\\images\\2330561.jpg"
            },
            {
                "VG_image_id": "2373044",
                "VG_object_id": "734528",
                "bbox": [183, 84, 287, 261],
                "image": "data\\images\\2373044.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is next to the cabinet", 2],
            ["what color is the cabinet", 1],
            ["what color is the floor under the cabinet", 1],
            ["what room is the cabinet in", 1],
            ["where is the cabinet", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", 1],
            ["what is the cabinet made of", -1],
            ["what color is the floor under the cabinet", 1],
            ["how many people are there", -1],
            ["what room is the cabinet in", 1],
            ["where is the cabinet", 1],
            ["what color is the ground", -1],
            ["who is in the photo", -1],
            ["what is next to the cabinet", 2],
            ["where was the photo taken", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a living room with a table, chairs and a table.",
            "a room with a desk and a chair."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2363901",
                "VG_object_id": "761209",
                "bbox": [1, 233, 498, 333],
                "image": "data\\images\\2363901.jpg"
            },
            {
                "VG_image_id": "2371582",
                "VG_object_id": "1747966",
                "bbox": [19, 144, 497, 354],
                "image": "data\\images\\2371582.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the ground", 1],
            ["what is in the distance", 1],
            ["when is this picture taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is on the ground", -1],
            ["what is in the distance", 1],
            ["how many people are there in the picture", 2],
            ["when is this picture taken", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a purple door with graffiti on it and a pair of shoes on a bench.",
            "a woman sitting on the ground holding an umbrella."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2317794",
                "VG_object_id": "3399362",
                "bbox": [11, 99, 183, 331],
                "image": "data\\images\\2317794.jpg"
            },
            {
                "VG_image_id": "2332024",
                "VG_object_id": "3516897",
                "bbox": [74, 3, 254, 266],
                "image": "data\\images\\2332024.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["what color is the woman's hair", 1],
            ["where is the woman", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1],
            ["what is the lady doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what color is the woman's hair", 1],
            ["where is the woman", 1],
            ["how many people are there", 1],
            ["What season is it", -1],
            ["what is the woman holding", 1],
            ["what is the lady doing", 1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1],
            ["what is on the woman's face", -1],
            ["what kind of shirt is the woman wearing", -1]
        ],
        "context": [
            "a man and a woman looking at a cell phone in a store.",
            "a woman and a child prepare food in a kitchen."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "107992",
                "VG_object_id": "1073799",
                "bbox": [83, 274, 370, 398],
                "image": "data\\images\\107992.jpg"
            },
            {
                "VG_image_id": "2327217",
                "VG_object_id": "3049366",
                "bbox": [362, 81, 499, 309],
                "image": "data\\images\\2327217.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horse are there", 2],
            ["what color is the sky", 2],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many horse are there", 2],
            ["what is in the background", 1],
            ["what color is the sky", 2],
            ["what are the horses doing", -1],
            ["where is the picture taken", -1],
            ["what is the land made of", -1],
            ["what color is the grass", -1],
            ["what type of animal is in the picture", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is the horse eating", -1],
            ["where are the horses", -1]
        ],
        "context": [
            "two horses standing in a field with a mountain in the background.",
            "a horse is standing in a field near a stream."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2388179",
                "VG_object_id": "1270172",
                "bbox": [53, 264, 270, 441],
                "image": "data\\images\\2388179.jpg"
            },
            {
                "VG_image_id": "2391216",
                "VG_object_id": "490666",
                "bbox": [266, 74, 476, 270],
                "image": "data\\images\\2391216.jpg"
            }
        ],
        "questions_with_scores": [["what is the ground covered with", 1]],
        "org_questions": [
            ["how many people are there", -1],
            ["what is the horse doing", -1],
            ["what is the ground covered with", 1],
            ["where is  the horse", -1],
            ["what is on the horse", -1],
            ["what is the horse standing on", -1],
            ["what type of animal is shown", -1],
            ["what is behind the horse", -1],
            ["what is on the horse's head", -1],
            ["what animal is shown", -1]
        ],
        "context": [
            "a group of horses standing next to each other.",
            "two men sitting on a bench with a white horse."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316209",
                "VG_object_id": "3710314",
                "bbox": [156, 107, 262, 226],
                "image": "data\\images\\2316209.jpg"
            },
            {
                "VG_image_id": "2410370",
                "VG_object_id": "3806197",
                "bbox": [163, 104, 279, 273],
                "image": "data\\images\\2410370.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color is the ground", 1],
            ["what kind of trousers is the man wearing", 1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1],
            ["What is man holding", 1],
            ["how many players are there", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the ground", 1],
            ["what kind of trousers is the man wearing", 1],
            ["what time is it", -1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1],
            ["What is man holding", 1],
            ["how many players are there", 1],
            ["who is holding the ball", -1],
            ["what sport is being played", 1],
            ["what is the player wearing", -1]
        ],
        "context": [
            "a group of basketball players playing a game of basketball.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2342236",
                "VG_object_id": "3145236",
                "bbox": [237, 78, 367, 281],
                "image": "data\\images\\2342236.jpg"
            },
            {
                "VG_image_id": "2372726",
                "VG_object_id": "2181815",
                "bbox": [288, 128, 399, 258],
                "image": "data\\images\\2372726.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person's posture", 2],
            ["how many people are there in the picture", 2],
            ["what color is the person's jacket", 1]
        ],
        "org_questions": [
            ["what color is the person's jacket", 1],
            ["when is the photo taken", -1],
            ["What is the man wearing on his head", -1],
            ["what is the man doing", -1],
            ["What is the man wearing on his hands", -1],
            ["who is in the photo", -1],
            ["how many people are there", -1],
            ["where is the skier", -1],
            ["what is on the ground", -1],
            ["what is the man wearing", -1],
            ["what is the person's posture", 2],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a snowboarder is going down a hill on a mountain.",
            "a snowboarder and a snowboarder sit in the snow."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2384981",
                "VG_object_id": "1302508",
                "bbox": [103, 279, 459, 332],
                "image": "data\\images\\2384981.jpg"
            },
            {
                "VG_image_id": "2410144",
                "VG_object_id": "225004",
                "bbox": [181, 273, 330, 347],
                "image": "data\\images\\2410144.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["what type of food is on the table", 1]
        ],
        "org_questions": [
            ["what are the people doing", 2],
            ["how many people are there", -1],
            ["what shape is the board", -1],
            ["what type of food is on the table", 1],
            ["what is the table made of", -1],
            ["what is in the bowl", -1],
            ["what is the food on", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a man and a woman looking at a tablet computer.",
            "a man in a kitchen preparing lobsters."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2402727",
                "VG_object_id": "387150",
                "bbox": [6, 157, 496, 331],
                "image": "data\\images\\2402727.jpg"
            },
            {
                "VG_image_id": "2343286",
                "VG_object_id": "3640926",
                "bbox": [15, 240, 485, 327],
                "image": "data\\images\\2343286.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are on the field", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["how many people are on the field", 1],
            ["what is in the distance", 1],
            ["what is the color of the grass", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1],
            ["where was this photo taken", -1],
            ["what is green", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a man in a red shirt is playing soccer.",
            "a small plane sitting on top of a runway."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2396160",
                "VG_object_id": "447492",
                "bbox": [0, 72, 497, 274],
                "image": "data\\images\\2396160.jpg"
            },
            {
                "VG_image_id": "2414003",
                "VG_object_id": "161646",
                "bbox": [115, 219, 417, 408],
                "image": "data\\images\\2414003.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many bowls are there on the table", 1],
            ["how many dishes are there on the table", 1]
        ],
        "org_questions": [
            ["how many bowls are there on the table", 1],
            ["what color is the table", 2],
            ["how many dishes are there on the table", 1],
            ["where is the food", -1],
            ["what is on the tray", -1],
            ["what is the table made of", -1],
            ["What is the background of image", -1],
            ["what color is the plate the food is placed on", -1],
            ["what type of food is on the table", -1],
            ["what is the food sitting on", -1],
            ["what is in the bowl", -1],
            ["what is the color of the plate", -1]
        ],
        "context": [
            "a table with a lot of food on it",
            "a bowl of soup with meat and vegetables."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2351289",
                "VG_object_id": "861760",
                "bbox": [117, 48, 348, 288],
                "image": "data\\images\\2351289.jpg"
            },
            {
                "VG_image_id": "2334744",
                "VG_object_id": "2721071",
                "bbox": [166, 153, 336, 219],
                "image": "data\\images\\2334744.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bears are there", 2],
            ["how many bears", 2]
        ],
        "org_questions": [
            ["how many bears are there", 2],
            ["what color is the background", -1],
            ["what is in front of the bear", -1],
            ["what is the bear doing", -1],
            ["what color is the the ground", -1],
            ["how many grasses are there on the ground", -1],
            ["when was this photo taken", -1],
            ["what type of animal is shown", -1],
            ["where was the photo taken", -1],
            ["what is the bear walking on", -1],
            ["how many bears", 2]
        ],
        "context": [
            "a brown bear standing next to a tree.",
            "two bears are standing next to each other in an enclosure."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2388502",
                "VG_object_id": "672934",
                "bbox": [0, 241, 499, 331],
                "image": "data\\images\\2388502.jpg"
            },
            {
                "VG_image_id": "2410357",
                "VG_object_id": "219664",
                "bbox": [8, 366, 371, 449],
                "image": "data\\images\\2410357.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bench", 2],
            ["how many people are there sitting on the bench", 1],
            ["what color is the man's clothes", 1],
            ["what is in the distance", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the bench", 2],
            ["how many people are there sitting on the bench", 1],
            ["what color is the man's clothes", 1],
            ["what time is it", -1],
            ["what is in the distance", 1],
            ["what is the bench made of", -1],
            ["what is the weather like", -1],
            ["what is under the bench", -1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["what is the man wearing", 1],
            ["what is the man sitting on", -1]
        ],
        "context": [
            "a man with a long beard and glasses sitting on a bench.",
            "a man and a woman sitting on a bench."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2372211",
                "VG_object_id": "593276",
                "bbox": [239, 178, 277, 277],
                "image": "data\\images\\2372211.jpg"
            },
            {
                "VG_image_id": "2339215",
                "VG_object_id": "2886923",
                "bbox": [230, 211, 327, 295],
                "image": "data\\images\\2339215.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's coat", 1],
            ["what color is the woman's hair", 1],
            ["what is in the distance", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's coat", 1],
            ["what color is the woman's hair", 1],
            ["what is in the distance", 1],
            ["how many people are there", 1],
            ["where is the woman", -1],
            ["what is the woman doing", -1],
            ["what is the woman wearing", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the weather like", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a man in green pants and a white shirt riding a skateboard.",
            "a clock tower in the distance"
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2321865",
                "VG_object_id": "2946767",
                "bbox": [198, 105, 365, 237],
                "image": "data\\images\\2321865.jpg"
            },
            {
                "VG_image_id": "2360456",
                "VG_object_id": "2171452",
                "bbox": [354, 251, 416, 309],
                "image": "data\\images\\2360456.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding the board wearing", 1],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the board", -1],
            ["where is the board", -1],
            ["what is the man holding the board wearing", 1],
            ["how many people are there in the picture", -1],
            ["what is the man doing", 1],
            ["where is the photo taken", -1],
            ["what color is the water", -1],
            ["when was the picture taken", -1],
            ["what is the man standing on", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man walking on the beach with a surfboard.",
            "a man riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2403009",
                "VG_object_id": "1128864",
                "bbox": [14, 336, 198, 443],
                "image": "data\\images\\2403009.jpg"
            },
            {
                "VG_image_id": "2379408",
                "VG_object_id": "1357826",
                "bbox": [214, 160, 326, 330],
                "image": "data\\images\\2379408.jpg"
            }
        ],
        "questions_with_scores": [["what color is the bus", 2]],
        "org_questions": [
            ["what color is the bus", 2],
            ["what is in the distance", -1],
            ["what is the weather like", -1],
            ["How many people are there", -1],
            ["what direction is the bus facing to", -1],
            ["where is the bus", -1],
            ["how many buses are there", -1],
            ["what type of vehicle is shown", -1],
            ["what is the bus doing", -1],
            ["what is on the front of the bus", -1],
            ["what is on the road", -1]
        ],
        "context": [
            "a bus driving down a road next to a castle.",
            "a red bus driving down a street next to a person riding a bike."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2335874",
                "VG_object_id": "2229338",
                "bbox": [316, 17, 498, 331],
                "image": "data\\images\\2335874.jpg"
            },
            {
                "VG_image_id": "2358390",
                "VG_object_id": "2190389",
                "bbox": [188, 183, 275, 461],
                "image": "data\\images\\2358390.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["where is the photo taken", 1],
            ["What is the man wearing", 1],
            ["What is man looking at", 1],
            ["what kind of clothes is the man wearing", 1],
            ["Where is the man", 1],
            ["what is the man on the right holding", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is man's shirt", -1],
            ["What is man doing", -1],
            ["where is the photo taken", 1],
            ["What is the man wearing", 1],
            ["What is man looking at", 1],
            ["what kind of clothes is the man wearing", 1],
            ["Where is the man", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is on the man's face", -1],
            ["what is the man on the right holding", 1]
        ],
        "context": [
            "an elephant eats greens from a cage.",
            "a man and woman pose for a picture in front of a clock."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2348978",
                "VG_object_id": "2200292",
                "bbox": [206, 44, 321, 272],
                "image": "data\\images\\2348978.jpg"
            },
            {
                "VG_image_id": "2390873",
                "VG_object_id": "493403",
                "bbox": [117, 9, 375, 309],
                "image": "data\\images\\2390873.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the cat sitting on", 1],
            ["where are the cats", 1],
            ["where is the cat", 1],
            ["where is the cat standing on", 1]
        ],
        "org_questions": [
            ["what color is the cat", -1],
            ["what is the cat sitting on", 1],
            ["what is the gesture of the cat", -1],
            ["How many cats are there", -1],
            ["where are the cats", 1],
            ["what is the cat doing", -1],
            ["what is in front of the cat", -1],
            ["what type of animal is this", -1],
            ["what is the cat's color", -1],
            ["where is the cat", 1],
            ["how many cats are in the picture", -1],
            ["where is the cat standing on", 1],
            ["what kind of animal is this", -1],
            ["what is on the cat's head", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a cat sitting on top of a computer tower.",
            "a cat sitting on top of a laptop computer."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2374528",
                "VG_object_id": "2430230",
                "bbox": [1, 260, 499, 414],
                "image": "data\\images\\2374528.jpg"
            },
            {
                "VG_image_id": "2331341",
                "VG_object_id": "2842246",
                "bbox": [2, 217, 499, 374],
                "image": "data\\images\\2331341.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 2],
            ["how many people are there in the picture", 2],
            ["what color is the thing on the table", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 2],
            ["what color is the thing on the table", 1],
            ["how many people are there", 1],
            ["where is the table", -1],
            ["how many plates are there on the table", -1],
            ["how many forks are there", -1],
            ["what material is the table made of", -1],
            ["where was the photo taken", -1],
            ["what is the table color", -1],
            ["what is covering the table", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "person with his car made from a cake.",
            "a table with vases of flowers on it."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2395111",
                "VG_object_id": "1207950",
                "bbox": [4, 27, 498, 374],
                "image": "data\\images\\2395111.jpg"
            },
            {
                "VG_image_id": "2352163",
                "VG_object_id": "855703",
                "bbox": [2, 371, 375, 499],
                "image": "data\\images\\2352163.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["what is the shape of the pizza", 1],
            ["what is next to the pizza", 1]
        ],
        "org_questions": [
            ["what is the color of the table", -1],
            ["what is on the table", -1],
            ["How many people are there", 1],
            ["what is the shape of the pizza", 1],
            ["where is the plate", -1],
            ["what is the plate sitting on", -1],
            ["what kind of food is this", -1],
            ["what is next to the pizza", 1]
        ],
        "context": [
            "a pizza sitting on top of a white plate.",
            "a woman sitting at a table with a plate of pizza."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2327377",
                "VG_object_id": "3441176",
                "bbox": [151, 41, 278, 330],
                "image": "data\\images\\2327377.jpg"
            },
            {
                "VG_image_id": "2367009",
                "VG_object_id": "2568642",
                "bbox": [190, 159, 293, 325],
                "image": "data\\images\\2367009.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's coat", 1],
            ["what is the person doing", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["what color is the person's coat", 1],
            ["what is the person doing", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what is the weather like", -1],
            ["where is the man", -1],
            ["what is the man wearing", -1],
            ["what color is the background", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man on", 1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man and woman riding a motorcycle.",
            "a man standing next to a stop sign."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2363998",
                "VG_object_id": "2012432",
                "bbox": [361, 72, 468, 193],
                "image": "data\\images\\2363998.jpg"
            },
            {
                "VG_image_id": "2352948",
                "VG_object_id": "2433350",
                "bbox": [123, 78, 253, 160],
                "image": "data\\images\\2352948.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is hanging on the shirt", 1],
            ["what kind of shirt is the man wearing", 1],
            ["what is the man holding", 1],
            ["what pattern is the shirt", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["what type of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["What is man wearing on his head", -1],
            ["where is the person", 1],
            ["what is hanging on the shirt", 1],
            ["what kind of shirt is the man wearing", 1],
            ["what is the man holding", 1],
            ["who is wearing the shirt", -1],
            ["what pattern is the shirt", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["what type of shirt is the man wearing", 1]
        ],
        "context": [
            "members cut the ribbon at the opening ceremony.",
            "a man riding a skateboard down the side of a ramp."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2322280",
                "VG_object_id": "3259194",
                "bbox": [196, 115, 373, 412],
                "image": "data\\images\\2322280.jpg"
            },
            {
                "VG_image_id": "2373899",
                "VG_object_id": "2310477",
                "bbox": [212, 132, 459, 331],
                "image": "data\\images\\2373899.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what sport is the man playing", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["WHat is man doing", 1],
            ["what kind of shoes is the man wearing", 1],
            ["who is in the photo", 1],
            ["what is on the man's head", 1],
            ["what sport is it", 1],
            ["where is the photo taken", 1],
            ["what is the man wearing on his head", 1]
        ],
        "org_questions": [
            ["what color is the man's trousers", -1],
            ["how many people are there", 1],
            ["what sport is the man playing", 1],
            ["where is the man", 1],
            ["what is the ground covered with", -1],
            ["how is the weather", -1],
            ["what is the man holding", 1],
            ["WHat is man doing", 1],
            ["when was the photo taken", -1],
            ["what kind of shoes is the man wearing", 1],
            ["who is in the photo", 1],
            ["what is on the man's head", 1],
            ["what sport is it", 1],
            ["where is the photo taken", 1],
            ["what is the man wearing on his head", 1]
        ],
        "context": [
            "a man swinging a tennis racket at a ball",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2391322",
                "VG_object_id": "1241535",
                "bbox": [142, 198, 234, 306],
                "image": "data\\images\\2391322.jpg"
            },
            {
                "VG_image_id": "2412339",
                "VG_object_id": "306772",
                "bbox": [135, 60, 239, 176],
                "image": "data\\images\\2412339.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 2],
            ["What sport is the man doing", 1],
            ["Where is the man", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 2],
            ["What sport is the man doing", 1],
            ["Where is the man", 1],
            ["what is the gender of the person", -1],
            ["what is the pattern of the man's shirt", -1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["what is the man wearing on his head", -1],
            ["when was the photo taken", -1],
            ["what type of shirt is the man wearing", -1]
        ],
        "context": [
            "a man riding a wave on top of a surfboard.",
            "a group of men playing hockey on a court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2371051",
                "VG_object_id": "2522352",
                "bbox": [338, 147, 469, 315],
                "image": "data\\images\\2371051.jpg"
            },
            {
                "VG_image_id": "2348848",
                "VG_object_id": "2542926",
                "bbox": [65, 17, 191, 175],
                "image": "data\\images\\2348848.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the man on", 1],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the ground covered with", 1],
            ["what is the man riding", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the man on", 1],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["who is wearing glasses", -1],
            ["where is the man", -1],
            ["what is in the background", -1],
            ["what is the ground covered with", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["how many people are there", 2],
            ["what is the man riding", 1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a boat carrying produce to the shore.",
            "a man riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2412958",
                "VG_object_id": "183266",
                "bbox": [109, 127, 174, 252],
                "image": "data\\images\\2412958.jpg"
            },
            {
                "VG_image_id": "2324335",
                "VG_object_id": "2793164",
                "bbox": [406, 92, 498, 323],
                "image": "data\\images\\2324335.jpg"
            }
        ],
        "questions_with_scores": [
            ["how is the weather", 1],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1],
            ["What is weather like", 1]
        ],
        "org_questions": [
            ["how is the weather", 1],
            ["what is the persion doing", 1],
            ["how many people are in the picture", -1],
            ["where is the person", -1],
            ["what color is the background", -1],
            ["what is the persion wearing", -1],
            ["what is the persion holding", 1],
            ["What is weather like", 1],
            ["when was this photo taken", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a person standing in a puddle of water with a child in it.",
            "a horse is walking in a field near a lake."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2410374",
                "VG_object_id": "219175",
                "bbox": [31, 108, 213, 358],
                "image": "data\\images\\2410374.jpg"
            },
            {
                "VG_image_id": "2346782",
                "VG_object_id": "1794795",
                "bbox": [343, 49, 494, 300],
                "image": "data\\images\\2346782.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the player's shirt", 2],
            ["What is the color of ground", 1],
            ["how many people are there", 1],
            ["what is the woman wearing on the head", 1]
        ],
        "org_questions": [
            ["What is the color of ground", 1],
            ["What color is the player's shirt", 2],
            ["how many people are there", 1],
            ["what is the woman wearing on the head", 1],
            ["what is the woman doing", -1],
            ["who is playing tennis", -1],
            ["where is the woman", -1],
            ["what sport is being played", -1],
            ["what is the woman playing", -1],
            ["what is the woman wearing", -1]
        ],
        "context": [
            "a woman holding a tennis racquet on a tennis court.",
            "a man standing on a tennis court holding a racquet."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2373392",
                "VG_object_id": "2059472",
                "bbox": [168, 229, 278, 400],
                "image": "data\\images\\2373392.jpg"
            },
            {
                "VG_image_id": "2358131",
                "VG_object_id": "805787",
                "bbox": [3, 1, 385, 57],
                "image": "data\\images\\2358131.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the pillow", 2],
            ["What color is the bed", 2],
            ["What is on the bed", 1],
            ["what color is the pillow in the front", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["What color is the pillow", 2],
            ["What color is the bed", 2],
            ["What is on the bed", 1],
            ["How many pillows are there", -1],
            ["Where is the pillow", -1],
            ["where is the pillow placed on", -1],
            ["what color is the pillow in the front", 1],
            ["what is in the background", -1],
            ["how many people are there", 1],
            ["where are the pillows", -1]
        ],
        "context": [
            "a woman and a child eating food in a bed.",
            "two cats sleeping on a pink blanket next to remotes."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2384146",
                "VG_object_id": "1312821",
                "bbox": [56, 248, 144, 327],
                "image": "data\\images\\2384146.jpg"
            },
            {
                "VG_image_id": "2326779",
                "VG_object_id": "2866758",
                "bbox": [192, 122, 283, 201],
                "image": "data\\images\\2326779.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the background", 2],
            ["what is on the man's feet", 2],
            ["what color is the trouser", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["how many trousers are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 1],
            ["what is the background", 2],
            ["how many people are there", 1],
            ["who is wearing the trousers", -1],
            ["what is the man doing", 1],
            ["how many trousers are there in the picture", 1],
            ["when was this picture taken", -1],
            ["what is the man wearing", -1],
            ["what is on the man's feet", 2],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man and a child skiing down a snow covered slope.",
            "a person riding a surfboard on a wave."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2372499",
                "VG_object_id": "591722",
                "bbox": [85, 47, 198, 271],
                "image": "data\\images\\2372499.jpg"
            },
            {
                "VG_image_id": "2396205",
                "VG_object_id": "446928",
                "bbox": [145, 50, 307, 399],
                "image": "data\\images\\2396205.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 1],
            ["what is the man holding", 1],
            ["what type of pants is the man wearing", 1],
            ["what are the people wearing", 1],
            ["what is the man doing", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["where is the photo taken", -1],
            ["what is the man wearing on the head", -1],
            ["what are the people doing", 1],
            ["how many animals are in front of the man", -1],
            ["what is the man holding", 1],
            ["what type of pants is the man wearing", 1],
            ["when was the picture taken", -1],
            ["what are the people wearing", 1],
            ["what is the man on the left wearing", -1],
            ["what is the man doing", 1],
            ["how many people are there in the picture", 1],
            ["what is the man wearing", -1],
            ["what gesture is the man", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man riding a sheep while another man watches.",
            "a man sitting on a horse in a dirt field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2386861",
                "VG_object_id": "1281704",
                "bbox": [206, 27, 276, 208],
                "image": "data\\images\\2386861.jpg"
            },
            {
                "VG_image_id": "2382381",
                "VG_object_id": "1329001",
                "bbox": [206, 7, 327, 304],
                "image": "data\\images\\2382381.jpg"
            }
        ],
        "questions_with_scores": [["what is the ground covered with", 1]],
        "org_questions": [
            ["what time is it", -1],
            ["what is the ground covered with", 1],
            ["what is the persion wearing", -1],
            ["how many people are there", -1],
            ["where is the skateboard", -1],
            ["who is on the skateboard", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a young man riding a skateboard down a street.",
            "a man riding a skateboard through an orange cone."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2332515",
                "VG_object_id": "969418",
                "bbox": [46, 230, 210, 303],
                "image": "data\\images\\2332515.jpg"
            },
            {
                "VG_image_id": "2409465",
                "VG_object_id": "241159",
                "bbox": [224, 232, 487, 299],
                "image": "data\\images\\2409465.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the counter", 2],
            ["what color is the sink", 2],
            ["what is on the counter", 1]
        ],
        "org_questions": [
            ["what color is the counter", 2],
            ["what color is the sink", 2],
            ["what is on the counter", 1],
            ["how many sinks are there", -1],
            ["where is the sink", -1],
            ["what is the sink on", -1],
            ["what is on the wall", -1],
            ["what is on the table", -1],
            ["what room is this", -1],
            ["what is next to the sink", -1],
            ["what is above the sink", -1]
        ],
        "context": [
            "a bathroom with a sink, toilet and a cabinet.",
            "a bathroom with a toilet, sink, and a plant."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2354893",
                "VG_object_id": "1686671",
                "bbox": [393, 224, 452, 301],
                "image": "data\\images\\2354893.jpg"
            },
            {
                "VG_image_id": "2337117",
                "VG_object_id": "2348896",
                "bbox": [258, 281, 319, 332],
                "image": "data\\images\\2337117.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person in the shirt", 2],
            ["what color is the shirt", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["where is the person in the shirt", 2],
            ["what is the person in the shirt wearing", -1],
            ["how many people are there", -1],
            ["what gender is the person who wears the shirt", -1],
            ["what is the man standing on", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["what is the man holding", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a large display of bananas",
            "a person standing in the grass next to a stop sign."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2398806",
                "VG_object_id": "1175677",
                "bbox": [159, 92, 224, 146],
                "image": "data\\images\\2398806.jpg"
            },
            {
                "VG_image_id": "2337556",
                "VG_object_id": "3157391",
                "bbox": [213, 71, 347, 271],
                "image": "data\\images\\2337556.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the land made of", 1],
            ["how many people are there in the picture", 1],
            ["What are people doing", 1],
            ["what is the person in the shirt holding", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the land made of", 1],
            ["how many people are there in the picture", 1],
            ["what gender is the person in the shirt", -1],
            ["What are people doing", 1],
            ["what is the person in the shirt holding", 1],
            ["who is wearing the shirt", -1],
            ["what is the gender of the person", -1],
            ["when was the picture taken", -1],
            ["what is on the man's head", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a group of people walking around a basketball court.",
            "a man is holding a surfboard on a table."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2405750",
                "VG_object_id": "653799",
                "bbox": [8, 199, 245, 275],
                "image": "data\\images\\2405750.jpg"
            },
            {
                "VG_image_id": "2350483",
                "VG_object_id": "1042103",
                "bbox": [1, 391, 333, 496],
                "image": "data\\images\\2350483.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there on the road", 2],
            ["how many people are there in the picture", 2],
            ["what is on the road", 1],
            ["what is on the side of the street", 1],
            ["how many people are on the street", 1],
            ["where is this scene", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is on the road", 1],
            ["how many cars are there on the road", 2],
            ["what time is it", -1],
            ["when is the photo taken", -1],
            ["what is on the side of the street", 1],
            ["how many people are on the street", 1],
            ["where is this scene", 1],
            ["what is the road made of", -1],
            ["where is the road", -1],
            ["where was the photo taken", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man holding an umbrella in the rain.",
            "a man riding a skateboard down a ramp."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2345402",
                "VG_object_id": "3213353",
                "bbox": [169, 155, 317, 216],
                "image": "data\\images\\2345402.jpg"
            },
            {
                "VG_image_id": "2410126",
                "VG_object_id": "225330",
                "bbox": [169, 184, 403, 234],
                "image": "data\\images\\2410126.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 1],
            ["what is beside the train", 1]
        ],
        "org_questions": [
            ["what color is the train", 1],
            ["where is the train", -1],
            ["what is in the background", -1],
            ["How many trains are there", -1],
            ["What is the weather like", -1],
            ["what direction is the train heading to", -1],
            ["how many people are there in the picture", -1],
            ["what is beside the train", 1],
            ["what type of vehicle is this", -1],
            ["what is the train doing", -1],
            ["what kind of train is this", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a train traveling over a bridge over a river.",
            "a train traveling down tracks next to a lush green field."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2358579",
                "VG_object_id": "1942129",
                "bbox": [269, 86, 330, 178],
                "image": "data\\images\\2358579.jpg"
            },
            {
                "VG_image_id": "2358339",
                "VG_object_id": "803840",
                "bbox": [40, 203, 112, 279],
                "image": "data\\images\\2358339.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the towel", 2],
            ["where is the towel", 1],
            ["what color is the ground", 1],
            ["what room is it", 1],
            ["what room is the towel in", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the towel", 2],
            ["where is the towel", 1],
            ["what color is the ground", 1],
            ["how many towels are there", -1],
            ["what room is it", 1],
            ["what room is the towel in", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a bathroom with a tub, toilet, and a shower.",
            "a kitchen with a stainless steel refrigerator and wooden cabinets."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2366279",
                "VG_object_id": "3879979",
                "bbox": [150, 220, 335, 338],
                "image": "data\\images\\2366279.jpg"
            },
            {
                "VG_image_id": "2324840",
                "VG_object_id": "3285495",
                "bbox": [122, 210, 232, 350],
                "image": "data\\images\\2324840.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many chairs are there", 2],
            ["what is the color of the floor", 2],
            ["what color is the chair", 1],
            ["what is the floor made of", 1],
            ["what room is it", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["how many chairs are there", 2],
            ["what is the floor made of", 1],
            ["what room is it", 1],
            ["where is the chair", -1],
            ["what is on the chair", -1],
            ["how many people are in  the picture", -1],
            ["what are the chairs made of", -1],
            ["where are the chairs", -1],
            ["what is the color of the floor", 2],
            ["what is on the floor", -1]
        ],
        "context": [
            "a kitchen with a table, refrigerator, and refrigerator.",
            "a living room with a fireplace and a chair."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2333022",
                "VG_object_id": "3293910",
                "bbox": [113, 72, 387, 257],
                "image": "data\\images\\2333022.jpg"
            },
            {
                "VG_image_id": "2403415",
                "VG_object_id": "353346",
                "bbox": [242, 108, 473, 241],
                "image": "data\\images\\2403415.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many animals are there in the picture", 2],
            ["what color is the animal", 1],
            ["What animal is on the land", 1],
            ["what kind of animals are they", 1]
        ],
        "org_questions": [
            ["how many animals are there in the picture", 2],
            ["what is the animal", -1],
            ["what color is the animal", 1],
            ["What animal is on the land", 1],
            ["what is in the distance", -1],
            ["what kind of animals are they", 1],
            ["when was the picture taken", -1],
            ["where is the picture taken", -1],
            ["who is in the picture", -1],
            ["where are the animals", -1]
        ],
        "context": [
            "an elephant standing in the dirt near trees.",
            "a zebra and a deer stand in a zoo enclosure."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2317879",
                "VG_object_id": "3207760",
                "bbox": [65, 43, 270, 195],
                "image": "data\\images\\2317879.jpg"
            },
            {
                "VG_image_id": "2324244",
                "VG_object_id": "2875830",
                "bbox": [68, 150, 129, 190],
                "image": "data\\images\\2324244.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["what is on the man's head", 2],
            ["What sports is man doing", 1],
            ["where is the person", 1],
            ["what is the persion in the shirt wearing on head", 1],
            ["what is the land covered with", 1],
            ["what sport is the man playing", 1],
            ["who is wearing a black shirt", 1]
        ],
        "org_questions": [
            ["What sports is man doing", 1],
            ["What color is man's shirt", 2],
            ["what gender is the person in the shirt", -1],
            ["where is the person", 1],
            ["what is the persion in the shirt wearing on head", 1],
            ["what is the land covered with", 1],
            ["what sport is the man playing", 1],
            ["when was the photo taken", -1],
            ["who is wearing a black shirt", 1],
            ["what is on the man's face", -1],
            ["what is the man wearing", -1],
            ["what is on the man's head", 2]
        ],
        "context": [
            "a man doing a trick on a skateboard.",
            "a baseball player throwing a ball on a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2416509",
                "VG_object_id": "3301364",
                "bbox": [83, 81, 332, 238],
                "image": "data\\images\\2416509.jpg"
            },
            {
                "VG_image_id": "2324161",
                "VG_object_id": "3017625",
                "bbox": [186, 55, 352, 195],
                "image": "data\\images\\2324161.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the board under the man", 1],
            ["what is the man wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the board under the man", 1],
            ["what is the man wearing", 1],
            ["how many people are there", -1],
            ["what is the man doing", -1],
            ["what is the man holding", 1],
            ["what is the man wearing on his head", -1],
            ["how many men are there", -1],
            ["when was the photo taken", -1],
            ["who is surfing", -1],
            ["where is the surfer", -1],
            ["what sport is this", -1],
            ["what is the persion standing on", -1]
        ],
        "context": [
            "a man laying on a surfboard in the water.",
            "a man riding a wave on top of a yellow surfboard."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2375813",
                "VG_object_id": "1875879",
                "bbox": [225, 72, 334, 234],
                "image": "data\\images\\2375813.jpg"
            },
            {
                "VG_image_id": "2391758",
                "VG_object_id": "1237737",
                "bbox": [89, 140, 195, 256],
                "image": "data\\images\\2391758.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's pant", 1],
            ["how many ears are there", 1]
        ],
        "org_questions": [
            ["what is the person doing", -1],
            ["where is the person", -1],
            ["what color is the person's pant", 1],
            ["how many ears are there", 1],
            ["what is the man holding", -1],
            ["What is person doing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's feet", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man and a child skiing down a snow covered slope.",
            "a person on skis doing a trick in the air."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2343986",
                "VG_object_id": "2579388",
                "bbox": [151, 0, 211, 91],
                "image": "data\\images\\2343986.jpg"
            },
            {
                "VG_image_id": "2407407",
                "VG_object_id": "280083",
                "bbox": [128, 29, 296, 286],
                "image": "data\\images\\2407407.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["what is in front of the woman", 1],
            ["what is the woman doing", 1],
            ["how many people are there", 1],
            ["how many women are there", 1],
            ["What is woman doing", 1],
            ["what is the woman on the left holding", 1],
            ["when was the photo taken", 1],
            ["who is in the photo", 1],
            ["where is the woman", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", -1],
            ["what is the woman holding", 1],
            ["what is in front of the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", -1],
            ["how many people are there", 1],
            ["when was this taken", -1],
            ["how many women are there", 1],
            ["What is woman doing", 1],
            ["what is the woman on the left holding", 1],
            ["when was the photo taken", 1],
            ["who is in the photo", 1],
            ["where is the woman", 1]
        ],
        "context": [
            "a red fire hydrant sitting on the ground.",
            "a woman sitting on a bench using her cell phone."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2403095",
                "VG_object_id": "1128058",
                "bbox": [3, 1, 492, 96],
                "image": "data\\images\\2403095.jpg"
            },
            {
                "VG_image_id": "2346824",
                "VG_object_id": "2008933",
                "bbox": [1, 169, 183, 394],
                "image": "data\\images\\2346824.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the court", 2],
            ["what color is the player's pants", 2]
        ],
        "org_questions": [
            ["what color is the court", 2],
            ["what color is the player's pants", 2],
            ["how many people are there", -1],
            ["how color is the player's shirt", -1],
            ["What color is the shirt", -1],
            ["where was this picture taken", -1],
            ["what is on the ground", -1],
            ["when was the picture taken", -1],
            ["where are the white lines", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man running to hit a tennis ball with a racket.",
            "a man swinging a tennis racket on a tennis court."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2392162",
                "VG_object_id": "1233291",
                "bbox": [20, 2, 291, 227],
                "image": "data\\images\\2392162.jpg"
            },
            {
                "VG_image_id": "1592234",
                "VG_object_id": "3030782",
                "bbox": [580, 373, 900, 608],
                "image": "data\\images\\1592234.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plant", 2],
            ["what is in the background", 1],
            ["what animal is in the picture", 1],
            ["what kind of plant is this", 1],
            ["where was this picture taken", 1],
            ["what is in front of the plant", 1]
        ],
        "org_questions": [
            ["what color is the plant", 2],
            ["where is the plant", -1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what animal is in the picture", 1],
            ["what is the land made of ", -1],
            ["what kind of plant is this", 1],
            ["when was this picture taken", -1],
            ["where was this picture taken", 1],
            ["what is in front of the plant", 1]
        ],
        "context": [
            "a blue and white vase with a tree in it.",
            "a dog is standing in the middle of a garden."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2352071",
                "VG_object_id": "1897714",
                "bbox": [1, 250, 51, 297],
                "image": "data\\images\\2352071.jpg"
            },
            {
                "VG_image_id": "2402632",
                "VG_object_id": "387956",
                "bbox": [56, 316, 174, 405],
                "image": "data\\images\\2402632.jpg"
            }
        ],
        "questions_with_scores": [["What is on the rug", 1]],
        "org_questions": [
            ["What color is the rug", -1],
            ["What is on the rug", 1],
            ["where is the rug", -1],
            ["what is the floor made of", -1],
            ["what is on the floor", -1],
            ["what color is the table", -1],
            ["what type of flooring is shown", -1],
            ["what material is the floor", -1],
            ["where is the picture taken", -1],
            ["what is the floor color", -1],
            ["what is covering the floor", -1]
        ],
        "context": [
            "a television stand with a flat screen tv on it.",
            "a person playing a video game on a television."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2405590",
                "VG_object_id": "1107048",
                "bbox": [184, 81, 239, 140],
                "image": "data\\images\\2405590.jpg"
            },
            {
                "VG_image_id": "2386712",
                "VG_object_id": "1283245",
                "bbox": [413, 200, 498, 316],
                "image": "data\\images\\2386712.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the container", 1],
            ["where is the container", 1],
            ["what is in the background", 1],
            ["what is on the container", 1],
            ["what is in the container", 1]
        ],
        "org_questions": [
            ["what color is the container", 1],
            ["where is the container", 1],
            ["what is in the background", 1],
            ["how many spoons are in the picture", -1],
            ["what is the container made of", -1],
            ["what is on the container", 1],
            ["what is in the container", 1],
            ["how many people are there", -1],
            ["what is to the left of the picture", -1]
        ],
        "context": [
            "a desk with a laptop, keyboard, mouse and a mouse.",
            "a bowl of food with a fork and a person holding a knife."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2408176",
                "VG_object_id": "266698",
                "bbox": [174, 188, 319, 308],
                "image": "data\\images\\2408176.jpg"
            },
            {
                "VG_image_id": "2405439",
                "VG_object_id": "1107798",
                "bbox": [180, 133, 270, 289],
                "image": "data\\images\\2405439.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is  the cat", 1],
            ["Where is the cat", 1],
            ["What is the cat doing", 1],
            ["what is the cat sitting on", 1],
            ["what gesture is the cat", 1],
            ["where are the cats", 1]
        ],
        "org_questions": [
            ["What color is  the cat", 1],
            ["Where is the cat", 1],
            ["What is the cat doing", 1],
            ["what is the cat sitting on", 1],
            ["what gesture is the cat", 1],
            ["what is in front of the cat", -1],
            ["where are the cats", 1],
            ["what kind of animal is in the picture", -1],
            ["how many cats are there", -1],
            ["who is in the photo", -1],
            ["what is on the cat's head", -1],
            ["what is the cat looking at", -1]
        ],
        "context": [
            "a black cat sleeping in a bag of ties.",
            "a cat is climbing on the refrigerator door."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2359066",
                "VG_object_id": "3100323",
                "bbox": [1, 406, 446, 498],
                "image": "data\\images\\2359066.jpg"
            },
            {
                "VG_image_id": "2340328",
                "VG_object_id": "948041",
                "bbox": [3, 139, 499, 333],
                "image": "data\\images\\2340328.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the player's shirt", 2],
            ["what color are the person's shoes", 1]
        ],
        "org_questions": [
            ["what color is the player's shirt", 2],
            ["how many people are there", -1],
            ["what is the tennis court made of", -1],
            ["what is the main color of the ground", -1],
            ["what color are the person's shoes", 1],
            ["where was this photo taken", -1],
            ["what is on the court", -1],
            ["where are the white lines", -1],
            ["where is the man", -1],
            ["where is this scene", -1]
        ],
        "context": [
            "a man hitting a tennis ball with a racquet.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2363066",
                "VG_object_id": "1788878",
                "bbox": [2, 20, 468, 440],
                "image": "data\\images\\2363066.jpg"
            },
            {
                "VG_image_id": "2395478",
                "VG_object_id": "454182",
                "bbox": [75, 377, 117, 437],
                "image": "data\\images\\2395478.jpg"
            }
        ],
        "questions_with_scores": [["What color is the counter", 1]],
        "org_questions": [
            ["What color is the counter", 1],
            ["What is on the counter", -1],
            ["how many bottles are there on the counter", -1],
            ["what is the ground covered with", -1],
            ["what on the counter", -1],
            ["where is the picture taken", -1],
            ["what room is this", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a bathroom sink with a mirror",
            "a bathroom with a toilet, sink, and window."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2368340",
                "VG_object_id": "2029973",
                "bbox": [249, 47, 405, 249],
                "image": "data\\images\\2368340.jpg"
            },
            {
                "VG_image_id": "2330113",
                "VG_object_id": "2864392",
                "bbox": [298, 166, 445, 487],
                "image": "data\\images\\2330113.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the man holding", 1],
            ["how is the weather", -1],
            ["how many people are there", 1],
            ["where is the man", -1],
            ["what is the man doing", -1],
            ["what color is the ground", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is on the man's head", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man walking across a street holding a briefcase.",
            "a group of people standing on a sidewalk holding umbrellas."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2415042",
                "VG_object_id": "147173",
                "bbox": [118, 293, 192, 374],
                "image": "data\\images\\2415042.jpg"
            },
            {
                "VG_image_id": "2347694",
                "VG_object_id": "1745662",
                "bbox": [12, 175, 72, 261],
                "image": "data\\images\\2347694.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bag", 2],
            ["where is the picture taken", 2],
            ["what is on the ground", 1],
            ["what color is the bag on the front of the photo", 1],
            ["what color is the man's backpack", 1]
        ],
        "org_questions": [
            ["what are the people doing", -1],
            ["what color is the bag", 2],
            ["how many bags are there", -1],
            ["where is the picture taken", 2],
            ["what is the floor made of", -1],
            ["what is the bag placed on", -1],
            ["where is the bag", -1],
            ["when was the photo taken", -1],
            ["what is on the ground", 1],
            ["how many people are in the photo", -1],
            ["what color is the bag on the front of the photo", 1],
            ["what color is the man's backpack", 1]
        ],
        "context": [
            "a group of people standing on top of a van.",
            "a man standing on the side of a train."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2407798",
                "VG_object_id": "1095937",
                "bbox": [417, 43, 468, 82],
                "image": "data\\images\\2407798.jpg"
            },
            {
                "VG_image_id": "2357510",
                "VG_object_id": "2538868",
                "bbox": [317, 62, 493, 217],
                "image": "data\\images\\2357510.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there", 1],
            ["what is behind the car", 1],
            ["where is the car", 1],
            ["where was the photo taken", 1],
            ["where are the cars parked", 1],
            ["how many cars are in the picture", 1],
            ["what is on the left of the car", 1]
        ],
        "org_questions": [
            ["what color is the car", -1],
            ["what color is the road that the car on", -1],
            ["what shape is the car", -1],
            ["how many cars are there", 1],
            ["when is the photo taken", -1],
            ["what is behind the car", 1],
            ["how is the weather", -1],
            ["where is the car", 1],
            ["what is parked on the street", -1],
            ["where was the photo taken", 1],
            ["what is on the street", -1],
            ["where are the cars parked", 1],
            ["how many cars are in the picture", 1],
            ["what time is it", -1],
            ["what is on the left of the car", 1]
        ],
        "context": [
            "a woman standing on a sidewalk while using her cell phone.",
            "a man riding a bike in a parking lot."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2336002",
                "VG_object_id": "2147242",
                "bbox": [12, 149, 233, 306],
                "image": "data\\images\\2336002.jpg"
            },
            {
                "VG_image_id": "2407462",
                "VG_object_id": "1097582",
                "bbox": [3, 74, 375, 253],
                "image": "data\\images\\2407462.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sofa", 2],
            ["what color are the pillows on the sofa", 2],
            ["what color is the table", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 2],
            ["what color are the pillows on the sofa", 2],
            ["what color is the table", 1],
            ["How many people are there", 1],
            ["what is on the ground", -1],
            ["What is in front of sofa", -1],
            ["how many cats are on the sofa", -1],
            ["What is on the sofa", -1],
            ["where was the photo taken", -1],
            ["what is the couch made of", -1],
            ["where is the pillow", -1]
        ],
        "context": [
            "an older woman and a boy playing a video game.",
            "a living room with a white table and pink flowers on it."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2322839",
                "VG_object_id": "3113015",
                "bbox": [187, 7, 494, 279],
                "image": "data\\images\\2322839.jpg"
            },
            {
                "VG_image_id": "2404491",
                "VG_object_id": "1115211",
                "bbox": [65, 7, 406, 302],
                "image": "data\\images\\2404491.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the television", 2],
            ["what is in front of the television", 2],
            ["what is in the tv", 1]
        ],
        "org_questions": [
            ["what is on the television", 2],
            ["what is in front of the television", 2],
            ["what time is it", -1],
            ["how many people are there in the picture", -1],
            ["where is the tv", -1],
            ["what is the television on", -1],
            ["what is behind the tv", -1],
            ["what is under the tv", -1],
            ["what is in the tv", 1],
            ["what is next to the tv", -1],
            ["what is the television sitting on", -1]
        ],
        "context": [
            "a television sitting on top of a wooden floor.",
            "a cat is standing in front of a television."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2318484",
                "VG_object_id": "2807158",
                "bbox": [227, 266, 364, 333],
                "image": "data\\images\\2318484.jpg"
            },
            {
                "VG_image_id": "2343053",
                "VG_object_id": "2884149",
                "bbox": [10, 223, 498, 304],
                "image": "data\\images\\2343053.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animal is it", 2],
            ["what is the ground color", 2],
            ["what color is the ground", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what animal is it", 2],
            ["how many people are there", -1],
            ["what color is the ground", 1],
            ["what is on the ground", -1],
            ["what is the weather like", -1],
            ["when was the picture taken", 1],
            ["what is the ground covered with", -1],
            ["how is the weather", -1],
            ["what is the ground color", 2],
            ["what is in the ground", -1]
        ],
        "context": [
            "a giraffe standing in a field next to a forest.",
            "a man riding on the back of a horse in a rodeo."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2401740",
                "VG_object_id": "396423",
                "bbox": [18, 156, 486, 500],
                "image": "data\\images\\2401740.jpg"
            },
            {
                "VG_image_id": "2358792",
                "VG_object_id": "2152959",
                "bbox": [99, 117, 394, 288],
                "image": "data\\images\\2358792.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of food is it", 1],
            ["what is the table made of", 1],
            ["what color is the table", 1],
            ["what color is the food on the plate", 1]
        ],
        "org_questions": [
            ["what kind of food is it", 1],
            ["what is the table made of", 1],
            ["what color is the table", 1],
            ["how many people are there", -1],
            ["what is the food on", -1],
            ["what is on the tray", -1],
            ["how many bowls are there on the table", -1],
            ["what color is the food on the plate", 1],
            ["what shape is the table", -1],
            ["where was the photo taken", -1],
            ["where is the food", -1],
            ["what is the plate sitting on", -1]
        ],
        "context": [
            "a table with a variety of foods and a drink.",
            "a table with a pizza, a tray of food and a container of orange juice."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2353415",
                "VG_object_id": "3579034",
                "bbox": [0, 13, 373, 215],
                "image": "data\\images\\2353415.jpg"
            },
            {
                "VG_image_id": "2404739",
                "VG_object_id": "3814733",
                "bbox": [6, 3, 332, 251],
                "image": "data\\images\\2404739.jpg"
            }
        ],
        "questions_with_scores": [
            ["what gender is the player", 2],
            ["what sport is it", 1],
            ["what is the player wearing on the head", 1],
            ["what color are the seats", 1],
            ["what sport is the player doing", 1],
            ["who is in the background", 1],
            ["what is behind the player", 1]
        ],
        "org_questions": [
            ["what sport is it", 1],
            ["what gender is the player", 2],
            ["what is the player wearing on the head", 1],
            ["How many seats are there", -1],
            ["where is the chair", -1],
            ["what color are the seats", 1],
            ["what sport is the player doing", 1],
            ["who is in the background", 1],
            ["what is behind the player", 1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a woman swinging a tennis racket at a ball.",
            "a baseball player is getting ready to hit a ball."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2381305",
                "VG_object_id": "705610",
                "bbox": [2, 198, 498, 372],
                "image": "data\\images\\2381305.jpg"
            },
            {
                "VG_image_id": "2330867",
                "VG_object_id": "2748633",
                "bbox": [5, 376, 464, 495],
                "image": "data\\images\\2330867.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what color is the laptop", 1],
            ["what is in the background", 1],
            ["where was the photo taken", 1],
            ["what is the table color", 1],
            ["what is in the foreground", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the laptop", 1],
            ["what is on the table", -1],
            ["how many people are in the picture", 2],
            ["what shape is the table", -1],
            ["where is the table", -1],
            ["how is the table made of", -1],
            ["what is the table made of", -1],
            ["what is in the background", 1],
            ["where was the photo taken", 1],
            ["what is the table color", 1],
            ["what is in the foreground", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a laptop computer sitting on top of a desk.",
            "a woman sitting at a table with a laptop."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2340341",
                "VG_object_id": "3430472",
                "bbox": [375, 265, 498, 327],
                "image": "data\\images\\2340341.jpg"
            },
            {
                "VG_image_id": "2348758",
                "VG_object_id": "2040244",
                "bbox": [148, 197, 499, 279],
                "image": "data\\images\\2348758.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["where is the photo taken", 2],
            ["What color is the counter", 1],
            ["What is on the counter", 1],
            ["where was the picture taken", 1],
            ["what is the main color of the table", 1]
        ],
        "org_questions": [
            ["What color is the counter", 1],
            ["What is on the counter", 1],
            ["How many people are there", 2],
            ["where is the photo taken", 2],
            ["how many wines are there on the counter", -1],
            ["where was the picture taken", 1],
            ["what is the table made of", -1],
            ["what is the main color of the table", 1]
        ],
        "context": [
            "a room with a desk, a desk, and a chair.",
            "a group of people standing around a counter."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2376750",
                "VG_object_id": "570083",
                "bbox": [203, 54, 261, 199],
                "image": "data\\images\\2376750.jpg"
            },
            {
                "VG_image_id": "2370214",
                "VG_object_id": "605401",
                "bbox": [164, 51, 373, 460],
                "image": "data\\images\\2370214.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 1],
            ["what is the man doing", 1],
            ["how many men are there", 1],
            ["where is the photo taken", 1],
            ["where is the man", 1],
            ["What is person doing", 1],
            ["how many people are there in the picture", 1],
            ["what is the persion sitting on", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man wearing", 1],
            ["what is the man doing", 1],
            ["how many men are there", 1],
            ["where is the photo taken", 1],
            ["where is the man", 1],
            ["What is person doing", 1],
            ["how many people are there in the picture", 1],
            ["what is the persion sitting on", 1],
            ["when was the picture taken", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a group of people riding on top of an elephant.",
            "a man dressed in a traditional costume standing next to a horse."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2321268",
                "VG_object_id": "3341791",
                "bbox": [144, 223, 242, 470],
                "image": "data\\images\\2321268.jpg"
            },
            {
                "VG_image_id": "2383435",
                "VG_object_id": "532838",
                "bbox": [126, 244, 204, 318],
                "image": "data\\images\\2383435.jpg"
            }
        ],
        "questions_with_scores": [
            ["WHat is man doing", 2],
            ["what color is the trouser", 1],
            ["how many people are in the picture", 1],
            ["what is the ground covered with", 1],
            ["what is the person holding", 1],
            ["what color are the man's trousers", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 1],
            ["how many people are in the picture", 1],
            ["WHat is man doing", 2],
            ["what is the ground covered with", 1],
            ["what is the person holding", 1],
            ["what is the man doing", -1],
            ["what color are the man's trousers", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a man and a woman standing in a room.",
            "a family posing for a picture while holding skis."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2332345",
                "VG_object_id": "2775595",
                "bbox": [244, 165, 350, 257],
                "image": "data\\images\\2332345.jpg"
            },
            {
                "VG_image_id": "2413801",
                "VG_object_id": "165965",
                "bbox": [113, 160, 299, 369],
                "image": "data\\images\\2413801.jpg"
            }
        ],
        "questions_with_scores": [
            ["what gender is the person in the shirt", 2],
            ["how many people are there in the photo", 2],
            ["what is the gender of the person", 1],
            ["who is in the photo", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what gender is the person in the shirt", 2],
            ["how many people are there in the photo", 2],
            ["where is the picture taken", -1],
            ["What is person doing", -1],
            ["what is the gender of the person", 1],
            ["what is the persion wearing", -1],
            ["who is in the photo", 1],
            ["what is the persion doing", 1]
        ],
        "context": [
            "three women are smiling and smiling in a kitchen.",
            "a man in a yellow shirt holding a plate of food."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2359575",
                "VG_object_id": "3765008",
                "bbox": [0, 170, 498, 330],
                "image": "data\\images\\2359575.jpg"
            },
            {
                "VG_image_id": "2327599",
                "VG_object_id": "3294977",
                "bbox": [9, 370, 329, 498],
                "image": "data\\images\\2327599.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the ground", 2],
            ["what is the color of the ground", 1],
            ["what is on the ground", 1],
            ["What is the color of road", 1],
            ["what is on the land", 1]
        ],
        "org_questions": [
            ["what is the color of the ground", 1],
            ["how many people are there on the ground", 2],
            ["what is on the ground", 1],
            ["what is the land made of", -1],
            ["What is the ground made of", -1],
            ["What is the color of road", 1],
            ["what is on the land", 1],
            ["what time of day is it", -1],
            ["where was this photo taken", -1],
            ["how is the weather", -1]
        ],
        "context": [
            "a bus station with buses parked in the middle of it.",
            "a clock tower on a sidewalk in a city."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2369468",
                "VG_object_id": "611180",
                "bbox": [181, 167, 287, 218],
                "image": "data\\images\\2369468.jpg"
            },
            {
                "VG_image_id": "2362313",
                "VG_object_id": "1737355",
                "bbox": [91, 342, 268, 498],
                "image": "data\\images\\2362313.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 2],
            ["what color is the trousers", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 1],
            ["what color is the man's shirt", 2],
            ["what is the man holding", 2],
            ["how many people are there", -1],
            ["when is this picture taken", -1],
            ["Where is man ", -1],
            ["what in the background", -1],
            ["What is man doing", -1],
            ["what is the player wearing", -1],
            ["when was the photo taken", -1],
            ["what color is the grass", -1],
            ["what is the main color of the pants", -1]
        ],
        "context": [
            "a baseball player pitching a ball on a field.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2347605",
                "VG_object_id": "889276",
                "bbox": [328, 21, 446, 338],
                "image": "data\\images\\2347605.jpg"
            },
            {
                "VG_image_id": "2316535",
                "VG_object_id": "2776669",
                "bbox": [326, 35, 443, 242],
                "image": "data\\images\\2316535.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what is the woman wearing", 1],
            ["what color is the ground the woman standing on", 1],
            ["how many person's are there in the photo", 1],
            ["where is the lady", 1],
            ["What is lady doing", 1],
            ["how many people are there in the picture", 1],
            ["what is the woman doing", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", 1],
            ["what is the woman holding", 2],
            ["what color is the ground the woman standing on", 1],
            ["how many person's are there in the photo", 1],
            ["where is the lady", 1],
            ["What is lady doing", 1],
            ["how many people are there in the picture", 1],
            ["what is the woman doing", 1],
            ["what is the gender of the person in the picture", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the gender of the person", -1]
        ],
        "context": [
            "a woman in a yellow bikini holding a surfboard.",
            "a woman swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2374307",
                "VG_object_id": "727657",
                "bbox": [94, 200, 259, 261],
                "image": "data\\images\\2374307.jpg"
            },
            {
                "VG_image_id": "2398348",
                "VG_object_id": "426143",
                "bbox": [132, 225, 226, 300],
                "image": "data\\images\\2398348.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the hair of the child", 2],
            ["Where is the child sitting", 2],
            ["How many children are there in the photo", 1],
            ["what is the persion doing", 1],
            ["what is the ground covered with", 1],
            ["what is the main color of the background", 1],
            ["what color is the floor", 1]
        ],
        "org_questions": [
            ["What color is the hair of the child", 2],
            ["How many children are there in the photo", 1],
            ["Where is the child sitting", 2],
            ["when is this picture taken", -1],
            ["what is the persion doing", 1],
            ["what is the ground covered with", 1],
            ["what is the main color of the background", 1],
            ["what is the child wearing", -1],
            ["what color is the girls shirt", -1],
            ["what is the little girl wearing", -1],
            ["what color is the floor", 1]
        ],
        "context": [
            "a little girl sitting at a table with a bowl of pizza.",
            "two children playing with a toy car in a living room."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2411809",
                "VG_object_id": "3805661",
                "bbox": [135, 33, 375, 369],
                "image": "data\\images\\2411809.jpg"
            },
            {
                "VG_image_id": "2387134",
                "VG_object_id": "1279035",
                "bbox": [63, 5, 329, 300],
                "image": "data\\images\\2387134.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the woman's head", 2],
            ["how many people are there", 1],
            ["WHat color is woman's hair", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what is in front of the woman", -1],
            ["what is on the woman's head", 2],
            ["how many people are there", 1],
            ["WHat color is woman's hair", 1],
            ["where is the woman ", -1],
            ["what is the woman wearing", -1],
            ["what is the woman holding", 1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "a woman holding a plate of food in front of her.",
            "a woman sitting in front of a birthday cake with lit candles."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2323310",
                "VG_object_id": "3423243",
                "bbox": [105, 27, 465, 331],
                "image": "data\\images\\2323310.jpg"
            },
            {
                "VG_image_id": "2342333",
                "VG_object_id": "3648661",
                "bbox": [14, 12, 423, 331],
                "image": "data\\images\\2342333.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the man doing", 2],
            ["what color is the ground", 1],
            ["what color are the man's trousers", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what color is the ground", 1],
            ["where is the horse", -1],
            ["what is the land made of", -1],
            ["what color are the man's trousers", 1],
            ["what is the ground covered with", -1],
            ["how many horses are there", -1],
            ["what is the man doing", 2],
            ["when was the photo taken", -1],
            ["what is on the horse's head", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a man riding a horse on a dirt field.",
            "a man and a little girl riding on the back of a horse."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2391780",
                "VG_object_id": "1237404",
                "bbox": [107, 177, 170, 338],
                "image": "data\\images\\2391780.jpg"
            },
            {
                "VG_image_id": "2342104",
                "VG_object_id": "937253",
                "bbox": [202, 40, 303, 358],
                "image": "data\\images\\2342104.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the man", 2],
            ["what are the people doing", 1],
            ["what is the ground covered with", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what are the people doing", 1],
            ["what is the ground covered with", 1],
            ["how many men are there", -1],
            ["what color is the man's shirt", -1],
            ["Where is the man", 2],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["what is the man standing on", -1],
            ["what is the man holding", 1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man and a woman standing in a field with a baby stroller.",
            "a man and a woman standing next to a motorcycle."
        ]
    },
    {
        "object_category": "ocean",
        "images": [
            {
                "VG_image_id": "2407358",
                "VG_object_id": "281028",
                "bbox": [0, 54, 500, 333],
                "image": "data\\images\\2407358.jpg"
            },
            {
                "VG_image_id": "2327623",
                "VG_object_id": "979872",
                "bbox": [0, 0, 500, 333],
                "image": "data\\images\\2327623.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the water", 1],
            ["what color is the board", 1]
        ],
        "org_questions": [
            ["what color is the water", 1],
            ["what color is the board", 1],
            ["how many people are there", -1],
            ["what is the persion doing", -1],
            ["what is the weather like", -1],
            ["what is in the background", -1],
            ["how is the water", -1],
            ["where was the picture taken", -1],
            ["what is in the water", -1],
            ["where is this scene", -1]
        ],
        "context": [
            "a woman riding a wave on top of a surfboard.",
            "a man riding a surfboard in the ocean."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2348432",
                "VG_object_id": "882168",
                "bbox": [78, 170, 218, 266],
                "image": "data\\images\\2348432.jpg"
            },
            {
                "VG_image_id": "2335705",
                "VG_object_id": "2094809",
                "bbox": [199, 32, 372, 277],
                "image": "data\\images\\2335705.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the background", 1],
            ["what color is the cat", 1],
            ["how many cats are there in the picture", 1],
            ["what is the cat sitting on", 1],
            ["where is the cat", 1],
            ["how many cats together", 1],
            ["what is the cat lying on", 1]
        ],
        "org_questions": [
            ["what color is the background", 1],
            ["what color is the cat", 1],
            ["how many cats are there in the picture", 1],
            ["what is the cat sitting on", 1],
            ["where is the cat", 1],
            ["what is the cat doing", -1],
            ["how many cats together", 1],
            ["what is the cat lying on", 1],
            ["what kind of animal is in the picture", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "two cats are standing on the beach near the water.",
            "a cat sitting on a wooden floor in the sun."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2324353",
                "VG_object_id": "2811779",
                "bbox": [4, 57, 499, 331],
                "image": "data\\images\\2324353.jpg"
            },
            {
                "VG_image_id": "2402198",
                "VG_object_id": "658528",
                "bbox": [72, 75, 292, 498],
                "image": "data\\images\\2402198.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what is the man doing", 1],
            ["where is the man playing", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man holding", 2],
            ["how many people are there", -1],
            ["what is the man wearing on the head", -1],
            ["what is the man wearing", -1],
            ["what is the man wearing on his face", -1],
            ["when was the picture taken", -1],
            ["where is the man playing", 1],
            ["who is in the picture", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a man holding a frisbee in his hand.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2377431",
                "VG_object_id": "717742",
                "bbox": [29, 23, 449, 318],
                "image": "data\\images\\2377431.jpg"
            },
            {
                "VG_image_id": "2395422",
                "VG_object_id": "1205590",
                "bbox": [404, 3, 499, 85],
                "image": "data\\images\\2395422.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the container", 1],
            ["what is in the container", 1],
            ["what color is the table", 1],
            ["what shape is the container", 1],
            ["what is the container made of", 1],
            ["what is the color of the thing on the left of the container", 1]
        ],
        "org_questions": [
            ["what color is the container", 1],
            ["what is in the container", 1],
            ["what color is the table", 1],
            ["how many containers are there", -1],
            ["what shape is the container", 1],
            ["where are the containers", -1],
            ["what is the container made of", 1],
            ["what is the color of the thing on the left of the container", 1],
            ["what is on the table", -1]
        ],
        "context": [
            "a lunch box with a banana, apple, and a banana.",
            "a sandwich with lettuce and tomato on a plate."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2385307",
                "VG_object_id": "1298596",
                "bbox": [1, 272, 498, 374],
                "image": "data\\images\\2385307.jpg"
            },
            {
                "VG_image_id": "2330546",
                "VG_object_id": "972931",
                "bbox": [0, 106, 498, 325],
                "image": "data\\images\\2330546.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing on the field", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what are the people doing on the field", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what is the color of the grass", -1],
            ["what is on the ground", 1],
            ["where was this photo taken", -1],
            ["what is the weather like", -1],
            ["where is the grass", -1],
            ["what is the ground covered with", -1],
            ["what is green", -1]
        ],
        "context": [
            "a woman flying a kite in a field.",
            "two men playing soccer on a field with a soccer ball."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2329852",
                "VG_object_id": "3724679",
                "bbox": [2, 138, 483, 392],
                "image": "data\\images\\2329852.jpg"
            },
            {
                "VG_image_id": "2376574",
                "VG_object_id": "2144079",
                "bbox": [223, 170, 498, 373],
                "image": "data\\images\\2376574.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["what is the desk sitting on", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is next to the desk", 1],
            ["what is in front of the desk", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 1],
            ["how many plates are there", -1],
            ["what is the table made of", -1],
            ["where was the photo taken", -1],
            ["what is the desk sitting on", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["where is the table", -1],
            ["how many bowls are there on the table", -1],
            ["what is next to the desk", 1],
            ["what is in front of the desk", 1]
        ],
        "context": [
            "a computer monitor sitting on top of a wooden desk.",
            "a man sitting in a chair with a laptop."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2405419",
                "VG_object_id": "332242",
                "bbox": [58, 181, 279, 321],
                "image": "data\\images\\2405419.jpg"
            },
            {
                "VG_image_id": "2369480",
                "VG_object_id": "3171064",
                "bbox": [296, 145, 485, 305],
                "image": "data\\images\\2369480.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are there", 2],
            ["where is the dog", 2],
            ["what is the dog sitting on", 1],
            ["what is the ground covered with", 1],
            ["what is behind the dog", 1]
        ],
        "org_questions": [
            ["how many dogs are there", 2],
            ["what is the dog sitting on", 1],
            ["what color is the dog", -1],
            ["what is the dog doing", -1],
            ["where is the dog", 2],
            ["what is the dog wearing", -1],
            ["what is the ground covered with", 1],
            ["what is the breed of the dog", -1],
            ["when was the photo taken", -1],
            ["what type of animal is shown", -1],
            ["what is behind the dog", 1],
            ["what is on the dog", -1]
        ],
        "context": [
            "two women sitting on the beach with their dogs.",
            "a white dog laying on a bench"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2317689",
                "VG_object_id": "1018415",
                "bbox": [146, 223, 443, 371],
                "image": "data\\images\\2317689.jpg"
            },
            {
                "VG_image_id": "2324650",
                "VG_object_id": "2814125",
                "bbox": [0, 61, 497, 373],
                "image": "data\\images\\2324650.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is the table", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["Where is photo taken", -1],
            ["How many people are there", 2],
            ["what is in the background", -1],
            ["what is the table made of", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a group of people sitting at a table with food.",
            "a plate of fruit and a glass of water on a table."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2332439",
                "VG_object_id": "969620",
                "bbox": [5, 155, 106, 240],
                "image": "data\\images\\2332439.jpg"
            },
            {
                "VG_image_id": "2412371",
                "VG_object_id": "3804852",
                "bbox": [148, 171, 340, 274],
                "image": "data\\images\\2412371.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bowl", 2],
            ["how many glasses are there on the table", 1],
            ["what kind of food is in the bowl", 1],
            ["what is in the white bowl", 1]
        ],
        "org_questions": [
            ["what color is the bowl", 2],
            ["what is in the bowl", -1],
            ["how many glasses are there on the table", 1],
            ["Where is the bowl", -1],
            ["what is the bowl made of", -1],
            ["what kind of vegetables is in the bowl", -1],
            ["what is on the counter", -1],
            ["what is inside the bowl", -1],
            ["what kind of food is in the bowl", 1],
            ["what is next to the bowl", -1],
            ["what is in the white bowl", 1],
            ["how many bowls are there", -1]
        ],
        "context": [
            "a turkey sitting on a rack next to a bowl of soup.",
            "a table with many plates of food on it"
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2405855",
                "VG_object_id": "328656",
                "bbox": [45, 203, 152, 257],
                "image": "data\\images\\2405855.jpg"
            },
            {
                "VG_image_id": "2347031",
                "VG_object_id": "894236",
                "bbox": [269, 92, 331, 194],
                "image": "data\\images\\2347031.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cat", 2],
            ["how many cats are there", 2],
            ["where is the picture taken", 1],
            ["where is the cat", 1],
            ["what color is the table the cat sitting on", 1]
        ],
        "org_questions": [
            ["what is the cat on", -1],
            ["what color is the cat", 2],
            ["how many cats together", -1],
            ["where is the picture taken", 1],
            ["what is the cat holding", -1],
            ["where is the cat", 1],
            ["what color is the table the cat sitting on", 1],
            ["what animal is in the picture", -1],
            ["what is the cat looking at", -1],
            ["what animal is shown", -1],
            ["how many cats are there", 2]
        ],
        "context": [
            "two cats sitting on a wooden fence near a river.",
            "a cat sitting on a table with a table cloth."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2409740",
                "VG_object_id": "1707860",
                "bbox": [320, 39, 392, 219],
                "image": "data\\images\\2409740.jpg"
            },
            {
                "VG_image_id": "2320090",
                "VG_object_id": "3291722",
                "bbox": [115, 22, 237, 220],
                "image": "data\\images\\2320090.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the guy doing", 1],
            ["what is the guy wearing", 1],
            ["what color is the man's shirt", 1],
            ["what is the man holding", 1],
            ["what is the persion riding on", 1],
            ["what is on the man's head", 1],
            ["what is the persion riding", 1]
        ],
        "org_questions": [
            ["what is the guy doing", 1],
            ["what is the guy wearing", 1],
            ["how many people are there", -1],
            ["where is the man", -1],
            ["what color is the man's shirt", 1],
            ["what is the man holding", 1],
            ["what is the man wearing on his face", -1],
            ["when was the photo taken", -1],
            ["what is the persion riding on", 1],
            ["what is on the man's head", 1],
            ["what is the persion riding", 1]
        ],
        "context": [
            "two men riding bicycles down a street with surfboards.",
            "a man riding a skateboard down a rail."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2319385",
                "VG_object_id": "2909048",
                "bbox": [269, 70, 439, 252],
                "image": "data\\images\\2319385.jpg"
            },
            {
                "VG_image_id": "2379892",
                "VG_object_id": "1353108",
                "bbox": [110, 40, 316, 332],
                "image": "data\\images\\2379892.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color are the man's trousers", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["What is man holding", 1],
            ["what is the man wearing", 1],
            ["how many players are there in the picture", 1],
            ["who is in the photo", 1],
            ["what is the man on the left doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color are the man's trousers", 1],
            ["what sport is the man playing", -1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["What is man holding", 1],
            ["what is the man wearing", 1],
            ["how many players are there in the picture", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the man on the left doing", 1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man riding skis down a snow covered slope.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2370214",
                "VG_object_id": "605420",
                "bbox": [218, 309, 343, 440],
                "image": "data\\images\\2370214.jpg"
            },
            {
                "VG_image_id": "2362055",
                "VG_object_id": "1965129",
                "bbox": [181, 243, 245, 308],
                "image": "data\\images\\2362055.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the gound made of", 1],
            ["how many people are there in the picture", 1],
            ["where is this photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the clothes of the man in the trousers", -1],
            ["what is the gound made of", 1],
            ["how many people are there in the picture", 1],
            ["where is this photo taken", 1],
            ["what is the man doing", -1],
            ["what is the man wearing on head", -1],
            ["what color is the ground", 2],
            ["what is the ground covered with", 1],
            ["when was the photo taken", -1],
            ["what type of pants are the people wearing", -1],
            ["what is on the ground", 1],
            ["what are the people wearing", -1]
        ],
        "context": [
            "a man dressed in a traditional costume standing next to a horse.",
            "a group of men standing together in a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2336539",
                "VG_object_id": "2411850",
                "bbox": [213, 230, 264, 327],
                "image": "data\\images\\2336539.jpg"
            },
            {
                "VG_image_id": "2357660",
                "VG_object_id": "1696114",
                "bbox": [201, 273, 237, 357],
                "image": "data\\images\\2357660.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["What is man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["how many people are there", -1],
            ["where is the picture taken", -1],
            ["What is man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", -1],
            ["where is the man standing", -1]
        ],
        "context": [
            "a man standing in front of a truck with a rope.",
            "a sign for a restaurant that reads \" the restaurant \"."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2376987",
                "VG_object_id": "718927",
                "bbox": [285, 3, 498, 199],
                "image": "data\\images\\2376987.jpg"
            },
            {
                "VG_image_id": "2369225",
                "VG_object_id": "743922",
                "bbox": [249, 14, 472, 184],
                "image": "data\\images\\2369225.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["what is in front of the building", 1],
            ["how many people are there in the picture", 1],
            ["what is the person doing", 1],
            ["what transportation is there", 1]
        ],
        "org_questions": [
            ["what color is the building", 2],
            ["what is in front of the building", 1],
            ["how many people are there in the picture", 1],
            ["what time is it", -1],
            ["what is the person doing", 1],
            ["what transportation is there", 1],
            ["where was the photo taken", -1],
            ["what is the building made of", -1],
            ["when was this picture taken", -1],
            ["where is the building", -1]
        ],
        "context": [
            "a group of men walking down a sidewalk.",
            "a man wearing a bow tie and a bow tie."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2356750",
                "VG_object_id": "817636",
                "bbox": [211, 1, 497, 330],
                "image": "data\\images\\2356750.jpg"
            },
            {
                "VG_image_id": "2409825",
                "VG_object_id": "2283396",
                "bbox": [24, 49, 209, 219],
                "image": "data\\images\\2409825.jpg"
            }
        ],
        "questions_with_scores": [["What color is the building", 1]],
        "org_questions": [
            ["What color is the building", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the weather like", -1],
            ["what is the building made of", -1],
            ["what is in the distance", -1],
            ["when was the picture taken", -1],
            ["what is on the building", -1],
            ["what is in front of the building", -1],
            ["what is behind the street", -1]
        ],
        "context": [
            "a street sign and a traffic light on a pole.",
            "a bike is parked on the side of the road."
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2416650",
                "VG_object_id": "3506426",
                "bbox": [61, 0, 339, 91],
                "image": "data\\images\\2416650.jpg"
            },
            {
                "VG_image_id": "2315696",
                "VG_object_id": "3624479",
                "bbox": [59, 200, 266, 373],
                "image": "data\\images\\2315696.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the tray", 2],
            ["what is the tray made of", 1],
            ["what is on the tray", 1],
            ["What food is on the plate", 1],
            ["what is in the tray", 1]
        ],
        "org_questions": [
            ["what color is the tray", 2],
            ["what is the tray made of", 1],
            ["what is on the tray", 1],
            ["where is the photo taken", -1],
            ["how many pieces are there on the plate", -1],
            ["What food is on the plate", 1],
            ["what is in the tray", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a sandwich with meat, cheese, and vegetables on a plate.",
            "three trays of food with different types of food."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2360011",
                "VG_object_id": "2339437",
                "bbox": [401, 177, 460, 242],
                "image": "data\\images\\2360011.jpg"
            },
            {
                "VG_image_id": "2353916",
                "VG_object_id": "1677309",
                "bbox": [0, 266, 498, 323],
                "image": "data\\images\\2353916.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what color is the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["how many people are in the picture", 2],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["What is on the table", -1],
            ["how many wine glasses are there on the table", -1],
            ["what is made of wood", -1],
            ["what is behind the table", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a kitchen with a sink, dishwasher and a window.",
            "two men sitting at a table with their arms crossed."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2403127",
                "VG_object_id": "657526",
                "bbox": [123, 152, 180, 210],
                "image": "data\\images\\2403127.jpg"
            },
            {
                "VG_image_id": "2327597",
                "VG_object_id": "3043402",
                "bbox": [162, 160, 252, 251],
                "image": "data\\images\\2327597.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the clock", 1],
            ["what time is it", 1],
            ["what kind of symbol is showed on the clock", 1]
        ],
        "org_questions": [
            ["what color is the clock", 1],
            ["where is the clock", -1],
            ["how many clocks are there in the picture", -1],
            ["what time is it", 1],
            ["what shape is the clock", -1],
            ["what is the clock made of", -1],
            ["what kind of symbol is showed on the clock", 1],
            ["when was the photo taken", -1],
            ["what is on the building", -1],
            ["what time does the clock say", -1],
            ["what is the shape of the clock", -1],
            ["what shape is the clock face", -1]
        ],
        "context": [
            "a colorful clock tower in a parking lot.",
            "a clock on the side of a building."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2371390",
                "VG_object_id": "597934",
                "bbox": [120, 195, 274, 453],
                "image": "data\\images\\2371390.jpg"
            },
            {
                "VG_image_id": "2382018",
                "VG_object_id": "699886",
                "bbox": [92, 36, 185, 200],
                "image": "data\\images\\2382018.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is the ground", 1],
            ["WHat is man doing", 1],
            ["what sport is the man playing", 1],
            ["What sports is man doing", 1],
            ["what is the color of the man's shirt", 1],
            ["what is the man holding", 1],
            ["what sport is this", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is the ground", 1],
            ["WHat is man doing", 1],
            ["where is the man", -1],
            ["what sport is the man playing", 1],
            ["what is the man wearing", -1],
            ["What sports is man doing", 1],
            ["what is the color of the man's shirt", 1],
            ["who is playing", -1],
            ["what is the man holding", 1],
            ["what is on the man's head", -1],
            ["what sport is this", 1]
        ],
        "context": [
            "a man holding a tennis racquet on a tennis court.",
            "two men playing frisbee on a field."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2339242",
                "VG_object_id": "2653906",
                "bbox": [135, 15, 487, 332],
                "image": "data\\images\\2339242.jpg"
            },
            {
                "VG_image_id": "2409035",
                "VG_object_id": "3808380",
                "bbox": [124, 83, 402, 303],
                "image": "data\\images\\2409035.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many elephants are there", 2],
            ["what color is the background", 1],
            ["what is the elephant standing on", 1]
        ],
        "org_questions": [
            ["How many elephants are there", 2],
            ["what color is the background", 1],
            ["what is on the elephant", -1],
            ["where are the elephants", -1],
            ["what are the elephants doing", -1],
            ["what is in front of the elephants", -1],
            ["what is in the distance", -1],
            ["when was the photo taken", -1],
            ["what animal is in the photo", -1],
            ["what is the elephant standing on", 1],
            ["where was the photo taken", -1],
            ["what is behind the elephant", -1]
        ],
        "context": [
            "a baby elephant standing in a field of tall grass.",
            "two elephants standing in a pond with trees in the background."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2348938",
                "VG_object_id": "2443311",
                "bbox": [125, 88, 239, 259],
                "image": "data\\images\\2348938.jpg"
            },
            {
                "VG_image_id": "2373168",
                "VG_object_id": "733995",
                "bbox": [77, 17, 499, 374],
                "image": "data\\images\\2373168.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is this photo taken", 2],
            ["what color is the girl's hair", 1],
            ["what is the girl doing", 1],
            ["what is in front of the girl", 1]
        ],
        "org_questions": [
            ["what color is the girl's hair", 1],
            ["where is this photo taken", 2],
            ["what is the girl doing", 1],
            ["how many people are there", -1],
            ["what is on the girl's head", -1],
            ["what is in front of the girl", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is the woman wearing", -1],
            ["where is the woman sitting", -1]
        ],
        "context": [
            "a girl is standing in front of a cake.",
            "a girl is eating a large sandwich"
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2334009",
                "VG_object_id": "3201821",
                "bbox": [33, 94, 253, 239],
                "image": "data\\images\\2334009.jpg"
            },
            {
                "VG_image_id": "2344199",
                "VG_object_id": "2601667",
                "bbox": [175, 37, 498, 318],
                "image": "data\\images\\2344199.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is the child holding", 2],
            ["what is the child doing", 1],
            ["what color is the child's pant", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what is the child doing", 1],
            ["how many people are in the picture", 2],
            ["what color is the child's pant", 1],
            ["what is the ground covered with", -1],
            ["where is the man", -1],
            ["what is the child holding", 2],
            ["what is the persion standing on", -1],
            ["when was the photo taken", -1],
            ["who is playing", -1],
            ["what sport is being played", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a boy is playing soccer on the field.",
            "a group of boys sitting on a bench holding baseball bats."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2410830",
                "VG_object_id": "1085144",
                "bbox": [87, 112, 413, 225],
                "image": "data\\images\\2410830.jpg"
            },
            {
                "VG_image_id": "2395277",
                "VG_object_id": "1206521",
                "bbox": [67, 114, 357, 245],
                "image": "data\\images\\2395277.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the truck", 1],
            ["what is on the truck", 1],
            ["what color is the truck", 1],
            ["what is the ground covered with", 1],
            ["what kind of land is the truck on", 1],
            ["what is in the distance", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["where is the truck", 1],
            ["what is on the truck", 1],
            ["what color is the truck", 1],
            ["how many people are there", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", 1],
            ["what kind of land is the truck on", 1],
            ["what is in the distance", 1],
            ["when was the photo taken", -1],
            ["what is the truck doing", -1],
            ["what type of vehicle is shown", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a large white truck parked in front of a building.",
            "a truck carrying logs on a dirt road."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2343756",
                "VG_object_id": "2841122",
                "bbox": [0, 101, 499, 374],
                "image": "data\\images\\2343756.jpg"
            },
            {
                "VG_image_id": "2359876",
                "VG_object_id": "2188754",
                "bbox": [59, 180, 497, 374],
                "image": "data\\images\\2359876.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there", 1],
            ["what is the weather like", 1],
            ["what is in the distance", 1],
            ["what color is the sky", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["how many cars are there", 1],
            ["what is the weather like", 1],
            ["what is in the distance", 1],
            ["what color is the sky", 1],
            ["which part of the car can we see in the picture", -1],
            ["what is the ground covered with", 1],
            ["what is on the side of the car", -1],
            ["where was the photo taken", -1],
            ["what kind of car is this", -1],
            ["when was the photo taken", -1],
            ["where is the car", -1]
        ],
        "context": [
            "a car parked next to a white car.",
            "a car parked in the snow next to a stop sign."
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2316831",
                "VG_object_id": "3725414",
                "bbox": [82, 194, 177, 239],
                "image": "data\\images\\2316831.jpg"
            },
            {
                "VG_image_id": "2339271",
                "VG_object_id": "951913",
                "bbox": [69, 245, 428, 373],
                "image": "data\\images\\2339271.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the tray", 1],
            ["where is the tray", 1],
            ["what is in the room", 1],
            ["what is the main color of the table", 1]
        ],
        "org_questions": [
            ["what is on the tray", 1],
            ["what is the tray made of", -1],
            ["what color is the wall", -1],
            ["how many people are there", -1],
            ["what shape is the tray", -1],
            ["where is the tray", 1],
            ["how many cakes are there in the picture", -1],
            ["what is next to the table", -1],
            ["where is the picture taken", -1],
            ["what is in the room", 1],
            ["what is the main color of the table", 1]
        ],
        "context": [
            "a room with a window and a sink",
            "a computer monitor sitting on top of a glass desk."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2376024",
                "VG_object_id": "2350006",
                "bbox": [11, 161, 340, 373],
                "image": "data\\images\\2376024.jpg"
            },
            {
                "VG_image_id": "2338234",
                "VG_object_id": "954862",
                "bbox": [1, 152, 220, 355],
                "image": "data\\images\\2338234.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person in the shirt holding", 2],
            ["what is the person holding", 2],
            ["what gender is the person in the shirt", 1],
            ["who is wearing the shirt", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what gender is the person in the shirt", 1],
            ["what is the person in the shirt holding", 2],
            ["who is wearing the shirt", 1],
            ["what is the person wearing on neck", -1],
            ["how many people are there", -1],
            ["what is the person holding", 2]
        ],
        "context": [
            "a woman holding a donut and a cup of coffee.",
            "a young boy poses with a car in front of the race car."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2351230",
                "VG_object_id": "2527661",
                "bbox": [74, 64, 267, 283],
                "image": "data\\images\\2351230.jpg"
            },
            {
                "VG_image_id": "2328519",
                "VG_object_id": "3660096",
                "bbox": [42, 31, 472, 383],
                "image": "data\\images\\2328519.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many players are in the picture", 2],
            ["what gender is the player", 1],
            ["what color is the court", 1]
        ],
        "org_questions": [
            ["what is the player doing", -1],
            ["what gender is the player", 1],
            ["how many players are in the picture", 2],
            ["what is in the background", -1],
            ["what sport is the person playing", -1],
            ["what is the player wearing", -1],
            ["what color is the court", 1],
            ["when was the picture taken", -1],
            ["where are the people", -1],
            ["what is the persion holding", -1]
        ],
        "context": [
            "a tennis player shaking hands on the court.",
            "a woman in a white tennis outfit holding a tennis racket."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "1159351",
                "VG_object_id": "1592124",
                "bbox": [60, 98, 383, 924],
                "image": "data\\images\\1159351.jpg"
            },
            {
                "VG_image_id": "2316789",
                "VG_object_id": "2942065",
                "bbox": [63, 99, 128, 307],
                "image": "data\\images\\2316789.jpg"
            }
        ],
        "questions_with_scores": [
            ["what time is it", 2],
            ["when was the photo taken", 2],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what color is the background", 1],
            ["What color is man's pant", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what time is it", 2],
            ["how many people are there", -1],
            ["what is on the person's head", -1],
            ["where is the person", -1],
            ["what color is the background", 1],
            ["What color is man's pant", 1],
            ["who is wearing a white shirt", -1],
            ["what is the man holding", 1],
            ["when was the photo taken", 2],
            ["what are the people wearing", -1]
        ],
        "context": [
            "a group of men standing next to each other holding wii controllers.",
            "two dogs on a leash standing next to a parking meter."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2321254",
                "VG_object_id": "3184199",
                "bbox": [198, 132, 490, 310],
                "image": "data\\images\\2321254.jpg"
            },
            {
                "VG_image_id": "2343909",
                "VG_object_id": "918810",
                "bbox": [17, 274, 368, 500],
                "image": "data\\images\\2343909.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the shape of the bed", 2],
            ["how many pillows are there on the bed", 2],
            ["what color is the pillow", 2],
            ["what color are the pillows on the bed", 1]
        ],
        "org_questions": [
            ["what is the shape of the bed", 2],
            ["what is on the bed", -1],
            ["how many pillows are there on the bed", 2],
            ["how many people are there on the bed", -1],
            ["what color are the pillows on the bed", 1],
            ["where was this photo taken", -1],
            ["what room is this", -1],
            ["what is the bed made of", -1],
            ["what kind of bed is this", -1],
            ["where are the pillows", -1],
            ["what color is the pillow", 2]
        ],
        "context": [
            "a bedroom with a bed, chair, and a chair.",
            "a bed with a chandelier and a chandelier."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2410712",
                "VG_object_id": "362722",
                "bbox": [197, 148, 324, 328],
                "image": "data\\images\\2410712.jpg"
            },
            {
                "VG_image_id": "2337578",
                "VG_object_id": "3205074",
                "bbox": [171, 8, 482, 461],
                "image": "data\\images\\2337578.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is beside the elephant", 1],
            ["what is on the elephant's head", 1]
        ],
        "org_questions": [
            ["what is on the elephant", -1],
            ["what is beside the elephant", 1],
            ["where is picture taken", -1],
            ["how many people are there", 2],
            ["what color is the elephants", -1],
            ["what are the elephants doing", -1],
            ["where is the elephant", -1],
            ["what is in front of the elephants", -1],
            ["what animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["what is on the elephant's head", 1]
        ],
        "context": [
            "a woman riding on the back of an elephant.",
            "an elephant standing next to a tire on a road."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2333819",
                "VG_object_id": "3461698",
                "bbox": [106, 41, 252, 151],
                "image": "data\\images\\2333819.jpg"
            },
            {
                "VG_image_id": "2317475",
                "VG_object_id": "3209426",
                "bbox": [215, 229, 297, 324],
                "image": "data\\images\\2317475.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color are the man's pants", 2],
            ["what color are the shorts", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color are the man's pants", 2],
            ["how many people are there", 1],
            ["What is the boy wearing on his head", -1],
            ["where is the man", -1],
            ["what is the person holding", -1],
            ["how many children are there in the picture", -1],
            ["when was the photo taken", -1],
            ["what is the man wearing", -1],
            ["what is on the man's head", -1],
            ["what color are the shorts", 2]
        ],
        "context": [
            "a man swinging a tennis racket at a tennis match.",
            "two men in white shirts and white shirts playing tennis."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2336379",
                "VG_object_id": "960463",
                "bbox": [122, 95, 376, 323],
                "image": "data\\images\\2336379.jpg"
            },
            {
                "VG_image_id": "2348467",
                "VG_object_id": "1946610",
                "bbox": [139, 1, 339, 233],
                "image": "data\\images\\2348467.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many animals are there", 2],
            ["How many giraffes are there", 2],
            ["how many giraffes", 1]
        ],
        "org_questions": [
            ["How many animals are there", 2],
            ["How many giraffes are there", 2],
            ["what is the ground covered with", -1],
            ["what kind of animal is it", -1],
            ["where is the animal", -1],
            ["what color is the animal", -1],
            ["what is the animal", -1],
            ["what are the giraffes doing", -1],
            ["where was this photo taken", -1],
            ["what are the giraffes standing on", -1],
            ["what animal is in the picture", -1],
            ["how many giraffes", 1]
        ],
        "context": [
            "a group of people watching giraffes in a zoo.",
            "a giraffe walking in a field near a tree."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2317385",
                "VG_object_id": "2972778",
                "bbox": [2, 42, 497, 332],
                "image": "data\\images\\2317385.jpg"
            },
            {
                "VG_image_id": "2360238",
                "VG_object_id": "1718602",
                "bbox": [2, 70, 499, 371],
                "image": "data\\images\\2360238.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the fork", 1],
            ["what type of food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the plate", -1],
            ["where is the fork", 1],
            ["how many plates are there", -1],
            ["what is the plate made of", -1],
            ["what is on the plate", -1],
            ["what is the plate on", -1],
            ["what is the color of the table", -1],
            ["what shape is the plate", -1],
            ["what type of food is on the plate", 1],
            ["where was the photo taken", -1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a plate of food with a fork on it.",
            "a plate with a sandwich and chips on it."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2394035",
                "VG_object_id": "467638",
                "bbox": [1, 250, 498, 330],
                "image": "data\\images\\2394035.jpg"
            },
            {
                "VG_image_id": "2319947",
                "VG_object_id": "997338",
                "bbox": [85, 233, 379, 330],
                "image": "data\\images\\2319947.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 2],
            ["what is standing on the floor", 1],
            ["what pattern is the floor", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["Where is the floor", -1],
            ["What color is the floor", -1],
            ["how many sofas are on the floor", -1],
            ["what is standing on the floor", 1],
            ["what pattern is the floor", 1],
            ["what is the ground covered with", 1],
            ["where is the picture taken", 2],
            ["what is covering the ground", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "an elephant standing in a dirt field next to a fence.",
            "a bathroom with a sink and a soap dispenser."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2382559",
                "VG_object_id": "538462",
                "bbox": [122, 230, 309, 385],
                "image": "data\\images\\2382559.jpg"
            },
            {
                "VG_image_id": "2394779",
                "VG_object_id": "461165",
                "bbox": [153, 406, 231, 478],
                "image": "data\\images\\2394779.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the toilet", 1],
            ["what is in the toilet", 1],
            ["what is the ground covered with", 1],
            ["where is the picture taken", 1],
            ["what is behind the toilet", 1],
            ["what is next to the toilet", 1]
        ],
        "org_questions": [
            ["where is the toilet", 1],
            ["what is in the toilet", 1],
            ["what is the ground covered with", 1],
            ["how many toilets are there", -1],
            ["where is the picture taken", 1],
            ["what is behind the toilet", 1],
            ["what is next to the toilet", 1]
        ],
        "context": [
            "a toilet with a wooden handle and a pair of black seats.",
            "a picture of jesus hanging above a toilet."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2333362",
                "VG_object_id": "3927748",
                "bbox": [205, 38, 309, 305],
                "image": "data\\images\\2333362.jpg"
            },
            {
                "VG_image_id": "2355168",
                "VG_object_id": "1707163",
                "bbox": [363, 104, 487, 277],
                "image": "data\\images\\2355168.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is person's shirt", 2],
            ["How many people are there", 1],
            ["what is on the person's head", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["What color is person's shirt", 2],
            ["How many people are there", 1],
            ["what is the person holding", -1],
            ["What is person doing", -1],
            ["what is on the person's head", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1],
            ["what is on the woman's feet", -1],
            ["what are the people doing", -1]
        ],
        "context": [
            "a woman on skis in the snow.",
            "a group of people walking across a snow covered field."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2393034",
                "VG_object_id": "1223563",
                "bbox": [175, 168, 351, 274],
                "image": "data\\images\\2393034.jpg"
            },
            {
                "VG_image_id": "2349417",
                "VG_object_id": "2393413",
                "bbox": [109, 170, 275, 239],
                "image": "data\\images\\2349417.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cow", 2],
            ["where is the cow", 1],
            ["what color is the ground the cow standing on", 1],
            ["where are the cows", 1]
        ],
        "org_questions": [
            ["what color is the cow", 2],
            ["where is the cow", 1],
            ["what is the cow doing", -1],
            ["how many cows are in the picture", -1],
            ["what is on the cow's head", -1],
            ["what color is the ground the cow standing on", 1],
            ["where are the cows", 1],
            ["what animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["what kind of animal is this", -1]
        ],
        "context": [
            "a cow walking down a street with people walking around.",
            "a cow is crossing a river in front of a boat."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2374049",
                "VG_object_id": "1870369",
                "bbox": [441, 203, 495, 269],
                "image": "data\\images\\2374049.jpg"
            },
            {
                "VG_image_id": "2368746",
                "VG_object_id": "616316",
                "bbox": [0, 1, 127, 374],
                "image": "data\\images\\2368746.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is on the sofa", 2],
            ["what color is the sofa", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 1],
            ["how many people are there in the picture", 2],
            ["what is on the sofa", 2]
        ],
        "context": [
            "a group of people standing in a living room.",
            "a black cat is curled up on a couch"
        ]
    },
    {
        "object_category": "banana",
        "images": [
            {
                "VG_image_id": "2337542",
                "VG_object_id": "2916069",
                "bbox": [46, 163, 253, 346],
                "image": "data\\images\\2337542.jpg"
            },
            {
                "VG_image_id": "2416704",
                "VG_object_id": "3308994",
                "bbox": [6, 32, 469, 348],
                "image": "data\\images\\2416704.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bananas are there", 2],
            ["what color is the table", 1],
            ["what shape is the container holding the banana", 1],
            ["where was the photo taken", 1],
            ["what is next to the bananas", 1]
        ],
        "org_questions": [
            ["how many bananas are there", 2],
            ["what color is the table", 1],
            ["what is on the table", -1],
            ["what shape is the container holding the banana", 1],
            ["where are the bananas", -1],
            ["what is under the banana", -1],
            ["What is the color of banana", -1],
            ["where is the banana placed", -1],
            ["what kind of fruit is this", -1],
            ["where was the photo taken", 1],
            ["what is next to the bananas", 1]
        ],
        "context": [
            "a plastic container filled with fruit and a sandwich.",
            "a table with several plates of bananas on it."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2373352",
                "VG_object_id": "2638944",
                "bbox": [154, 142, 208, 267],
                "image": "data\\images\\2373352.jpg"
            },
            {
                "VG_image_id": "2352824",
                "VG_object_id": "3008507",
                "bbox": [27, 9, 199, 316],
                "image": "data\\images\\2352824.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the gender of the person", 1],
            ["what color is the background", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what color is the ground", 2],
            ["what is the gender of the person", 1],
            ["what color is the background", 1],
            ["how many people are there", -1],
            ["what is the persion wearing", -1],
            ["What is weather like", -1],
            ["what color are the person's clothes", -1],
            ["what sport is being played", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", 1],
            ["what are the people doing", -1]
        ],
        "context": [
            "a woman and two children playing tennis on a grass court.",
            "a man sitting on a bench holding a tennis racket."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2404738",
                "VG_object_id": "338618",
                "bbox": [59, 34, 292, 465],
                "image": "data\\images\\2404738.jpg"
            },
            {
                "VG_image_id": "2406861",
                "VG_object_id": "288993",
                "bbox": [91, 55, 267, 499],
                "image": "data\\images\\2406861.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["How many people are there", -1],
            ["what is the man wearing", 1],
            ["what sport is the man doing", -1],
            ["when was the photo taken", -1],
            ["who is playing", -1],
            ["what is behind the man", -1],
            ["what is the man holding", -1],
            ["what is the weather like", -1],
            ["What is man doing", -1],
            ["what is the gender of the person", -1],
            ["what is the man standing on", -1],
            ["what is on the man's face", 1]
        ],
        "context": [
            "a man jumping in the air with a tennis racket.",
            "a man holding a tennis racket on a street."
        ]
    },
    {
        "object_category": "computer",
        "images": [
            {
                "VG_image_id": "2324771",
                "VG_object_id": "3209358",
                "bbox": [37, 281, 257, 369],
                "image": "data\\images\\2324771.jpg"
            },
            {
                "VG_image_id": "2322008",
                "VG_object_id": "2871686",
                "bbox": [271, 181, 463, 329],
                "image": "data\\images\\2322008.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the computer", 2],
            ["who is using the computer", 2],
            ["how many computers are there in the photo", 1]
        ],
        "org_questions": [
            ["what color is the computer", 2],
            ["who is using the computer", 2],
            ["how many computers are there", -1],
            ["where is the computer", -1],
            ["how many people are there in the picture", -1],
            ["how many computers are there in the photo", 1],
            ["what type of computer is shown", -1],
            ["what is the laptop sitting on", -1],
            ["what is on the laptop", -1],
            ["what is next to the laptop", -1]
        ],
        "context": [
            "a woman sitting on a couch with two laptops.",
            "a man sitting at a desk using a laptop."
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2330636",
                "VG_object_id": "3301219",
                "bbox": [168, 297, 277, 382],
                "image": "data\\images\\2330636.jpg"
            },
            {
                "VG_image_id": "2396668",
                "VG_object_id": "441627",
                "bbox": [46, 29, 371, 338],
                "image": "data\\images\\2396668.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the boat doing", 2],
            ["What color is the boat", 1],
            ["Where is the boat", 1],
            ["what is in the distance", 1],
            ["what is the boat on", 1]
        ],
        "org_questions": [
            ["What color is the boat", 1],
            ["Where is the boat", 1],
            ["How many boats are there", -1],
            ["what is the boat doing", 2],
            ["what is in the distance", 1],
            ["what is the boat on", 1],
            ["what is the background", -1],
            ["what is on the boat", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["what is behind the boat", -1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a group of sailboats in the water near a hill.",
            "a small orange boat sitting on top of a beach."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2405752",
                "VG_object_id": "329339",
                "bbox": [17, 246, 484, 482],
                "image": "data\\images\\2405752.jpg"
            },
            {
                "VG_image_id": "2358311",
                "VG_object_id": "804048",
                "bbox": [1, 1, 499, 330],
                "image": "data\\images\\2358311.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are there", 2],
            ["what color is the food", 2],
            ["what is the plate made of", 1]
        ],
        "org_questions": [
            ["how many plates are there", 2],
            ["what color is the food", 2],
            ["what is on the table", -1],
            ["what is the table made of", -1],
            ["what pattern is the plate", -1],
            ["what is the food on", -1],
            ["where was the photo taken", -1],
            ["what kind of food is on the table", -1],
            ["where is the plate", -1],
            ["what is the plate sitting on", -1],
            ["what is the plate made of", 1]
        ],
        "context": [
            "a person eating a sandwich and a bowl of food.",
            "a donut with rainbow sprinkles on a paper plate."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2327986",
                "VG_object_id": "3053689",
                "bbox": [3, 121, 499, 273],
                "image": "data\\images\\2327986.jpg"
            },
            {
                "VG_image_id": "2406252",
                "VG_object_id": "1717425",
                "bbox": [111, 143, 359, 374],
                "image": "data\\images\\2406252.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 2],
            ["what color is the table", 1],
            ["what shape is the plate", 1],
            ["what type of food is this", 1]
        ],
        "org_questions": [
            ["what color is the plate", 2],
            ["what color is the table", 1],
            ["what is on the plate", -1],
            ["how many plates are in the picture", -1],
            ["what shape is the plate", 1],
            ["where is the plate", -1],
            ["what type of food is this", 1],
            ["what is the plate sitting on", -1],
            ["where was the photo taken", -1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a cake with white frosting and a white frosting.",
            "a plate of food with a fork on it."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2331960",
                "VG_object_id": "3215676",
                "bbox": [231, 140, 285, 202],
                "image": "data\\images\\2331960.jpg"
            },
            {
                "VG_image_id": "2413440",
                "VG_object_id": "173506",
                "bbox": [216, 159, 273, 210],
                "image": "data\\images\\2413440.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the elephant", 1],
            ["what is the elephant doing", 1]
        ],
        "org_questions": [
            ["where is the elephant", 1],
            ["what is the elephant doing", 1],
            ["how many people are there", -1],
            ["what color is the background", -1],
            ["what is in front of the elephants", -1],
            ["What is on the elephant", -1],
            ["how many trees are there", -1],
            ["what type of animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what animal is shown", -1],
            ["what animal is in the photo", -1]
        ],
        "context": [
            "an elephant walking along a river bank next to a house.",
            "an elephant standing in the middle of a river."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2346093",
                "VG_object_id": "902608",
                "bbox": [186, 104, 232, 188],
                "image": "data\\images\\2346093.jpg"
            },
            {
                "VG_image_id": "2359055",
                "VG_object_id": "2409287",
                "bbox": [194, 235, 230, 311],
                "image": "data\\images\\2359055.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 1],
            ["what is the man holding", 1],
            ["What color is the boy's shirt", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["what is the man doing", -1],
            ["What color is the boy's shirt", 1],
            ["what color is the background", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["how many people are in the picture", -1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a boy is doing a trick on a skateboard.",
            "a boy riding a skateboard down a street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "1592794",
                "VG_object_id": "3615018",
                "bbox": [388, 573, 939, 677],
                "image": "data\\images\\1592794.jpg"
            },
            {
                "VG_image_id": "2412289",
                "VG_object_id": "198055",
                "bbox": [12, 379, 493, 492],
                "image": "data\\images\\2412289.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what kind of animal is on the land", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is on the ground", -1],
            ["how many people are there", -1],
            ["what kind of animal is on the land", 1],
            ["where is the land", -1],
            ["what is the weather like", -1],
            ["What is the on the land", -1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["how is the weather", -1]
        ],
        "context": [
            "a picnic table by the lake",
            "a large elephant standing next to a large elephant."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2372512",
                "VG_object_id": "1943821",
                "bbox": [0, 29, 115, 232],
                "image": "data\\images\\2372512.jpg"
            },
            {
                "VG_image_id": "2392614",
                "VG_object_id": "1227886",
                "bbox": [367, 107, 481, 263],
                "image": "data\\images\\2392614.jpg"
            }
        ],
        "questions_with_scores": [["how many people are there", 2]],
        "org_questions": [
            ["how many people are there", 2],
            ["what are the people doing", -1],
            ["where is the photo taken", -1],
            ["what is the gender of the person", -1],
            ["what is the land made of", -1],
            ["what is the man holding", -1],
            ["What are people doing", -1],
            ["who is wearing a blue shirt", -1],
            ["what type of pants is the man wearing", -1],
            ["when was the picture taken", -1],
            ["what type of shirt is the man wearing", -1]
        ],
        "context": [
            "a young girl standing next to a fire hydrant.",
            "a man taking a picture of an elephant"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2400127",
                "VG_object_id": "1162695",
                "bbox": [127, 1, 499, 374],
                "image": "data\\images\\2400127.jpg"
            },
            {
                "VG_image_id": "2399380",
                "VG_object_id": "1170127",
                "bbox": [48, 38, 275, 373],
                "image": "data\\images\\2399380.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's hair", 2],
            ["how many people are there", 2],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1],
            ["what color are the man's trousers", 1],
            ["where is the picture taken", 1],
            ["what is in the background", 1],
            ["what is the man looking at", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's hair", 2],
            ["what is the man doing", 1],
            ["how many people are there", 2],
            ["what is the ground covered with", 1],
            ["what is the man wearing", -1],
            ["what color are the man's trousers", 1],
            ["where is the picture taken", 1],
            ["who is in the photo", -1],
            ["what is in the background", 1],
            ["what is the man looking at", 1],
            ["what is on the man's face", 1]
        ],
        "context": [
            "a man sitting at a table with two laptops.",
            "two men standing in a field eating food."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2385232",
                "VG_object_id": "689087",
                "bbox": [65, 12, 457, 442],
                "image": "data\\images\\2385232.jpg"
            },
            {
                "VG_image_id": "2377999",
                "VG_object_id": "2168483",
                "bbox": [57, 27, 331, 365],
                "image": "data\\images\\2377999.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the elephant", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["how many trees are there", 1],
            ["what is the elephant standing on", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["where is the elephant", 1],
            ["what is the elephant doing", -1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["how many trees are there", 1],
            ["what are the elephants doing", -1],
            ["what animal is in the photo", -1],
            ["when was this picture taken", -1],
            ["what is the elephant standing on", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "an elephant standing in the sand in a zoo.",
            "a man standing next to an elephant in a river."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2363998",
                "VG_object_id": "3742037",
                "bbox": [351, 31, 472, 310],
                "image": "data\\images\\2363998.jpg"
            },
            {
                "VG_image_id": "2371430",
                "VG_object_id": "2474097",
                "bbox": [215, 109, 366, 311],
                "image": "data\\images\\2371430.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 2],
            ["how many people are there", 2],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["What color is person's shirt", 1],
            ["what is the person wearing", 1],
            ["what is in front of the person", 1],
            ["What is person doing", 1],
            ["what is the persion on the left holding", 1],
            ["where is the man standing", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what is the person holding", 2],
            ["where is the person", 1],
            ["how many people are there", 2],
            ["What color is person's shirt", 1],
            ["what is the person wearing", 1],
            ["what is in front of the person", 1],
            ["What is person doing", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is the persion on the left holding", 1],
            ["where is the man standing", 1]
        ],
        "context": [
            "members cut the ribbon at the opening ceremony.",
            "a man holding a stop sign on a road."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2374239",
                "VG_object_id": "2028931",
                "bbox": [21, 248, 499, 448],
                "image": "data\\images\\2374239.jpg"
            },
            {
                "VG_image_id": "2360276",
                "VG_object_id": "2155479",
                "bbox": [0, 271, 497, 331],
                "image": "data\\images\\2360276.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what animal is standing on the ground", 1],
            ["where is the picture taken", 1],
            ["what is the shadow of", 1],
            ["What are the animals", 1],
            ["where was the photo taken", 1],
            ["what is in the background", 1],
            ["what animals are there", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what is on the ground", -1],
            ["how many people are there", 1],
            ["what animal is standing on the ground", 1],
            ["what is the ground covered with", -1],
            ["where is the picture taken", 1],
            ["how is the weather", -1],
            ["what is the shadow of", 1],
            ["where are the shadows", -1],
            ["what is the weather like", -1],
            ["What are the animals", 1],
            ["where was the photo taken", 1],
            ["what is in the background", 1],
            ["what animals are there", 1]
        ],
        "context": [
            "a man milking a cow in a ring.",
            "a family of elephants standing next to each other."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2337664",
                "VG_object_id": "956781",
                "bbox": [0, 247, 332, 498],
                "image": "data\\images\\2337664.jpg"
            },
            {
                "VG_image_id": "2385375",
                "VG_object_id": "1297694",
                "bbox": [0, 0, 500, 332],
                "image": "data\\images\\2385375.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table besides the plate", 1],
            ["how many people are there", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what food is on the plate", -1],
            ["what is on the table besides the plate", 1],
            ["how many people are there", 1],
            ["where is the photo taken", -1],
            ["where is the food", -1],
            ["how many wine glasses are there on the table", -1],
            ["what shape is the plate", -1],
            ["what is the plate sitting on", -1],
            ["what is next to the plate", 1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a woman standing in front of a counter with a cake on it.",
            "a hot dog on a piece of bread on a plate."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2372813",
                "VG_object_id": "1866213",
                "bbox": [71, 96, 188, 223],
                "image": "data\\images\\2372813.jpg"
            },
            {
                "VG_image_id": "2319808",
                "VG_object_id": "3279082",
                "bbox": [298, 88, 409, 245],
                "image": "data\\images\\2319808.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["what is the ground covered with", 1],
            ["what is the person playing", 1],
            ["what color is the ground ", 1],
            ["what color are the shirts", 1],
            ["who is wearing a white shirt", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["what is the ground covered with", 1],
            ["where is the person", -1],
            ["what is the person playing", 1],
            ["what color is the ground ", 1],
            ["what color are the shirts", 1],
            ["when was the photo taken", -1],
            ["who is wearing a white shirt", 1],
            ["what is on the man's head", 1],
            ["what kind of shirt is the man wearing", -1]
        ],
        "context": [
            "a man holding a baseball bat on a field.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2374381",
                "VG_object_id": "586916",
                "bbox": [100, 120, 309, 354],
                "image": "data\\images\\2374381.jpg"
            },
            {
                "VG_image_id": "2413971",
                "VG_object_id": "162360",
                "bbox": [152, 84, 221, 304],
                "image": "data\\images\\2413971.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the man's clothes", 2],
            ["what is the man doing", 1],
            ["where is the photo taken", 1],
            ["what gesture is the man", 1],
            ["where is the man", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["how many people are there in the picture", 2],
            ["what color is the man's clothes", 2],
            ["where is the photo taken", 1],
            ["what is the man wearing on his head", -1],
            ["how is the weather", -1],
            ["what gesture is the man", 1],
            ["where is the man", 1],
            ["when was the picture taken", -1],
            ["what is the persion standing on", 1],
            ["who is in the picture", -1],
            ["what is the man holding", -1]
        ],
        "context": [
            "two men sitting on a bench in front of a brick wall.",
            "a man standing on a bike next to a fence."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2327975",
                "VG_object_id": "3034990",
                "bbox": [336, 207, 427, 294],
                "image": "data\\images\\2327975.jpg"
            },
            {
                "VG_image_id": "2396160",
                "VG_object_id": "447488",
                "bbox": [273, 122, 407, 247],
                "image": "data\\images\\2396160.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the container", 1],
            ["what is the container made of", 1],
            ["what type of food is shown", 1]
        ],
        "org_questions": [
            ["what color is the container", 1],
            ["what is the container made of", 1],
            ["where is the container", -1],
            ["What is container on", -1],
            ["what is in the container", -1],
            ["where is the photo taken", -1],
            ["what is on the container", -1],
            ["what shape is the plate", -1],
            ["what type of food is shown", 1]
        ],
        "context": [
            "a basket of food with a sandwich and a drink.",
            "a table with a lot of food on it"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2322143",
                "VG_object_id": "993244",
                "bbox": [138, 8, 183, 81],
                "image": "data\\images\\2322143.jpg"
            },
            {
                "VG_image_id": "2322291",
                "VG_object_id": "992457",
                "bbox": [157, 190, 330, 331],
                "image": "data\\images\\2322291.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 1],
            ["What color is the man's tie", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 1],
            ["What color is the man's tie", 1],
            ["How many people are there", 1],
            ["what is the gender of the person", -1],
            ["where is the person", -1],
            ["who is wearing a tie", -1],
            ["what is on the man's neck", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man wearing a red jacket and gray pants.",
            "a man and a woman standing in front of a building."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2354169",
                "VG_object_id": "2546315",
                "bbox": [111, 76, 186, 132],
                "image": "data\\images\\2354169.jpg"
            },
            {
                "VG_image_id": "2392178",
                "VG_object_id": "1233172",
                "bbox": [171, 165, 262, 209],
                "image": "data\\images\\2392178.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the motocycle", 1],
            ["What color is the ground", 1],
            ["How many people are there", 1],
            ["what is the color of the motorcycle", 1]
        ],
        "org_questions": [
            ["What color is the motocycle", 1],
            ["What color is the ground", 1],
            ["How many people are there", 1],
            ["what is the color of the motorcycle", 1],
            ["how many motorcycles are there", -1],
            ["when was this taken", -1],
            ["what is on the back of the motorcycle", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a motorcycle parked on the grass next to other motorcycles.",
            "a man riding a motorcycle on a beach."
        ]
    },
    {
        "object_category": "banana",
        "images": [
            {
                "VG_image_id": "2341858",
                "VG_object_id": "939280",
                "bbox": [160, 82, 255, 424],
                "image": "data\\images\\2341858.jpg"
            },
            {
                "VG_image_id": "2317664",
                "VG_object_id": "1018660",
                "bbox": [155, 125, 255, 189],
                "image": "data\\images\\2317664.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bananas are there", 1],
            ["what is the bowl made of", 1]
        ],
        "org_questions": [
            ["how many bananas are there", 1],
            ["what is the bowl made of", 1],
            ["what shape is the container holding the banana", -1],
            ["where are the bananas", -1],
            ["what color are the bananas", -1],
            ["what is on the table", -1],
            ["what is the banana sitting on", -1],
            ["what fruit is in the picture", -1],
            ["what type of fruit is shown", -1],
            ["what fruit is this", -1],
            ["what is the yellow fruit", -1]
        ],
        "context": [
            "a bowl of oranges and a banana on a table.",
            "a bowl of apples and bananas on a table."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2330844",
                "VG_object_id": "3243542",
                "bbox": [2, 102, 117, 329],
                "image": "data\\images\\2330844.jpg"
            },
            {
                "VG_image_id": "2322805",
                "VG_object_id": "3448819",
                "bbox": [2, 1, 328, 344],
                "image": "data\\images\\2322805.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in front of the plant", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the flower", -1],
            ["what is in front of the plant", 1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["how many plants are there", -1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "three clocks on a pink wall with three different times.",
            "a chocolate cake with chocolate frosting and flowers."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2364663",
                "VG_object_id": "639841",
                "bbox": [3, 342, 484, 500],
                "image": "data\\images\\2364663.jpg"
            },
            {
                "VG_image_id": "2342787",
                "VG_object_id": "930810",
                "bbox": [70, 170, 441, 215],
                "image": "data\\images\\2342787.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 1],
            ["what is on the floor", 1],
            ["what color is the floor", 1],
            ["how many tables are there in the picture", 1],
            ["where is the photo taken", 1],
            ["how many people are there in the picture", 1],
            ["what type of floor is this", 1],
            ["where is the floor", 1]
        ],
        "org_questions": [
            ["what is the floor made of", 1],
            ["what is on the floor", 1],
            ["what color is the floor", 1],
            ["what time is it", -1],
            ["how many tables are there in the picture", 1],
            ["where is the photo taken", 1],
            ["how many people are there in the picture", 1],
            ["what type of floor is this", 1],
            ["where is the floor", 1]
        ],
        "context": [
            "a train is parked at a train station.",
            "a living room with a couch, a lamp and a window."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2411921",
                "VG_object_id": "206899",
                "bbox": [3, 100, 255, 371],
                "image": "data\\images\\2411921.jpg"
            },
            {
                "VG_image_id": "2360238",
                "VG_object_id": "2139322",
                "bbox": [274, 241, 381, 352],
                "image": "data\\images\\2360238.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the container", 2],
            ["what is the main color of the plate", 2],
            ["what is in the container", 1],
            ["what is the tray made of", 1],
            ["what kind of food is in the tray", 1],
            ["What is container on", 1],
            ["what is next to the plate", 1],
            ["what is in the middle of the bowl", 1]
        ],
        "org_questions": [
            ["what is in the container", 1],
            ["what color is the table", -1],
            ["what shape is the container", 2],
            ["how many spoons are in the picture", -1],
            ["what is the tray made of", 1],
            ["what kind of food is in the tray", 1],
            ["What is container on", 1],
            ["where is the food", -1],
            ["what is on the table", -1],
            ["what is next to the plate", 1],
            ["what is in the middle of the bowl", 1],
            ["what is the main color of the plate", 2]
        ],
        "context": [
            "a banana next to a container of fruit.",
            "a plate with a sandwich and chips on it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2352736",
                "VG_object_id": "1684471",
                "bbox": [344, 44, 474, 400],
                "image": "data\\images\\2352736.jpg"
            },
            {
                "VG_image_id": "2335098",
                "VG_object_id": "3620223",
                "bbox": [259, 158, 364, 317],
                "image": "data\\images\\2335098.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man holding", 2],
            ["What is the weather like", -1],
            ["how many people are there", 1],
            ["what is the man wearing", -1],
            ["what is behind the man", 1],
            ["what is on the man's face", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man looking at", -1]
        ],
        "context": [
            "a man standing next to a motorcycle.",
            "a man and a woman standing in a kitchen."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2344386",
                "VG_object_id": "2004706",
                "bbox": [93, 0, 385, 331],
                "image": "data\\images\\2344386.jpg"
            },
            {
                "VG_image_id": "2364880",
                "VG_object_id": "1667977",
                "bbox": [13, 5, 138, 332],
                "image": "data\\images\\2364880.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is being played", 2],
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["How many people are there", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["How many people are there", 1],
            ["where is the man", -1],
            ["what is the man standing at", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what sport is being played", 2],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man swinging a tennis racket on a court.",
            "a little boy holding a bat on a baseball field."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2380952",
                "VG_object_id": "542898",
                "bbox": [117, 133, 336, 382],
                "image": "data\\images\\2380952.jpg"
            },
            {
                "VG_image_id": "2392941",
                "VG_object_id": "476092",
                "bbox": [28, 63, 273, 256],
                "image": "data\\images\\2392941.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's shirt", 2],
            ["what color is the child's trousers", 2],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", 2],
            ["what is in the background", 1],
            ["what color is the child's trousers", 2],
            ["how many people are there", -1],
            ["what is the man doing", -1],
            ["where is the person", -1],
            ["what is on the child's head", -1],
            ["what is the boy holding", -1],
            ["when was the photo taken", -1],
            ["who is in the air", -1],
            ["what is the boy wearing", -1],
            ["what is in the air", -1]
        ],
        "context": [
            "a boy doing a trick on a skateboard.",
            "a man riding a skateboard on top of a cement wall."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2373743",
                "VG_object_id": "730957",
                "bbox": [6, 408, 44, 463],
                "image": "data\\images\\2373743.jpg"
            },
            {
                "VG_image_id": "2362357",
                "VG_object_id": "2392786",
                "bbox": [306, 305, 481, 496],
                "image": "data\\images\\2362357.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bag", 1],
            ["how many people are there", 1],
            ["what is the bag placed on", 1],
            ["where was the photo taken", 1],
            ["what is in the background", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the bag", -1],
            ["where is the bag", 1],
            ["how many people are there", 1],
            ["what is the bag made of", -1],
            ["what is the bag placed on", 1],
            ["where was the photo taken", 1],
            ["how many bags are in the picture", -1],
            ["what is in the background", 1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a bathroom with a sink, mirror and toilet.",
            "a man holding a sign with a picture of a woman sitting on a step."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2350040",
                "VG_object_id": "1738594",
                "bbox": [366, 93, 471, 236],
                "image": "data\\images\\2350040.jpg"
            },
            {
                "VG_image_id": "2405829",
                "VG_object_id": "1105980",
                "bbox": [6, 136, 48, 279],
                "image": "data\\images\\2405829.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["Where is the woman", 1],
            ["what color is the background", 1],
            ["what is the woman wearing", 1],
            ["what is the woman doing", 1],
            ["what color is the woman's hair", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["Where is the woman", 1],
            ["what color is the background", 1],
            ["what is the woman wearing", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding", -1],
            ["who is in the photo", -1],
            ["what is on the woman's head", -1],
            ["what color is the woman's hair", 1]
        ],
        "context": [
            "a bathroom with three urinals and a woman in a blue dress.",
            "a long table with a blackboard and a chalkboard on it."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2408017",
                "VG_object_id": "269702",
                "bbox": [3, 0, 498, 373],
                "image": "data\\images\\2408017.jpg"
            },
            {
                "VG_image_id": "2357938",
                "VG_object_id": "807517",
                "bbox": [2, 111, 477, 330],
                "image": "data\\images\\2357938.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the sofa", 1],
            ["what is on the sofa", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 1],
            ["what is on the sofa", 1],
            ["how many people are there", 2],
            ["What is in the back of sofa", -1],
            ["when was the photo taken", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a black dog laying on a couch with a floral pattern.",
            "a man and two children sitting on a couch with a dog."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2365537",
                "VG_object_id": "633415",
                "bbox": [339, 107, 474, 214],
                "image": "data\\images\\2365537.jpg"
            },
            {
                "VG_image_id": "2404634",
                "VG_object_id": "340126",
                "bbox": [223, 109, 354, 191],
                "image": "data\\images\\2404634.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the horse", 2],
            ["how many people are there in the picture", 2],
            ["how many horses are there in the picture", 1],
            ["what is the ground covered with", 1],
            ["what is behind the horse", 1]
        ],
        "org_questions": [
            ["what color is the horse", 2],
            ["what color is the background", -1],
            ["how many horses are there in the picture", 1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["what animal is in the picture", -1],
            ["when was the picture taken", -1],
            ["what is behind the horse", 1],
            ["what is the horse doing", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man sitting at a table with a horse behind him.",
            "a horse and a horse grazing on a hill."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2380971",
                "VG_object_id": "708233",
                "bbox": [41, 230, 320, 394],
                "image": "data\\images\\2380971.jpg"
            },
            {
                "VG_image_id": "2378619",
                "VG_object_id": "558641",
                "bbox": [358, 46, 433, 159],
                "image": "data\\images\\2378619.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the background", 1],
            ["what is on the table", 1],
            ["What color is person;s shirt", 1],
            ["What is man doing", 1],
            ["where is the man", 1],
            ["what is in the distance", 1],
            ["who is in the picture", 1],
            ["what is the man wearing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is the gender of the person", -1],
            ["what is the color of the background", 1],
            ["how many people are in the photo", -1],
            ["what is the table made of", -1],
            ["what is on the table", 1],
            ["how many people are there in the picture", -1],
            ["How many people are there", -1],
            ["What color is person;s shirt", 1],
            ["What is man doing", 1],
            ["where is the man", 1],
            ["what is in the distance", 1],
            ["who is in the picture", 1],
            ["what is the man wearing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "context": [
            "a young boy eating a donut outside.",
            "a man laying on a bed with two cats."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2393410",
                "VG_object_id": "3394832",
                "bbox": [134, 167, 364, 325],
                "image": "data\\images\\2393410.jpg"
            },
            {
                "VG_image_id": "2395011",
                "VG_object_id": "458622",
                "bbox": [197, 162, 387, 237],
                "image": "data\\images\\2395011.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["how many plates are there", 1],
            ["what type of food is shown", 1],
            ["what kind of food is on the plate", 1]
        ],
        "org_questions": [
            ["what is in the bowl", -1],
            ["what color is the table", 1],
            ["how many plates are there", 1],
            ["what is the shape of the plate", -1],
            ["where is the food", -1],
            ["what is on the plate", -1],
            ["what type of food is shown", 1],
            ["what is the food sitting on", -1],
            ["what is on top of the plate", -1],
            ["what kind of food is on the plate", 1],
            ["what is the main color of the plate", -1]
        ],
        "context": [
            "a table with a bowl of salad and a bottle of wine.",
            "a plate of food on a table with a pitcher of coffee."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2328436",
                "VG_object_id": "977630",
                "bbox": [125, 282, 277, 351],
                "image": "data\\images\\2328436.jpg"
            },
            {
                "VG_image_id": "2371207",
                "VG_object_id": "2550246",
                "bbox": [217, 269, 409, 331],
                "image": "data\\images\\2371207.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["How many people are there", 1],
            ["what color is the plate that the cake on", 1],
            ["what is next to the cake", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["what color is the table", 2],
            ["what is on the table", -1],
            ["how many candles are there on the cake", -1],
            ["What is cake on", -1],
            ["what color is the plate that the cake on", 1],
            ["where was the photo taken", -1],
            ["what kind of food is this", -1],
            ["what is the cake made of", -1],
            ["where is the cake", -1],
            ["what is next to the cake", 1]
        ],
        "context": [
            "a little girl is trying to eat a cake.",
            "a group of men in uniform cutting a cake."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2370883",
                "VG_object_id": "2623971",
                "bbox": [51, 89, 123, 264],
                "image": "data\\images\\2370883.jpg"
            },
            {
                "VG_image_id": "2398839",
                "VG_object_id": "421607",
                "bbox": [224, 108, 311, 358],
                "image": "data\\images\\2398839.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the man's shirt", 2],
            ["what is the man wearing", 2],
            ["where is the photo taken", 1],
            ["who is wearing a black shirt", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 1],
            ["what are the people doing", -1],
            ["how many people are there", -1],
            ["what is the color of the man's shirt", 2],
            ["what is the man wearing", 2],
            ["what is the man holding", -1],
            ["who is wearing a black shirt", 1],
            ["what is the man standing on", -1],
            ["where is the man standing", -1]
        ],
        "context": [
            "a woman with a suitcase standing next to a subway train.",
            "a group of men standing in a parking lot with an umbrella."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2408261",
                "VG_object_id": "2538136",
                "bbox": [328, 163, 473, 330],
                "image": "data\\images\\2408261.jpg"
            },
            {
                "VG_image_id": "2372312",
                "VG_object_id": "2649435",
                "bbox": [181, 2, 335, 280],
                "image": "data\\images\\2372312.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["What are men doing", 2],
            ["What is man looking", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", 2],
            ["What are men doing", 2],
            ["What is man looking", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a group of people standing in front of a jet.",
            "a group of people riding bicycles down a street."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2381620",
                "VG_object_id": "703165",
                "bbox": [350, 121, 485, 234],
                "image": "data\\images\\2381620.jpg"
            },
            {
                "VG_image_id": "1592214",
                "VG_object_id": "1040207",
                "bbox": [304, 0, 504, 162],
                "image": "data\\images\\1592214.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the television", 1],
            ["what color is the screen of the television", 1],
            ["what is hanging on the wall", 1]
        ],
        "org_questions": [
            ["Where is the television", 1],
            ["How many people are there", -1],
            ["what is on the television", -1],
            ["what is in front of the tv", -1],
            ["what color is the screen of the television", 1],
            ["how many television are there", -1],
            ["when was the picture taken", -1],
            ["what is the tv on", -1],
            ["where was the photo taken", -1],
            ["what is hanging on the wall", 1]
        ],
        "context": [
            "a man standing in a living room playing a video game.",
            "two men sitting in a room with a fireplace and television."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2355334",
                "VG_object_id": "830571",
                "bbox": [0, 323, 499, 499],
                "image": "data\\images\\2355334.jpg"
            },
            {
                "VG_image_id": "2362635",
                "VG_object_id": "1693606",
                "bbox": [35, 210, 240, 300],
                "image": "data\\images\\2362635.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["where is the photo taken", 1],
            ["where is the table", 1]
        ],
        "org_questions": [
            ["what are on the table", -1],
            ["how many people are there in the picture", 2],
            ["where is the photo taken", 1],
            ["what color is the table", -1],
            ["what is the table made of", -1],
            ["where is the table", 1],
            ["when was the photo taken", -1],
            ["what is next to the table", -1]
        ],
        "context": [
            "a man sitting at a table with a lot of cans and a table full of items.",
            "a group of people dressed as santas are gathered around a food truck."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2369995",
                "VG_object_id": "607388",
                "bbox": [178, 72, 259, 160],
                "image": "data\\images\\2369995.jpg"
            },
            {
                "VG_image_id": "2392444",
                "VG_object_id": "668051",
                "bbox": [254, 209, 333, 318],
                "image": "data\\images\\2392444.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sky", 1],
            ["what time is it", 1],
            ["what time does the clock say", 1],
            ["what time is on the clock", 1]
        ],
        "org_questions": [
            ["what shape is the clock", -1],
            ["what color is the sky", 1],
            ["how many people are there in the picture", -1],
            ["what is the building made of", -1],
            ["where is the clock", -1],
            ["when is this picture taken", -1],
            ["what kind of symbol is showed on the clock", -1],
            ["what time is it", 1],
            ["what time does the clock say", 1],
            ["what is on top of the clock", -1],
            ["what time is on the clock", 1]
        ],
        "context": [
            "a clock on a pole on a sidewalk.",
            "a clock on a pole in a city."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2322795",
                "VG_object_id": "2836749",
                "bbox": [172, 137, 331, 283],
                "image": "data\\images\\2322795.jpg"
            },
            {
                "VG_image_id": "2338600",
                "VG_object_id": "2720335",
                "bbox": [178, 267, 279, 400],
                "image": "data\\images\\2338600.jpg"
            }
        ],
        "questions_with_scores": [
            ["what time is it", 2],
            ["where is the clock", 1],
            ["what is on the side of the clock", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the wall behind the clock", -1],
            ["where is the clock", 1],
            ["what time is it", 2],
            ["what is the shape of the clock", -1],
            ["what is the building made of", -1],
            ["what is on the clock", -1],
            ["when is this picture taken", -1],
            ["how many clocks are there", -1],
            ["what is on the side of the clock", 1],
            ["when was the photo taken", 1],
            ["what is the clock made of", -1],
            ["what shape is the clock face", -1]
        ],
        "context": [
            "a clock with angels on the side of it.",
            "a clock tower with a sky background"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2333660",
                "VG_object_id": "3672533",
                "bbox": [387, 189, 444, 275],
                "image": "data\\images\\2333660.jpg"
            },
            {
                "VG_image_id": "2402548",
                "VG_object_id": "1134570",
                "bbox": [194, 88, 256, 132],
                "image": "data\\images\\2402548.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is on the table", 2],
            ["where was the photo taken", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["What is on the table", 2],
            ["What is next to the table", -1],
            ["how many forks are there on the table", -1],
            ["what is the table made of", -1],
            ["where was the photo taken", 1],
            ["what room is this", 1],
            ["what is in the room", -1],
            ["what color is the floor", -1]
        ],
        "context": [
            "a large bed sitting in a room next to a window.",
            "a kitchen with a stove, a table and a chair."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2392443",
                "VG_object_id": "1230047",
                "bbox": [196, 290, 340, 370],
                "image": "data\\images\\2392443.jpg"
            },
            {
                "VG_image_id": "498012",
                "VG_object_id": "1039252",
                "bbox": [1, 1, 346, 375],
                "image": "data\\images\\498012.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the table", 2],
            ["what color is the table", 1],
            ["what color is the thing in front of the bag", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what color is the thing in front of the bag", 1],
            ["what color is the bag", -1],
            ["how many people are there", -1],
            ["what is the persion doing", -1],
            ["where is the bag", -1],
            ["how is the weather", -1],
            ["what is the table made of", -1],
            ["what is in the picture", -1],
            ["what is written on the table", -1],
            ["what is in the plastic bag", -1],
            ["how many people are there on the table", 2]
        ],
        "context": [
            "a table with a hot dog and a bottle of ketchup.",
            "a picnic table with a person sitting at it"
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2416769",
                "VG_object_id": "2824308",
                "bbox": [6, 54, 497, 328],
                "image": "data\\images\\2416769.jpg"
            },
            {
                "VG_image_id": "2378847",
                "VG_object_id": "1363830",
                "bbox": [56, 338, 299, 416],
                "image": "data\\images\\2378847.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many benches are there", 2],
            ["how many people are there sitting on the bench", 2],
            ["what is the weather like", 1],
            ["what is the ground covered with", 1],
            ["what is sitting on the bench", 1]
        ],
        "org_questions": [
            ["how many benches are there", 2],
            ["what is the weather like", 1],
            ["what is the bench made of", -1],
            ["how many people are there sitting on the bench", 2],
            ["what is the ground covered with", 1],
            ["What color is the ground", -1],
            ["where was the picture taken", -1],
            ["what is sitting on the bench", 1],
            ["how is the bench", -1],
            ["what is behind the bench", -1]
        ],
        "context": [
            "a white bird standing next to a bench.",
            "two people sitting on a bench under an umbrella."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2379302",
                "VG_object_id": "553916",
                "bbox": [136, 328, 229, 367],
                "image": "data\\images\\2379302.jpg"
            },
            {
                "VG_image_id": "2393873",
                "VG_object_id": "1215994",
                "bbox": [261, 159, 376, 209],
                "image": "data\\images\\2393873.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the plate", 1],
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["How many people are there", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what is on the plate", 1],
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["How many people are there", 1],
            ["what shape is the plate", -1],
            ["where is the plate", -1],
            ["what is the plate on", -1],
            ["what is in the distance", 1],
            ["what is the table made of", -1],
            ["what kind of food is on the table", -1],
            ["what is the food on the table", -1],
            ["what is the food on", -1]
        ],
        "context": [
            "two women sitting at a table eating food.",
            "a table full of desserts and pastries."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2351893",
                "VG_object_id": "857473",
                "bbox": [140, 101, 256, 292],
                "image": "data\\images\\2351893.jpg"
            },
            {
                "VG_image_id": "2359892",
                "VG_object_id": "3763535",
                "bbox": [0, 13, 118, 325],
                "image": "data\\images\\2359892.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", 2],
            ["what is on the person's head", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["what is the man looking at", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", 2],
            ["what is on the person's head", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["who is in the photo", -1],
            ["what is the man on the right wearing", -1],
            ["what is the man looking at", 1]
        ],
        "context": [
            "a group of chefs preparing food in a kitchen.",
            "a man playing a video game on a computer."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2330815",
                "VG_object_id": "972519",
                "bbox": [139, 71, 226, 169],
                "image": "data\\images\\2330815.jpg"
            },
            {
                "VG_image_id": "2405865",
                "VG_object_id": "1105647",
                "bbox": [257, 130, 321, 219],
                "image": "data\\images\\2405865.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person staying on", 2],
            ["what is the person doing", 1],
            ["what gender is the person", 1],
            ["how many people are there", 1],
            ["who is wearing a white shirt", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what gender is the person", 1],
            ["how many people are there", 1],
            ["what is the persion wearing on the head", -1],
            ["what color is the shirt", -1],
            ["who is wearing a white shirt", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the picture", 1],
            ["when was this picture taken", -1],
            ["what is the person staying on", 2]
        ],
        "context": [
            "a boy and a girl standing next to an elephant.",
            "a man playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2374540",
                "VG_object_id": "726147",
                "bbox": [11, 264, 485, 371],
                "image": "data\\images\\2374540.jpg"
            },
            {
                "VG_image_id": "2355082",
                "VG_object_id": "1661965",
                "bbox": [0, 104, 499, 374],
                "image": "data\\images\\2355082.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many glasses are on the table", 2],
            ["how many cell phones are on the table", 2],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["how many glasses are on the table", 2],
            ["how many cell phones are on the table", 2],
            ["what color is the table", -1],
            ["What is on the table", -1],
            ["what is the table made of", -1],
            ["How many people are there", 1],
            ["where is the photo taken", -1],
            ["what is sitting on the table", -1]
        ],
        "context": [
            "a group of people standing around a table.",
            "a laptop computer sitting on top of a wooden table."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2343124",
                "VG_object_id": "2498398",
                "bbox": [3, 1, 203, 179],
                "image": "data\\images\\2343124.jpg"
            },
            {
                "VG_image_id": "2341040",
                "VG_object_id": "3657311",
                "bbox": [0, 1, 453, 184],
                "image": "data\\images\\2341040.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["What is person doing", 1],
            ["how many hands are there", 1],
            ["what is the man holding", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is the table made of", -1],
            ["what is on the table", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the weather like", -1],
            ["when was the picture taken", -1],
            ["What is person doing", 1],
            ["What is color of table", -1],
            ["how many hands are there", 1],
            ["where is the person", -1],
            ["what is the man holding", 1],
            ["what are the people doing", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "two hot dogs with condiments on them sitting on a table.",
            "a person is eating a piece of cake on a plate."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2353357",
                "VG_object_id": "2625037",
                "bbox": [13, 22, 493, 497],
                "image": "data\\images\\2353357.jpg"
            },
            {
                "VG_image_id": "2346263",
                "VG_object_id": "901111",
                "bbox": [43, 222, 339, 350],
                "image": "data\\images\\2346263.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the plate", 1],
            ["what is the shape of the plate", 1],
            ["what kind of food is this", 1],
            ["what kind of food is on the plate", 1]
        ],
        "org_questions": [
            ["what is on the plate", 1],
            ["what is the shape of the plate", 1],
            ["how many plate are there", -1],
            ["What color is the table", -1],
            ["what is the table made of", -1],
            ["where is the plate", -1],
            ["what is the plate on", -1],
            ["what kind of food is this", 1],
            ["what is the food sitting on", -1],
            ["what kind of food is on the plate", 1],
            ["what is the plate made of", -1]
        ],
        "context": [
            "three donuts sitting on a plate with a newspaper on it.",
            "a piece of cake on a plate with a fork."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2355788",
                "VG_object_id": "2002522",
                "bbox": [8, 80, 498, 272],
                "image": "data\\images\\2355788.jpg"
            },
            {
                "VG_image_id": "2351998",
                "VG_object_id": "3585850",
                "bbox": [62, 8, 499, 373],
                "image": "data\\images\\2351998.jpg"
            }
        ],
        "questions_with_scores": [["what shape is the table", 1]],
        "org_questions": [
            ["what color is the table", -1],
            ["what is the table made of", -1],
            ["how many people are in the picture", -1],
            ["what is on the table", -1],
            ["how many kinds of vegetables are there on the table", -1],
            ["what is the color of the chair", -1],
            ["where was the photo taken", -1],
            ["what shape is the table", 1],
            ["what is the pizza sitting on", -1],
            ["where is the pizza sitting", -1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["where is the table", -1],
            ["how many plates are there on the table", -1],
            ["what is in the photo", -1]
        ],
        "context": ["a glass of red wine", "a pizza on a pan on a table"]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2395510",
                "VG_object_id": "453618",
                "bbox": [110, 82, 171, 154],
                "image": "data\\images\\2395510.jpg"
            },
            {
                "VG_image_id": "2395393",
                "VG_object_id": "3823218",
                "bbox": [428, 172, 486, 302],
                "image": "data\\images\\2395393.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the light", 1],
            ["what color are the traffic lights", 1]
        ],
        "org_questions": [
            ["what color is the light", 1],
            ["what is the weather like", -1],
            ["how many people are there", -1],
            ["what room is the light in", -1],
            ["what is in the distance", -1],
            ["where is the light", -1],
            ["what is the traffic light on", -1],
            ["when was the picture taken", -1],
            ["how many traffic lights are there", -1],
            ["what color is the sky", -1],
            ["what color are the traffic lights", 1]
        ],
        "context": [
            "a traffic light hanging from a pole with a brick building in the background.",
            "a street sign is hanging from a pole."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2417133",
                "VG_object_id": "3198694",
                "bbox": [7, 88, 227, 373],
                "image": "data\\images\\2417133.jpg"
            },
            {
                "VG_image_id": "2391930",
                "VG_object_id": "484894",
                "bbox": [317, 189, 453, 301],
                "image": "data\\images\\2391930.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what is the persion doing", 1],
            ["what is on the couch", 1]
        ],
        "org_questions": [
            ["what color is the couch", -1],
            ["how many people are there", 1],
            ["what is the floor made of", -1],
            ["what is the persion doing", 1],
            ["how many chairs are there", -1],
            ["who is in the room", -1],
            ["what is in the room", -1],
            ["what is on the couch", 1]
        ],
        "context": [
            "two men standing in a living room.",
            "a man sitting on a couch in a living room."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2394365",
                "VG_object_id": "666949",
                "bbox": [41, 0, 234, 216],
                "image": "data\\images\\2394365.jpg"
            },
            {
                "VG_image_id": "2385216",
                "VG_object_id": "1299480",
                "bbox": [0, 86, 295, 497],
                "image": "data\\images\\2385216.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the woman's head", 2],
            ["what is the girl wearing on head", 2],
            ["what color is the woman's shirt", 1],
            ["where is the woman", 1],
            ["what is the girl doing", 1],
            ["What is the woman wearing", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what is on the woman's head", 2],
            ["How many people are there", -1],
            ["where is the woman", 1],
            ["what is the girl doing", 1],
            ["What is the woman wearing", 1],
            ["what is the girl wearing on head", 2],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is in the background", 1],
            ["what is in the woman's hand", -1]
        ],
        "context": [
            "a woman wearing sunglasses and a gray tank top.",
            "a woman with red hair talking on a cell phone."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2327616",
                "VG_object_id": "3374316",
                "bbox": [88, 210, 277, 267],
                "image": "data\\images\\2327616.jpg"
            },
            {
                "VG_image_id": "2317133",
                "VG_object_id": "2800472",
                "bbox": [1, 80, 346, 494],
                "image": "data\\images\\2317133.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 2],
            ["what is the ground covered with", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the train", 2],
            ["where is the train", -1],
            ["what is the ground covered with", 1],
            ["How many people are there", -1],
            ["what is in the background", 1],
            ["what is the train on", -1],
            ["how many deckers does the train have", -1],
            ["how many trains are in the picture", -1],
            ["when was this picture taken", -1],
            ["how is the weather", -1],
            ["what is the train doing", -1],
            ["what is on the tracks", -1]
        ],
        "context": [
            "a train going over a bridge in the woods.",
            "a red train is parked at a station."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2333225",
                "VG_object_id": "2711863",
                "bbox": [135, 48, 470, 300],
                "image": "data\\images\\2333225.jpg"
            },
            {
                "VG_image_id": "2381533",
                "VG_object_id": "703983",
                "bbox": [42, 95, 323, 209],
                "image": "data\\images\\2381533.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bus", 2],
            ["how many buses are there", 2],
            ["what is the bus doing", 1],
            ["How many decks do the bus have", 1],
            ["how many buses are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the bus", 2],
            ["how many buses are there", 2],
            ["When is the picture taken", -1],
            ["where is the bus", -1],
            ["what is the bus doing", 1],
            ["How many decks do the bus have", 1],
            ["how many buses are there in the picture", 1],
            ["what is in the background", -1],
            ["what kind of bus is this", -1],
            ["what is on the street", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a couple of buses that are parked in a lot",
            "a red fire hydrant on a city street."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2389343",
                "VG_object_id": "505292",
                "bbox": [162, 234, 202, 341],
                "image": "data\\images\\2389343.jpg"
            },
            {
                "VG_image_id": "2315548",
                "VG_object_id": "2871133",
                "bbox": [49, 2, 316, 202],
                "image": "data\\images\\2315548.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's trouser", 1],
            ["what is the ground the person standing on made of", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the ground covered with", 1],
            ["where is the man", 1],
            ["What color is the man's pair of trousers", 1]
        ],
        "org_questions": [
            ["what color is the person's trouser", 1],
            ["what is the ground the person standing on made of", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the ground covered with", 1],
            ["where is the man", 1],
            ["What color is the man's pair of trousers", 1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a group of children flying kites in a park.",
            "a person wearing green and white shoes and a skateboard"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2382883",
                "VG_object_id": "1325350",
                "bbox": [334, 36, 416, 130],
                "image": "data\\images\\2382883.jpg"
            },
            {
                "VG_image_id": "2392695",
                "VG_object_id": "1226964",
                "bbox": [25, 83, 97, 177],
                "image": "data\\images\\2392695.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", 1],
            ["where is the photo taken", 1],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", 1],
            ["what is the person doing", -1],
            ["where is the photo taken", 1],
            ["how many people are there", -1],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["when was the picture taken", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a woman holding a tennis racket on a tennis court.",
            "a group of people standing around a motorcycle."
        ]
    },
    {
        "object_category": "toilet",
        "images": [
            {
                "VG_image_id": "2413157",
                "VG_object_id": "3145584",
                "bbox": [115, 285, 189, 446],
                "image": "data\\images\\2413157.jpg"
            },
            {
                "VG_image_id": "2368190",
                "VG_object_id": "2504200",
                "bbox": [97, 56, 373, 458],
                "image": "data\\images\\2368190.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the toilet", 2],
            ["what color is the wall", 1],
            ["what color is the floor", 1]
        ],
        "org_questions": [
            ["what color is the wall", 1],
            ["what color is the floor", 1],
            ["what is on the toilet", 2],
            ["what shape is the toilet", -1],
            ["what is the floor made of", -1],
            ["what kind of toilet is it", -1],
            ["what is fixed on the wall", -1],
            ["what is the ground covered with", -1],
            ["how many toilets are there", -1],
            ["where was this picture taken", -1],
            ["what room is this", -1],
            ["where is the toilet", -1]
        ],
        "context": [
            "a bathroom with a toilet and a sink",
            "a toilet with a pink basket on top of it"
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2398627",
                "VG_object_id": "3819982",
                "bbox": [23, 3, 496, 103],
                "image": "data\\images\\2398627.jpg"
            },
            {
                "VG_image_id": "2415206",
                "VG_object_id": "143892",
                "bbox": [262, 0, 498, 212],
                "image": "data\\images\\2415206.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the wall", 1],
            ["how many layers does the shelf have", 1]
        ],
        "org_questions": [
            ["what color is the shelf", -1],
            ["what color is the wall", 1],
            ["what is sitting around the shelf", -1],
            ["how many layers does the shelf have", 1],
            ["what room is it", -1],
            ["where is the shelf placing", -1],
            ["what is the shelf made of", -1],
            ["What are on the shelf", -1],
            ["where was this photo taken", -1],
            ["what is in the background", -1],
            ["what is hanging on the wall", -1],
            ["what is on top of the shelf", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man sitting in a chair in front of a computer.",
            "a black dog laying on a bed with a cat on it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2329768",
                "VG_object_id": "3382559",
                "bbox": [180, 5, 279, 318],
                "image": "data\\images\\2329768.jpg"
            },
            {
                "VG_image_id": "2350898",
                "VG_object_id": "1659941",
                "bbox": [59, 98, 124, 250],
                "image": "data\\images\\2350898.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man head", 2],
            ["What is man holding", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["where is the man standing", 1]
        ],
        "org_questions": [
            ["What is man holding", 1],
            ["how many people are there", 1],
            ["where is the photo taken", -1],
            ["what is the man doing", 1],
            ["what is on the man head", 2],
            ["what gesture is the man", 1],
            ["when was the picture taken", -1],
            ["what is the man standing on", -1],
            ["what is the man wearing", 1],
            ["where is the man standing", 1]
        ],
        "context": [
            "a man and woman standing on a sidewalk.",
            "two boys are walking on a dock near a house."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2397643",
                "VG_object_id": "1188075",
                "bbox": [2, 381, 361, 498],
                "image": "data\\images\\2397643.jpg"
            },
            {
                "VG_image_id": "2373506",
                "VG_object_id": "2074716",
                "bbox": [213, 286, 333, 363],
                "image": "data\\images\\2373506.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what is the color of the ground", -1],
            ["what is on the ground", 1],
            ["when is this photo taken", -1],
            ["what sport are the people playing on the land", -1],
            ["How many people are there", 1],
            ["What animal are there", -1],
            ["What are people doing", -1],
            ["What is land made of", -1],
            ["where are the people", -1],
            ["what is in the background", -1],
            ["where was the picture taken", -1],
            ["what is the ground covered with", -1]
        ],
        "context": [
            "a man riding a skateboard through orange cones.",
            "a man flying through the air while riding a skateboard."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2368493",
                "VG_object_id": "2098317",
                "bbox": [187, 74, 332, 132],
                "image": "data\\images\\2368493.jpg"
            },
            {
                "VG_image_id": "2316823",
                "VG_object_id": "2760528",
                "bbox": [245, 95, 310, 171],
                "image": "data\\images\\2316823.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["WHat is man holding", 1],
            ["What is man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["who is wearing the shirt", -1],
            ["WHat is man holding", 1],
            ["What is man doing", 1],
            ["when was this picture taken", -1],
            ["what is on the man's head", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a skier in the air doing a trick.",
            "a man riding a horse on the beach at sunset."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2330371",
                "VG_object_id": "973088",
                "bbox": [334, 153, 449, 263],
                "image": "data\\images\\2330371.jpg"
            },
            {
                "VG_image_id": "2403103",
                "VG_object_id": "2680962",
                "bbox": [269, 132, 383, 202],
                "image": "data\\images\\2403103.jpg"
            }
        ],
        "questions_with_scores": [
            ["when is this picture taken", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what is behind the car", -1],
            ["when is this picture taken", 1],
            ["what shape is the car", -1],
            ["How many cars are there", -1],
            ["What is the weather like", -1],
            ["what is the ground covered with", -1],
            ["where was the photo taken", -1],
            ["what is on the road", -1],
            ["when was the picture taken", 1]
        ],
        "context": [
            "two women walking down the street while looking at their cell phones.",
            "a car driving down a street at night."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2387540",
                "VG_object_id": "512652",
                "bbox": [258, 56, 333, 155],
                "image": "data\\images\\2387540.jpg"
            },
            {
                "VG_image_id": "2404799",
                "VG_object_id": "337802",
                "bbox": [113, 93, 197, 193],
                "image": "data\\images\\2404799.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what sport is the man doing", 2],
            ["what is the man holding", 1],
            ["where is the photo taken", 1],
            ["what is the man doing", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what sport is the man doing", 2],
            ["what is the man holding", 1],
            ["how many elephants are in the background", -1],
            ["where is the photo taken", 1],
            ["what is the person wearing on his head", -1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["when was the picture taken", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a baseball player swinging a bat on a field.",
            "a man riding a horse in a fenced in area."
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2322816",
                "VG_object_id": "3293359",
                "bbox": [0, 2, 500, 329],
                "image": "data\\images\\2322816.jpg"
            },
            {
                "VG_image_id": "2357321",
                "VG_object_id": "2400573",
                "bbox": [1, 45, 498, 332],
                "image": "data\\images\\2357321.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many televisions are in the room", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["how many televisions are in the room", 1],
            ["what is on the wall", 1],
            ["what is the wall made of", -1],
            ["what is the color of the wall", -1],
            ["what color are the pillows", -1],
            ["where was this taken", -1],
            ["what room is this", -1],
            ["who is in the room", -1],
            ["where is the couch", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a living room with a couch, coffee table, television and a couch.",
            "a living room with a couch, coffee table, and a lamp."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2384277",
                "VG_object_id": "1311466",
                "bbox": [110, 11, 209, 324],
                "image": "data\\images\\2384277.jpg"
            },
            {
                "VG_image_id": "2374626",
                "VG_object_id": "1816953",
                "bbox": [22, 148, 188, 478],
                "image": "data\\images\\2374626.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 1],
            ["what is the woman wearing", 1],
            ["where is the girl", 1],
            ["when was the photo taken", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", -1],
            ["how many people are there in the photo", 1],
            ["what is the woman wearing", 1],
            ["where is the girl", 1],
            ["What is woman doing", -1],
            ["What is the background of image", -1],
            ["what is the woman holding", -1],
            ["when was the photo taken", 1],
            ["what is the persion doing", -1],
            ["what is the main color of the shirt", -1],
            ["what is the woman doing", -1],
            ["where is the woman ", -1],
            ["how many people are there", 1],
            ["what is the woman wearing on her face", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a vase with a yellow flower and purple flowers.",
            "a man looking at his cell phone"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2375276",
                "VG_object_id": "3842314",
                "bbox": [316, 110, 387, 251],
                "image": "data\\images\\2375276.jpg"
            },
            {
                "VG_image_id": "2346208",
                "VG_object_id": "2038238",
                "bbox": [105, 148, 210, 240],
                "image": "data\\images\\2346208.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man's posture", 2],
            ["what is the ground covered with", 1],
            ["how many umbrellas are there in the picture", 1],
            ["Where is the man", 1],
            ["what is in the distance", 1],
            ["what is the man doing", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["what is the man's posture", 2],
            ["how many umbrellas are there in the picture", 1],
            ["Where is the man", 1],
            ["what is on the man's head", -1],
            ["what is in the distance", 1],
            ["what is the man doing", 1],
            ["what is the persion standing on", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what color is the man's shirt", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a person walking across a street holding an umbrella.",
            "a group of people sitting on a grass covered field."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2318967",
                "VG_object_id": "2888018",
                "bbox": [4, 2, 496, 372],
                "image": "data\\images\\2318967.jpg"
            },
            {
                "VG_image_id": "2360855",
                "VG_object_id": "2132691",
                "bbox": [0, 135, 500, 325],
                "image": "data\\images\\2360855.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is on the bed", 2]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what is on the bed", 2],
            ["what is the pattern of the sheet", -1],
            ["how many pillows are there on the bed", -1],
            ["What is the pattern of bed", -1],
            ["where was the picture taken", -1],
            ["what is the bed made of", -1],
            ["where is the blanket", -1]
        ],
        "context": [
            "a man laying in bed with a tray of food.",
            "two women jumping on a bed"
        ]
    },
    {
        "object_category": "book",
        "images": [
            {
                "VG_image_id": "2347016",
                "VG_object_id": "1666721",
                "bbox": [18, 266, 307, 478],
                "image": "data\\images\\2347016.jpg"
            },
            {
                "VG_image_id": "2332843",
                "VG_object_id": "3494563",
                "bbox": [5, 1, 275, 283],
                "image": "data\\images\\2332843.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cat", 1],
            ["what color is the table", 1],
            ["how many books are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the cat", 1],
            ["what is the color of the book", -1],
            ["what color is the table", 1],
            ["how many people are there", -1],
            ["where is the book", -1],
            ["how many books are in the picture", 1],
            ["what is the table made of", -1],
            ["what is behind the cat", -1]
        ],
        "context": [
            "a cat is sitting on a desk with a pencil.",
            "a cat is standing next to a book."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2350461",
                "VG_object_id": "2350423",
                "bbox": [3, 198, 498, 373],
                "image": "data\\images\\2350461.jpg"
            },
            {
                "VG_image_id": "2373322",
                "VG_object_id": "1933441",
                "bbox": [31, 242, 488, 272],
                "image": "data\\images\\2373322.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is on the street", 1]
        ],
        "org_questions": [
            ["what is on the street", 1],
            ["what is on the side of the street", -1],
            ["how many people are there in the picture", 2],
            ["what time is it", -1],
            ["how is the weather", -1],
            ["where was the picture taken", -1],
            ["what is the road made of", -1],
            ["when was the picture taken", -1],
            ["where are the white lines", -1]
        ],
        "context": [
            "a small car parked in a parking space.",
            "a man is standing on a sidewalk next to a pile of dirt."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2370542",
                "VG_object_id": "2103988",
                "bbox": [232, 314, 373, 399],
                "image": "data\\images\\2370542.jpg"
            },
            {
                "VG_image_id": "2363990",
                "VG_object_id": "760390",
                "bbox": [88, 133, 177, 265],
                "image": "data\\images\\2363990.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the chair", 1],
            ["what is behind the chair", 1],
            ["what is in front of the chair", 1],
            ["where is the picture taken", 1],
            ["what is next to the chair", 1]
        ],
        "org_questions": [
            ["where is the chair", 1],
            ["what is behind the chair", 1],
            ["what is in front of the chair", 1],
            ["how many chairs are in the picture", -1],
            ["what color is the wall", -1],
            ["what is the persion on the chair wearing", -1],
            ["what is the chair made of", -1],
            ["how many people are in  the picture", -1],
            ["what is on the floor", -1],
            ["where is the picture taken", 1],
            ["what is in the room", -1],
            ["what is next to the chair", 1]
        ],
        "context": [
            "a stainless steel refrigerator in a kitchen with wooden cabinets.",
            "a bathroom with a sink, a rug and a rug."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2409090",
                "VG_object_id": "249208",
                "bbox": [212, 232, 293, 316],
                "image": "data\\images\\2409090.jpg"
            },
            {
                "VG_image_id": "2317860",
                "VG_object_id": "2740821",
                "bbox": [5, 177, 253, 374],
                "image": "data\\images\\2317860.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the man doing", 2],
            ["Where is the man", 2],
            ["what shape is the collar of the shirt", 2],
            ["what shape does the shirt's collar have", 1],
            ["how many people are there", 1],
            ["what is the man holding", 1],
            ["what is around the man's neck", 1],
            ["what type of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what shape does the shirt's collar have", 1],
            ["how many people are there", 1],
            ["What is the man doing", 2],
            ["Where is the man", 2],
            ["What is the color of the man's shirt", -1],
            ["what is on the man's face", -1],
            ["what is the man holding", 1],
            ["what is around the man's neck", 1],
            ["what type of shirt is the man wearing", 1],
            ["what is the man wearing on his head", -1],
            ["what shape is the collar of the shirt", 2]
        ],
        "context": [
            "a man in a black suit and tie is standing on the sidewalk.",
            "a man talking on a cell phone while wearing a hat."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2412519",
                "VG_object_id": "192638",
                "bbox": [40, 17, 185, 202],
                "image": "data\\images\\2412519.jpg"
            },
            {
                "VG_image_id": "2362804",
                "VG_object_id": "1692432",
                "bbox": [363, 132, 488, 270],
                "image": "data\\images\\2362804.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the dog's mouth", 2],
            ["where is the dog", 1],
            ["what color is the ground", 1],
            ["what is the dog doing ", 1],
            ["what is the ground the dog standing on made of", 1],
            ["what gesture is the dog", 1],
            ["what is the dog on", 1]
        ],
        "org_questions": [
            ["what is in the dog's mouth", 2],
            ["where is the dog", 1],
            ["what color is the ground", 1],
            ["what is the dog doing ", 1],
            ["what is the ground the dog standing on made of", 1],
            ["what is the dog wearing", -1],
            ["what gesture is the dog", 1],
            ["what is the dog on", 1],
            ["what kind of animal is in the picture", -1],
            ["how many dogs are there", -1],
            ["when was the photo taken", -1],
            ["what is on the dog's head", -1]
        ],
        "context": [
            "a man playing with a dog in a field.",
            "a man walking along a dock next to a marina."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2414803",
                "VG_object_id": "151625",
                "bbox": [55, 321, 333, 411],
                "image": "data\\images\\2414803.jpg"
            },
            {
                "VG_image_id": "2328337",
                "VG_object_id": "3084361",
                "bbox": [1, 294, 373, 499],
                "image": "data\\images\\2328337.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 2],
            ["what are people doing", 1],
            ["what is on the ground", 1],
            ["how many people are there", 1],
            ["What is the background of image", 1],
            ["what is the person doing on the ground", 1],
            ["where was this picture taken", 1],
            ["what is the weather like", 1],
            ["what is the persion standing on", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["what color is the land", 2],
            ["what are people doing", 1],
            ["what is on the ground", 1],
            ["how many people are there", 1],
            ["What is the background of image", 1],
            ["what is land made of", -1],
            ["what is the person doing on the ground", 1],
            ["where was this picture taken", 1],
            ["what is the weather like", 1],
            ["what is the persion standing on", 1],
            ["where are the people", 1]
        ],
        "context": [
            "a man riding a skateboard down the side of a rail.",
            "a person walking with an umbrella in the rain."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316781",
                "VG_object_id": "2772478",
                "bbox": [0, 80, 294, 499],
                "image": "data\\images\\2316781.jpg"
            },
            {
                "VG_image_id": "2326943",
                "VG_object_id": "2786710",
                "bbox": [19, 5, 110, 217],
                "image": "data\\images\\2326943.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is the man standing at", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 2],
            ["where is the man", 1],
            ["How many people are there", -1],
            ["What is the man wearing on his head", -1],
            ["what is the man standing at", 1],
            ["who is in the photo", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a man wearing a white shirt and sunglasses.",
            "a statue of a bear sitting on a box"
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2359525",
                "VG_object_id": "1763886",
                "bbox": [0, 10, 499, 375],
                "image": "data\\images\\2359525.jpg"
            },
            {
                "VG_image_id": "2399259",
                "VG_object_id": "417741",
                "bbox": [5, 307, 499, 370],
                "image": "data\\images\\2399259.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["what animal is on the land", 1],
            ["what is the ground covered with", 1],
            ["where was this photo taken", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what is on the land", -1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what animal is on the land", 1],
            ["what is the ground covered with", 1],
            ["when was the picture taken", -1],
            ["where was this photo taken", 1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "three sheep grazing in a field of tall grass.",
            "a stuffed animal sitting on the shore of a lake."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2360312",
                "VG_object_id": "788635",
                "bbox": [157, 131, 353, 266],
                "image": "data\\images\\2360312.jpg"
            },
            {
                "VG_image_id": "2369807",
                "VG_object_id": "2121918",
                "bbox": [165, 88, 358, 315],
                "image": "data\\images\\2369807.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bear", 2],
            ["what is the ground covered with", 1],
            ["what is the main color of the background", 1],
            ["what is in front of the bear", 1],
            ["what is behind the bears", 1]
        ],
        "org_questions": [
            ["what color is the bear", 2],
            ["what is the ground covered with", 1],
            ["what is the main color of the background", 1],
            ["how many bears are there", -1],
            ["what is the bear doing", -1],
            ["what is in front of the bear", 1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["what is behind the bears", 1]
        ],
        "context": [
            "a bear is walking around in the rocks.",
            "a bear walking through a lush green field."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2363208",
                "VG_object_id": "2502952",
                "bbox": [2, 244, 124, 331],
                "image": "data\\images\\2363208.jpg"
            },
            {
                "VG_image_id": "2319323",
                "VG_object_id": "3147350",
                "bbox": [113, 103, 480, 253],
                "image": "data\\images\\2319323.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the bag placed on", 1],
            ["What is in the bag", 1],
            ["where is the bag", 1]
        ],
        "org_questions": [
            ["What is the bag placed on", 1],
            ["What is in the bag", 1],
            ["how many people are there", -1],
            ["where is the picture taken", -1],
            ["what is the bag made of", -1],
            ["where is the bag", 1],
            ["who is in the picture", -1],
            ["what type of bag is this", -1],
            ["what is on the back of the bag", -1]
        ],
        "context": [
            "a living room with a couch, television and a window.",
            "a cat is laying in a purse with a red handle."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2330928",
                "VG_object_id": "3293537",
                "bbox": [91, 240, 152, 482],
                "image": "data\\images\\2330928.jpg"
            },
            {
                "VG_image_id": "2318029",
                "VG_object_id": "1015027",
                "bbox": [272, 177, 415, 326],
                "image": "data\\images\\2318029.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where are the men", 2],
            ["what is the man doing", 2],
            ["What color is the man's tie", 1]
        ],
        "org_questions": [
            ["What color is the man's tie", 1],
            ["What color is the man", -1],
            ["Where are the men", 2],
            ["how many people are there", -1],
            ["what is the man doing", 2],
            ["what type of necktie is it", -1],
            ["what is the man wearing", -1],
            ["how many neckties are there", -1],
            ["who is wearing a tie", -1],
            ["what is around the man's neck", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is on the man's neck", -1]
        ],
        "context": [
            "two men in suits are posing for a picture.",
            "a man in a suit and tie looking at his cell phone."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2329138",
                "VG_object_id": "3375241",
                "bbox": [132, 257, 255, 496],
                "image": "data\\images\\2329138.jpg"
            },
            {
                "VG_image_id": "2349627",
                "VG_object_id": "2343536",
                "bbox": [160, 173, 240, 328],
                "image": "data\\images\\2349627.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's trousers", 1],
            ["what color is the ground", 1],
            ["what is the man holding", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color are the man's trousers", 1],
            ["what color is the ground", 1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["where is the person", -1],
            ["what is the ground covered with", 1],
            ["What is weather like", -1],
            ["when was the picture taken", -1],
            ["what kind of pants is the man wearing", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man carrying two suitcases down a street.",
            "a group of people sitting on the floor"
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2345720",
                "VG_object_id": "3440306",
                "bbox": [62, 67, 452, 332],
                "image": "data\\images\\2345720.jpg"
            },
            {
                "VG_image_id": "2401223",
                "VG_object_id": "1150075",
                "bbox": [2, 172, 500, 374],
                "image": "data\\images\\2401223.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["where was the picture taken", 1],
            ["what is laying on the floor", 1],
            ["what is the floor made of", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is the ground", -1],
            ["what color is the wall", -1],
            ["where was the picture taken", 1],
            ["what is laying on the floor", 1],
            ["where are the shoes", -1],
            ["what is the floor made of", 1]
        ],
        "context": [
            "a cat sitting on the floor next to a pair of shoes.",
            "a person sitting in a chair with a bird on it."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2411641",
                "VG_object_id": "308818",
                "bbox": [51, 97, 164, 375],
                "image": "data\\images\\2411641.jpg"
            },
            {
                "VG_image_id": "2403262",
                "VG_object_id": "3815921",
                "bbox": [77, 1, 198, 313],
                "image": "data\\images\\2403262.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child doing", 1],
            ["what is the child holding", 1],
            ["what color is the child's trousers", 1],
            ["how many children are there in the picture", 1]
        ],
        "org_questions": [
            ["what is the child doing", 1],
            ["what is the child holding", 1],
            ["what color is the child's trousers", 1],
            ["how many children are there in the picture", 1],
            ["where is the child", -1],
            ["what is the ground covered with", -1],
            ["what is the person wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what kind of shirt is the man wearing", -1]
        ],
        "context": [
            "a group of men playing basketball in a park.",
            "a young man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2384803",
                "VG_object_id": "690887",
                "bbox": [194, 82, 302, 302],
                "image": "data\\images\\2384803.jpg"
            },
            {
                "VG_image_id": "2395806",
                "VG_object_id": "2421009",
                "bbox": [333, 32, 488, 414],
                "image": "data\\images\\2395806.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman holding", 2],
            ["Where is the woman", 2],
            ["How many people are there", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["what gesture is the woman", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What is woman holding", 2],
            ["Where is the woman", 2],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["what gesture is the woman", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman standing on", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a woman walking on the beach with a surfboard.",
            "students and staff pose for a photo with their dog."
        ]
    },
    {
        "object_category": "keyboard",
        "images": [
            {
                "VG_image_id": "2363021",
                "VG_object_id": "1041555",
                "bbox": [97, 221, 272, 290],
                "image": "data\\images\\2363021.jpg"
            },
            {
                "VG_image_id": "2417158",
                "VG_object_id": "1057529",
                "bbox": [231, 254, 372, 309],
                "image": "data\\images\\2417158.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many screens are on the table", 1],
            ["How many keyboards are there", 1],
            ["how many computers are there", 1]
        ],
        "org_questions": [
            ["what color is the keyboard", -1],
            ["how many screens are on the table", 1],
            ["How many keyboards are there", 1],
            ["what color is the table", -1],
            ["what is on the keyboard", -1],
            ["what is under the keyboard", -1],
            ["what is next to the keyboard", -1],
            ["how many computers are there", 1],
            ["Where is the keyboard", -1],
            ["how many people are there in the picture", -1],
            ["how many people are there", -1],
            ["what is on the desk", -1],
            ["what is the desk made of", -1]
        ],
        "context": [
            "a computer monitor sitting on top of a desk.",
            "a desk with a computer and a monitor on it."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2347268",
                "VG_object_id": "892137",
                "bbox": [1, 288, 500, 374],
                "image": "data\\images\\2347268.jpg"
            },
            {
                "VG_image_id": "2352447",
                "VG_object_id": "3583345",
                "bbox": [1, 354, 332, 498],
                "image": "data\\images\\2352447.jpg"
            }
        ],
        "questions_with_scores": [
            ["where was this photo taken", 2],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what are the people doing", 1],
            ["what color is the ground", -1],
            ["what is on the ground", -1],
            ["what is the ground covered with", -1],
            ["What color is land", -1],
            ["where was this photo taken", 2],
            ["what are the people standing on", -1],
            ["what is green", -1],
            ["where is the grass", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a woman flying a kite in a field."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2392178",
                "VG_object_id": "1233172",
                "bbox": [171, 165, 262, 209],
                "image": "data\\images\\2392178.jpg"
            },
            {
                "VG_image_id": "2415302",
                "VG_object_id": "3387797",
                "bbox": [111, 65, 243, 130],
                "image": "data\\images\\2415302.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many motorcycles are in the picture", 2],
            ["what color is the motorcycle", 1],
            ["how many people are there", 1],
            ["how many people are there sitting on the seat", 1]
        ],
        "org_questions": [
            ["what color is the motorcycle", 1],
            ["how many motorcycles are in the picture", 2],
            ["What is next to the seat", -1],
            ["Where is the seat", -1],
            ["how many people are there", 1],
            ["how many people are there sitting on the seat", 1],
            ["when was this picture taken", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man riding a motorcycle on a beach.",
            "a blue motorcycle parked on the grass."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2354705",
                "VG_object_id": "836306",
                "bbox": [1, 162, 499, 334],
                "image": "data\\images\\2354705.jpg"
            },
            {
                "VG_image_id": "2375941",
                "VG_object_id": "3841056",
                "bbox": [9, 142, 367, 319],
                "image": "data\\images\\2375941.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["how many animals are there on the ground", 1],
            ["what is in the background of the picture", 1],
            ["what is the ground covered with", 1],
            ["what is standing on the ground", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["what is on the ground", 1],
            ["how many animals are there on the ground", 1],
            ["what is in the background of the picture", 1],
            ["what is the ground covered with", 1],
            ["what is standing on the ground", 1],
            ["when was the picture taken", -1],
            ["where was this picture taken", 1]
        ],
        "context": [
            "a fire hydrant sitting next to a wooden bench.",
            "two sheep are sitting in a field of grass."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2343266",
                "VG_object_id": "925605",
                "bbox": [101, 195, 398, 277],
                "image": "data\\images\\2343266.jpg"
            },
            {
                "VG_image_id": "2401606",
                "VG_object_id": "1146080",
                "bbox": [262, 123, 414, 184],
                "image": "data\\images\\2401606.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the plane", 1],
            ["what color is the background", 1],
            ["what is beside the plane", 1],
            ["What color is airplane", 1]
        ],
        "org_questions": [
            ["what color is the plane", 1],
            ["what color is the background", 1],
            ["how many airplanes are there", -1],
            ["what is the ground covered with", -1],
            ["what is beside the plane", 1],
            ["What color is airplane", 1],
            ["how many engines are there on the plane", -1],
            ["when was the photo taken", -1],
            ["what are the planes doing", -1],
            ["where was the photo taken", -1],
            ["where is the airplane", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "two large blue and white airplanes parked at an airport.",
            "a woman taking a picture of a flower on a runway."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2374077",
                "VG_object_id": "1718220",
                "bbox": [20, 79, 165, 203],
                "image": "data\\images\\2374077.jpg"
            },
            {
                "VG_image_id": "2367784",
                "VG_object_id": "3872274",
                "bbox": [11, 173, 373, 412],
                "image": "data\\images\\2367784.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is in the photo", 2],
            ["what color is the shirt", 1],
            ["what color is the person's hair", 1],
            ["what is the person holding", 1],
            ["How many people are there", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what color is the person's hair", 1],
            ["what is the person holding", 1],
            ["How many people are there", 1],
            ["how many children are there in the picture", -1],
            ["who is in the photo", 2],
            ["what is the persion doing", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "two women in a kitchen cooking food together.",
            "a man holding a sandwich in his hands."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2368808",
                "VG_object_id": "615795",
                "bbox": [202, 143, 261, 309],
                "image": "data\\images\\2368808.jpg"
            },
            {
                "VG_image_id": "2389482",
                "VG_object_id": "1260431",
                "bbox": [304, 160, 356, 305],
                "image": "data\\images\\2389482.jpg"
            }
        ],
        "questions_with_scores": [["What color is the woman's shirt", 2]],
        "org_questions": [
            ["What color is the woman's shirt", 2],
            ["How many people are there", -1],
            ["What color is the ground", -1],
            ["what is the woman doing", -1],
            ["what is the woman wearing on head", -1],
            ["where is the woman", -1],
            ["what is the woman holding", -1],
            ["when was this picture taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a woman standing next to a moving truck.",
            "a man and a woman walking down a street."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2408201",
                "VG_object_id": "2115254",
                "bbox": [2, 1, 428, 263],
                "image": "data\\images\\2408201.jpg"
            },
            {
                "VG_image_id": "2411587",
                "VG_object_id": "309687",
                "bbox": [61, 27, 362, 151],
                "image": "data\\images\\2411587.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["when is this picture taken", 2],
            ["how many people are there in the picture", 1],
            ["what time is it", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what color is the building", 2],
            ["when is this picture taken", 2],
            ["how many people are there in the picture", 1],
            ["What is behind the building", -1],
            ["what is the building made of", -1],
            ["what time is it", 1],
            ["how is the weather", -1],
            ["who is in the picture", 1],
            ["what is on the building", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man riding a bike down a street next to a car.",
            "a bench in the middle of a street at night."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2344471",
                "VG_object_id": "3015580",
                "bbox": [5, 127, 497, 332],
                "image": "data\\images\\2344471.jpg"
            },
            {
                "VG_image_id": "2372214",
                "VG_object_id": "3853261",
                "bbox": [0, 110, 498, 330],
                "image": "data\\images\\2372214.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the animal", 2],
            ["what kind of animals are there in the background", 2],
            ["how many people are there", 1],
            ["what is behind the grass", 1]
        ],
        "org_questions": [
            ["what is on the field", -1],
            ["what is on the animal", 2],
            ["what is on the animal's neck", -1],
            ["how many people are there", 1],
            ["what color is the grass", -1],
            ["what kind of animals are there in the background", 2],
            ["what color is the ground", -1],
            ["where are the flowers", -1],
            ["how is the weather", -1],
            ["what is behind the grass", 1]
        ],
        "context": [
            "a woman riding a horse through a lush green forest.",
            "a cow with a collar standing in a field of flowers."
        ]
    },
    {
        "object_category": "toilet",
        "images": [
            {
                "VG_image_id": "2372738",
                "VG_object_id": "2047106",
                "bbox": [33, 37, 305, 449],
                "image": "data\\images\\2372738.jpg"
            },
            {
                "VG_image_id": "2354395",
                "VG_object_id": "838833",
                "bbox": [254, 69, 395, 235],
                "image": "data\\images\\2354395.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the color of the floor", 2],
            ["what is the ground covered with", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["what color is the wall", -1],
            ["what is the ground covered with", 1],
            ["what color is the ground", 2],
            ["what is on the wall", 1],
            ["what kind of toilet is it", -1],
            ["What is the floor made of", -1],
            ["what is the color of the floor", 2],
            ["how many toilets are there", -1],
            ["where was the photo taken", -1],
            ["what room is this", -1],
            ["what is next to the toilet", -1]
        ],
        "context": [
            "a toilet with a white seat and lid.",
            "a bathroom with a toilet and a sink"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2380040",
                "VG_object_id": "548277",
                "bbox": [31, 72, 258, 498],
                "image": "data\\images\\2380040.jpg"
            },
            {
                "VG_image_id": "2351730",
                "VG_object_id": "1694480",
                "bbox": [151, 120, 262, 326],
                "image": "data\\images\\2351730.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["how many men are there", 2],
            ["what color is the man's jacket", 1],
            ["what is the man holding", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what color is the man's jacket", 1],
            ["how many people are in the picture", 2],
            ["what is the man wearing on his head", -1],
            ["where is the man", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["how many men are there", 2],
            ["when was the photo taken", -1],
            ["who is in the picture", 1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man in a green jacket and hat standing in the snow.",
            "a man and woman are standing in the snow on skis."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2414932",
                "VG_object_id": "149215",
                "bbox": [127, 137, 206, 211],
                "image": "data\\images\\2414932.jpg"
            },
            {
                "VG_image_id": "2351504",
                "VG_object_id": "2507539",
                "bbox": [110, 131, 248, 215],
                "image": "data\\images\\2351504.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cat", 2],
            ["where are the cats", 1],
            ["what is the cat doing", 1],
            ["where is the cat", 1],
            ["what is behind the cat", 1]
        ],
        "org_questions": [
            ["what color is the cat", 2],
            ["how many cats are in the picture", -1],
            ["where are the cats", 1],
            ["what is on the cat's neck", -1],
            ["what is the cat doing", 1],
            ["where is the cat", 1],
            ["what type of animal is shown", -1],
            ["when was the photo taken", -1],
            ["what is behind the cat", 1],
            ["what is the cat looking at", -1],
            ["what is on the cat", -1]
        ],
        "context": [
            "a white cat sitting on top of a car.",
            "a cat sitting on a window sill next to a lamp."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2322853",
                "VG_object_id": "2853763",
                "bbox": [0, 259, 497, 327],
                "image": "data\\images\\2322853.jpg"
            },
            {
                "VG_image_id": "2372536",
                "VG_object_id": "2338576",
                "bbox": [18, 237, 492, 327],
                "image": "data\\images\\2372536.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is next to the train", 2],
            ["what color is the train on the land", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the train on the land", 1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["what is on the track", -1],
            ["what is the train on", -1],
            ["what is next to the train", 2]
        ],
        "context": [
            "a train is traveling through a lush green forest.",
            "a red train is coming down the tracks."
        ]
    },
    {
        "object_category": "toilet",
        "images": [
            {
                "VG_image_id": "2382559",
                "VG_object_id": "538460",
                "bbox": [56, 2, 374, 499],
                "image": "data\\images\\2382559.jpg"
            },
            {
                "VG_image_id": "2413157",
                "VG_object_id": "3145584",
                "bbox": [115, 285, 189, 446],
                "image": "data\\images\\2413157.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the toilet", 2],
            ["where was this picture taken", 2],
            ["what is in the toilet", 1],
            ["what color is the ground", 1],
            ["what is the floor made of", 1],
            ["what color is the floor under the toilet", 1],
            ["what is behind the toilet", 1]
        ],
        "org_questions": [
            ["where is the toilet", 2],
            ["what is in the toilet", 1],
            ["what color is the ground", 1],
            ["what is the floor made of", 1],
            ["what color is the floor under the toilet", 1],
            ["how many toilets are there", -1],
            ["what shape is the toilet", -1],
            ["where was this picture taken", 2],
            ["what is behind the toilet", 1]
        ],
        "context": [
            "a toilet with a wooden handle and a pair of black seats.",
            "a bathroom with a toilet and a sink"
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2394602",
                "VG_object_id": "462708",
                "bbox": [333, 158, 452, 297],
                "image": "data\\images\\2394602.jpg"
            },
            {
                "VG_image_id": "2322285",
                "VG_object_id": "3051106",
                "bbox": [281, 124, 417, 374],
                "image": "data\\images\\2322285.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the lady doing", 2],
            ["where is the lady", 2],
            ["what color is the lady's shirt", 1],
            ["how many people are there", 1],
            ["how many person's are there in the photo", 1]
        ],
        "org_questions": [
            ["what is the lady doing", 2],
            ["where is the lady", 2],
            ["what color is the lady's shirt", 1],
            ["how many people are there", 1],
            ["What season is it", -1],
            ["what is the weather like", -1],
            ["what is the woman holding", -1],
            ["how many person's are there in the photo", 1],
            ["who is in the photo", -1],
            ["what is the woman wearing on the face", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man in a red shirt",
            "a woman playing a video game in a living room."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2386737",
                "VG_object_id": "1282994",
                "bbox": [6, 176, 272, 499],
                "image": "data\\images\\2386737.jpg"
            },
            {
                "VG_image_id": "2344206",
                "VG_object_id": "2413259",
                "bbox": [0, 0, 282, 218],
                "image": "data\\images\\2344206.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cat", 2],
            ["where is the cat", 1],
            ["where are the cats", 1]
        ],
        "org_questions": [
            ["what color is the cat", 2],
            ["what is the cat doing", -1],
            ["where is the cat", 1],
            ["How many animals are there", -1],
            ["how many cats are in the picture", -1],
            ["where are the cats", 1],
            ["what animal is in the photo", -1],
            ["what is on the cat's head", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a cat watching a man on a computer screen.",
            "a black cat is sitting on a toilet seat."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2371791",
                "VG_object_id": "2426988",
                "bbox": [35, 182, 105, 323],
                "image": "data\\images\\2371791.jpg"
            },
            {
                "VG_image_id": "2364733",
                "VG_object_id": "2482739",
                "bbox": [310, 200, 499, 365],
                "image": "data\\images\\2364733.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["who is sitting on the chair", 2],
            ["how many people are in the picture", 1],
            ["how many chairs are there in the picture", 1],
            ["where was this photo taken", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["who is sitting on the chair", 2],
            ["how many people are in the picture", 1],
            ["where is the chair", -1],
            ["how many chairs are there in the picture", 1],
            ["what color is the wall", -1],
            ["where was this photo taken", 1],
            ["what is on the table", 1],
            ["what is the chair made of", -1]
        ],
        "context": [
            "a family sitting at a table with food and drinks.",
            "a young boy holding a frisbee in a toy store."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2331746",
                "VG_object_id": "3491125",
                "bbox": [46, 78, 168, 307],
                "image": "data\\images\\2331746.jpg"
            },
            {
                "VG_image_id": "2350722",
                "VG_object_id": "1924723",
                "bbox": [417, 113, 498, 367],
                "image": "data\\images\\2350722.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["What color is the ground", 1],
            ["what color are the man's trousers", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man wearing", -1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["What color is the ground", 1],
            ["what color are the man's trousers", 1],
            ["what is the persion holding", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man pulling a boat on the beach.",
            "a woman and a boy are looking at a display of donuts."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2393491",
                "VG_object_id": "1219493",
                "bbox": [396, 33, 464, 97],
                "image": "data\\images\\2393491.jpg"
            },
            {
                "VG_image_id": "2349344",
                "VG_object_id": "2932376",
                "bbox": [85, 80, 184, 201],
                "image": "data\\images\\2349344.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is behind the person", 2],
            ["what color is the person's shirt", 1],
            ["what is the pattern of the person's shirt", 1],
            ["where is the photo taken", 1],
            ["who is wearing the shirt", 1],
            ["what is the persion wearing on his head", 1],
            ["what is the persion wearing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what is the pattern of the person's shirt", 1],
            ["what is the color of the background", -1],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["who is wearing the shirt", 1],
            ["what is the persion wearing on his head", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", 1],
            ["who is in the photo", 1],
            ["what is behind the person", 2]
        ],
        "context": [
            "a man riding a skateboard down a ramp.",
            "a man riding a skateboard on top of a sign."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2414962",
                "VG_object_id": "148637",
                "bbox": [10, 289, 132, 393],
                "image": "data\\images\\2414962.jpg"
            },
            {
                "VG_image_id": "2356146",
                "VG_object_id": "823068",
                "bbox": [338, 43, 460, 200],
                "image": "data\\images\\2356146.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the wall", 1],
            ["how many people are there in the picture", 1],
            ["what shape is the sink", -1],
            ["what is the sink on", -1],
            ["how many taps are there on the sink", -1],
            ["what is on the wall", -1],
            ["what room is this", -1],
            ["where was the picture taken", -1],
            ["what is the sink made of", -1],
            ["what is next to the sink", -1]
        ],
        "context": [
            "a man taking a picture of himself in a bathroom mirror.",
            "a bathroom with a toilet, sink, and a towel rack."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2412524",
                "VG_object_id": "192544",
                "bbox": [37, 74, 467, 300],
                "image": "data\\images\\2412524.jpg"
            },
            {
                "VG_image_id": "2407861",
                "VG_object_id": "272303",
                "bbox": [127, 102, 387, 257],
                "image": "data\\images\\2407861.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 1],
            ["what is the ground covered with", 1],
            ["How many people are there", 1],
            ["what is on the side of the train", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what color is the train", 1],
            ["where is the train", -1],
            ["what is the ground covered with", 1],
            ["How many people are there", 1],
            ["what is on the side of the train", 1],
            ["what is the train on", -1],
            ["what is in the distance", 1],
            ["what is on the ground", -1],
            ["when was the photo taken", -1],
            ["what type of vehicle is this", -1],
            ["what is the train doing", -1]
        ],
        "context": [
            "a train sitting in a building with a table and chairs.",
            "a train is traveling down the tracks in a field."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2374025",
                "VG_object_id": "1796090",
                "bbox": [1, 60, 499, 330],
                "image": "data\\images\\2374025.jpg"
            },
            {
                "VG_image_id": "2357161",
                "VG_object_id": "2424351",
                "bbox": [7, 166, 499, 367],
                "image": "data\\images\\2357161.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["what is in front of the trees", 1],
            ["how many animals are there on the land", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what animal is on the ground", -1],
            ["How many people are there", 1],
            ["What is on the ground", -1],
            ["What is the weather like", -1],
            ["where was this photo taken", -1],
            ["where are the trees", -1],
            ["what is the ground covered with", -1],
            ["what is in the background", -1],
            ["what is in front of the trees", 1],
            ["how many animals are there on the land", 1],
            ["what is the weather like", -1],
            ["where is this scene", -1],
            ["what is on the ground", -1],
            ["how is the weather", -1]
        ],
        "context": [
            "a zebra is grazing in the dirt near a tree.",
            "a large dirt field with zebras in it."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2407180",
                "VG_object_id": "1098597",
                "bbox": [65, 417, 272, 500],
                "image": "data\\images\\2407180.jpg"
            },
            {
                "VG_image_id": "2343118",
                "VG_object_id": "3471621",
                "bbox": [237, 404, 374, 497],
                "image": "data\\images\\2343118.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is next to the sink", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what shape is the sink", -1],
            ["how many people are there in the picture", -1],
            ["what is sink made of", -1],
            ["what is above the sink", -1],
            ["what is next to the sink", 1],
            ["what is in the sink", -1]
        ],
        "context": [
            "a bathroom with a large mirror and a sink.",
            "a man taking a picture of himself in the mirror."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "713188",
                "VG_object_id": "1580127",
                "bbox": [64, 327, 281, 580],
                "image": "data\\images\\713188.jpg"
            },
            {
                "VG_image_id": "2378736",
                "VG_object_id": "1365527",
                "bbox": [142, 152, 266, 299],
                "image": "data\\images\\2378736.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 1],
            ["what is the gender of the person", 1],
            ["what is behind the person", 1],
            ["what is the persion holding", 1],
            ["who is wearing a blue shirt", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["where is the person", 1],
            ["what is the gender of the person", 1],
            ["how many people are there", -1],
            ["what is behind the person", 1],
            ["what is the persion holding", 1],
            ["who is wearing a blue shirt", 1],
            ["when was the photo taken", -1],
            ["what is the persion doing", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a woman and her dog on a surfboard.",
            "a man holding a kite in a field."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2346804",
                "VG_object_id": "896390",
                "bbox": [0, 0, 500, 318],
                "image": "data\\images\\2346804.jpg"
            },
            {
                "VG_image_id": "2357315",
                "VG_object_id": "1903577",
                "bbox": [4, 2, 496, 332],
                "image": "data\\images\\2357315.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the building", 2],
            ["What the color of building", 1]
        ],
        "org_questions": [
            ["What color is the building", 2],
            ["What is in front of the building", -1],
            ["what time is it", -1],
            ["What is the weather like", -1],
            ["What is behind the building", -1],
            ["What the color of building", 1],
            ["where was the photo taken", -1],
            ["when was this picture taken", -1],
            ["where is the building", -1],
            ["what is on the wall", -1]
        ],
        "context": [
            "a man in a jacket talking on a cell phone.",
            "a red and white sign"
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2396574",
                "VG_object_id": "1196875",
                "bbox": [437, 170, 495, 212],
                "image": "data\\images\\2396574.jpg"
            },
            {
                "VG_image_id": "2396197",
                "VG_object_id": "1199743",
                "bbox": [226, 6, 492, 372],
                "image": "data\\images\\2396197.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["Where is the cow", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the cow", -1],
            ["what is the cow doing", -1],
            ["what color is the ground", 1],
            ["how many cows are there", -1],
            ["Where is the cow", 1],
            ["what is the ground covered with", 1],
            ["what is the weather like", -1],
            ["what animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["what is the animal", -1],
            ["what animal is this", -1]
        ],
        "context": [
            "a woman standing next to a fence with cows in the background.",
            "a cow and a calf are standing in a pen."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2344962",
                "VG_object_id": "3653188",
                "bbox": [8, 40, 238, 345],
                "image": "data\\images\\2344962.jpg"
            },
            {
                "VG_image_id": "2367784",
                "VG_object_id": "2607643",
                "bbox": [13, 28, 351, 496],
                "image": "data\\images\\2367784.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["where is the man", 1],
            ["what is the person holding", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 2],
            ["what is on the man's head", 1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["how old is the man", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person holding", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a man and a woman looking at cell phones in a store.",
            "a man holding a sandwich in his hands."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2353671",
                "VG_object_id": "843979",
                "bbox": [171, 335, 335, 386],
                "image": "data\\images\\2353671.jpg"
            },
            {
                "VG_image_id": "2383741",
                "VG_object_id": "530642",
                "bbox": [250, 157, 444, 279],
                "image": "data\\images\\2383741.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color are the animals", 1],
            ["What color are the trees in the background", 1],
            ["What is the animal in the image", 1],
            ["what type of animal is shown", 1],
            ["what animals are shown", 1],
            ["what animal is in the picture", 1]
        ],
        "org_questions": [
            ["What color are the animals", 1],
            ["What color are the trees in the background", 1],
            ["where is this animal", -1],
            ["How many animals are there", -1],
            ["what is the ground covered with", -1],
            ["What is the animal in the image", 1],
            ["what type of animal is shown", 1],
            ["what animals are shown", 1],
            ["what animal is in the picture", 1]
        ],
        "context": [
            "a sheep grazes in a field of grass.",
            "a herd of cattle grazing in a field of grass."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2348068",
                "VG_object_id": "3607591",
                "bbox": [163, 244, 230, 294],
                "image": "data\\images\\2348068.jpg"
            },
            {
                "VG_image_id": "2412084",
                "VG_object_id": "307072",
                "bbox": [73, 90, 194, 295],
                "image": "data\\images\\2412084.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the container made of", 1],
            ["what color is the table", 1],
            ["where was the photo taken", 1],
            ["what is sitting on the table", 1]
        ],
        "org_questions": [
            ["what color is the container", -1],
            ["what is the container made of", 1],
            ["what color is the table", 1],
            ["how many people are there", 2],
            ["how many containers are there", -1],
            ["where was the photo taken", 1],
            ["what is sitting on the table", 1]
        ],
        "context": [
            "a woman in a red shirt",
            "a tray with a clock and tea on it"
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2352127",
                "VG_object_id": "2132503",
                "bbox": [166, 59, 318, 143],
                "image": "data\\images\\2352127.jpg"
            },
            {
                "VG_image_id": "2328643",
                "VG_object_id": "3254286",
                "bbox": [87, 7, 457, 374],
                "image": "data\\images\\2328643.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many giraffes are there in the picture", 2],
            ["what color is the truck", 1],
            ["what color is the ground", 1],
            ["what color is the background", 1],
            ["what is the ground covered with", 1],
            ["what kind of land is the truck on", 1]
        ],
        "org_questions": [
            ["what color is the truck", 1],
            ["what color is the ground", 1],
            ["what color is the background", 1],
            ["where is the truck", -1],
            ["what is in the distance", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", 1],
            ["what kind of land is the truck on", 1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["what is on the ground", -1],
            ["how many giraffes are there in the picture", 2]
        ],
        "context": [
            "a giraffe standing next to a tree in a field.",
            "a truck with a statue of a man in a hat and a cowboy hat."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "713013",
                "VG_object_id": "1577606",
                "bbox": [735, 244, 944, 614],
                "image": "data\\images\\713013.jpg"
            },
            {
                "VG_image_id": "2405276",
                "VG_object_id": "333491",
                "bbox": [9, 12, 321, 331],
                "image": "data\\images\\2405276.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["how many people are there", 2],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["how many people are there", 2],
            ["what is the man holding", -1],
            ["what animal is in the photo", -1],
            ["where is the man", -1],
            ["what is the man doing", -1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is the man sitting on", -1],
            ["what is on the man's face", -1],
            ["what is behind the man", 1]
        ],
        "context": [
            "the crowd at the event",
            "a man holding up a donut in front of his face."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2334903",
                "VG_object_id": "2128876",
                "bbox": [1, 236, 498, 334],
                "image": "data\\images\\2334903.jpg"
            },
            {
                "VG_image_id": "2320154",
                "VG_object_id": "995706",
                "bbox": [11, 346, 333, 469],
                "image": "data\\images\\2320154.jpg"
            }
        ],
        "questions_with_scores": [["how many people are on the street", 2]],
        "org_questions": [
            ["how many people are on the street", 2],
            ["how many arrows are on the street", -1],
            ["what color is the street", -1],
            ["what is on the side of the street", -1],
            ["what is the road made of", -1],
            ["what is the weather like", -1],
            ["what is on the ground", -1],
            ["where was this photo taken", -1],
            ["when was the photo taken", -1],
            ["where are the white lines", -1]
        ],
        "context": [
            "a construction site with a clock tower in the background.",
            "a tall clock tower in the middle of a city."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2358520",
                "VG_object_id": "3048136",
                "bbox": [21, 88, 298, 291],
                "image": "data\\images\\2358520.jpg"
            },
            {
                "VG_image_id": "2331432",
                "VG_object_id": "2863763",
                "bbox": [0, 355, 498, 498],
                "image": "data\\images\\2331432.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are in the picture", 1],
            ["what is the main color of the plate", 1],
            ["what is the food on", 1]
        ],
        "org_questions": [
            ["how many plates are in the picture", 1],
            ["what is on the plate", -1],
            ["What color is the table", -1],
            ["what shape is the plate", -1],
            ["what is the table made of", -1],
            ["what is the plate on", -1],
            ["what is under the plate", -1],
            ["what is the plate made of", -1],
            ["what is the main color of the plate", 1],
            ["what is the food on", 1]
        ],
        "context": [
            "two plates of food on a table with drinks.",
            "a bowl of broccoli on a table"
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2397332",
                "VG_object_id": "1190721",
                "bbox": [6, 99, 500, 363],
                "image": "data\\images\\2397332.jpg"
            },
            {
                "VG_image_id": "2402573",
                "VG_object_id": "1134215",
                "bbox": [1, 2, 498, 329],
                "image": "data\\images\\2402573.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is on the ground", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what is on the ground", 1],
            ["where is the picture taken", -1],
            ["what is the ground covered with", 1],
            ["what is the ground of the ground", -1],
            ["how many motorcycles are there on the ground", -1],
            ["What color is the background of image", -1],
            ["when was the photo taken", -1],
            ["what is the weather like", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a man and two children standing on a beach with a kite.",
            "a blue book on a bench"
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2334727",
                "VG_object_id": "2064647",
                "bbox": [216, 243, 426, 330],
                "image": "data\\images\\2334727.jpg"
            },
            {
                "VG_image_id": "2327866",
                "VG_object_id": "2760978",
                "bbox": [178, 193, 375, 370],
                "image": "data\\images\\2327866.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["what kind of food is on the plate", 1],
            ["What is on he plate", 1],
            ["how many plates are on the table", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["what kind of food is on the plate", 1],
            ["How many people are there", -1],
            ["what shape is the plate", -1],
            ["what is the table made of", -1],
            ["What is on he plate", 1],
            ["where is the food", -1],
            ["what is the food on", -1],
            ["how many plates are on the table", 1]
        ],
        "context": [
            "a woman eating a plate of food",
            "a table topped with plates of food and vegetables."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2416688",
                "VG_object_id": "3234058",
                "bbox": [0, 410, 332, 498],
                "image": "data\\images\\2416688.jpg"
            },
            {
                "VG_image_id": "2416834",
                "VG_object_id": "3193823",
                "bbox": [42, 192, 497, 323],
                "image": "data\\images\\2416834.jpg"
            }
        ],
        "questions_with_scores": [
            ["What animals are there", 2],
            ["How many animals are there", 2]
        ],
        "org_questions": [
            ["What animals are there", 2],
            ["How many animals are there", 2],
            ["what is covering the ground", -1],
            ["What color is the land", -1],
            ["when is this picture taken", -1],
            ["what is the animal doing", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1],
            ["what is the ground", -1],
            ["what is in the ground", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a giraffe standing in a doorway next to a brick building.",
            "a baby elephant standing next to an adult elephant."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2404534",
                "VG_object_id": "1114971",
                "bbox": [58, 135, 280, 353],
                "image": "data\\images\\2404534.jpg"
            },
            {
                "VG_image_id": "2366153",
                "VG_object_id": "2489864",
                "bbox": [240, 162, 356, 282],
                "image": "data\\images\\2366153.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["what is the land made of ", 1],
            ["where is  the horse", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 2],
            ["what is behind the horse", -1],
            ["how many horses are there", -1],
            ["what color are the horses", -1],
            ["what is the land made of ", 1],
            ["what is the horse standing on", -1],
            ["where is  the horse", 1],
            ["what type of animal is shown", -1],
            ["when was the picture taken", -1],
            ["what are the horses doing", -1],
            ["who is in the picture", -1],
            ["what is on the horses", -1]
        ],
        "context": [
            "a man is riding a horse drawn cart with a dog.",
            "a horse drawn carriage is standing in the middle of a city."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2358871",
                "VG_object_id": "1851056",
                "bbox": [157, 240, 373, 390],
                "image": "data\\images\\2358871.jpg"
            },
            {
                "VG_image_id": "2385020",
                "VG_object_id": "3846337",
                "bbox": [3, 227, 259, 371],
                "image": "data\\images\\2385020.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is on the ground", -1],
            ["where is the picture taken", -1],
            ["what is the ground covered with", 1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["how is the weather", -1]
        ],
        "context": [
            "a man and a bear are looking at something.",
            "a bench in the woods with a tree in the background."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2376730",
                "VG_object_id": "570147",
                "bbox": [222, 152, 366, 355],
                "image": "data\\images\\2376730.jpg"
            },
            {
                "VG_image_id": "2398375",
                "VG_object_id": "1180471",
                "bbox": [121, 91, 292, 224],
                "image": "data\\images\\2398375.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 1],
            ["what color is the man's hair", 1],
            ["What sport is the man doing ", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt inside", -1],
            ["what is the man holding", 1],
            ["what color is the man's hair", 1],
            ["where is the man", -1],
            ["What sport is the man doing ", 1],
            ["what is the person wearing", -1],
            ["how many people are in the picture", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man holding a surfboard in front of a tree.",
            "a man holding a skateboard on a handrail."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2344650",
                "VG_object_id": "3156766",
                "bbox": [3, 195, 484, 373],
                "image": "data\\images\\2344650.jpg"
            },
            {
                "VG_image_id": "2407516",
                "VG_object_id": "366792",
                "bbox": [3, 306, 497, 372],
                "image": "data\\images\\2407516.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animals are in the field", 1],
            ["how many animals are on the field", 1],
            ["what is on the field", 1],
            ["what is in the field", 1],
            ["where was this picture taken", 1],
            ["how is the weather", 1],
            ["what is the ground covered with", 1],
            ["how many horses are there on the field", 1],
            ["What is the background of photo", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the field", -1],
            ["what animals are in the field", 1],
            ["how many animals are on the field", 1],
            ["what is on the field", 1],
            ["what is in the field", 1],
            ["when was the picture taken", -1],
            ["where was this picture taken", 1],
            ["how is the weather", 1],
            ["what is the ground covered with", 1],
            ["where is the shadow", -1],
            ["how many horses are there on the field", 1],
            ["What is the background of photo", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a group of zebras standing around in a fenced in area.",
            "a group of cows grazing in a field."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2367369",
                "VG_object_id": "2989302",
                "bbox": [2, 325, 493, 482],
                "image": "data\\images\\2367369.jpg"
            },
            {
                "VG_image_id": "2378619",
                "VG_object_id": "558644",
                "bbox": [46, 170, 488, 235],
                "image": "data\\images\\2378619.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cats are there on the bed", 2],
            ["what color is the bed", 1],
            ["what color is the wall", 1],
            ["what is on the blanket", 1]
        ],
        "org_questions": [
            ["what color is the bed", 1],
            ["what is on the bed", -1],
            ["how many cats are there on the bed", 2],
            ["Where is the person", -1],
            ["What is the person wearing", -1],
            ["what color is the wall", 1],
            ["how many pillows are on the bed", -1],
            ["what is on the blanket", 1],
            ["what is the bed made of", -1],
            ["what is on top of the bed", -1],
            ["what is covering the bed", -1]
        ],
        "context": [
            "a small child sitting on a bed with a pillow.",
            "a man laying on a bed with two cats."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2379227",
                "VG_object_id": "712878",
                "bbox": [371, 1, 499, 54],
                "image": "data\\images\\2379227.jpg"
            },
            {
                "VG_image_id": "2409760",
                "VG_object_id": "234235",
                "bbox": [406, 72, 497, 264],
                "image": "data\\images\\2409760.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the plant", 2],
            ["How many animals are there in the photo", 2],
            ["what is in the picture", 1],
            ["where is the photo taken", 1],
            ["what is in front of the plant", 1]
        ],
        "org_questions": [
            ["Where is the plant", 2],
            ["How many animals are there in the photo", 2],
            ["what is in the picture", 1],
            ["where is the photo taken", 1],
            ["what is in front of the plant", 1],
            ["when was the photo taken", -1],
            ["how is the weather", -1],
            ["what is green", -1]
        ],
        "context": [
            "a zebra standing on a dirt bank next to a body of water.",
            "a bike is laying in a garden with flowers."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2345657",
                "VG_object_id": "905929",
                "bbox": [156, 144, 235, 214],
                "image": "data\\images\\2345657.jpg"
            },
            {
                "VG_image_id": "2388228",
                "VG_object_id": "674708",
                "bbox": [68, 102, 209, 218],
                "image": "data\\images\\2388228.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the man doing", 2],
            ["What color is the man's pant", 1],
            ["what is the persion holding", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["What is the man doing", 2],
            ["How many people are there", -1],
            ["What color is the man's pant", 1],
            ["who is wearing the shirt", -1],
            ["what is the land made of ", -1],
            ["what is the persion holding", 1],
            ["what color is the shirt", -1],
            ["when was this photo taken", -1],
            ["what is on the man's head", 1],
            ["what is the boy wearing", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a baseball player is swinging a bat at a ball.",
            "a man jumping in the air on a skateboard."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2394474",
                "VG_object_id": "3008662",
                "bbox": [63, 182, 173, 332],
                "image": "data\\images\\2394474.jpg"
            },
            {
                "VG_image_id": "2366683",
                "VG_object_id": "626798",
                "bbox": [139, 205, 287, 318],
                "image": "data\\images\\2366683.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trouser", 2],
            ["what is the person in the trousers holding in hand", 2],
            ["what is the person doing in the picture", 1],
            ["what is the persion in the trouser doing", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 2],
            ["what is the person doing in the picture", 1],
            ["how many people are there in the photo", -1],
            ["where is the person in the trouser", -1],
            ["what is the person in the trousers holding in hand", 2],
            ["what is the persion in the trouser doing", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a woman throwing a frisbee in a wooded area.",
            "a man in a red and white baseball uniform is throwing a ball."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2364974",
                "VG_object_id": "637316",
                "bbox": [1, 49, 169, 308],
                "image": "data\\images\\2364974.jpg"
            },
            {
                "VG_image_id": "2405884",
                "VG_object_id": "1105539",
                "bbox": [88, 74, 331, 444],
                "image": "data\\images\\2405884.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is the woman holding", 2],
            ["what is the woman doing", 1],
            ["Where is the woman", 1],
            ["what is the woman wearing", 1],
            ["what kind of shirt is the woman wearing", 1],
            ["what is in the woman's hand", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 2],
            ["what is the woman doing", 1],
            ["What is the woman wearing on her head", -1],
            ["Where is the woman", 1],
            ["what is the woman holding", 2],
            ["what is the woman wearing", 1],
            ["what color is the background", -1],
            ["who is in the photo", -1],
            ["what kind of shirt is the woman wearing", 1],
            ["what is on the woman's wrist", -1],
            ["what is in the woman's hand", 1]
        ],
        "context": [
            "a woman is putting a pizza in an oven.",
            "two women cutting a cake together."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2374091",
                "VG_object_id": "2370926",
                "bbox": [16, 223, 496, 372],
                "image": "data\\images\\2374091.jpg"
            },
            {
                "VG_image_id": "2358415",
                "VG_object_id": "2130330",
                "bbox": [378, 290, 498, 373],
                "image": "data\\images\\2358415.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the counter made of", 1],
            ["what color is the counter", 1],
            ["what is on the counter", 1],
            ["where is the picture taken", 1],
            ["how many people are there in the picture", 1],
            ["where is the counter", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["what is the counter made of", 1],
            ["what color is the counter", 1],
            ["what is on the counter", 1],
            ["where is the picture taken", 1],
            ["how many people are there in the picture", 1],
            ["what is the counter on", -1],
            ["how many bottles are there on the counter", -1],
            ["where is the counter", 1],
            ["where was this photo taken", 1]
        ],
        "context": [
            "a bathroom sink with a large mirror and a girl taking a picture.",
            "a group of people working in a kitchen."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2380849",
                "VG_object_id": "3320197",
                "bbox": [178, 184, 263, 264],
                "image": "data\\images\\2380849.jpg"
            },
            {
                "VG_image_id": "2395806",
                "VG_object_id": "1938793",
                "bbox": [17, 201, 114, 438],
                "image": "data\\images\\2395806.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the person holding", 2]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["what is the person holding", 2],
            ["what time is it", -1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a baseball player throwing a ball to another player.",
            "students and staff pose for a photo with their dog."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2371871",
                "VG_object_id": "2133452",
                "bbox": [135, 5, 344, 287],
                "image": "data\\images\\2371871.jpg"
            },
            {
                "VG_image_id": "2315910",
                "VG_object_id": "2744191",
                "bbox": [0, 57, 187, 347],
                "image": "data\\images\\2315910.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's shirt", 2],
            ["what is the girl doing", 1],
            ["what is the girl wearing on the head", 1],
            ["where is the photo taken", 1],
            ["how old is the girl", 1],
            ["What is the woman holding", 1],
            ["who is in the photo", 1],
            ["what is on the woman's feet", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", 2],
            ["what is the girl doing", 1],
            ["what is the girl wearing on the head", 1],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["What is the ground made of", -1],
            ["how old is the girl", 1],
            ["What is the woman holding", 1],
            ["who is in the photo", 1],
            ["when was the picture taken", -1],
            ["what is on the woman's feet", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a woman sitting on a motorcycle with her hand up.",
            "a young girl riding a skateboard on a ramp."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2377316",
                "VG_object_id": "3838516",
                "bbox": [32, 81, 193, 246],
                "image": "data\\images\\2377316.jpg"
            },
            {
                "VG_image_id": "2360828",
                "VG_object_id": "1712752",
                "bbox": [157, 40, 376, 157],
                "image": "data\\images\\2360828.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what time is it", 1],
            ["what color is the background", 1],
            ["what time is showed on the clock", 1],
            ["what is in the background", 1],
            ["what time of day is it", 1]
        ],
        "org_questions": [
            ["what time is it", 1],
            ["how many people are in the picture", 2],
            ["what color is the background", 1],
            ["where is the clock", -1],
            ["what is the clock made of", -1],
            ["what time is showed on the clock", 1],
            ["when is this picture taken", -1],
            ["what shape is the clock", -1],
            ["what is in the background", 1],
            ["what is on the wall", -1],
            ["what is above the clock", -1],
            ["what time of day is it", 1]
        ],
        "context": [
            "a clock on a wall next to a box.",
            "a clock hanging from the ceiling"
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2354177",
                "VG_object_id": "3574466",
                "bbox": [0, 0, 498, 372],
                "image": "data\\images\\2354177.jpg"
            },
            {
                "VG_image_id": "2348229",
                "VG_object_id": "2632149",
                "bbox": [2, 0, 491, 337],
                "image": "data\\images\\2348229.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 2],
            ["how many people are there", 2],
            ["what is in the room", 1],
            ["what is on the wall", 1],
            ["what color is the table", 1],
            ["who is in the photo", 1],
            ["what type of room is this", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["what is in the room", 1],
            ["what color is the wall", 2],
            ["how many people are there", 2],
            ["what is the floor made of", -1],
            ["what is on the wall", 1],
            ["what color is the table", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what type of room is this", 1],
            ["where was this picture taken", 1]
        ],
        "context": [
            "a large group of people sitting at tables eating food.",
            "a living room with a piano and a piano."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2364268",
                "VG_object_id": "642104",
                "bbox": [23, 42, 253, 259],
                "image": "data\\images\\2364268.jpg"
            },
            {
                "VG_image_id": "2358113",
                "VG_object_id": "2053501",
                "bbox": [231, 15, 433, 186],
                "image": "data\\images\\2358113.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's trouser", 1],
            ["what color is the man's shirt", 1]
        ],
        "org_questions": [
            ["what color is the man's trouser", 1],
            ["what color is the man's shirt", 1],
            ["how many people are there", -1],
            ["when is this picture taken", -1],
            ["what is the man standing on", -1],
            ["where is the man", -1],
            ["what is the man wearing", -1],
            ["what sport is this", -1],
            ["who is in the air", -1],
            ["what is the man doing", -1],
            ["what is the person riding", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man riding a skateboard on top of a ramp.",
            "a man doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2366750",
                "VG_object_id": "2309811",
                "bbox": [2, 141, 497, 373],
                "image": "data\\images\\2366750.jpg"
            },
            {
                "VG_image_id": "2369565",
                "VG_object_id": "2183141",
                "bbox": [2, 171, 497, 331],
                "image": "data\\images\\2369565.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 2],
            ["what is the ground covered with", 1],
            ["how many people are there", 1],
            ["what is on the land", 1]
        ],
        "org_questions": [
            ["what is on the ground", -1],
            ["what is the ground covered with", 1],
            ["how many people are there", 1],
            ["what color is the land", -1],
            ["where is the land", -1],
            ["what is in the distance", -1],
            ["what is on the land", 1],
            ["when was this picture taken", -1],
            ["how is the weather", -1],
            ["where was the picture taken", -1],
            ["what is the weather like", -1],
            ["where is the picture taken", 2]
        ],
        "context": [
            "a man is taking a picture of himself.",
            "an elephant standing next to a tree with a wooden fence."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2383780",
                "VG_object_id": "1317141",
                "bbox": [2, 206, 100, 245],
                "image": "data\\images\\2383780.jpg"
            },
            {
                "VG_image_id": "2374077",
                "VG_object_id": "2271549",
                "bbox": [0, 160, 67, 195],
                "image": "data\\images\\2374077.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the plate", 1],
            ["what color is the table", 1],
            ["what kind of food it is", 1],
            ["what color is the food on the plate", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is on the plate", 1],
            ["where is the plate", -1],
            ["what color is the table", 1],
            ["what shape is the plate", -1],
            ["what kind of food it is", 1],
            ["what color is the food on the plate", 1],
            ["what are the plates made of", -1],
            ["where was the photo taken", 1],
            ["how many plates are there", -1]
        ],
        "context": [
            "a table with a plate of food and cups of tea.",
            "two women in a kitchen cooking food together."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2328827",
                "VG_object_id": "3245950",
                "bbox": [8, 169, 493, 367],
                "image": "data\\images\\2328827.jpg"
            },
            {
                "VG_image_id": "2374917",
                "VG_object_id": "1889500",
                "bbox": [10, 155, 485, 367],
                "image": "data\\images\\2374917.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the court", 1],
            ["how many people are there", 1],
            ["what color are the person's shirt", 1],
            ["what is the main color of the ground", 1],
            ["what color is the ground", 1],
            ["How many people are there in the image", 1]
        ],
        "org_questions": [
            ["what color is the court", 1],
            ["how many people are there", 1],
            ["what is the tennis court made of", -1],
            ["what is the persion doing", -1],
            ["what color are the person's shirt", 1],
            ["what is the main color of the ground", 1],
            ["what color is the ground", 1],
            ["How many people are there in the image", 1],
            ["where was this photo taken", -1],
            ["when was the picture taken", -1],
            ["what sport is being played", -1],
            ["who is playing tennis", -1]
        ],
        "context": [
            "a man holding a tennis racquet on a tennis court.",
            "a woman playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2318324",
                "VG_object_id": "2849518",
                "bbox": [128, 30, 356, 364],
                "image": "data\\images\\2318324.jpg"
            },
            {
                "VG_image_id": "2406037",
                "VG_object_id": "1104328",
                "bbox": [78, 72, 331, 405],
                "image": "data\\images\\2406037.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["what is the man's right hand doing", 1],
            ["what are the people doing", 1],
            ["what color is the background", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["what is the man wearing", -1],
            ["what is the man's right hand doing", 1],
            ["what are the people doing", 1],
            ["what color is the background", 1],
            ["where was the photo taken", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man is using a blender in a kitchen.",
            "a man holding two glasses of ice cream"
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2328729",
                "VG_object_id": "2964450",
                "bbox": [11, 80, 118, 209],
                "image": "data\\images\\2328729.jpg"
            },
            {
                "VG_image_id": "2386989",
                "VG_object_id": "679829",
                "bbox": [180, 283, 303, 363],
                "image": "data\\images\\2386989.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bus", 2],
            ["when is this photo taken", 1],
            ["How many decks does the bus have", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the bus", 2],
            ["when is this photo taken", 1],
            ["what color is the ground", -1],
            ["How many decks does the bus have", 1],
            ["what is on the side of the bus", -1],
            ["what is in front of the bus", -1],
            ["how many people are there", -1],
            ["what color of the road does the bus park on", -1],
            ["where is the bus", -1],
            ["what kind of bus is this", -1],
            ["what is on the road", -1],
            ["when was the picture taken", 1]
        ],
        "context": [
            "a large street sign",
            "a red double decker bus driving down a street."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2417117",
                "VG_object_id": "2824520",
                "bbox": [107, 393, 151, 478],
                "image": "data\\images\\2417117.jpg"
            },
            {
                "VG_image_id": "2325932",
                "VG_object_id": "984267",
                "bbox": [127, 91, 374, 486],
                "image": "data\\images\\2325932.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is in the sky", 1],
            ["how many people are there", 1],
            ["what is the woman wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is in the sky", 1],
            ["how many people are there", 1],
            ["where is the woman", -1],
            ["what is the woman wearing on her head", -1],
            ["what is the woman doing", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman standing on", -1],
            ["where was this photo taken", -1],
            ["what is the woman wearing", 1]
        ],
        "context": [
            "a large green kite flying over a lush green field.",
            "a woman throwing a frisbee in a field."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2371999",
                "VG_object_id": "594764",
                "bbox": [4, 164, 286, 216],
                "image": "data\\images\\2371999.jpg"
            },
            {
                "VG_image_id": "2416720",
                "VG_object_id": "3096787",
                "bbox": [393, 12, 495, 141],
                "image": "data\\images\\2416720.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there ", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the car", -1],
            ["what is the car doing", -1],
            ["How many people are there ", 1],
            ["what kind of vehicle are they", -1],
            ["what is on the car", -1],
            ["how many cars are in the picture", -1],
            ["when was the photo taken", 1],
            ["where is the car", -1],
            ["what is parked in the street", -1],
            ["where was the photo taken", -1],
            ["what is in front of the car", -1]
        ],
        "context": [
            "a street scene with bicycles and people on it.",
            "a fire hydrant on the sidewalk next to a fire hydrant."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2396028",
                "VG_object_id": "665387",
                "bbox": [341, 129, 421, 180],
                "image": "data\\images\\2396028.jpg"
            },
            {
                "VG_image_id": "2372097",
                "VG_object_id": "594061",
                "bbox": [1, 404, 83, 494],
                "image": "data\\images\\2372097.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the pillow", 1],
            ["what is the pillow on", 1],
            ["Where is the pillow", 1],
            ["where is the pillow placed on", 1],
            ["what is the pillow placed on", 1],
            ["where are the pillows", 1]
        ],
        "org_questions": [
            ["what color is the pillow", 1],
            ["what is the pillow on", 1],
            ["how many pillows are there", -1],
            ["Where is the pillow", 1],
            ["where is the pillow placed on", 1],
            ["what is the pillow placed on", 1],
            ["what is the pillow made of", -1],
            ["where are the pillows", 1]
        ],
        "context": [
            "a dog standing on the floor next to a person.",
            "a window with a view of a building outside."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2359106",
                "VG_object_id": "3540135",
                "bbox": [271, 115, 388, 203],
                "image": "data\\images\\2359106.jpg"
            },
            {
                "VG_image_id": "2373362",
                "VG_object_id": "732892",
                "bbox": [214, 175, 272, 277],
                "image": "data\\images\\2373362.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["what is the man wearing on his head", 2],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the ground covered with", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is on the man's head", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["how many persons are there", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the man holding", 1],
            ["what is the man wearing on his head", 2]
        ],
        "context": [
            "a group of skateboarders riding down a road.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2348229",
                "VG_object_id": "2632149",
                "bbox": [2, 0, 491, 337],
                "image": "data\\images\\2348229.jpg"
            },
            {
                "VG_image_id": "2405304",
                "VG_object_id": "3812708",
                "bbox": [7, 17, 457, 217],
                "image": "data\\images\\2405304.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the room", 2],
            ["where is the clock", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is in the room", -1],
            ["where is the clock", 1],
            ["how many people are there in the room", 2],
            ["what color is the table", -1],
            ["what is the ground covered with", 1],
            ["how many lights are on in the room", -1],
            ["what color is the wall", -1],
            ["when was this picture taken", -1],
            ["what room is this", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a living room with a piano and a piano.",
            "a group of people sitting around a table eating food."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2411045",
                "VG_object_id": "317294",
                "bbox": [125, 111, 243, 215],
                "image": "data\\images\\2411045.jpg"
            },
            {
                "VG_image_id": "2375007",
                "VG_object_id": "2400403",
                "bbox": [224, 191, 356, 278],
                "image": "data\\images\\2375007.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["what color is the clock", 1],
            ["what time is it", 1],
            ["what time is on the clock", 1]
        ],
        "org_questions": [
            ["what color is the clock", 1],
            ["what color is the building", 2],
            ["what time is it", 1],
            ["what shape is the clock", -1],
            ["where is the clock", -1],
            ["what is the building made of", -1],
            ["what is on the clock", -1],
            ["how many clocks are there", -1],
            ["what time is on the clock", 1],
            ["what is on top of the clock", -1],
            ["what is the clock made of", -1],
            ["what is the shape of the clock", -1]
        ],
        "context": [
            "a clock tower with a bird statue on top.",
            "a clock tower on a building with a sky background"
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2407622",
                "VG_object_id": "276482",
                "bbox": [313, 274, 396, 382],
                "image": "data\\images\\2407622.jpg"
            },
            {
                "VG_image_id": "2351522",
                "VG_object_id": "1889863",
                "bbox": [388, 203, 446, 280],
                "image": "data\\images\\2351522.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the dog", 2],
            ["what color is the dog", 2],
            ["what is the dogs posture", 1],
            ["what is in the background", 1],
            ["what is the ground the dog standing on made of", 1],
            ["what color are the dog's legs", 1],
            ["what is the gound made of", 1],
            ["what is the dog doing", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is the dogs posture", 1],
            ["where is the dog", 2],
            ["what is in the background", 1],
            ["what is the ground the dog standing on made of", 1],
            ["what color are the dog's legs", 1],
            ["what is in the dog's mouth", -1],
            ["what is the gound made of", 1],
            ["what is the dog doing", 1],
            ["what kind of animal is in the picture", -1],
            ["how many dogs are there", -1],
            ["when was the picture taken", -1],
            ["what is the ground covered with", 1],
            ["what color is the dog", 2]
        ],
        "context": [
            "a man is standing next to a dog on a leash.",
            "three people riding horses on a beach with a dog"
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2372270",
                "VG_object_id": "2552630",
                "bbox": [35, 41, 498, 361],
                "image": "data\\images\\2372270.jpg"
            },
            {
                "VG_image_id": "2377857",
                "VG_object_id": "2357728",
                "bbox": [81, 122, 471, 274],
                "image": "data\\images\\2377857.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train's wheels", 2],
            ["how many people are there on the train", 2],
            ["what color is the train", 1]
        ],
        "org_questions": [
            ["what color is the train", 1],
            ["what color is the train's wheels", 2],
            ["how many people are there on the train", 2],
            ["What time is it", -1],
            ["where is the photo taken", -1],
            ["what is the land made of", -1],
            ["what is the train doing", -1],
            ["Where is the train", -1],
            ["what kind of vehicle is this", -1],
            ["what is behind the train", -1],
            ["what is on the train tracks", -1],
            ["what is the train on", -1]
        ],
        "context": [
            "a train on the tracks with a man in a hat.",
            "a train is on the tracks in the mountains."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2345704",
                "VG_object_id": "2407751",
                "bbox": [133, 68, 307, 255],
                "image": "data\\images\\2345704.jpg"
            },
            {
                "VG_image_id": "2346546",
                "VG_object_id": "3616551",
                "bbox": [87, 55, 331, 352],
                "image": "data\\images\\2346546.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's pants", 1],
            ["what color is the surfboard", 1],
            ["how many people are there", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color are the man's pants", 1],
            ["what color is the surfboard", 1],
            ["what color is the water", -1],
            ["how many people are there", 1],
            ["What is person doing", -1],
            ["what is on the person's head", -1],
            ["Where is the person", -1],
            ["what is the person doing", -1],
            ["when was the photo taken", -1],
            ["who is on the surfboard", -1],
            ["what is the man standing on", -1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man riding a surfboard on top of a wave.",
            "a man on a surfboard riding a wave."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2363085",
                "VG_object_id": "768472",
                "bbox": [246, 3, 368, 165],
                "image": "data\\images\\2363085.jpg"
            },
            {
                "VG_image_id": "2379808",
                "VG_object_id": "1354091",
                "bbox": [440, 196, 499, 280],
                "image": "data\\images\\2379808.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's clothes", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's clothes", 1],
            ["who is wearing a bag", -1],
            ["how many people are there", 1],
            ["what is the weather like", -1],
            ["where is the man", 1],
            ["how many hands are holding the racket", -1],
            ["when was the photo taken", -1],
            ["what is the man wearing", -1],
            ["where was the photo taken", 1],
            ["who is in the photo", -1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a man riding a skateboard up the side of a building.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2343410",
                "VG_object_id": "2150159",
                "bbox": [87, 29, 260, 118],
                "image": "data\\images\\2343410.jpg"
            },
            {
                "VG_image_id": "2332439",
                "VG_object_id": "969620",
                "bbox": [5, 155, 106, 240],
                "image": "data\\images\\2332439.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bowl", 1],
            ["what is in the bowl", 1],
            ["what is beside the bowl", 1],
            ["how many people are there", 1],
            ["what is on the bowl", 1],
            ["how many bowls are in the picture", 1],
            ["what kind of food is in the bowl", 1],
            ["what is in the white bowl", 1]
        ],
        "org_questions": [
            ["what color is the bowl", 1],
            ["what is in the bowl", 1],
            ["what is beside the bowl", 1],
            ["how many people are there", 1],
            ["what is the bowl made of", -1],
            ["what is on the bowl", 1],
            ["how many bowls are in the picture", 1],
            ["what color is the table", -1],
            ["where are the bowls", -1],
            ["what kind of food is in the bowl", 1],
            ["what is in the white bowl", 1]
        ],
        "context": [
            "a pizza with meat and cheese on a plate.",
            "a turkey sitting on a rack next to a bowl of soup."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2384154",
                "VG_object_id": "1312633",
                "bbox": [84, 58, 311, 469],
                "image": "data\\images\\2384154.jpg"
            },
            {
                "VG_image_id": "2380671",
                "VG_object_id": "1343845",
                "bbox": [57, 35, 162, 124],
                "image": "data\\images\\2380671.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 1],
            ["how many chairs are in the picture", 1],
            ["where is the chair", 1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["what is next to the chair", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["how many chairs are in the picture", 1],
            ["where is the chair", 1],
            ["where is the photo taken", 1],
            ["how many people are there in the picture", -1],
            ["what is the chair made of ", -1],
            ["what is in the background", 1],
            ["what is next to the chair", 1]
        ],
        "context": [
            "a blue rocking chair with a clock on it.",
            "a slice of pizza on a plate next to a beer."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2352507",
                "VG_object_id": "853522",
                "bbox": [235, 275, 308, 339],
                "image": "data\\images\\2352507.jpg"
            },
            {
                "VG_image_id": "2326531",
                "VG_object_id": "2832622",
                "bbox": [34, 245, 114, 374],
                "image": "data\\images\\2326531.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["who is wearing the trousers", 2],
            ["what color is the bus in the background", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["who is wearing the trousers", 2],
            ["what color is the bus in the background", 1],
            ["what is the persion wearing on his face", -1],
            ["where is the picture taken", -1],
            ["what is the persion doing", 1],
            ["how many people are there", -1],
            ["what is the ground covered with", -1],
            ["when was the photo taken", -1],
            ["what are the people wearing", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a woman standing in front of a school bus.",
            "a man standing in front of a bus with his hand in his pocket."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2326894",
                "VG_object_id": "3169787",
                "bbox": [78, 1, 248, 366],
                "image": "data\\images\\2326894.jpg"
            },
            {
                "VG_image_id": "2402212",
                "VG_object_id": "391733",
                "bbox": [0, 0, 332, 499],
                "image": "data\\images\\2402212.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 2],
            ["What color is floor", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["what color is the floor", 2],
            ["what is on the floor", -1],
            ["what color is the wall", -1],
            ["how many glasses are there", -1],
            ["What is the texture of wall", -1],
            ["What color is floor", 1],
            ["what is on the wall", 1],
            ["where was this photo taken", -1],
            ["what room is this", -1],
            ["what is the wall made of", -1],
            ["what is behind the toilet", -1]
        ],
        "context": [
            "a bathroom with a toilet and a wall paper.",
            "a bathroom with a toilet and a sink."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2379818",
                "VG_object_id": "1353922",
                "bbox": [61, 183, 204, 336],
                "image": "data\\images\\2379818.jpg"
            },
            {
                "VG_image_id": "2362621",
                "VG_object_id": "2309698",
                "bbox": [98, 115, 208, 241],
                "image": "data\\images\\2362621.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is on the bicycle", 2],
            ["how many people are there", 2],
            ["what is the bicycle doing", 1]
        ],
        "org_questions": [
            ["who is on the bicycle", 2],
            ["where is the bicycle", -1],
            ["what is the bicycle doing", 1],
            ["what is the ground under the bicycle made of", -1],
            ["when is this picture taken", -1],
            ["how many people are there", 2],
            ["what is on the bicycle", -1],
            ["What is the background of image", -1],
            ["what type of vehicle is shown", -1],
            ["what color is the bike", -1],
            ["what is parked on the street", -1],
            ["where are the motorcycles parked", -1]
        ],
        "context": [
            "a motorcycle parked on the side of a street.",
            "two police officers on motorcycles on a city street."
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2378231",
                "VG_object_id": "2479949",
                "bbox": [114, 87, 249, 231],
                "image": "data\\images\\2378231.jpg"
            },
            {
                "VG_image_id": "2323224",
                "VG_object_id": "3340193",
                "bbox": [168, 99, 490, 306],
                "image": "data\\images\\2323224.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the motorcycle", 2],
            ["what is in the distance", 1],
            ["what is the ground covered with", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the motorcycle", -1],
            ["where is the motorcycle", 2],
            ["what is in the distance", 1],
            ["how many motorcycles are there", -1],
            ["what is the ground covered with", 1],
            ["where is the photo taken", 1],
            ["what color is the rider's shirt", -1],
            ["where are the motor ", -1],
            ["what is the man doing", -1],
            ["what is the man wearing", -1],
            ["what is the man riding on", -1]
        ],
        "context": [
            "a man riding a motorcycle down a street.",
            "a man riding on the back of a motorcycle in a field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2365434",
                "VG_object_id": "634133",
                "bbox": [158, 7, 342, 255],
                "image": "data\\images\\2365434.jpg"
            },
            {
                "VG_image_id": "2326415",
                "VG_object_id": "3939624",
                "bbox": [1, 46, 222, 417],
                "image": "data\\images\\2326415.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["what color is the table", 1],
            ["how many plates are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what color is the table", 1],
            ["how many plates are there", 1],
            ["what is the woman doing", -1],
            ["Where is the woman", -1],
            ["what is the woman wearing", -1],
            ["What is woman holding", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman sitting on", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a woman sitting at a table with a plate of food.",
            "a woman smiling at a table with a piece of cake."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2346158",
                "VG_object_id": "2231918",
                "bbox": [78, 26, 349, 498],
                "image": "data\\images\\2346158.jpg"
            },
            {
                "VG_image_id": "2414365",
                "VG_object_id": "296001",
                "bbox": [34, 51, 232, 280],
                "image": "data\\images\\2414365.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what is on the man's head", 1],
            ["who is in the photo", 1],
            ["what is on the head on the man", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", -1],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what is on the man's head", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the man wearing", -1],
            ["what is the man looking at", -1],
            ["what is in front of the man", -1],
            ["What is man doing", -1],
            ["what is on the head on the man", 1],
            ["what is on the man's face", 1]
        ],
        "context": [
            "a man holding a baseball bat.",
            "a man and woman sitting at a table with laptops."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2358695",
                "VG_object_id": "3768043",
                "bbox": [33, 3, 142, 332],
                "image": "data\\images\\2358695.jpg"
            },
            {
                "VG_image_id": "2325358",
                "VG_object_id": "3147814",
                "bbox": [95, 16, 275, 498],
                "image": "data\\images\\2325358.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the tower", 1],
            ["What is main color of tower", 1]
        ],
        "org_questions": [
            ["what color is the tower", 1],
            ["how many towers are there", -1],
            ["how many clocks are on the tower", -1],
            ["What time is it", -1],
            ["what is on the top of the tower", -1],
            ["What is main color of tower", 1],
            ["what color is the sky", -1],
            ["what color is the clock on the tower", -1],
            ["where are the clocks", -1],
            ["what is the building made of", -1],
            ["what is in front of the tower", -1],
            ["what is on the building", -1]
        ],
        "context": [
            "a flag and a clock tower with a flag on top.",
            "a tall clock tower with a clock on it's face."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2318139",
                "VG_object_id": "1013739",
                "bbox": [105, 329, 215, 499],
                "image": "data\\images\\2318139.jpg"
            },
            {
                "VG_image_id": "2408819",
                "VG_object_id": "1091443",
                "bbox": [73, 61, 229, 153],
                "image": "data\\images\\2408819.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the gender of the person", 1],
            ["what color is the person's clothes", 1],
            ["where is the person", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what is the gender of the person", 1],
            ["what color is the person's clothes", 1],
            ["where is the person", 1],
            ["How many people are there", 1],
            ["what is the persion wearing", -1],
            ["what is the shirt color", -1]
        ],
        "context": [
            "a group of people sitting at a table eating food.",
            "a man riding a skateboard on top of a wooden ramp."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2374841",
                "VG_object_id": "585408",
                "bbox": [184, 57, 499, 158],
                "image": "data\\images\\2374841.jpg"
            },
            {
                "VG_image_id": "2407443",
                "VG_object_id": "279399",
                "bbox": [12, 3, 500, 192],
                "image": "data\\images\\2407443.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in front of the building", 2],
            ["What is in front of the building", 1],
            ["what is on the ground", 1],
            ["what are the people doing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["how many people are there in front of the building", 2],
            ["What is in front of the building", 1],
            ["what is the weather like", -1],
            ["what is on the ground", 1],
            ["what is at the middle", -1],
            ["when was the picture taken", -1],
            ["where are the people", -1],
            ["what are the people doing", 1],
            ["what is the building made of", -1],
            ["what is behind the man", 1]
        ],
        "context": [
            "a man in a field with a frisbee.",
            "a man talking on a cell phone while standing in a crowd."
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2399423",
                "VG_object_id": "416401",
                "bbox": [240, 199, 389, 292],
                "image": "data\\images\\2399423.jpg"
            },
            {
                "VG_image_id": "2348536",
                "VG_object_id": "1949417",
                "bbox": [195, 51, 476, 243],
                "image": "data\\images\\2348536.jpg"
            }
        ],
        "questions_with_scores": [["what color is the boat", 1]],
        "org_questions": [
            ["what color is the boat", 1],
            ["what is in the distance", -1],
            ["how many boats are there", -1],
            ["where is the boat", -1],
            ["what is the boat doing", -1],
            ["what is the boat placed on", -1],
            ["what is on the boat", -1],
            ["when was the photo taken", -1],
            ["what kind of boat is this", -1],
            ["where was the photo taken", -1],
            ["what is the boat in", -1],
            ["what is the color of the water", -1]
        ],
        "context": [
            "a boat floating on top of a body of water.",
            "a fishing boat is docked in the ocean."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2349842",
                "VG_object_id": "3596764",
                "bbox": [68, 337, 497, 489],
                "image": "data\\images\\2349842.jpg"
            },
            {
                "VG_image_id": "2361627",
                "VG_object_id": "1677931",
                "bbox": [0, 202, 498, 373],
                "image": "data\\images\\2361627.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the ground", 2],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["how many people are there on the ground", 2],
            ["what color is the ground", -1],
            ["what is the ground covered with", -1],
            ["what is on the land", -1],
            ["what is the land covered with", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", 1],
            ["how is the weather", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a horse standing next to a fence in a field.",
            "a woman holding a kite string in a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2376267",
                "VG_object_id": "574447",
                "bbox": [262, 321, 344, 374],
                "image": "data\\images\\2376267.jpg"
            },
            {
                "VG_image_id": "2383053",
                "VG_object_id": "1324050",
                "bbox": [126, 379, 219, 499],
                "image": "data\\images\\2383053.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is man ", 2],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what are the people doing", 1],
            ["what is the gender of the person wearing the trousers", -1],
            ["Where is man ", 2],
            ["who is wearing the trousers", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man and a little boy playing with frisbees.",
            "a young boy in a baseball uniform holding a baseball glove."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2417345",
                "VG_object_id": "3343966",
                "bbox": [76, 147, 486, 311],
                "image": "data\\images\\2417345.jpg"
            },
            {
                "VG_image_id": "2363208",
                "VG_object_id": "2502953",
                "bbox": [0, 220, 228, 331],
                "image": "data\\images\\2363208.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the bed", 2],
            ["what color is the bed", 1],
            ["how many people are there on the bed", 1],
            ["who is on the bed", 1]
        ],
        "org_questions": [
            ["what is on the bed", 2],
            ["what color is the bed", 1],
            ["when is this photo taken", -1],
            ["how many pillows are there on the bed", -1],
            ["where is this bed", -1],
            ["what type of bed is it", -1],
            ["how many people are there on the bed", 1],
            ["who is on the bed", 1],
            ["what is the bed made of", -1],
            ["what is next to the bed", -1]
        ],
        "context": [
            "a woman laying on the floor with a laptop.",
            "a living room with a couch, television and a window."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2378429",
                "VG_object_id": "2338970",
                "bbox": [228, 49, 434, 315],
                "image": "data\\images\\2378429.jpg"
            },
            {
                "VG_image_id": "2343639",
                "VG_object_id": "3703369",
                "bbox": [287, 80, 488, 350],
                "image": "data\\images\\2343639.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["where is the man", 2],
            ["what is the man wearing", 1],
            ["What is man doing", 1],
            ["what is the man looking at", 1],
            ["where is the man looking", 1]
        ],
        "org_questions": [
            ["what is the man holding", 2],
            ["what is the man wearing", 1],
            ["where is the man", 2],
            ["how many people are there", -1],
            ["what color is the man's shirt", -1],
            ["What is man doing", 1],
            ["what is the man wearing on his face", -1],
            ["what is the man wearing on the head", -1],
            ["who is in the photo", -1],
            ["what is the man looking at", 1],
            ["where is the man looking", 1]
        ],
        "context": [
            "two men in aprons are preparing food in a kitchen.",
            "a man and a woman sitting on a bus talking on a cell phone."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2361876",
                "VG_object_id": "1658431",
                "bbox": [289, 288, 379, 368],
                "image": "data\\images\\2361876.jpg"
            },
            {
                "VG_image_id": "2404399",
                "VG_object_id": "343126",
                "bbox": [361, 283, 445, 340],
                "image": "data\\images\\2404399.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bag", 1],
            ["what is the bag made of", 1],
            ["where is the bag", 1],
            ["how many people are there", 1],
            ["what is in the photo", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the bag", 1],
            ["what is the bag made of", 1],
            ["where is the bag", 1],
            ["how many people are there", 1],
            ["what is the person doing", -1],
            ["what is in the photo", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a woman sitting on a couch playing a video game.",
            "a man sitting on a beach under an umbrella."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2416480",
                "VG_object_id": "3487435",
                "bbox": [193, 2, 497, 219],
                "image": "data\\images\\2416480.jpg"
            },
            {
                "VG_image_id": "2392384",
                "VG_object_id": "1230730",
                "bbox": [430, 0, 500, 166],
                "image": "data\\images\\2392384.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 1],
            ["how many people are there", 1],
            ["what color is the building", 1],
            ["what is on the ground", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what is the building made of", -1],
            ["what is the weather like", 1],
            ["how many people are there", 1],
            ["where is the photo taken", -1],
            ["what are the people doing", -1],
            ["what color is the building", 1],
            ["what is in the distance", -1],
            ["when was the photo taken", -1],
            ["what is on the ground", 1],
            ["how is the weather", 1],
            ["what is behind the building", -1]
        ],
        "context": [
            "a couple of people walking down a street holding umbrellas.",
            "a blue bus is stopped at a bus stop."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2361469",
                "VG_object_id": "781652",
                "bbox": [191, 149, 305, 498],
                "image": "data\\images\\2361469.jpg"
            },
            {
                "VG_image_id": "2371072",
                "VG_object_id": "2266188",
                "bbox": [363, 72, 454, 231],
                "image": "data\\images\\2371072.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 2],
            ["How many people are there", 1],
            ["Where is the man", 1],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 2],
            ["How many people are there", 1],
            ["Where is the man", 1],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a man holding a child while standing next to an elephant.",
            "a reflection of a surfboard and people sitting in a window."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2335461",
                "VG_object_id": "962835",
                "bbox": [267, 218, 442, 332],
                "image": "data\\images\\2335461.jpg"
            },
            {
                "VG_image_id": "2362719",
                "VG_object_id": "1700138",
                "bbox": [403, 79, 498, 278],
                "image": "data\\images\\2362719.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 2],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 2],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the woman holding", 1],
            ["what color is the ground", -1],
            ["when was the photo taken", -1],
            ["what is the girl wearing", -1]
        ],
        "context": [
            "a woman is feeding a giraffe at the zoo.",
            "a man holding a skateboard next to a woman."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2352113",
                "VG_object_id": "3585114",
                "bbox": [162, 195, 218, 304],
                "image": "data\\images\\2352113.jpg"
            },
            {
                "VG_image_id": "2376273",
                "VG_object_id": "3686494",
                "bbox": [110, 174, 217, 324],
                "image": "data\\images\\2376273.jpg"
            }
        ],
        "questions_with_scores": [
            ["WHat is man holding", 2],
            ["What color is man's shirt", 1],
            ["What is man doing", 1],
            ["what sport is the man playing", 1],
            ["what is the person doing", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["WHat is man holding", 2],
            ["What color is man's shirt", 1],
            ["What is man doing", 1],
            ["how many people are there", -1],
            ["who is wearing the shirt", -1],
            ["what sport is the man playing", 1],
            ["what is the person doing", 1],
            ["where is the man", 1],
            ["what kind of shirt is the man wearing", -1],
            ["what is on the man's head", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man swinging a tennis racket on a tennis court.",
            "a man holding a frisbee and giving a thumbs up sign."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2402548",
                "VG_object_id": "1134570",
                "bbox": [194, 88, 256, 132],
                "image": "data\\images\\2402548.jpg"
            },
            {
                "VG_image_id": "2346499",
                "VG_object_id": "2564105",
                "bbox": [413, 202, 498, 373],
                "image": "data\\images\\2346499.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 1],
            ["how many people are there", 1],
            ["where is the table", 1],
            ["what is on the desk", 1],
            ["how many chairs are beside the table", 1],
            ["what room is this", 1],
            ["where is the picture taken", 1],
            ["what is in the room", 1],
            ["what is on the table", 1],
            ["how many people are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the wall", -1],
            ["what color is the table", -1],
            ["what is the floor made of", 1],
            ["how many people are there", 1],
            ["what shape is the table", -1],
            ["where is the table", 1],
            ["what is on the desk", 1],
            ["how many chairs are beside the table", 1],
            ["what room is this", 1],
            ["where is the picture taken", 1],
            ["what is in the room", 1],
            ["what is the table made of", -1],
            ["what is on the table", 1],
            ["how many people are in the picture", 1],
            ["how is the table made of", -1]
        ],
        "context": [
            "a kitchen with a stove, a table and a chair.",
            "a group of women playing a video game."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2324928",
                "VG_object_id": "3942434",
                "bbox": [0, 64, 330, 494],
                "image": "data\\images\\2324928.jpg"
            },
            {
                "VG_image_id": "2333374",
                "VG_object_id": "2945951",
                "bbox": [125, 6, 449, 330],
                "image": "data\\images\\2333374.jpg"
            }
        ],
        "questions_with_scores": [["what is the woman wearing", 1]],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what is the woman wearing", 1],
            ["how many people are in the photo", -1],
            ["what is the woman holding", -1],
            ["what is the woman looking at", -1],
            ["who is holding the phone", -1],
            ["where is the woman looking", -1],
            ["who is in the photo", -1],
            ["what color is the woman's hair", -1]
        ],
        "context": [
            "a woman in a green shirt taking a selfie with her cell phone.",
            "a woman in a black coat looking at her cell phone."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2317634",
                "VG_object_id": "1018957",
                "bbox": [93, 264, 432, 375],
                "image": "data\\images\\2317634.jpg"
            },
            {
                "VG_image_id": "2319202",
                "VG_object_id": "1003801",
                "bbox": [10, 176, 496, 375],
                "image": "data\\images\\2319202.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["what animal is on the floor", 1],
            ["what is the floor made of", 1],
            ["where is the photo taken", 1],
            ["what is on the floor", 1],
            ["what type of floor is this", 1],
            ["where is the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what animal is on the floor", 1],
            ["what is the floor made of", 1],
            ["how many tables are there in the picture", -1],
            ["where is the photo taken", 1],
            ["what is the pattern of the floor", -1],
            ["what is on the floor", 1],
            ["what type of floor is this", 1],
            ["where is the floor", 1]
        ],
        "context": [
            "a toilet with a black bag on top of it.",
            "a black and white cat standing next to a black bag."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2324660",
                "VG_object_id": "2710733",
                "bbox": [7, 21, 313, 463],
                "image": "data\\images\\2324660.jpg"
            },
            {
                "VG_image_id": "2352145",
                "VG_object_id": "1664632",
                "bbox": [91, 107, 218, 240],
                "image": "data\\images\\2352145.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's pant", 2],
            ["what color is the clothes", 1]
        ],
        "org_questions": [
            ["what color is the man's pant", 2],
            ["how many people are there", -1],
            ["where is the man", -1],
            ["what is the boy holding", -1],
            ["what is in the background", -1],
            ["what is the boy wearing", -1],
            ["what is the child on", -1],
            ["what color is the clothes", 1],
            ["when was this picture taken", -1],
            ["who is on the skateboard", -1],
            ["what is the man doing", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a skateboarder doing a trick at a skate park.",
            "a young man is jumping a skateboard over a hill."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2359739",
                "VG_object_id": "1866704",
                "bbox": [131, 197, 276, 340],
                "image": "data\\images\\2359739.jpg"
            },
            {
                "VG_image_id": "2389669",
                "VG_object_id": "3828508",
                "bbox": [129, 232, 184, 442],
                "image": "data\\images\\2389669.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the towel", 2],
            ["what color is the wall", 2],
            ["what color is the background", 1],
            ["how many towels are there", 1],
            ["what color is the towel on the shower", 1]
        ],
        "org_questions": [
            ["what color is the towel", 2],
            ["what color is the wall", 2],
            ["what room is the towel in", -1],
            ["what is behind the towel", -1],
            ["Where is the toilet", -1],
            ["what color is the background", 1],
            ["how many towels are there", 1],
            ["what is hanging on the wall", -1],
            ["where was the photo taken", -1],
            ["what is on the towel", -1],
            ["what color is the towel on the shower", 1]
        ],
        "context": [
            "a bathroom with a toilet, window and towel rack.",
            "a bathroom with a toilet, sink and a mirror."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2330589",
                "VG_object_id": "2877487",
                "bbox": [119, 157, 318, 397],
                "image": "data\\images\\2330589.jpg"
            },
            {
                "VG_image_id": "2378619",
                "VG_object_id": "558641",
                "bbox": [358, 46, 433, 159],
                "image": "data\\images\\2378619.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the gender of the person", 1],
            ["what is in the distance", 1],
            ["who is in the picture", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the gender of the person", 1],
            ["where is the photo taken", -1],
            ["what is the persion doing", -1],
            ["what is in the distance", 1],
            ["where is the person", -1],
            ["how many people are in the photo", -1],
            ["who is in the picture", 1],
            ["what is the persion wearing", -1],
            ["what is the main color of the shirt", 1]
        ],
        "context": [
            "a young girl holding a wii remote in her hand.",
            "a man laying on a bed with two cats."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2391904",
                "VG_object_id": "1236194",
                "bbox": [2, 3, 500, 331],
                "image": "data\\images\\2391904.jpg"
            },
            {
                "VG_image_id": "2379129",
                "VG_object_id": "554796",
                "bbox": [0, 3, 498, 499],
                "image": "data\\images\\2379129.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many chips are there", 2],
            ["how many scoops are there", 2],
            ["how many forks are on the table", 1],
            ["how many scoops are on the table", 1]
        ],
        "org_questions": [
            ["how many forks are on the table", 1],
            ["how many chips are on the table", -1],
            ["how many scoops are on the table", 1],
            ["what color is the table", -1],
            ["what shape is the table", -1],
            ["what is on the table", -1],
            ["what is the table made of", -1],
            ["what is the table sitting on", -1],
            ["where was this photo taken", -1],
            ["what is next to the plate", -1],
            ["what is the table color", -1],
            ["how many chips are there", 2],
            ["how many scoops are there", 2]
        ],
        "context": [
            "a sandwich and salad on a plate on a table.",
            "a plate of food with a sandwich and some fries."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2361225",
                "VG_object_id": "2695411",
                "bbox": [217, 184, 312, 331],
                "image": "data\\images\\2361225.jpg"
            },
            {
                "VG_image_id": "2403225",
                "VG_object_id": "355459",
                "bbox": [277, 35, 345, 98],
                "image": "data\\images\\2403225.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what kind of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the gender of the person", -1],
            ["what is the person holding", -1],
            ["where is the photo taken", 1],
            ["when was the picture taken", -1],
            ["what kind of shirt is the man wearing", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man in a blue shirt catching a frisbee.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2404236",
                "VG_object_id": "345052",
                "bbox": [67, 257, 499, 319],
                "image": "data\\images\\2404236.jpg"
            },
            {
                "VG_image_id": "2330182",
                "VG_object_id": "3722581",
                "bbox": [11, 100, 362, 487],
                "image": "data\\images\\2330182.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what is the land covered with", 1],
            ["what is on the land", 1],
            ["where is the picture taken", 1],
            ["what is in the middle of the picture", 1],
            ["how is the weather", 1],
            ["what is the condition of the ground", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is the land covered with", 1],
            ["what is on the land", 1],
            ["how many people are on the land", -1],
            ["where is the picture taken", 1],
            ["what is in the middle of the picture", 1],
            ["when was the photo taken", -1],
            ["how is the weather", 1],
            ["what is the condition of the ground", 1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a jeep is parked on a snowy hill.",
            "a red and white fire hydrant sitting in the grass."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2345826",
                "VG_object_id": "2292749",
                "bbox": [179, 92, 299, 166],
                "image": "data\\images\\2345826.jpg"
            },
            {
                "VG_image_id": "2364498",
                "VG_object_id": "1889656",
                "bbox": [67, 91, 244, 228],
                "image": "data\\images\\2364498.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 2],
            ["what weather is it", 2],
            ["what time is it", 1],
            ["how many people are there", 1],
            ["which part of the car can we see in the picture", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the car", 2],
            ["what weather is it", 2],
            ["what time is it", 1],
            ["how many people are there", 1],
            ["what is on the side of the car", -1],
            ["what is the ground covered with", -1],
            ["which part of the car can we see in the picture", 1],
            ["what is parked on the road", -1],
            ["when was the photo taken", 1],
            ["where was this photo taken", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a row of parked bicycles on a city street.",
            "a man and a woman walking across a crosswalk in the rain."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2323579",
                "VG_object_id": "3068311",
                "bbox": [168, 212, 411, 301],
                "image": "data\\images\\2323579.jpg"
            },
            {
                "VG_image_id": "2379565",
                "VG_object_id": "1356502",
                "bbox": [0, 310, 331, 374],
                "image": "data\\images\\2379565.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the distance", 1],
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["what is on the plate", 1],
            ["what is next to the table", 1]
        ],
        "org_questions": [
            ["what is on the plant", -1],
            ["what is in the distance", 1],
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["what shape is the plate", -1],
            ["where is the plate", -1],
            ["what is on the plate", 1],
            ["what is the table made of", -1],
            ["what is white and white", -1],
            ["what is on top of the table", -1],
            ["what is next to the table", 1]
        ],
        "context": [
            "a baby sitting in a high chair with a piece of cake on it.",
            "a table with a cup of coffee and bread."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2361305",
                "VG_object_id": "2097761",
                "bbox": [21, 19, 221, 296],
                "image": "data\\images\\2361305.jpg"
            },
            {
                "VG_image_id": "2360403",
                "VG_object_id": "2652020",
                "bbox": [138, 251, 229, 484],
                "image": "data\\images\\2360403.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["what is the woman holding", 1],
            ["what is the woman wearing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", -1],
            ["what is the woman doing", 1],
            ["where is the woman", -1],
            ["how many people are there", -1],
            ["what is the woman holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is the woman wearing", 1]
        ],
        "context": [
            "a woman sitting on a bench in a park.",
            "a woman walking down a street with a bag in her hand."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2411587",
                "VG_object_id": "309705",
                "bbox": [230, 194, 461, 323],
                "image": "data\\images\\2411587.jpg"
            },
            {
                "VG_image_id": "2394845",
                "VG_object_id": "460486",
                "bbox": [178, 351, 331, 494],
                "image": "data\\images\\2394845.jpg"
            }
        ],
        "questions_with_scores": [
            ["when is this photo taken", 2],
            ["what is on the ground", 1],
            ["What is the object on the ground", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["when is this photo taken", 2],
            ["what is on the ground", 1],
            ["how many people are there", -1],
            ["where is the land", -1],
            ["what is the land made of", -1],
            ["What is the object on the ground", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a bench in the middle of a street at night.",
            "a man wearing a suit and tie standing in front of a bush."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2382139",
                "VG_object_id": "1330703",
                "bbox": [210, 24, 417, 283],
                "image": "data\\images\\2382139.jpg"
            },
            {
                "VG_image_id": "2387775",
                "VG_object_id": "511194",
                "bbox": [291, 197, 395, 353],
                "image": "data\\images\\2387775.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child doing", 1],
            ["what is the child sitting on", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what is the child wearing", -1],
            ["what is the child doing", 1],
            ["what is the child sitting on", 1],
            ["how many children are there in the picture", -1],
            ["what is the gender of the person in the picture", -1],
            ["who is in the photo", -1],
            ["what is the persion holding", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "two boys sitting on a bunk bed reading a book.",
            "a man holding a wii remote control while others watch."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2361772",
                "VG_object_id": "2530613",
                "bbox": [5, 150, 285, 469],
                "image": "data\\images\\2361772.jpg"
            },
            {
                "VG_image_id": "2414031",
                "VG_object_id": "3800019",
                "bbox": [165, 167, 295, 306],
                "image": "data\\images\\2414031.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what is next to the laptop", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["how many laptops are there", -1],
            ["what is on the screen of the laptop", -1],
            ["where is the laptop", -1],
            ["what color is the laptop's screen", -1],
            ["what is on the laptop's screen", -1],
            ["what color is the laptop", -1],
            ["what type of computer is this", -1],
            ["what is the laptop on", -1],
            ["what is next to the laptop", 1],
            ["what is on the table", -1],
            ["what is in front of the laptop", -1]
        ],
        "context": [
            "a woman is looking at a laptop screen.",
            "a cat sleeping on a desk next to a laptop."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2364577",
                "VG_object_id": "1825397",
                "bbox": [12, 54, 202, 374],
                "image": "data\\images\\2364577.jpg"
            },
            {
                "VG_image_id": "2388502",
                "VG_object_id": "672918",
                "bbox": [20, 13, 341, 331],
                "image": "data\\images\\2388502.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 1],
            ["what is on the side of the street", 1],
            ["what is the man wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What color is man's shirt", 1],
            ["What is man doing", -1],
            ["what is on the side of the street", 1],
            ["where is the man", -1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["what is the man sitting on", -1],
            ["what is on the man's face", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "two men sitting at a table eating food.",
            "a man with a long beard and glasses sitting on a bench."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2380687",
                "VG_object_id": "1343638",
                "bbox": [240, 5, 404, 230],
                "image": "data\\images\\2380687.jpg"
            },
            {
                "VG_image_id": "2357503",
                "VG_object_id": "811617",
                "bbox": [218, 74, 321, 500],
                "image": "data\\images\\2357503.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is on the man's head", 2],
            ["what color is the man's shirt", 1],
            ["what is the man on", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is on the man's head", 2],
            ["what color is the man's shirt", 1],
            ["what animal is the man with", -1],
            ["what is the ground covered with", -1],
            ["where is the man", -1],
            ["what is the man on", 1],
            ["what is the man wearing", 1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a man riding a skateboard next to a dog.",
            "a man in a colorful outfit holding an umbrella."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2347517",
                "VG_object_id": "890122",
                "bbox": [1, 211, 496, 375],
                "image": "data\\images\\2347517.jpg"
            },
            {
                "VG_image_id": "2361825",
                "VG_object_id": "1781112",
                "bbox": [192, 0, 497, 372],
                "image": "data\\images\\2361825.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color of the building", 1]
        ],
        "org_questions": [
            ["what is the main color of the building", 1],
            ["what is on the building", -1],
            ["where is the building", -1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what transportation is there", -1],
            ["How many giraffes are there in the picture", -1],
            ["what is the building made of", -1],
            ["where was the photo taken", -1],
            ["what kind of building is this", -1]
        ],
        "context": [
            "a bunch of street signs on a pole",
            "a street sign on a pole in front of a building."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2405829",
                "VG_object_id": "1106024",
                "bbox": [7, 136, 42, 281],
                "image": "data\\images\\2405829.jpg"
            },
            {
                "VG_image_id": "2319411",
                "VG_object_id": "2877540",
                "bbox": [67, 160, 142, 280],
                "image": "data\\images\\2319411.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person wearing", 2],
            ["what is the person doing", 2],
            ["where is the photo taken", 1],
            ["what is the person on", 1],
            ["what is on the person's head", 1],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the person wearing", 2],
            ["what is the person doing", 2],
            ["where is the photo taken", 1],
            ["what time is it", -1],
            ["what is the person on", 1],
            ["what is on the person's head", 1],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a long table with a blackboard and a chalkboard on it.",
            "a couple of horses pulling a wagon full of people."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2414959",
                "VG_object_id": "148721",
                "bbox": [9, 3, 496, 491],
                "image": "data\\images\\2414959.jpg"
            },
            {
                "VG_image_id": "2320536",
                "VG_object_id": "2953819",
                "bbox": [219, 107, 325, 341],
                "image": "data\\images\\2320536.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what sport is being played", 2],
            ["what color are the man's pants", 1],
            ["what color is the ground", 1],
            ["what is the man doing", 1],
            ["what is in the background", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color are the man's pants", 1],
            ["what color is the ground", 1],
            ["how many people are there", -1],
            ["what is the man doing", 1],
            ["what is in the background", 1],
            ["where is the man", -1],
            ["how old is the man", -1],
            ["when was this picture taken", -1],
            ["what is on the man's head", 1],
            ["who is in the picture", -1],
            ["what sport is being played", 2]
        ],
        "context": [
            "a man in red shirt catching a rugby ball.",
            "a man is swinging a bat at a ball."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2354997",
                "VG_object_id": "1988223",
                "bbox": [168, 72, 273, 302],
                "image": "data\\images\\2354997.jpg"
            },
            {
                "VG_image_id": "2397305",
                "VG_object_id": "1190982",
                "bbox": [7, 392, 49, 498],
                "image": "data\\images\\2397305.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 2],
            ["where is the photo taken", 2],
            ["what color is the person's coat", 1],
            ["how many people are there", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what is the person wearing", -1],
            ["what color is the person's coat", 1],
            ["where is the person", 2],
            ["how many people are there", 1],
            ["what is the gender of the person", -1],
            ["what is the ground covered with", -1],
            ["what is on the person's head", -1],
            ["where is the photo taken", 2],
            ["what are the people doing", -1],
            ["who is in the photo", -1],
            ["when was the picture taken", 1]
        ],
        "context": [
            "a woman is standing in a bathroom with a man in a purple suit.",
            "a group of people standing around a tall tower."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2378953",
                "VG_object_id": "1362566",
                "bbox": [140, 178, 252, 451],
                "image": "data\\images\\2378953.jpg"
            },
            {
                "VG_image_id": "2362072",
                "VG_object_id": "1980129",
                "bbox": [224, 112, 291, 267],
                "image": "data\\images\\2362072.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 2],
            ["what is the man wearing on the head", 2],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is in the background", 2],
            ["what color is the man's clothes", -1],
            ["what is the man wearing", -1],
            ["what is the man wearing on the head", 2],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["where is the man", 1],
            ["what is the persion standing on", 1]
        ],
        "context": [
            "a man standing in a field with a cow.",
            "a couple sitting on a bench looking out to the ocean."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2404936",
                "VG_object_id": "336424",
                "bbox": [37, 105, 321, 296],
                "image": "data\\images\\2404936.jpg"
            },
            {
                "VG_image_id": "2399830",
                "VG_object_id": "1165711",
                "bbox": [160, 170, 425, 499],
                "image": "data\\images\\2399830.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are there", 2],
            ["what is the person doing", 2],
            ["what is on the horse's head", 1],
            ["what is on the horse", 1],
            ["what is behind the horse", 1],
            ["how many horses", 1]
        ],
        "org_questions": [
            ["how many horses are there", 2],
            ["what is the person doing", 2],
            ["what color is the ground", -1],
            ["where is  the horse", -1],
            ["what is in the background", -1],
            ["what is the horse carrying", -1],
            ["who is in the picture", -1],
            ["what is on the horse's head", 1],
            ["what is on the horse", 1],
            ["what is behind the horse", 1],
            ["how many horses", 1]
        ],
        "context": [
            "a woman feeding a horse a carrot in a field.",
            "a man riding a white horse in a field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2379686",
                "VG_object_id": "1355238",
                "bbox": [277, 90, 339, 199],
                "image": "data\\images\\2379686.jpg"
            },
            {
                "VG_image_id": "2417228",
                "VG_object_id": "3297766",
                "bbox": [353, 92, 433, 277],
                "image": "data\\images\\2417228.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 2],
            ["what color is the woman's shirt", 2],
            ["what is the color of the woman's pants", 1],
            ["what is the woman wearing on the head", 1],
            ["what is the woman's posture", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 2],
            ["what color is the woman's shirt", 2],
            ["what is the color of the woman's pants", 1],
            ["how many people are there", -1],
            ["what is the woman wearing on the head", 1],
            ["what is the woman on the left holding", -1],
            ["what is the woman's posture", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a woman and child are feeding cows in a pen.",
            "two people riding horses on a road near a stone wall."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2350676",
                "VG_object_id": "2263752",
                "bbox": [5, 103, 414, 281],
                "image": "data\\images\\2350676.jpg"
            },
            {
                "VG_image_id": "2381596",
                "VG_object_id": "1335466",
                "bbox": [0, 25, 463, 246],
                "image": "data\\images\\2381596.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many planes are there", 2],
            ["what color is the plane's tail", 1]
        ],
        "org_questions": [
            ["how many planes are there", 2],
            ["what color is the plane's tail", 1],
            ["what is the ground covered with", -1],
            ["what is the plane doing", -1],
            ["what is the weather like", -1],
            ["WHat color is the ground", -1],
            ["where was this photo taken", -1],
            ["when was the picture taken", -1],
            ["what is behind the plane", -1],
            ["where is the airplane", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a group of airplanes are on the runway.",
            "a large white airplane on a runway."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2380589",
                "VG_object_id": "1344947",
                "bbox": [205, 81, 345, 359],
                "image": "data\\images\\2380589.jpg"
            },
            {
                "VG_image_id": "2407802",
                "VG_object_id": "273376",
                "bbox": [239, 112, 341, 250],
                "image": "data\\images\\2407802.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is person's shirt", 2],
            ["how many players are there in the photo", 2],
            ["What color is the shirt of the man", 1],
            ["where is the ball", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what is the player doing", -1],
            ["What color is person's shirt", 2],
            ["what is the player holding", -1],
            ["What sport is it", -1],
            ["how many players are there in the photo", 2],
            ["What color is the shirt of the man", 1],
            ["where is the ball", 1],
            ["what is on the player's head", -1],
            ["who is in the picture", -1],
            ["what is the man wearing", -1],
            ["what game is being played", -1]
        ],
        "context": [
            "a man pitching a baseball on a field.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2413769",
                "VG_object_id": "166561",
                "bbox": [240, 97, 293, 146],
                "image": "data\\images\\2413769.jpg"
            },
            {
                "VG_image_id": "2340653",
                "VG_object_id": "3433400",
                "bbox": [140, 65, 237, 145],
                "image": "data\\images\\2340653.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of sport is the man playing", 2],
            ["what sport is the man doing", 1],
            ["where is the picture taken", 1],
            ["What is in the background of image", 1],
            ["What is man doing", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["what sport is the man doing", 1],
            ["where is the picture taken", 1],
            ["How many people are there", -1],
            ["What is in the background of image", 1],
            ["What is man doing", 1],
            ["what kind of sport is the man playing", 2],
            ["when was the photo taken", -1],
            ["how is the weather", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1],
            ["what is the man on", 1]
        ],
        "context": [
            "a man riding a wave on top of a surfboard.",
            "a man flying through the air while riding a skateboard."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2369447",
                "VG_object_id": "611277",
                "bbox": [76, 122, 139, 178],
                "image": "data\\images\\2369447.jpg"
            },
            {
                "VG_image_id": "2398460",
                "VG_object_id": "1179634",
                "bbox": [174, 95, 309, 200],
                "image": "data\\images\\2398460.jpg"
            }
        ],
        "questions_with_scores": [
            ["what device does the screen belong to", 2],
            ["what is the main color on the screen", 1],
            ["how many people are there", 1],
            ["What color is the screen", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the main color on the screen", 1],
            ["what device does the screen belong to", 2],
            ["how many people are there", 1],
            ["how many phones are in the picture", -1],
            ["What color is the screen", 1],
            ["when was the photo taken", -1],
            ["where is the picture taken", -1],
            ["who is in the photo", 1],
            ["how is the photo", -1]
        ],
        "context": [
            "a black desk with a computer on top of it.",
            "a cell phone with a picture of a woman on it."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2368152",
                "VG_object_id": "2752410",
                "bbox": [0, 355, 332, 496],
                "image": "data\\images\\2368152.jpg"
            },
            {
                "VG_image_id": "2328757",
                "VG_object_id": "1046083",
                "bbox": [18, 203, 460, 499],
                "image": "data\\images\\2328757.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["What is the background of photo", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", -1],
            ["How many people are there", -1],
            ["what is the table made of", -1],
            ["What is the background of photo", 1],
            ["how many plates are there on the table", -1],
            ["what shape is the table", -1],
            ["what is the table sitting on", -1],
            ["how is the table made", -1],
            ["where was the photo taken", 1],
            ["what material is the table", -1]
        ],
        "context": [
            "a blue jug on display in a museum.",
            "a cup and saucer on a table at the beach."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2376721",
                "VG_object_id": "570209",
                "bbox": [119, 189, 205, 266],
                "image": "data\\images\\2376721.jpg"
            },
            {
                "VG_image_id": "2405590",
                "VG_object_id": "1107043",
                "bbox": [197, 84, 316, 161],
                "image": "data\\images\\2405590.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the bowl", 2],
            ["Where is the bowl", 2],
            ["how many people are there", 1],
            ["what is the bowl made of", 1],
            ["what is beside the bowl", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["What color is the bowl", 2],
            ["Where is the bowl", 2],
            ["What is in the bowl", -1],
            ["how many people are there", 1],
            ["what is the bowl made of", 1],
            ["what the bowl is on", -1],
            ["how many bowls are there in the picture", -1],
            ["what is beside the bowl", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a woman sitting on a bench eating a doughnut.",
            "a desk with a laptop, keyboard, mouse and a mouse."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2354948",
                "VG_object_id": "834202",
                "bbox": [1, 305, 496, 374],
                "image": "data\\images\\2354948.jpg"
            },
            {
                "VG_image_id": "2351847",
                "VG_object_id": "2810456",
                "bbox": [0, 364, 368, 497],
                "image": "data\\images\\2351847.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["where is the photo taken", 2],
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["how many people are in the picture", 1],
            ["what is standing on the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["what is the person doing", 2],
            ["how many people are in the picture", 1],
            ["where is the photo taken", 2],
            ["what is the floor made of", -1],
            ["what is standing on the floor", 1],
            ["what is the ground covered with", -1],
            ["what type of floor is this", -1],
            ["where is the floor", -1]
        ],
        "context": [
            "a man standing in a living room holding a nintendo wii controller.",
            "a woman sitting on a bench with her luggage."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2376720",
                "VG_object_id": "2358914",
                "bbox": [212, 322, 441, 372],
                "image": "data\\images\\2376720.jpg"
            },
            {
                "VG_image_id": "2372331",
                "VG_object_id": "592490",
                "bbox": [47, 395, 330, 499],
                "image": "data\\images\\2372331.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 1],
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["what room is the floor in", 1],
            ["how is the floor made", 1]
        ],
        "org_questions": [
            ["what is the floor made of", 1],
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["how many doors are there", -1],
            ["where is the picture taken", -1],
            ["what room is the floor in", 1],
            ["how many people are there on the floor", -1],
            ["what shape is the floor", -1],
            ["how is the floor made", 1]
        ],
        "context": [
            "a public restroom with a sink and toilet.",
            "a cat playing with a toy on the floor"
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2410752",
                "VG_object_id": "321581",
                "bbox": [315, 28, 430, 190],
                "image": "data\\images\\2410752.jpg"
            },
            {
                "VG_image_id": "2317879",
                "VG_object_id": "3005258",
                "bbox": [1, 6, 332, 389],
                "image": "data\\images\\2317879.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the guy doing", 2],
            ["what is the person riding", 2],
            ["what color is the guy's shirt", 1],
            ["what color is the guy's trousers", 1],
            ["what is the guy standing on ", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the guy doing", 2],
            ["what color is the guy's shirt", 1],
            ["what color is the guy's trousers", 1],
            ["how many people are there", -1],
            ["what is the man wearing on his face", -1],
            ["where is the guy", -1],
            ["what is the guy standing on ", 1],
            ["what is the man holding", 1],
            ["when was this photo taken", -1],
            ["who is in the picture", -1],
            ["what is the man wearing", -1],
            ["what is the person riding", 2]
        ],
        "context": [
            "a man riding a bike down a street.",
            "a man doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2319070",
                "VG_object_id": "3299379",
                "bbox": [1, 32, 498, 330],
                "image": "data\\images\\2319070.jpg"
            },
            {
                "VG_image_id": "2351077",
                "VG_object_id": "2260460",
                "bbox": [88, 118, 298, 223],
                "image": "data\\images\\2351077.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 1],
            ["what shape is the tray", 1],
            ["what is the tray made of", 1],
            ["what is under the pizza", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what color is the table", -1],
            ["where is this photo taken", -1],
            ["what shape is the tray", 1],
            ["what is on the tray", -1],
            ["what is the tray made of", 1],
            ["what is in the tray", -1],
            ["how many pizzas are there", -1],
            ["what is the pizza sitting on", -1],
            ["what kind of food is on the table", -1],
            ["where is the pizza", -1],
            ["what is under the pizza", 1]
        ],
        "context": [
            "a pizza with cheese and spinach on a plate.",
            "a table with a plate of food and a bowl of salad."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2362335",
                "VG_object_id": "775090",
                "bbox": [76, 278, 175, 499],
                "image": "data\\images\\2362335.jpg"
            },
            {
                "VG_image_id": "2371300",
                "VG_object_id": "2589141",
                "bbox": [109, 128, 274, 473],
                "image": "data\\images\\2371300.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the tie", 2],
            ["what color is the shirt", 2],
            ["what type of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the tie", 2],
            ["what color is the shirt", 2],
            ["how many people are there", -1],
            ["what shape is the necktie", -1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["how many neckties are there in the photo", -1],
            ["who is wearing a tie", -1],
            ["what is around the man's neck", -1],
            ["what type of shirt is the man wearing", 1],
            ["what is the man wearing", -1],
            ["what is on the man's shirt", -1]
        ],
        "context": [
            "a man with a beard and glasses smiling.",
            "a man wearing a tie with a picture of a bird on it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2354639",
                "VG_object_id": "836892",
                "bbox": [244, 141, 466, 396],
                "image": "data\\images\\2354639.jpg"
            },
            {
                "VG_image_id": "2410752",
                "VG_object_id": "321582",
                "bbox": [315, 31, 427, 193],
                "image": "data\\images\\2410752.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many men are there", 1],
            ["what is the ground covered with", 1],
            ["what is the persion sitting on", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many men are there", 1],
            ["what is the ground covered with", 1],
            ["what is the man wearing", -1],
            ["what is the persion sitting on", 1],
            ["when was the photo taken", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man shearing a sheep with a black hat.",
            "a man riding a bike down a street."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2373451",
                "VG_object_id": "2644329",
                "bbox": [69, 109, 181, 326],
                "image": "data\\images\\2373451.jpg"
            },
            {
                "VG_image_id": "2320536",
                "VG_object_id": "2827638",
                "bbox": [215, 104, 333, 349],
                "image": "data\\images\\2320536.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the person's head", 2],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what color is the person's shirt", 1],
            ["what is the persion holding", 1],
            ["who is in the picture", 1],
            ["what sport is being played", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what are the people doing", -1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is on the person's head", 2],
            ["what color is the person's shirt", 1],
            ["what is the persion holding", 1],
            ["what is the person wearing", -1],
            ["when was this picture taken", -1],
            ["who is in the picture", 1],
            ["what sport is being played", 1],
            ["what is the man standing on", 1]
        ],
        "context": [
            "a group of people playing frisbee in a park.",
            "a man is swinging a bat at a ball."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2388162",
                "VG_object_id": "675206",
                "bbox": [56, 35, 499, 310],
                "image": "data\\images\\2388162.jpg"
            },
            {
                "VG_image_id": "2363152",
                "VG_object_id": "767689",
                "bbox": [6, 393, 283, 478],
                "image": "data\\images\\2363152.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the seat", 1],
            ["how many motorcycles are there", 1],
            ["what is in the photo", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what is on the seat", 1],
            ["where is the seat", -1],
            ["how many people are there", -1],
            ["how many different colors do the seats have", -1],
            ["how many motorcycles are there", 1],
            ["what color are the seats", -1],
            ["when was the photo taken", -1],
            ["what is in the photo", 1],
            ["where is the photo taken", 1]
        ],
        "context": [
            "a piece of luggage sitting on top of a car seat.",
            "a cat sitting on a motorcycle seat."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2334034",
                "VG_object_id": "3296265",
                "bbox": [253, 29, 340, 104],
                "image": "data\\images\\2334034.jpg"
            },
            {
                "VG_image_id": "2319182",
                "VG_object_id": "1004023",
                "bbox": [97, 344, 158, 418],
                "image": "data\\images\\2319182.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person doing", 2],
            ["what is the gender of the person", 1],
            ["who is in the photo", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person doing", 2],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the gender of the person", 1],
            ["where is the person", -1],
            ["what is the color of the man's pants", -1],
            ["who is in the photo", 1],
            ["when was the photo taken", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man jumping a skateboard over some steps.",
            "a woman standing next to a giant chair with a horse on top."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2412502",
                "VG_object_id": "3312047",
                "bbox": [215, 223, 428, 304],
                "image": "data\\images\\2412502.jpg"
            },
            {
                "VG_image_id": "2366279",
                "VG_object_id": "2140277",
                "bbox": [154, 216, 337, 324],
                "image": "data\\images\\2366279.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what color is the wall", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what color is the wall", 1],
            ["How many plates are there", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", 1],
            ["what room is this", -1],
            ["what is in the room", -1],
            ["what is on the table", -1],
            ["what is the table made out of", -1]
        ],
        "context": [
            "a living room with two couches and a coffee table.",
            "a kitchen with a table, refrigerator, and refrigerator."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2359930",
                "VG_object_id": "2299408",
                "bbox": [220, 53, 313, 306],
                "image": "data\\images\\2359930.jpg"
            },
            {
                "VG_image_id": "2414763",
                "VG_object_id": "152455",
                "bbox": [363, 100, 465, 220],
                "image": "data\\images\\2414763.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's jacket", 1],
            ["when is the picture taken", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his head", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the person's jacket", 1],
            ["when is the picture taken", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his head", 1],
            ["what is the man doing", -1],
            ["Where is the man", -1],
            ["what kind of clothes is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is the person holding", -1],
            ["what is on the person's feet", -1],
            ["what is the person wearing", -1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "two people on skis in the snow at night.",
            "a group of people jumping on skis in the air."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2359622",
                "VG_object_id": "1745842",
                "bbox": [76, 381, 269, 498],
                "image": "data\\images\\2359622.jpg"
            },
            {
                "VG_image_id": "2388430",
                "VG_object_id": "673422",
                "bbox": [174, 115, 269, 271],
                "image": "data\\images\\2388430.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the horse", 1],
            ["how many horses are in the picture", 1],
            ["where is the horse", 1],
            ["what is the land made of ", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the horse", 1],
            ["how many horses are in the picture", 1],
            ["where is the horse", 1],
            ["what is the land made of ", 1],
            ["what is behind the horse", -1],
            ["what is the horse standing on", -1],
            ["how many people are there in the picture", 1],
            ["what are the horses doing", -1],
            ["what is on the horse", -1],
            ["what is the horse pulling", -1]
        ],
        "context": [
            "a group of horses standing in front of a building.",
            "a horse and carriage traveling down a country road."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2395539",
                "VG_object_id": "3823005",
                "bbox": [9, 159, 395, 372],
                "image": "data\\images\\2395539.jpg"
            },
            {
                "VG_image_id": "2355077",
                "VG_object_id": "3241029",
                "bbox": [254, 178, 479, 331],
                "image": "data\\images\\2355077.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["What is man doing", 2],
            ["where is the person", 1],
            ["what is the persion holding", 1],
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["what color is the jacket", 1],
            ["what is the color of the shirt", 1],
            ["what color is the shirt of the man in the foreground", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what color is the man's shirt", 2],
            ["What is man doing", 2],
            ["where is the person", 1],
            ["what is the persion holding", 1],
            ["what is the man wearing", -1],
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["what color is the jacket", 1],
            ["what is the color of the shirt", 1],
            ["what color is the shirt of the man in the foreground", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a man sitting at a table with a cup of coffee.",
            "a boy looks out the window of a train as it passes by."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2361998",
                "VG_object_id": "3229694",
                "bbox": [341, 20, 499, 177],
                "image": "data\\images\\2361998.jpg"
            },
            {
                "VG_image_id": "2353305",
                "VG_object_id": "847376",
                "bbox": [123, 143, 245, 280],
                "image": "data\\images\\2353305.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is in the distance", 1],
            ["how many people are there", 1],
            ["where are the people", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is in the distance", 1],
            ["what shape is the shirt's collar", -1],
            ["how many people are there", 1],
            ["where are the people", 1],
            ["what pattern is the shirt", -1],
            ["when was this photo taken", -1],
            ["who is in the picture", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man and a little girl playing tee ball",
            "a woman flying a kite on the beach."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2393720",
                "VG_object_id": "1217508",
                "bbox": [215, 89, 271, 292],
                "image": "data\\images\\2393720.jpg"
            },
            {
                "VG_image_id": "2353772",
                "VG_object_id": "843279",
                "bbox": [156, 214, 252, 485],
                "image": "data\\images\\2353772.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the background", 2],
            ["what is the man wearing on the head", 2],
            ["how many people are there in the picture", 2],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["where is the man", -1],
            ["what color is the background", 2],
            ["how many people are there", -1],
            ["what is the man wearing on the head", 2],
            ["what is the man doing", -1],
            ["what gesture is the man", -1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the man standing on", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a group of people standing outside of a food truck.",
            "a man holding an umbrella standing in the water."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2329125",
                "VG_object_id": "3699408",
                "bbox": [4, 349, 279, 496],
                "image": "data\\images\\2329125.jpg"
            },
            {
                "VG_image_id": "2387045",
                "VG_object_id": "1279939",
                "bbox": [42, 392, 181, 497],
                "image": "data\\images\\2387045.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 1],
            ["what kind of flooring is this", 1],
            ["what material is the floor made of", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is on the floor", -1],
            ["How many people are there in the picture", -1],
            ["Where is the picture taken", -1],
            ["What is the pattern of the floor", -1],
            ["what is the floor made of", 1],
            ["what is the flooring", -1],
            ["what kind of flooring is this", 1],
            ["what is in the room", -1],
            ["what is covering the floor", -1],
            ["what material is the floor made of", 1]
        ],
        "context": [
            "a refrigerator with magnets on it in a kitchen.",
            "a gray suitcase sitting on top of a wooden floor."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2391897",
                "VG_object_id": "1236258",
                "bbox": [104, 27, 186, 368],
                "image": "data\\images\\2391897.jpg"
            },
            {
                "VG_image_id": "2403127",
                "VG_object_id": "657519",
                "bbox": [88, 78, 237, 457],
                "image": "data\\images\\2403127.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the tower", 2],
            ["What color is the clock", 2],
            ["What is main color of tower", 1],
            ["what time is it", 1]
        ],
        "org_questions": [
            ["What color is the tower", 2],
            ["What color is the clock", 2],
            ["how many clocks are on the tower", -1],
            ["what is behind the tower", -1],
            ["where is the clock", -1],
            ["what is on the top of the tower", -1],
            ["What is main color of tower", 1],
            ["what time is it", 1],
            ["what is the tower made of", -1],
            ["when was the photo taken", -1],
            ["what is on the clock", -1]
        ],
        "context": [
            "a clock tower in a large city.",
            "a colorful clock tower in a parking lot."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2384782",
                "VG_object_id": "1305027",
                "bbox": [1, 337, 374, 499],
                "image": "data\\images\\2384782.jpg"
            },
            {
                "VG_image_id": "2361628",
                "VG_object_id": "1722995",
                "bbox": [82, 297, 269, 496],
                "image": "data\\images\\2361628.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["where is the photo taken", 1],
            ["how many people are there in the picture", 1],
            ["where was the photo taken", 1],
            ["where is the table", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is the table made of", -1],
            ["what is on the table", 1],
            ["how many keyboards are there on the table", -1],
            ["where is the photo taken", 1],
            ["how many people are there in the picture", 1],
            ["what is in the background", -1],
            ["where was the photo taken", 1],
            ["where is the table", 1]
        ],
        "context": [
            "a metal cup holding toothbrushes in it.",
            "a little girl writing on a piece of paper."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2386962",
                "VG_object_id": "679937",
                "bbox": [108, 64, 382, 341],
                "image": "data\\images\\2386962.jpg"
            },
            {
                "VG_image_id": "2378219",
                "VG_object_id": "3671328",
                "bbox": [22, 29, 462, 475],
                "image": "data\\images\\2378219.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many forks are there on the plate", 1],
            ["how many knives are there on the plate", 1],
            ["What food is on the plate", 1],
            ["what kind of food is this", 1]
        ],
        "org_questions": [
            ["how many forks are there on the plate", 1],
            ["how many knives are there on the plate", 1],
            ["What food is on the plate", 1],
            ["where are the plates", -1],
            ["what is on the plates", -1],
            ["where was the photo taken", -1],
            ["what kind of food is this", 1]
        ],
        "context": [
            "a sandwich on a plate with a banana and a carton of juice",
            "a sandwich and a knife on a plate with a knife."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2390619",
                "VG_object_id": "495461",
                "bbox": [97, 118, 216, 307],
                "image": "data\\images\\2390619.jpg"
            },
            {
                "VG_image_id": "2323224",
                "VG_object_id": "3151122",
                "bbox": [167, 119, 469, 330],
                "image": "data\\images\\2323224.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is next to the motorcycle", 2],
            ["What is background of image", 2],
            ["Where is the bicycle", 1],
            ["What is the background of image", 1],
            ["what color is the bicycle", 1],
            ["what is the bicycle leaning on", 1],
            ["what is on the back of the motorcycle", 1]
        ],
        "org_questions": [
            ["Where is the bicycle", 1],
            ["How many people are there", -1],
            ["What is the background of image", 1],
            ["what color is the bicycle", 1],
            ["what is on the motorcycle", -1],
            ["what is the bicycle leaning on", 1],
            ["what is next to the motorcycle", 2],
            ["What is background of image", 2],
            ["what type of vehicle is shown", -1],
            ["what is the man doing", -1],
            ["what is the man riding", -1],
            ["what is on the back of the motorcycle", 1]
        ],
        "context": [
            "a man and a woman riding a motorcycle.",
            "a man riding on the back of a motorcycle in a field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2401026",
                "VG_object_id": "1152460",
                "bbox": [298, 136, 414, 270],
                "image": "data\\images\\2401026.jpg"
            },
            {
                "VG_image_id": "2349738",
                "VG_object_id": "1882536",
                "bbox": [145, 127, 235, 213],
                "image": "data\\images\\2349738.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the boy's shirt", 2],
            ["What sport is the boy doing", 1],
            ["where is the photo taken", 1],
            ["what is the man doing", 1],
            ["where is the person", 1],
            ["who is wearing a white shirt", 1]
        ],
        "org_questions": [
            ["What color is the boy's shirt", 2],
            ["What sport is the boy doing", 1],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["what is the gender of the person", -1],
            ["what is the man doing", 1],
            ["where is the person", 1],
            ["what is the man wearing on his head", -1],
            ["who is wearing a white shirt", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man and a woman walking down a street with a child on a skateboard.",
            "a baseball player swinging a bat at a ball"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2344664",
                "VG_object_id": "2847626",
                "bbox": [171, 238, 301, 487],
                "image": "data\\images\\2344664.jpg"
            },
            {
                "VG_image_id": "2407012",
                "VG_object_id": "286834",
                "bbox": [35, 151, 254, 499],
                "image": "data\\images\\2407012.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is on the man's head", 1],
            ["how many people are there", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is on the man's head", 1],
            ["how many people are there", 1],
            ["how is the weather", -1],
            ["where is the man", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is behind the man", 1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a man reaching up to catch a frisbee."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2339739",
                "VG_object_id": "2282760",
                "bbox": [4, 258, 497, 332],
                "image": "data\\images\\2339739.jpg"
            },
            {
                "VG_image_id": "2359428",
                "VG_object_id": "1702183",
                "bbox": [24, 211, 492, 318],
                "image": "data\\images\\2359428.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bulls are there on the ground", 2],
            ["what is in the background", 1],
            ["what color is the sky", 1],
            ["where was the photo taken", 1],
            ["what is green", 1]
        ],
        "org_questions": [
            ["what is on the field", -1],
            ["what is in the background", 1],
            ["what color is the sky", 1],
            ["how many bulls are there on the ground", 2],
            ["what is the ground covered with", -1],
            ["what is the color of the grass", -1],
            ["where was the photo taken", 1],
            ["what is the weather like", -1],
            ["what is the green stuff on the ground", -1],
            ["what is green", 1]
        ],
        "context": [
            "a stop sign on a black post in a field.",
            "cows grazing in a field with a castle in the background."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2349366",
                "VG_object_id": "875306",
                "bbox": [157, 165, 339, 444],
                "image": "data\\images\\2349366.jpg"
            },
            {
                "VG_image_id": "2370381",
                "VG_object_id": "3858876",
                "bbox": [281, 153, 337, 203],
                "image": "data\\images\\2370381.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman doing", 2],
            ["what is the woman holding", 2],
            ["what color is the shirt", 1],
            ["what is the person doing", 1],
            ["what is the girl wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the person doing", 1],
            ["who is wearing the shirt", -1],
            ["what is hanging on the shirt", -1],
            ["how is the weather", -1],
            ["What is woman doing", 2],
            ["when was the photo taken", -1],
            ["how many people are in the photo", -1],
            ["what is the woman holding", 2],
            ["what is the girl wearing", 1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a woman holding a tennis racket on a court.",
            "a dog sitting next to a girl on a bench."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2407393",
                "VG_object_id": "280394",
                "bbox": [360, 5, 487, 129],
                "image": "data\\images\\2407393.jpg"
            },
            {
                "VG_image_id": "2368261",
                "VG_object_id": "2566207",
                "bbox": [254, 0, 398, 188],
                "image": "data\\images\\2368261.jpg"
            }
        ],
        "questions_with_scores": [
            ["when is this photo taken", 2],
            ["what is the color of the building", 1],
            ["how many people are there", 1],
            ["who is in the picture", 1],
            ["what is on the street", 1]
        ],
        "org_questions": [
            ["what is the color of the building", 1],
            ["how many people are there", 1],
            ["when is this photo taken", 2],
            ["what is the wall of the building made of", -1],
            ["what is in front of the building", -1],
            ["what is the building made of", -1],
            ["what color is the ground in front of the building", -1],
            ["where was this picture taken", -1],
            ["who is in the picture", 1],
            ["where are the buildings", -1],
            ["what is on the street", 1]
        ],
        "context": [
            "a tree with a sign on it next to a tree.",
            "a double decker bus driving down a street."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2341628",
                "VG_object_id": "3215433",
                "bbox": [70, 97, 241, 347],
                "image": "data\\images\\2341628.jpg"
            },
            {
                "VG_image_id": "2338494",
                "VG_object_id": "3321642",
                "bbox": [67, 177, 251, 332],
                "image": "data\\images\\2338494.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 1],
            ["how many people are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what color is the ground", -1],
            ["how many people are in the picture", 1],
            ["Where is the horse", -1],
            ["what is in the distance", -1],
            ["What is the horse doing", -1],
            ["How many horses are there", -1],
            ["what is the color of the horse", -1],
            ["what type of animal is shown", -1],
            ["what is on the horse's head", -1],
            ["who is on the horse", -1]
        ],
        "context": [
            "a man riding a horse in a field with people watching.",
            "a person riding a horse in a corral."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2378608",
                "VG_object_id": "2043902",
                "bbox": [256, 234, 316, 374],
                "image": "data\\images\\2378608.jpg"
            },
            {
                "VG_image_id": "2363066",
                "VG_object_id": "2314585",
                "bbox": [332, 15, 396, 102],
                "image": "data\\images\\2363066.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bottle", 1],
            ["where is the bottle", 1],
            ["what is in the background", 1],
            ["How many bottles are there", 1],
            ["what is on the right side of the photo", 1]
        ],
        "org_questions": [
            ["what color is the bottle", 1],
            ["where is the bottle", 1],
            ["what is in the background", 1],
            ["How many bottles are there", 1],
            ["what  is the bottle made of", -1],
            ["when was the photo taken", -1],
            ["what is on the right side of the photo", 1]
        ],
        "context": [
            "a woman sitting on a train while talking on a cell phone.",
            "a bathroom sink with a mirror"
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2366279",
                "VG_object_id": "2774565",
                "bbox": [26, 198, 105, 352],
                "image": "data\\images\\2366279.jpg"
            },
            {
                "VG_image_id": "2409003",
                "VG_object_id": "250895",
                "bbox": [310, 4, 490, 80],
                "image": "data\\images\\2409003.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the cabinet", 2],
            ["WHat color is the wall", 2]
        ],
        "org_questions": [
            ["What color is the cabinet", 2],
            ["WHat color is the wall", 2],
            ["what is hanging on the cabinet", -1],
            ["what is on the wall", -1],
            ["where is the cabinet", -1],
            ["what room is this", -1],
            ["what are the cabinets made of", -1],
            ["where was this picture taken", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a kitchen with a table, refrigerator, and refrigerator.",
            "a kitchen with a stove, refrigerator, and a stove."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2358215",
                "VG_object_id": "2035949",
                "bbox": [329, 179, 424, 304],
                "image": "data\\images\\2358215.jpg"
            },
            {
                "VG_image_id": "2358892",
                "VG_object_id": "3541508",
                "bbox": [142, 127, 345, 327],
                "image": "data\\images\\2358892.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", -1],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["what is the gender of the person", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a family sitting at a table with a plate of food.",
            "a man is brushing his teeth in a park."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2371099",
                "VG_object_id": "1787285",
                "bbox": [2, 305, 373, 498],
                "image": "data\\images\\2371099.jpg"
            },
            {
                "VG_image_id": "2337785",
                "VG_object_id": "3734818",
                "bbox": [0, 393, 331, 498],
                "image": "data\\images\\2337785.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 2],
            ["what is the land made of", 1],
            ["what is in the foreground", 1],
            ["what is the condition of the ground", 1]
        ],
        "org_questions": [
            ["what color is the land", 2],
            ["how many giraffes are in the picture", -1],
            ["what time is it", -1],
            ["where is the land", -1],
            ["what is the land made of", 1],
            ["what is the weather like", -1],
            ["what is in the foreground", 1],
            ["how is the grass", -1],
            ["what is on the ground", -1],
            ["what is the condition of the ground", 1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a giraffe standing next to a tree in a fenced in area.",
            "a giraffe standing next to a large rock."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2384612",
                "VG_object_id": "525578",
                "bbox": [384, 286, 449, 332],
                "image": "data\\images\\2384612.jpg"
            },
            {
                "VG_image_id": "2396629",
                "VG_object_id": "442109",
                "bbox": [321, 191, 392, 237],
                "image": "data\\images\\2396629.jpg"
            }
        ],
        "questions_with_scores": [
            ["when is this photo taken", 2],
            ["what color is the truck", 1],
            ["where is the truck", 1],
            ["where was the photo taken", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the truck", 1],
            ["what is behind the truck", -1],
            ["where is the truck", 1],
            ["how many people are there", -1],
            ["when is this photo taken", 2],
            ["What is the weather like", -1],
            ["what is the ground covered with", -1],
            ["what is on the truck", -1],
            ["what kind of vehicle is in the picture", -1],
            ["where was the photo taken", 1],
            ["what type of vehicle is shown", -1],
            ["when was the picture taken", 1]
        ],
        "context": [
            "a large jetliner sitting on top of an airport tarmac.",
            "a city at night with a car parked in front of it."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2405431",
                "VG_object_id": "1107865",
                "bbox": [130, 77, 357, 304],
                "image": "data\\images\\2405431.jpg"
            },
            {
                "VG_image_id": "2388894",
                "VG_object_id": "669930",
                "bbox": [213, 310, 326, 499],
                "image": "data\\images\\2388894.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bowl", 1],
            ["what is the bowl made of", 1],
            ["where is the bowl", 1],
            ["what is beside the bowl", 1],
            ["where was the photo taken", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the bowl", 1],
            ["what is the bowl made of", 1],
            ["where is the bowl", 1],
            ["how many bowls are in the picture", -1],
            ["what the bowl is on", -1],
            ["what is beside the bowl", 1],
            ["What is in the bowl", -1],
            ["where was the photo taken", 1],
            ["what is on the table", -1],
            ["how many people are there", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "apples and a bowl of apples on a counter.",
            "a man making a clay pot on a pottery wheel"
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2337870",
                "VG_object_id": "3704998",
                "bbox": [5, 0, 455, 122],
                "image": "data\\images\\2337870.jpg"
            },
            {
                "VG_image_id": "2336230",
                "VG_object_id": "960831",
                "bbox": [299, 3, 498, 237],
                "image": "data\\images\\2336230.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the curtain", -1],
            ["what is the pattern on the curtain", -1],
            ["what color is the wall", 1],
            ["how many people are there", 1],
            ["where is the picture taken", -1],
            ["where is the curtain", -1],
            ["what is the wall made of", -1],
            ["what is behind the window", -1],
            ["what is in the room", -1],
            ["what is hanging on the wall", -1]
        ],
        "context": [
            "a man and a child reading a book in bed.",
            "a teddy bear sitting on top of a pile of pillows."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2395382",
                "VG_object_id": "455224",
                "bbox": [43, 0, 276, 115],
                "image": "data\\images\\2395382.jpg"
            },
            {
                "VG_image_id": "2367799",
                "VG_object_id": "750117",
                "bbox": [33, 118, 242, 215],
                "image": "data\\images\\2367799.jpg"
            }
        ],
        "questions_with_scores": [["what is in front of the building", 1]],
        "org_questions": [
            ["what color is the building", -1],
            ["what is in front of the building", 1],
            ["what is in the background", -1],
            ["How many people are there", -1],
            ["what is the weather like", -1],
            ["What is the building made of", -1],
            ["what is on the building", -1],
            ["how many doors are in the building", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a cow standing in a pen with a building in the background.",
            "a bridge over a river with a clock tower."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2357510",
                "VG_object_id": "2111768",
                "bbox": [277, 206, 484, 296],
                "image": "data\\images\\2357510.jpg"
            },
            {
                "VG_image_id": "2361128",
                "VG_object_id": "2273712",
                "bbox": [18, 199, 304, 249],
                "image": "data\\images\\2361128.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["where is the board", 1],
            ["What is the background of image", 1],
            ["what is on the back of the man's head", 1],
            ["what is the person on", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["where is the board", 1],
            ["what is the ground the board on made of", -1],
            ["how many people are there in the photo", -1],
            ["what direction does the board face", -1],
            ["What is the background of image", 1],
            ["what direction is the board heading to", -1],
            ["how many boards are there in the photo", -1],
            ["what is on the back of the man's head", 1],
            ["what is the man doing", 2],
            ["what is the person on", 1],
            ["what is on the ground", -1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a man riding a bike in a parking lot.",
            "a man sitting on the beach with a surfboard."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2364663",
                "VG_object_id": "639856",
                "bbox": [411, 272, 468, 404],
                "image": "data\\images\\2364663.jpg"
            },
            {
                "VG_image_id": "2324008",
                "VG_object_id": "988640",
                "bbox": [317, 6, 368, 55],
                "image": "data\\images\\2324008.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["where is the person", 1],
            ["what is the ground covered with", 1],
            ["what is the man doing", 1],
            ["what color is the person's shirt", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["where is the person", 1],
            ["what is the person doing", 2],
            ["what is the person wearing", -1],
            ["what is the ground covered with", 1],
            ["what is the man doing", 1],
            ["what color is the person's shirt", 1],
            ["what is the man holding", 1],
            ["how many people are shown", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a train is parked at a train station.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2317030",
                "VG_object_id": "3664194",
                "bbox": [99, 124, 454, 319],
                "image": "data\\images\\2317030.jpg"
            },
            {
                "VG_image_id": "2317482",
                "VG_object_id": "2713291",
                "bbox": [223, 133, 325, 364],
                "image": "data\\images\\2317482.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what sport is being played", 2],
            ["what is the child doing", 1],
            ["what color is the child's shirt", 1],
            ["what color are the child's pants", 1],
            ["What is child holding", 1],
            ["What is the gender of person", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the child doing", 1],
            ["what color is the child's shirt", 1],
            ["what color are the child's pants", 1],
            ["how many people are there", 2],
            ["What is child holding", 1],
            ["What is the gender of person", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what sport is being played", 2]
        ],
        "context": [
            "a group of people standing on top of a lush green field.",
            "a young girl playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2362928",
                "VG_object_id": "1857954",
                "bbox": [248, 177, 429, 226],
                "image": "data\\images\\2362928.jpg"
            },
            {
                "VG_image_id": "2386316",
                "VG_object_id": "682613",
                "bbox": [120, 258, 182, 330],
                "image": "data\\images\\2386316.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many hot dogs are there", 2],
            ["where is the hot dog", 2]
        ],
        "org_questions": [
            ["how many hot dogs are there", 2],
            ["where is the hot dog", 2],
            ["what kind of food is this", -1],
            ["what is the man holding", -1],
            ["where is the picture taken", -1],
            ["what is on the sandwich", -1],
            ["what is in the man's hand", -1]
        ],
        "context": [
            "a man holding a plate of hot dogs covered in mustard.",
            "a man eating a hot dog in a tent."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2410340",
                "VG_object_id": "220119",
                "bbox": [113, 117, 267, 250],
                "image": "data\\images\\2410340.jpg"
            },
            {
                "VG_image_id": "2346072",
                "VG_object_id": "902769",
                "bbox": [299, 202, 478, 332],
                "image": "data\\images\\2346072.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trouser", 1],
            ["what is the man doing", 1],
            ["what is on the person's head", 1],
            ["WHat are people doing", 1],
            ["what color are the man's pants", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 1],
            ["what is the man wearing", -1],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["what is on the person's head", 1],
            ["WHat are people doing", 1],
            ["what type of pants is the man wearing", -1],
            ["what color are the man's pants", 1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a man and a cat sitting on top of a hill."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2386449",
                "VG_object_id": "682101",
                "bbox": [34, 51, 227, 257],
                "image": "data\\images\\2386449.jpg"
            },
            {
                "VG_image_id": "2358666",
                "VG_object_id": "2623907",
                "bbox": [156, 108, 336, 276],
                "image": "data\\images\\2358666.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 2],
            ["where is the dog", 2],
            ["what is the dog doing", 1],
            ["what is the ground covered with", 1],
            ["What is the dog looking at", 1],
            ["what gesture is the dog", 1],
            ["what is behind the dog", 1]
        ],
        "org_questions": [
            ["what color is the dog", 2],
            ["where is the dog", 2],
            ["what is the dog doing", 1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["what is the dog wearing", -1],
            ["What is the dog looking at", 1],
            ["what gesture is the dog", 1],
            ["what animal is in the picture", -1],
            ["what is behind the dog", 1],
            ["what is on the dog's face", -1],
            ["what is the dog on", -1]
        ],
        "context": [
            "a dog laying on the floor next to a teddy bear.",
            "a dog jumping up to catch a frisbee."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2397096",
                "VG_object_id": "1192816",
                "bbox": [85, 67, 174, 136],
                "image": "data\\images\\2397096.jpg"
            },
            {
                "VG_image_id": "2337984",
                "VG_object_id": "955764",
                "bbox": [282, 321, 369, 401],
                "image": "data\\images\\2337984.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many planes are in  the picture", 2],
            ["what is in the distance", 1],
            ["what color is the body of the plane", 1],
            ["what is the number of planes", 1]
        ],
        "org_questions": [
            ["how many planes are in  the picture", 2],
            ["what is in the distance", 1],
            ["what color is the sky", -1],
            ["when is the picture taken", -1],
            ["what color is the body of the plane", 1],
            ["what is beside the plane", -1],
            ["what direction is the plane facing", -1],
            ["what are the planes doing", -1],
            ["who is flying the plane", -1],
            ["where are the planes", -1],
            ["what is the number of planes", 1]
        ],
        "context": [
            "a plane flying over a house with power lines.",
            "a group of planes flying in formation in the sky."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2355545",
                "VG_object_id": "1709414",
                "bbox": [0, 0, 63, 244],
                "image": "data\\images\\2355545.jpg"
            },
            {
                "VG_image_id": "2331997",
                "VG_object_id": "3212603",
                "bbox": [276, 41, 363, 161],
                "image": "data\\images\\2331997.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there is the picture", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["what is on the building", -1],
            ["How many people are there is the picture", 1],
            ["where is the building", -1],
            ["what is the building made of", -1],
            ["what is in front of the building", -1],
            ["How many motorcycles are there in the picture", -1],
            ["what time of day is it", -1],
            ["when was the picture taken", -1],
            ["what is on the right side of the building", -1],
            ["what is in the background", -1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a sign on a sidewalk next to a street.",
            "a person riding a bike on a city street."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2414191",
                "VG_object_id": "158118",
                "bbox": [28, 150, 360, 451],
                "image": "data\\images\\2414191.jpg"
            },
            {
                "VG_image_id": "2374035",
                "VG_object_id": "2543534",
                "bbox": [1, 258, 193, 498],
                "image": "data\\images\\2374035.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the bicycle", 2],
            ["What is the background of image", 1],
            ["how many people are there", 1],
            ["what is the ground under the bicycle made of", 1],
            ["where is the bike", 1],
            ["what is the floor made of", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["What color is the bicycle", 2],
            ["What is the background of image", 1],
            ["how many people are there", 1],
            ["when is this picture taken", -1],
            ["what is the ground under the bicycle made of", 1],
            ["what is the bicycle doing", -1],
            ["what is on the bike", -1],
            ["where is the bike", 1],
            ["what is behind the bike", -1],
            ["what is the floor made of", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a pink bike parked next to a parking meter.",
            "a man standing next to a bike in a room."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2410087",
                "VG_object_id": "226255",
                "bbox": [181, 103, 389, 259],
                "image": "data\\images\\2410087.jpg"
            },
            {
                "VG_image_id": "2337635",
                "VG_object_id": "2382699",
                "bbox": [18, 173, 471, 369],
                "image": "data\\images\\2337635.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are in the picture", 2],
            ["what is in the background", 1],
            ["what main color is the background", 1]
        ],
        "org_questions": [
            ["how many zebras are in the picture", 2],
            ["what is in the background", 1],
            ["what color is the grass", -1],
            ["where is the zebra", -1],
            ["what is the zebras doing", -1],
            ["what is the zebra eating", -1],
            ["what main color is the background", 1],
            ["what type of animal is shown", -1],
            ["when was the picture taken", -1],
            ["what is the zebra standing on", -1],
            ["where was the photo taken", -1],
            ["what color are the zebras", -1]
        ],
        "context": [
            "a zebra grazing on grass in a field.",
            "a group of zebras and giraffes in a zoo enclosure."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2399190",
                "VG_object_id": "418411",
                "bbox": [331, 145, 384, 275],
                "image": "data\\images\\2399190.jpg"
            },
            {
                "VG_image_id": "2363936",
                "VG_object_id": "3742556",
                "bbox": [339, 44, 419, 218],
                "image": "data\\images\\2363936.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["where is the photo taken", 1],
            ["what is the man wearing", 1],
            ["What is man holding", 1],
            ["Where is the man", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["what is the man wearing", 1],
            ["What is man holding", 1],
            ["Where is the man", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is on the man's head", 1],
            ["what is the man carrying", -1]
        ],
        "context": [
            "a man standing next to a small airplane.",
            "a group of police officers riding horses down a street."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2402306",
                "VG_object_id": "390813",
                "bbox": [0, 1, 497, 57],
                "image": "data\\images\\2402306.jpg"
            },
            {
                "VG_image_id": "2404236",
                "VG_object_id": "345048",
                "bbox": [28, 139, 264, 190],
                "image": "data\\images\\2404236.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what is in the distance", -1],
            ["what color is the ground", 1],
            ["what is on the ground", 1],
            ["how many trees are there in the distance", -1],
            ["What time is it", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", -1],
            ["how is the weather", -1],
            ["when was this photo taken", -1],
            ["where are the trees", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a train traveling down tracks next to a mountain.",
            "a jeep is parked on a snowy hill."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2365619",
                "VG_object_id": "2359848",
                "bbox": [7, 4, 498, 329],
                "image": "data\\images\\2365619.jpg"
            },
            {
                "VG_image_id": "2403251",
                "VG_object_id": "382687",
                "bbox": [1, 53, 351, 431],
                "image": "data\\images\\2403251.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["what color is the sofa", 1],
            ["how many cats are there on the sofa", 1],
            ["What is in front of sofa", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 1],
            ["what is on the sofa", -1],
            ["how many cats are there on the sofa", 1],
            ["what is the pattern on the sofa", -1],
            ["What is in front of sofa", 1],
            ["How many people are there", 2],
            ["who is in the photo", -1],
            ["what is the cat doing", -1],
            ["what is the cat sitting on", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a woman sitting on a couch with a cat.",
            "a black cat laying on a couch with its head on the couch."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2376445",
                "VG_object_id": "3685199",
                "bbox": [66, 109, 189, 339],
                "image": "data\\images\\2376445.jpg"
            },
            {
                "VG_image_id": "2358627",
                "VG_object_id": "800667",
                "bbox": [186, 128, 274, 285],
                "image": "data\\images\\2358627.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2]
        ],
        "org_questions": [
            ["how many people are there in the picture", 2],
            ["what is the child doing", -1],
            ["where is the person", -1],
            ["what is the child on", -1],
            ["who is playing", -1],
            ["what sport is being played", -1],
            ["what is the man wearing", -1],
            ["what is the man playing", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "two men are walking on a tennis court.",
            "a young boy playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2371906",
                "VG_object_id": "3517852",
                "bbox": [187, 26, 257, 106],
                "image": "data\\images\\2371906.jpg"
            },
            {
                "VG_image_id": "2367931",
                "VG_object_id": "2167444",
                "bbox": [148, 240, 298, 321],
                "image": "data\\images\\2367931.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the board", 1],
            ["what is in the background", 1],
            ["what is the board on", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the board", -1],
            ["where is the board", 1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what is the board on", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a sculpture of a kangaroo sitting on a bench.",
            "a desk with two monitors and a microphone."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2323479",
                "VG_object_id": "3525949",
                "bbox": [98, 6, 166, 197],
                "image": "data\\images\\2323479.jpg"
            },
            {
                "VG_image_id": "2334940",
                "VG_object_id": "3673007",
                "bbox": [245, 0, 458, 374],
                "image": "data\\images\\2334940.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the building", 2],
            ["what is the color of the sky", 2],
            ["how many clocks are in the picture", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what is the color of the building", 2],
            ["what is the color of the sky", 2],
            ["how many clocks are in the picture", 1],
            ["What time is it", -1],
            ["what is on the top of the tower", -1],
            ["what is the tower made of", -1],
            ["what is beside the tower", -1],
            ["What is on the building", -1],
            ["how is the weather", 1],
            ["where are the windows", -1],
            ["when was the photo taken", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a stop sign in front of a church.",
            "a clock tower on a building in the city."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2360444",
                "VG_object_id": "2045681",
                "bbox": [228, 296, 490, 464],
                "image": "data\\images\\2360444.jpg"
            },
            {
                "VG_image_id": "2321949",
                "VG_object_id": "1047060",
                "bbox": [54, 185, 290, 294],
                "image": "data\\images\\2321949.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the cars", 2],
            ["how many cars are in the picture", 1],
            ["what is on top of the car", 1]
        ],
        "org_questions": [
            ["what color are the cars", 2],
            ["what color is the road", -1],
            ["What time is it", -1],
            ["how many cars are in the picture", 1],
            ["where is the car", -1],
            ["how is the weather", -1],
            ["When is the photo taken", -1],
            ["what is on the side of the car", -1],
            ["what kind of car is this", -1],
            ["what is on the road", -1],
            ["what is on top of the car", 1]
        ],
        "context": [
            "a food truck and a food truck on a city street.",
            "a car that is parked in a parking lot."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2408746",
                "VG_object_id": "255813",
                "bbox": [42, 64, 159, 190],
                "image": "data\\images\\2408746.jpg"
            },
            {
                "VG_image_id": "2391243",
                "VG_object_id": "490498",
                "bbox": [3, 0, 287, 188],
                "image": "data\\images\\2391243.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cabinet", 2],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", 2],
            ["where is the cabinet", -1],
            ["how many people are there in the picture", -1],
            ["what room is the cabinet in", -1],
            ["who is in the photo", 1],
            ["what is in the background", -1],
            ["where was the photo taken", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a young boy sitting at a table with a plate of food.",
            "a woman standing in a kitchen making food."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2327135",
                "VG_object_id": "2965135",
                "bbox": [8, 165, 466, 280],
                "image": "data\\images\\2327135.jpg"
            },
            {
                "VG_image_id": "1160119",
                "VG_object_id": "1601782",
                "bbox": [0, 776, 960, 1275],
                "image": "data\\images\\1160119.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are on the table", 2],
            ["how many people are in the picture", 2],
            ["What is next to the table", 1]
        ],
        "org_questions": [
            ["how many plates are on the table", 2],
            ["how many people are in the picture", 2],
            ["What is next to the table", 1],
            ["what is the table made of", -1],
            ["What is on the plate", -1],
            ["where was this photo taken", -1],
            ["where are the plates", -1],
            ["what is covering the table", -1]
        ],
        "context": [
            "a table with a large silver bowl and plates on it.",
            "a man sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2357379",
                "VG_object_id": "3554350",
                "bbox": [5, 108, 498, 331],
                "image": "data\\images\\2357379.jpg"
            },
            {
                "VG_image_id": "2400335",
                "VG_object_id": "409540",
                "bbox": [347, 211, 448, 276],
                "image": "data\\images\\2400335.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the blanket placed on", 1],
            ["how many people are there", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is the blanket placed on", 1],
            ["what color is the blanket", -1],
            ["when is this photo taken", -1],
            ["how many people are there", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a child laying on a bed eating food.",
            "a living room with a couch, coffee table and a television."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2402889",
                "VG_object_id": "385369",
                "bbox": [1, 220, 498, 329],
                "image": "data\\images\\2402889.jpg"
            },
            {
                "VG_image_id": "2368863",
                "VG_object_id": "615599",
                "bbox": [3, 336, 374, 499],
                "image": "data\\images\\2368863.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the weather like", 2],
            ["how is the weather", 2],
            ["What is in the middle of photo", 1]
        ],
        "org_questions": [
            ["What is on the road", -1],
            ["What is the weather like", 2],
            ["What is in the middle of photo", 1],
            ["how many people are there", -1],
            ["when is the photo taken", -1],
            ["what time is it", -1],
            ["where was this picture taken", -1],
            ["what is the road made of", -1],
            ["where are the white lines", -1],
            ["how is the weather", 2]
        ],
        "context": [
            "a red van parked on the side of a road.",
            "a stop sign covered in snow on a street."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2404975",
                "VG_object_id": "335976",
                "bbox": [136, 68, 437, 109],
                "image": "data\\images\\2404975.jpg"
            },
            {
                "VG_image_id": "2344962",
                "VG_object_id": "2264655",
                "bbox": [254, 13, 497, 339],
                "image": "data\\images\\2344962.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the shelf", 2],
            ["how many people are in the picture", 1],
            ["What is in the shelf", 1],
            ["what is near the shelf", 1]
        ],
        "org_questions": [
            ["what is on the shelf", 2],
            ["how many people are in the picture", 1],
            ["where is the shelf", -1],
            ["what is in front of the shelf", -1],
            ["What is in the shelf", 1],
            ["what is near the shelf", 1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a display case filled with lots of different types of pastries.",
            "a man and a woman looking at cell phones in a store."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2387301",
                "VG_object_id": "1277352",
                "bbox": [50, 89, 146, 228],
                "image": "data\\images\\2387301.jpg"
            },
            {
                "VG_image_id": "2397363",
                "VG_object_id": "434818",
                "bbox": [20, 92, 143, 272],
                "image": "data\\images\\2397363.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing on her face", 2],
            ["what is the persion doing", 1],
            ["what color is the woman's shirt", 1]
        ],
        "org_questions": [
            ["what are on the table", -1],
            ["what color is the table", -1],
            ["what is the woman wearing on her face", 2],
            ["how many people are there", -1],
            ["where is the woman", -1],
            ["what is the persion doing", 1],
            ["what is the woman holding", -1],
            ["what is the woman wearing", -1],
            ["what is the gender of the person in the picture", -1],
            ["who is wearing a black shirt", -1],
            ["who is in the photo", -1],
            ["what color is the woman's shirt", 1]
        ],
        "context": [
            "a woman cutting a cake with two young girls.",
            "a group of people sitting at a table eating pizza."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2397533",
                "VG_object_id": "433126",
                "bbox": [65, 8, 467, 313],
                "image": "data\\images\\2397533.jpg"
            },
            {
                "VG_image_id": "2345241",
                "VG_object_id": "909701",
                "bbox": [378, 22, 489, 225],
                "image": "data\\images\\2345241.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is on the man's head", 1],
            ["what is the man wearing on his face", 1],
            ["what is the man wearing on the head", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is on the man's head", 1],
            ["how many people are there", -1],
            ["what is the man holding", -1],
            ["what is the man wearing", -1],
            ["what is the man wearing on his face", 1],
            ["what is the man wearing on the head", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["where is the man", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man in a uniform is playing soccer.",
            "a frisbee flying through the air in a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2368707",
                "VG_object_id": "2621549",
                "bbox": [155, 54, 385, 317],
                "image": "data\\images\\2368707.jpg"
            },
            {
                "VG_image_id": "2350270",
                "VG_object_id": "869029",
                "bbox": [27, 1, 326, 328],
                "image": "data\\images\\2350270.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the man doing", 2],
            ["how many people are there", 2],
            ["what color is the man's shirt", 1],
            ["What is man holding", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["what is the man doing", 1],
            ["who is in the photo", 1],
            ["what sport is this", 1]
        ],
        "org_questions": [
            ["what sport is the man doing", 2],
            ["what color is the man's shirt", 1],
            ["how many people are there", 2],
            ["What is man holding", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["what is the man doing", 1],
            ["what is the man weaing on his neck", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", 1],
            ["where was the photo taken", -1],
            ["what sport is this", 1]
        ],
        "context": [
            "a group of young men playing a game of frisbee.",
            "a man in a red baseball uniform swinging a bat."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2409182",
                "VG_object_id": "247249",
                "bbox": [348, 202, 493, 253],
                "image": "data\\images\\2409182.jpg"
            },
            {
                "VG_image_id": "2408667",
                "VG_object_id": "3808848",
                "bbox": [64, 289, 179, 359],
                "image": "data\\images\\2408667.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 1],
            ["what is beside the sink", 1],
            ["where is the sink", 1]
        ],
        "org_questions": [
            ["where is the picture taken", 1],
            ["what is above the sink", -1],
            ["how many taps are there on the sink", -1],
            ["what shape is the sink", -1],
            ["what is beside the sink", 1],
            ["where is the sink", 1]
        ],
        "context": [
            "a kitchen with a wooden table and chairs.",
            "a bathroom with a sink, mirror, and a bathtub."
        ]
    },
    {
        "object_category": "skier",
        "images": [
            {
                "VG_image_id": "2372413",
                "VG_object_id": "592041",
                "bbox": [137, 122, 263, 430],
                "image": "data\\images\\2372413.jpg"
            },
            {
                "VG_image_id": "2331183",
                "VG_object_id": "3312336",
                "bbox": [263, 39, 360, 296],
                "image": "data\\images\\2331183.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the person's trousers", 2],
            ["what is the gender of the person", 1],
            ["how many people are there", 1],
            ["who is in the photo", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color are the person's clothes", -1],
            ["what color are the person's trousers", 2],
            ["what is the gender of the person", 1],
            ["how many people are there", 1],
            ["where is the person", -1],
            ["what is on the ground", -1],
            ["what is the skier doing", -1],
            ["what time is it", -1],
            ["who is in the photo", 1],
            ["what is the persion wearing", 1],
            ["when was this photo taken", -1],
            ["what is the persion holding", -1]
        ],
        "context": [
            "a man on skis standing on a snowy slope",
            "two people standing on skis in the snow."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2366686",
                "VG_object_id": "3050610",
                "bbox": [4, 68, 129, 280],
                "image": "data\\images\\2366686.jpg"
            },
            {
                "VG_image_id": "2320316",
                "VG_object_id": "994266",
                "bbox": [324, 2, 381, 109],
                "image": "data\\images\\2320316.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in front of the building", 1],
            ["what color is the vehicle in front of the building", 1],
            ["how many people are there", 1],
            ["what is on the side of the building", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["what is in front of the building", 1],
            ["what color is the vehicle in front of the building", 1],
            ["how many people are there", 1],
            ["what time is it", -1],
            ["what is the building made of", -1],
            ["how is the weather", -1],
            ["when was this photo taken", -1],
            ["what is on the side of the building", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a blue train is on the tracks near a building.",
            "a school bus is parked on the side of the road."
        ]
    },
    {
        "object_category": "bird",
        "images": [
            {
                "VG_image_id": "2316495",
                "VG_object_id": "2796123",
                "bbox": [303, 112, 413, 207],
                "image": "data\\images\\2316495.jpg"
            },
            {
                "VG_image_id": "2374014",
                "VG_object_id": "729346",
                "bbox": [299, 14, 378, 110],
                "image": "data\\images\\2374014.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bird", 1],
            ["what is the bird doing", 1],
            ["what is the bird standing on", 1],
            ["when was this photo taken", 1],
            ["where was the photo taken", 1],
            ["what is in the background", 1],
            ["what is the main color of the background", 1]
        ],
        "org_questions": [
            ["what color is the bird", -1],
            ["where is the bird", 1],
            ["how many birds are there", -1],
            ["what is the bird doing", 1],
            ["what is the bird standing on", 1],
            ["what kind of animal is this", -1],
            ["when was this photo taken", 1],
            ["who is in the photo", -1],
            ["where was the photo taken", 1],
            ["how long is the beak of the bird", -1],
            ["what is in the background", 1],
            ["what is the main color of the background", 1],
            ["when was the photo taken", -1],
            ["what type of bird is shown", -1],
            ["what is the bird in the photo", -1]
        ],
        "context": [
            "a bird sitting on a boat in the water.",
            "a city with a lot of buildings in the background."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2413440",
                "VG_object_id": "173506",
                "bbox": [216, 159, 273, 210],
                "image": "data\\images\\2413440.jpg"
            },
            {
                "VG_image_id": "2391708",
                "VG_object_id": "1238273",
                "bbox": [321, 26, 436, 128],
                "image": "data\\images\\2391708.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the elephant standing", 2],
            ["how many elephants are there in the picture", 2],
            ["what is behind the elephant", 1],
            ["what is in the background", 1],
            ["what is the elephant standing on", 1]
        ],
        "org_questions": [
            ["where is the elephant standing", 2],
            ["how many elephants are there in the picture", 2],
            ["what is on the elephant's back", -1],
            ["What is elephant doing", -1],
            ["what is behind the elephant", 1],
            ["what is in the background", 1],
            ["when was the picture taken", -1],
            ["what animal is in the picture", -1],
            ["what color are the elephants", -1],
            ["who is in the photo", -1],
            ["what is the elephant standing on", 1]
        ],
        "context": [
            "an elephant standing in the middle of a river.",
            "a white truck driving down a road next to a herd of elephants."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2358252",
                "VG_object_id": "804660",
                "bbox": [78, 75, 200, 325],
                "image": "data\\images\\2358252.jpg"
            },
            {
                "VG_image_id": "2339067",
                "VG_object_id": "2673872",
                "bbox": [117, 0, 327, 342],
                "image": "data\\images\\2339067.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing on her head", 2],
            ["what is the woman wearing", 2],
            ["what is the woman doing", 1],
            ["what is the color of the woman's shirt", 1],
            ["where is the woman", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what is the color of the woman's shirt", 1],
            ["what is the woman on", -1],
            ["how many people are there", -1],
            ["what is the woman wearing on her head", 2],
            ["what is the woman on the left holding", -1],
            ["what is the woman wearing", 2],
            ["What is woman holding", -1],
            ["when was this picture taken", -1],
            ["who is in the picture", -1],
            ["where is the woman", 1],
            ["what is the persion holding", -1]
        ],
        "context": [
            "a man and a woman sitting under an umbrella.",
            "a woman is surfing on a yellow surfboard."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2415398",
                "VG_object_id": "1056057",
                "bbox": [25, 119, 140, 169],
                "image": "data\\images\\2415398.jpg"
            },
            {
                "VG_image_id": "2394212",
                "VG_object_id": "466141",
                "bbox": [343, 166, 457, 224],
                "image": "data\\images\\2394212.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["where was the photo taken", 2],
            ["what color is the ground under the chair", 1],
            ["how many people are there", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["what color is the ground under the chair", 1],
            ["how many people are there", 1],
            ["who is seating on the chair", -1],
            ["What is above the chair", -1],
            ["when was the picture taken", -1],
            ["where was the photo taken", 2],
            ["what is in the background", 1]
        ],
        "context": [
            "a woman playing tennis on a clay court.",
            "a woman walking on a beach next to a man."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2333806",
                "VG_object_id": "3273248",
                "bbox": [280, 5, 403, 98],
                "image": "data\\images\\2333806.jpg"
            },
            {
                "VG_image_id": "2317361",
                "VG_object_id": "2796889",
                "bbox": [230, 250, 475, 470],
                "image": "data\\images\\2317361.jpg"
            }
        ],
        "questions_with_scores": [
            ["where are the pillows", 2],
            ["how many pillows are there", 2],
            ["what color are the pillows", 1],
            ["what room is the pillow in", 1],
            ["where is the pillow", 1],
            ["what color is the wall", 1],
            ["what is the main color of the pillow", 1]
        ],
        "org_questions": [
            ["what color are the pillows", 1],
            ["where are the pillows", 2],
            ["how many pillows are there", 2],
            ["what room is the pillow in", 1],
            ["what is the pillow on", -1],
            ["where is the pillow", 1],
            ["what color is the wall", 1],
            ["what is the main color of the pillow", 1]
        ],
        "context": [
            "a young girl laying on a bed with white sheets.",
            "a living room with a couch, table, and chairs."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316781",
                "VG_object_id": "2772478",
                "bbox": [0, 80, 294, 499],
                "image": "data\\images\\2316781.jpg"
            },
            {
                "VG_image_id": "2378782",
                "VG_object_id": "1364807",
                "bbox": [66, 50, 449, 375],
                "image": "data\\images\\2378782.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man wearing on his head", 2],
            ["what is on the man's face", 2],
            ["What is the background of image", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", -1],
            ["What is man wearing on his head", 2],
            ["What is the background of image", 1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["What is the man doing", -1],
            ["what is the man wearing", 1],
            ["who is in the picture", -1],
            ["what is around the man's neck", -1],
            ["what is behind the man", 1],
            ["what is on the man's face", 2]
        ],
        "context": [
            "a man wearing a white shirt and sunglasses.",
            "a man in a suit and bow tie standing in front of a building."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2388894",
                "VG_object_id": "669934",
                "bbox": [1, 342, 142, 426],
                "image": "data\\images\\2388894.jpg"
            },
            {
                "VG_image_id": "2379360",
                "VG_object_id": "3835093",
                "bbox": [173, 3, 284, 176],
                "image": "data\\images\\2379360.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the towel", 2],
            ["where is the towel", 1],
            ["what is hanging on the wall", 1],
            ["what is below the towel", 1],
            ["what color is the wall", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the towel", 2],
            ["where is the towel", 1],
            ["what is hanging on the wall", 1],
            ["what is below the towel", 1],
            ["what color is the wall", 1],
            ["how many people are there", -1],
            ["where is the picture taken", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man making a clay pot on a pottery wheel",
            "a young boy is brushing his teeth with a toothbrush."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2380792",
                "VG_object_id": "1342287",
                "bbox": [126, 122, 317, 314],
                "image": "data\\images\\2380792.jpg"
            },
            {
                "VG_image_id": "2373130",
                "VG_object_id": "734241",
                "bbox": [13, 138, 146, 347],
                "image": "data\\images\\2373130.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person in the shirt holding", 2],
            ["How many people are there", 1],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["How many people are there", 1],
            ["where is the person", -1],
            ["what is the persion wearing on his head", -1],
            ["what is the persion doing", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1],
            ["what is the persion holding", 1],
            ["what is the person in the shirt holding", 2]
        ],
        "context": [
            "a man sitting in a chair holding a hot dog.",
            "a woman holding a cell phone while walking down the street."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2374805",
                "VG_object_id": "2394566",
                "bbox": [225, 183, 273, 250],
                "image": "data\\images\\2374805.jpg"
            },
            {
                "VG_image_id": "2320266",
                "VG_object_id": "994656",
                "bbox": [229, 112, 319, 206],
                "image": "data\\images\\2320266.jpg"
            }
        ],
        "questions_with_scores": [["what color is the man's shirt", 2]],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["How many people are there", -1],
            ["where is the man", -1],
            ["what is the man doing", -1],
            ["when was the photo taken", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man wearing", -1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a man and a woman playing tennis on a tennis court.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2353788",
                "VG_object_id": "1678565",
                "bbox": [128, 274, 182, 346],
                "image": "data\\images\\2353788.jpg"
            },
            {
                "VG_image_id": "2410098",
                "VG_object_id": "225999",
                "bbox": [126, 47, 292, 107],
                "image": "data\\images\\2410098.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is person's shirt", 2],
            ["What is person doing", 2],
            ["What is the background of image", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["What color is person's shirt", 2],
            ["What is person doing", 2],
            ["how many people are there", -1],
            ["what is on the person's head", -1],
            ["What is the background of image", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the person wearing", -1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a girl walking in a line of buses.",
            "a man flying through the air while riding a skateboard."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2414997",
                "VG_object_id": "148020",
                "bbox": [17, 0, 498, 257],
                "image": "data\\images\\2414997.jpg"
            },
            {
                "VG_image_id": "2414545",
                "VG_object_id": "156278",
                "bbox": [1, 52, 499, 373],
                "image": "data\\images\\2414545.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are on the table", 2],
            ["how many glasses are on the table", 1],
            ["how many plates are there", 1]
        ],
        "org_questions": [
            ["how many plates are on the table", 2],
            ["how many forks are on the table", -1],
            ["how many glasses are on the table", 1],
            ["What color is the table", -1],
            ["what is on the table", -1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["how many plates are there", 1],
            ["what is the pizza sitting on", -1],
            ["where was the photo taken", -1],
            ["where is the pizza sitting", -1]
        ],
        "context": [
            "a pizza with spinach and cheese on a plate.",
            "a table with two pizzas and a glass of water."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2321254",
                "VG_object_id": "3184199",
                "bbox": [198, 132, 490, 310],
                "image": "data\\images\\2321254.jpg"
            },
            {
                "VG_image_id": "2366303",
                "VG_object_id": "3879890",
                "bbox": [16, 197, 484, 367],
                "image": "data\\images\\2366303.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many quilts are there on the bed", 2],
            ["how many covers are on the bed", 1]
        ],
        "org_questions": [
            ["what is the main color of the bed", -1],
            ["how many covers are on the bed", 1],
            ["What is on the bed", -1],
            ["how many people are there in the picture", -1],
            ["what color are the pillows on the bed", -1],
            ["where was this photo taken", -1],
            ["what room is this", -1],
            ["when was the picture taken", -1],
            ["what is the bed made of", -1],
            ["where are the pillows", -1],
            ["how many quilts are there on the bed", 2]
        ],
        "context": [
            "a bedroom with a bed, chair, and a chair.",
            "a bedroom with a bed, window, and a lamp."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2334437",
                "VG_object_id": "3677526",
                "bbox": [1, 163, 496, 370],
                "image": "data\\images\\2334437.jpg"
            },
            {
                "VG_image_id": "2379078",
                "VG_object_id": "3835539",
                "bbox": [0, 152, 499, 329],
                "image": "data\\images\\2379078.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is onthe land", 2],
            ["What is in the background", 2],
            ["What is on the land", 2],
            ["How many people are there", 1],
            ["where is the photo taken", 1],
            ["What is on the ground", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["What is onthe land", 2],
            ["What is in the background", 2],
            ["How many people are there", 1],
            ["where is the photo taken", 1],
            ["What is the ground made of", -1],
            ["What is on the ground", 1],
            ["who is in the picture", 1],
            ["what is the weather like", -1],
            ["What is on the land", 2]
        ],
        "context": [
            "a blue airplane is parked at the airport.",
            "a person with a suitcase walking along a train track."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2396254",
                "VG_object_id": "1199446",
                "bbox": [407, 165, 500, 241],
                "image": "data\\images\\2396254.jpg"
            },
            {
                "VG_image_id": "2325649",
                "VG_object_id": "3489213",
                "bbox": [341, 167, 393, 204],
                "image": "data\\images\\2325649.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there in the picture", 2],
            ["what color is the car", 1],
            ["what color is the road", 1],
            ["what color is the background", 1],
            ["what is in front of the cars", 1],
            ["which part of the car can we see in the picture", 1]
        ],
        "org_questions": [
            ["what color is the car", 1],
            ["what color is the road", 1],
            ["what color is the background", 1],
            ["how many cars are there", -1],
            ["what time is it", -1],
            ["what is in front of the cars", 1],
            ["what is the weather like", -1],
            ["which part of the car can we see in the picture", 1],
            ["where is the car", -1],
            ["when was this picture taken", -1],
            ["what is parked on the road", -1],
            ["what is on the road", -1],
            ["how many cars are there in the picture", 2]
        ],
        "context": [
            "a group of people riding motorcycles down a road.",
            "a bus is driving down the street in the city."
        ]
    },
    {
        "object_category": "lamp",
        "images": [
            {
                "VG_image_id": "2325167",
                "VG_object_id": "2885756",
                "bbox": [309, 15, 373, 268],
                "image": "data\\images\\2325167.jpg"
            },
            {
                "VG_image_id": "2316602",
                "VG_object_id": "2948756",
                "bbox": [287, 90, 378, 185],
                "image": "data\\images\\2316602.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the lamp", 2],
            ["when is this picture taken", 1],
            ["what is the lamp standing on", 1],
            ["where was the picture taken", 1]
        ],
        "org_questions": [
            ["where is the lamp", 2],
            ["how many people are there", -1],
            ["when is this picture taken", 1],
            ["what is the lamp standing on", 1],
            ["what is in the photo", -1],
            ["where was the picture taken", 1]
        ],
        "context": [
            "a woman takes a picture of herself in a bathroom mirror.",
            "a large open air market with tents in the background."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2374841",
                "VG_object_id": "585399",
                "bbox": [150, 108, 209, 308],
                "image": "data\\images\\2374841.jpg"
            },
            {
                "VG_image_id": "2389405",
                "VG_object_id": "3828641",
                "bbox": [1, 62, 203, 277],
                "image": "data\\images\\2389405.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the gender of person", 2],
            ["WHat color is person's shirt", 2],
            ["How many people are there", 1],
            ["what is on the person's head", 1],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What is the gender of person", 2],
            ["WHat color is person's shirt", 2],
            ["where is the photo taken", -1],
            ["what is on the person's head", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", -1],
            ["where is the person", -1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is the persion holding", 1],
            ["what is the persion playing", -1]
        ],
        "context": [
            "a man in a field with a frisbee.",
            "two people playing frisbee in a grassy field."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2368739",
                "VG_object_id": "2486557",
                "bbox": [35, 44, 498, 354],
                "image": "data\\images\\2368739.jpg"
            },
            {
                "VG_image_id": "2382424",
                "VG_object_id": "1328777",
                "bbox": [160, 184, 281, 256],
                "image": "data\\images\\2382424.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the computer", 2],
            ["where is the computer", 1],
            ["how many people are there", 1],
            ["how is the laptop", 1]
        ],
        "org_questions": [
            ["what color is the computer", 2],
            ["where is the computer", 1],
            ["how many people are there", 1],
            ["how many laptops are in the picture", -1],
            ["what color is the screen of the laptop", -1],
            ["what shape is the laptop", -1],
            ["what type of computer is shown", -1],
            ["what is the laptop on", -1],
            ["how is the laptop", 1]
        ],
        "context": [
            "a laptop computer sitting on top of a desk.",
            "two men standing in the woods"
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2336497",
                "VG_object_id": "2153101",
                "bbox": [168, 7, 259, 142],
                "image": "data\\images\\2336497.jpg"
            },
            {
                "VG_image_id": "2365535",
                "VG_object_id": "3883744",
                "bbox": [191, 112, 498, 372],
                "image": "data\\images\\2365535.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing on her head", 2],
            ["how many elephants are in the picture", 2],
            ["what is the woman doing", 2],
            ["what color is the woman's shirt", 1],
            ["What is woman doing", 1],
            ["whatis the woman holding", 1],
            ["who is wearing a hat", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman wearing on her head", 2],
            ["how many elephants are in the picture", 2],
            ["when is the picture taken", -1],
            ["what color is the woman's shirt", 1],
            ["What is woman doing", 1],
            ["whatis the woman holding", 1],
            ["where are the people", -1],
            ["who is wearing a hat", 1],
            ["what is the woman holding", 1],
            ["what is the persion on the left wearing", -1],
            ["what is the woman doing", 2]
        ],
        "context": [
            "two people riding on the back of an elephant.",
            "a woman and child looking at elephants in a field."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2416923",
                "VG_object_id": "2904225",
                "bbox": [135, 207, 187, 332],
                "image": "data\\images\\2416923.jpg"
            },
            {
                "VG_image_id": "2361481",
                "VG_object_id": "2630353",
                "bbox": [409, 215, 471, 312],
                "image": "data\\images\\2361481.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 1],
            ["what  is the person holding", 1],
            ["what is the person wearing", 1],
            ["Where are people", 1],
            ["what is on the person's head", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what  is the person holding", 1],
            ["what is the person wearing", 1],
            ["How many people are there", -1],
            ["Where are people", 1],
            ["what is the gender of the person", -1],
            ["what is the man doing", -1],
            ["what is on the person's head", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["where is the man", 1]
        ],
        "context": [
            "a man walking down a path next to a park.",
            "two horses pulling a man in a harness."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2318792",
                "VG_object_id": "3456206",
                "bbox": [141, 314, 205, 389],
                "image": "data\\images\\2318792.jpg"
            },
            {
                "VG_image_id": "2346613",
                "VG_object_id": "2658532",
                "bbox": [292, 212, 347, 262],
                "image": "data\\images\\2346613.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 1],
            ["what is the dog doing", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the dog", 1],
            ["what is the dog standing on", -1],
            ["what is the dog doing", 1],
            ["how many hot dogs are there", -1],
            ["how is the weather", -1],
            ["where is the dog", -1],
            ["what is in the background", -1],
            ["what is the ground covered with", 1],
            ["what animal is in the photo", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what kind of animal is this", -1]
        ],
        "context": [
            "a dog sitting on a beach with a bird flying in the background.",
            "a black dog walking along a beach next to a row of boats."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2335484",
                "VG_object_id": "2471500",
                "bbox": [42, 264, 314, 358],
                "image": "data\\images\\2335484.jpg"
            },
            {
                "VG_image_id": "2332355",
                "VG_object_id": "3354315",
                "bbox": [298, 92, 473, 284],
                "image": "data\\images\\2332355.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bag placed", 2],
            ["what color is the bag", 2],
            ["how many people are there", 1],
            ["what is the persion doing", 1],
            ["what is in the distance", 1],
            ["what is the persion wearing", 1],
            ["where was this photo taken", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["where is the bag placed", 2],
            ["what color is the bag", 2],
            ["how many people are there", 1],
            ["what is the persion doing", 1],
            ["what is the bag made of", -1],
            ["how is the weather", -1],
            ["what is in the distance", 1],
            ["what is the persion wearing", 1],
            ["where was this photo taken", 1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a group of monks sitting in an airport.",
            "a man is bending over"
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2360375",
                "VG_object_id": "788314",
                "bbox": [0, 142, 499, 354],
                "image": "data\\images\\2360375.jpg"
            },
            {
                "VG_image_id": "2388604",
                "VG_object_id": "508522",
                "bbox": [5, 124, 496, 375],
                "image": "data\\images\\2388604.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many animals are there", 2],
            ["what animal is in the piture", 1],
            ["what is the color of the grass", 1],
            ["what kind of animal is there", 1]
        ],
        "org_questions": [
            ["what animal is in the piture", 1],
            ["what is the color of the grass", 1],
            ["How many animals are there", 2],
            ["what is the ground covered with", -1],
            ["how many cows are there on the ground", -1],
            ["what kind of animal is there", 1],
            ["when was the picture taken", -1],
            ["where are the trees", -1],
            ["how is the weather", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "a herd of cattle grazing in a field of tall grass.",
            "a zebra standing in a field with trees in the background."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2394367",
                "VG_object_id": "464805",
                "bbox": [287, 168, 374, 264],
                "image": "data\\images\\2394367.jpg"
            },
            {
                "VG_image_id": "2352572",
                "VG_object_id": "2440449",
                "bbox": [115, 124, 257, 242],
                "image": "data\\images\\2352572.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the dog doing", 1],
            ["where is the dog", 1],
            ["what is in front of the dog", 1],
            ["what is under the dog", 1],
            ["what is around the dog's neck", 1],
            ["what is the dog looking at", 1]
        ],
        "org_questions": [
            ["what is the dog doing", 1],
            ["where is the dog", 1],
            ["how many animals are in the picture", -1],
            ["what is the dog's posture", -1],
            ["what is in front of the dog", 1],
            ["what is under the dog", 1],
            ["what kind of animal is in the picture", -1],
            ["what is on the dog's head", -1],
            ["what is around the dog's neck", 1],
            ["what is the dog looking at", 1]
        ],
        "context": [
            "a cat and dog sitting on a toilet",
            "a dog running in the grass with its mouth open."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2381680",
                "VG_object_id": "1334774",
                "bbox": [90, 42, 203, 159],
                "image": "data\\images\\2381680.jpg"
            },
            {
                "VG_image_id": "2355153",
                "VG_object_id": "832300",
                "bbox": [172, 239, 254, 332],
                "image": "data\\images\\2355153.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person doing", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["what color is the sky", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person doing", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["what shape is the shirt's collar", -1],
            ["who is wearing the shirt", -1],
            ["where is the person", 1],
            ["what is the person wearing", -1],
            ["when was this photo taken", -1],
            ["what color are the man's pants", -1],
            ["when was the photo taken", -1],
            ["what color is the sky", 1]
        ],
        "context": [
            "a man riding a skateboard up the side of a ramp.",
            "a man holding a surfboard on a beach."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2356346",
                "VG_object_id": "3772531",
                "bbox": [54, 107, 218, 492],
                "image": "data\\images\\2356346.jpg"
            },
            {
                "VG_image_id": "2322973",
                "VG_object_id": "990276",
                "bbox": [141, 89, 210, 272],
                "image": "data\\images\\2322973.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the girl playing", 2],
            ["What is girl doing", 2],
            ["what color are the girl's clothes", 1],
            ["what is the woman holding", 1],
            ["what is the woman doing", 1],
            ["what is the girl wearing on the head", 1]
        ],
        "org_questions": [
            ["what color are the girl's clothes", 1],
            ["what sport is the girl playing", 2],
            ["how many children are there", -1],
            ["Where is the girl", -1],
            ["what is the woman holding", 1],
            ["What is girl doing", 2],
            ["what color is the ground", -1],
            ["what is the woman doing", 1],
            ["when was the photo taken", -1],
            ["what is the girl wearing on the head", 1],
            ["who is in the photo", -1],
            ["what is on the girl's feet", -1]
        ],
        "context": [
            "a little girl holding tennis balls and tennis balls.",
            "a group of young girls playing a game of soccer."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2365923",
                "VG_object_id": "1885878",
                "bbox": [48, 96, 149, 234],
                "image": "data\\images\\2365923.jpg"
            },
            {
                "VG_image_id": "2373130",
                "VG_object_id": "734231",
                "bbox": [22, 128, 120, 340],
                "image": "data\\images\\2373130.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bag", 1],
            ["what color is the woman's clothes", 1],
            ["what is the persion doing", 1],
            ["what is the girl wearing", 1],
            ["what is the persion wearing", 1],
            ["what is the woman wearing", 1]
        ],
        "org_questions": [
            ["what color is the bag", 1],
            ["what is the woman holding", -1],
            ["what color is the woman's clothes", 1],
            ["how many people are there", -1],
            ["where is the bag", -1],
            ["how is the weather", -1],
            ["what is the persion doing", 1],
            ["when was the photo taken", -1],
            ["what is the girl wearing", 1],
            ["what is the persion wearing", 1],
            ["what is the woman wearing", 1]
        ],
        "context": [
            "a man and woman standing next to each other.",
            "a woman holding a cell phone while walking down the street."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2399959",
                "VG_object_id": "1164409",
                "bbox": [262, 251, 467, 373],
                "image": "data\\images\\2399959.jpg"
            },
            {
                "VG_image_id": "2349084",
                "VG_object_id": "2039149",
                "bbox": [30, 80, 206, 205],
                "image": "data\\images\\2349084.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the sofa", 2],
            ["what is the main color of the couch", 2],
            ["what is in front of the couch", 1],
            ["What is on the sofa", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["How many sofas are there", -1],
            ["What color is the sofa", 2],
            ["what is in front of the couch", 1],
            ["What is on the sofa", 1],
            ["how many people are there", 1],
            ["what is sitting on the couch", -1],
            ["what is the main color of the couch", 2],
            ["what is on the floor", -1]
        ],
        "context": [
            "two girls playing a video game in a living room.",
            "a living room with a fireplace and a chair."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2318248",
                "VG_object_id": "1012678",
                "bbox": [246, 178, 384, 306],
                "image": "data\\images\\2318248.jpg"
            },
            {
                "VG_image_id": "2413141",
                "VG_object_id": "179452",
                "bbox": [216, 79, 459, 275],
                "image": "data\\images\\2413141.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what is the food on the plate", 1],
            ["How many pots are there", 1],
            ["what is the main color of the food", 1],
            ["what kind of food is in the picture", 1]
        ],
        "org_questions": [
            ["what is the food on the plate", 1],
            ["what is color of the plate", -1],
            ["what color is the table", 2],
            ["How many pots are there", 1],
            ["what is on the tray", -1],
            ["what is the main color of the food", 1],
            ["what color is the plate", -1],
            ["where is the picture taken", -1],
            ["what kind of food is in the picture", 1],
            ["what is the food on", -1],
            ["where are the plates", -1]
        ],
        "context": [
            "a man and woman preparing food in a kitchen.",
            "a table topped with plates of food and a bowl of chips."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2376618",
                "VG_object_id": "1844000",
                "bbox": [1, 37, 292, 159],
                "image": "data\\images\\2376618.jpg"
            },
            {
                "VG_image_id": "2353550",
                "VG_object_id": "3133276",
                "bbox": [417, 14, 499, 279],
                "image": "data\\images\\2353550.jpg"
            }
        ],
        "questions_with_scores": [
            ["what time is it", 2],
            ["how many floors does the building have", 1],
            ["what color is the sky", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the wall of the building", -1],
            ["what is in front of the building", -1],
            ["what time is it", 2],
            ["how many people are there", -1],
            ["how many floors does the building have", 1],
            ["what color is the sky", 1],
            ["how many clocks are there", -1],
            ["when was the photo taken", 1],
            ["what kind of building is in the background", -1]
        ],
        "context": [
            "a stop sign on a city street at night.",
            "a bus is parked on the side of the road."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2340673",
                "VG_object_id": "2049976",
                "bbox": [213, 1, 325, 318],
                "image": "data\\images\\2340673.jpg"
            },
            {
                "VG_image_id": "2317042",
                "VG_object_id": "3008875",
                "bbox": [119, 62, 291, 324],
                "image": "data\\images\\2317042.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["how many men are there in the picture", 2],
            ["What color is man's shirt", 1],
            ["What is the man wearing on his head", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", 1],
            ["What is the man wearing on his head", 1],
            ["how many people are there", 2],
            ["where is the man", -1],
            ["what vehicle is beside the men", -1],
            ["what is the man wearing", -1],
            ["how many men are there in the picture", 2],
            ["who is wearing a white shirt", -1],
            ["when was the photo taken", -1],
            ["what is the man doing", -1],
            ["what sport is the man playing", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a man jumping up to hit a tennis ball.",
            "a man hitting a tennis ball with a racquet."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2366871",
                "VG_object_id": "753393",
                "bbox": [0, 25, 373, 495],
                "image": "data\\images\\2366871.jpg"
            },
            {
                "VG_image_id": "2328138",
                "VG_object_id": "978660",
                "bbox": [3, 190, 499, 331],
                "image": "data\\images\\2328138.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the photo", 2],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", -1],
            ["How many plates are there", -1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["what food is on the table", -1],
            ["where was this picture taken", -1],
            ["what is in the background", 1],
            ["how many people are in the photo", 2],
            ["where are the books", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a table with a cell phone, a charger, and some other items.",
            "a laptop computer sitting on top of a table."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2359128",
                "VG_object_id": "3766919",
                "bbox": [108, 66, 353, 191],
                "image": "data\\images\\2359128.jpg"
            },
            {
                "VG_image_id": "2367129",
                "VG_object_id": "752704",
                "bbox": [70, 166, 335, 354],
                "image": "data\\images\\2367129.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the bicycle", 1],
            ["what is the bicycle on", 1],
            ["how many bicycles are there", 1],
            ["what is the ground covered with", 1],
            ["what is beside the bicycle", 1],
            ["where is the picture taken", 1],
            ["where is the bike", 1]
        ],
        "org_questions": [
            ["what is on the bicycle", 1],
            ["what is the bicycle on", 1],
            ["how many bicycles are there", 1],
            ["what is the persion doing", -1],
            ["what is the ground covered with", 1],
            ["what is beside the bicycle", 1],
            ["when was the photo taken", -1],
            ["where is the picture taken", 1],
            ["what is the persion holding", -1],
            ["where is the bike", 1]
        ],
        "context": [
            "a car with bicycles on top of it",
            "a man standing next to a surfboard on a beach."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2399826",
                "VG_object_id": "413475",
                "bbox": [44, 106, 90, 388],
                "image": "data\\images\\2399826.jpg"
            },
            {
                "VG_image_id": "2385384",
                "VG_object_id": "1297570",
                "bbox": [344, 0, 401, 210],
                "image": "data\\images\\2385384.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 2],
            ["what is on the left of the curtain", 1],
            ["what is the color of the floor", 1]
        ],
        "org_questions": [
            ["what is on the left of the curtain", 1],
            ["what is the color of the floor", 1],
            ["how many people are there", -1],
            ["where is the picture taken", -1],
            ["what is under the curtain", -1],
            ["where is the curtain", -1],
            ["who is in the photo", -1],
            ["what is white", -1],
            ["what is the floor made of", 2]
        ],
        "context": [
            "a bathroom with a blue bathtub and a white curtain.",
            "a bathroom with a sink and a mirror"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2343196",
                "VG_object_id": "2277654",
                "bbox": [3, 4, 331, 497],
                "image": "data\\images\\2343196.jpg"
            },
            {
                "VG_image_id": "2415869",
                "VG_object_id": "2712656",
                "bbox": [4, 39, 499, 359],
                "image": "data\\images\\2415869.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are on the table", 2],
            ["what color is the plate", 2],
            ["What is on the table", 1],
            ["what is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["how many plates are on the table", 2],
            ["what color is the plate", 2],
            ["where is the picture taken", -1],
            ["What is on the table", 1],
            ["what is in the photo", 1],
            ["how many people are there", -1]
        ],
        "context": [
            "a banana on a plate with a floral pattern.",
            "a table with plates of food on it"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2405172",
                "VG_object_id": "1110059",
                "bbox": [121, 164, 243, 332],
                "image": "data\\images\\2405172.jpg"
            },
            {
                "VG_image_id": "2369937",
                "VG_object_id": "607859",
                "bbox": [62, 121, 234, 251],
                "image": "data\\images\\2369937.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["where is the person", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["what are the people sitting on", -1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what is the person doing ", -1],
            ["where is the person", 1],
            ["what is the woman holding", -1],
            ["when was the photo taken", -1],
            ["what type of shirt is the girl wearing", -1],
            ["what is the girl wearing", -1],
            ["what is the main color of the shirt", 1]
        ],
        "context": [
            "a woman sitting in a stadium eating a hot dog.",
            "two young girls sitting on the steps eating food."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2405039",
                "VG_object_id": "335513",
                "bbox": [106, 92, 233, 222],
                "image": "data\\images\\2405039.jpg"
            },
            {
                "VG_image_id": "2380594",
                "VG_object_id": "3833565",
                "bbox": [197, 32, 260, 164],
                "image": "data\\images\\2380594.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the man", 2],
            ["what is the man wearing", 2],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the ground covered with", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["what is the man wearing", 2],
            ["when was the photo taken", -1],
            ["what is the persion standing on", 1],
            ["what is on the person's head", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man riding a skateboard down a wet road.",
            "a man in a boat is standing on the water."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2318386",
                "VG_object_id": "2737200",
                "bbox": [335, 150, 447, 334],
                "image": "data\\images\\2318386.jpg"
            },
            {
                "VG_image_id": "2411317",
                "VG_object_id": "313435",
                "bbox": [218, 355, 373, 499],
                "image": "data\\images\\2411317.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the gender of person", 2],
            ["WHat is person holding", 2],
            ["what is the persion doing", 1],
            ["what is the pattern on the chair", 1],
            ["what is in the background", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["What is the gender of person", 2],
            ["WHat is person holding", 2],
            ["How many plates are there", -1],
            ["where is the chair", -1],
            ["what is the persion doing", 1],
            ["how many pillows are there on the chair", -1],
            ["what is the chair made out of", -1],
            ["what is the persion sitting on", -1],
            ["what is the pattern on the chair", 1],
            ["what is in the background", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a woman sitting at a table with an umbrella.",
            "a man holding a cake on a plate."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2354820",
                "VG_object_id": "835273",
                "bbox": [219, 57, 280, 116],
                "image": "data\\images\\2354820.jpg"
            },
            {
                "VG_image_id": "2400924",
                "VG_object_id": "1153549",
                "bbox": [136, 242, 187, 310],
                "image": "data\\images\\2400924.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["Where is the person", 1],
            ["what is the man holding", 1],
            ["when was this photo taken", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["how many people are in the picture", -1],
            ["what is the man doing", 1],
            ["What gender is the person", -1],
            ["what is the man wearing around his neck", -1],
            ["Where is the person", 1],
            ["what is the pattern of the shirt", -1],
            ["what is the man holding", 1],
            ["when was this photo taken", 1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1],
            ["where is the man", 1],
            ["how many shirt are there in the picture", -1],
            ["what is the gender of the person", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man riding a skateboard on top of a cement block.",
            "a man standing on a tennis court holding a racquet."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2357750",
                "VG_object_id": "2225184",
                "bbox": [142, 112, 265, 253],
                "image": "data\\images\\2357750.jpg"
            },
            {
                "VG_image_id": "2390293",
                "VG_object_id": "499033",
                "bbox": [156, 110, 413, 264],
                "image": "data\\images\\2390293.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the  horse", 2],
            ["what color is the man's helmet", 2],
            ["what is the land made of", 1],
            ["what is behind the horse", 1]
        ],
        "org_questions": [
            ["what color is the  horse", 2],
            ["where is the horse", -1],
            ["what color is the man's helmet", 2],
            ["how many horses are there", -1],
            ["what is the land made of", 1],
            ["what is in the background", -1],
            ["what is behind the horse", 1],
            ["how many people are there in the picture", -1],
            ["what kind of animal is this", -1],
            ["when was the picture taken", -1],
            ["what is on the horse's head", -1],
            ["what is the horse doing", -1]
        ],
        "context": [
            "a man riding a horse through a forest.",
            "a person riding a horse through a pond."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2322081",
                "VG_object_id": "993612",
                "bbox": [6, 56, 480, 374],
                "image": "data\\images\\2322081.jpg"
            },
            {
                "VG_image_id": "2327135",
                "VG_object_id": "2965135",
                "bbox": [8, 165, 466, 280],
                "image": "data\\images\\2327135.jpg"
            }
        ],
        "questions_with_scores": [["what color is the table", 1]],
        "org_questions": [
            ["what color is the table", 1],
            ["how many people are in the picture", -1],
            ["where is the photo taken", -1],
            ["what kind of food is there", -1],
            ["what type of table is this", -1],
            ["what is on top of the table", -1],
            ["where are the plates", -1],
            ["what is sitting on the table", -1]
        ],
        "context": [
            "a table with a large amount of food on it.",
            "a table with a large silver bowl and plates on it."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2373409",
                "VG_object_id": "3251484",
                "bbox": [189, 242, 281, 344],
                "image": "data\\images\\2373409.jpg"
            },
            {
                "VG_image_id": "2330844",
                "VG_object_id": "2854285",
                "bbox": [59, 54, 494, 204],
                "image": "data\\images\\2330844.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many clocks are there in the picture", 2],
            ["what color is the wall", 1],
            ["what color is the edge of the clock", 1],
            ["what is behind the clock", 1]
        ],
        "org_questions": [
            ["how many clocks are there in the picture", 2],
            ["what color is the wall", 1],
            ["what color is the edge of the clock", 1],
            ["what time is it", -1],
            ["what shape is the clock", -1],
            ["what is the building made of", -1],
            ["where is the clock", -1],
            ["what is on the clock", -1],
            ["when was the photo taken", -1],
            ["what is behind the clock", 1],
            ["what is above the clock", -1],
            ["where are the clocks", -1]
        ],
        "context": [
            "a man is standing on a balcony above a clock.",
            "three clocks on a pink wall with three different times."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2410228",
                "VG_object_id": "223037",
                "bbox": [212, 117, 265, 234],
                "image": "data\\images\\2410228.jpg"
            },
            {
                "VG_image_id": "2391269",
                "VG_object_id": "1242157",
                "bbox": [195, 169, 296, 235],
                "image": "data\\images\\2391269.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 1],
            ["what is in the dog's mouth", 1]
        ],
        "org_questions": [
            ["what color is the dog", 1],
            ["what is in the dog's mouth", 1],
            ["what color is the ground", -1],
            ["how many black dogs are there on the ground", -1],
            ["where is the dog", -1],
            ["what is the dog doing", -1],
            ["what is the dog sitting on", -1],
            ["what animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the dog's head", -1]
        ],
        "context": [
            "a dog holding a frisbee in its mouth.",
            "a dog standing next to a kayak in the woods."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2368659",
                "VG_object_id": "2175895",
                "bbox": [372, 216, 477, 280],
                "image": "data\\images\\2368659.jpg"
            },
            {
                "VG_image_id": "2367813",
                "VG_object_id": "3872160",
                "bbox": [177, 334, 285, 470],
                "image": "data\\images\\2367813.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is on the man's head", 2],
            ["what is the person doing", 1],
            ["What is ground made of", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person doing", 1],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what pattern is the shirt", -1],
            ["what is the man wearing", -1],
            ["What is ground made of", 1],
            ["when was the picture taken", -1],
            ["what is the man holding", 1],
            ["what is on the man's head", 2]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a dog catching a frisbee in a dog park."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2360226",
                "VG_object_id": "3534790",
                "bbox": [2, 283, 496, 330],
                "image": "data\\images\\2360226.jpg"
            },
            {
                "VG_image_id": "2412885",
                "VG_object_id": "1617692",
                "bbox": [1, 147, 410, 344],
                "image": "data\\images\\2412885.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["what is the person doing on the ground", 1],
            ["what are people doing", 1],
            ["where is this scene", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["what is the person doing on the ground", 1],
            ["What is the ground made of", -1],
            ["what are people doing", 1],
            ["how is the weather", -1],
            ["what is in the background", -1],
            ["where is this scene", 1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a man and woman loading luggage onto a train.",
            "a man riding a skateboard up the side of a ramp."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2359470",
                "VG_object_id": "793086",
                "bbox": [180, 142, 224, 174],
                "image": "data\\images\\2359470.jpg"
            },
            {
                "VG_image_id": "2358743",
                "VG_object_id": "2376961",
                "bbox": [315, 137, 392, 212],
                "image": "data\\images\\2358743.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["who is wearing a white shirt", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["How many people are there", -1],
            ["how many mans are in the picture", -1],
            ["who is wearing a white shirt", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man riding an elephant down a street.",
            "a woman swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2402212",
                "VG_object_id": "391727",
                "bbox": [0, 415, 332, 500],
                "image": "data\\images\\2402212.jpg"
            },
            {
                "VG_image_id": "2387784",
                "VG_object_id": "3830008",
                "bbox": [1, 193, 361, 496],
                "image": "data\\images\\2387784.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["what is in the background", 1],
            ["what animal is in the picture", 1],
            ["what pattern is the floor", 1],
            ["where is the photo taken", 1],
            ["what is the floor made of", 1],
            ["what kind of floor is this", 1],
            ["where is the floor", 1],
            ["what is the floor color", 1]
        ],
        "org_questions": [
            ["what is on the ground", 1],
            ["what is in the background", 1],
            ["how many tables are on the floor", -1],
            ["what animal is in the picture", 1],
            ["what pattern is the floor", 1],
            ["where is the photo taken", 1],
            ["what is the floor made of", 1],
            ["what kind of floor is this", 1],
            ["where is the floor", 1],
            ["what is the floor color", 1]
        ],
        "context": [
            "a bathroom with a toilet and a sink.",
            "two dogs playing with a bowl of water."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2351168",
                "VG_object_id": "1873049",
                "bbox": [161, 203, 253, 496],
                "image": "data\\images\\2351168.jpg"
            },
            {
                "VG_image_id": "2319372",
                "VG_object_id": "1002540",
                "bbox": [1, 31, 454, 333],
                "image": "data\\images\\2319372.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there", 2],
            ["how many people are there", 2],
            ["what are on the children's heads", 1],
            ["what are the children doing", 1],
            ["what color is the child's shirt", 1],
            ["how many people are there in the photo", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["how many children are there", 2],
            ["what are on the children's heads", 1],
            ["what are the children doing", 1],
            ["what color is the child's shirt", 1],
            ["how many people are there in the photo", 1],
            ["how many people are there", 2],
            ["what are the people doing", 1],
            ["where was this photo taken", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the girl wearing", -1]
        ],
        "context": [
            "a little girl holding an umbrella in a kitchen.",
            "a group of children sitting at a table eating pizza."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2383082",
                "VG_object_id": "695018",
                "bbox": [16, 102, 235, 374],
                "image": "data\\images\\2383082.jpg"
            },
            {
                "VG_image_id": "2342764",
                "VG_object_id": "931009",
                "bbox": [2, 53, 165, 499],
                "image": "data\\images\\2342764.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["what is the woman wearing", 1],
            ["where are the people", 1],
            ["what is the woman on the left doing", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 1],
            ["how many people are there", -1],
            ["Where is the photo taken", -1],
            ["What is the woman doing", -1],
            ["what is the woman wearing", 1],
            ["what is on the woman's face", -1],
            ["what is the woman on", -1],
            ["where are the people", 1],
            ["who is in the photo", -1],
            ["what is the woman on the left doing", 1]
        ],
        "context": [
            "a man and a woman playing a video game.",
            "a man and woman shaking hands."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2386336",
                "VG_object_id": "1287104",
                "bbox": [40, 351, 294, 499],
                "image": "data\\images\\2386336.jpg"
            },
            {
                "VG_image_id": "2372231",
                "VG_object_id": "593074",
                "bbox": [65, 299, 213, 492],
                "image": "data\\images\\2372231.jpg"
            }
        ],
        "questions_with_scores": [
            ["what room is this", 2],
            ["what color is the floor", 1],
            ["what is the floor made of", 1],
            ["what is the ground covered with", 1],
            ["what kind of flooring is this", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["where is the floor", -1],
            ["what is the floor made of", 1],
            ["what is the ground covered with", 1],
            ["what room is this", 2],
            ["how is the floor made", -1],
            ["what kind of flooring is this", 1]
        ],
        "context": [
            "a bathroom with two sinks and a mirror.",
            "a kitchen with a sink and a window"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "1592687",
                "VG_object_id": "2820826",
                "bbox": [361, 403, 498, 538],
                "image": "data\\images\\1592687.jpg"
            },
            {
                "VG_image_id": "2386501",
                "VG_object_id": "1285530",
                "bbox": [13, 132, 67, 209],
                "image": "data\\images\\2386501.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["what is on the man's head", 2],
            ["How many people are there", 1],
            ["What is man doing", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["What sport is the man doing ", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is man's shirt", 2],
            ["What is man doing", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["What sport is the man doing ", 1],
            ["when was the photo taken", -1],
            ["what is on the man's head", 2],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man riding a skateboard on a sidewalk.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2369096",
                "VG_object_id": "613971",
                "bbox": [159, 115, 350, 268],
                "image": "data\\images\\2369096.jpg"
            },
            {
                "VG_image_id": "2371991",
                "VG_object_id": "737795",
                "bbox": [11, 46, 500, 364],
                "image": "data\\images\\2371991.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the children doing", 2],
            ["what color are the children's shirts", 2],
            ["how many people are there", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["what are the children doing", 2],
            ["what color are the children's shirts", 2],
            ["who is wearing a helmet", -1],
            ["how many people are there", 1],
            ["what is the boy wearing on his head", -1],
            ["when was the photo taken", -1],
            ["what is the gender of the person in the photo", -1],
            ["where are the people", 1]
        ],
        "context": [
            "two children sitting under an umbrella on a dirt ground.",
            "a mascot with a toothbrush in his mouth"
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2404704",
                "VG_object_id": "1113794",
                "bbox": [6, 400, 321, 499],
                "image": "data\\images\\2404704.jpg"
            },
            {
                "VG_image_id": "2375625",
                "VG_object_id": "580964",
                "bbox": [103, 247, 496, 333],
                "image": "data\\images\\2375625.jpg"
            }
        ],
        "questions_with_scores": [["what color is the floor", 1]],
        "org_questions": [
            ["what color is the floor", 1],
            ["what color is the wall", -1],
            ["how many sofas are there on the ground", -1],
            ["what is on the ground", -1],
            ["What is the pattern of the floor", -1],
            ["what is the floor made of", -1],
            ["what room is this", -1],
            ["where are the tiles", -1],
            ["how is the floor made", -1],
            ["what type of floor is this", -1]
        ],
        "context": [
            "a bathroom with a double sink and a large mirror.",
            "a bathroom with a blue shower curtain and a toilet."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2343181",
                "VG_object_id": "926493",
                "bbox": [194, 132, 264, 222],
                "image": "data\\images\\2343181.jpg"
            },
            {
                "VG_image_id": "2373813",
                "VG_object_id": "2493167",
                "bbox": [219, 105, 361, 190],
                "image": "data\\images\\2373813.jpg"
            }
        ],
        "questions_with_scores": [["What color is the train", 2]],
        "org_questions": [
            ["What color is the train", 2],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the weather like", -1],
            ["what is the train doing", -1],
            ["what kind of vehicle is this", -1],
            ["where is the train", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a yellow train is coming down the tracks.",
            "a train traveling down tracks next to a body of water."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2396720",
                "VG_object_id": "1195740",
                "bbox": [289, 289, 405, 360],
                "image": "data\\images\\2396720.jpg"
            },
            {
                "VG_image_id": "2342764",
                "VG_object_id": "931010",
                "bbox": [4, 320, 141, 500],
                "image": "data\\images\\2342764.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the person doing", 2],
            ["Who is wearing those trousers", 1],
            ["Where is the person", 1],
            ["what is the ground covered with", 1],
            ["what is behind the trousers", 1],
            ["what is on the ground", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["Who is wearing those trousers", 1],
            ["What is the person doing", 2],
            ["Where is the person", 1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["what is behind the trousers", 1],
            ["what is on the ground", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a man sitting on a motorcycle on a road.",
            "a man and woman shaking hands."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2367527",
                "VG_object_id": "751355",
                "bbox": [1, 1, 278, 299],
                "image": "data\\images\\2367527.jpg"
            },
            {
                "VG_image_id": "2354590",
                "VG_object_id": "837338",
                "bbox": [118, 87, 340, 393],
                "image": "data\\images\\2354590.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["What color is the cake", 1],
            ["What color is the table", 1]
        ],
        "org_questions": [
            ["What color is the cake", 1],
            ["What color is the table", 1],
            ["how many people are there", 2],
            ["what shape is the cake", -1],
            ["What is cake on", -1],
            ["What is on the cake", -1],
            ["how many candles are there on the cake", -1],
            ["where is the cake", -1],
            ["what is the cake made of", -1]
        ],
        "context": [
            "a cake with a pink and white flower on it.",
            "a cake with a train on it"
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "150357",
                "VG_object_id": "1038285",
                "bbox": [0, 178, 1022, 574],
                "image": "data\\images\\150357.jpg"
            },
            {
                "VG_image_id": "2350839",
                "VG_object_id": "2574516",
                "bbox": [3, 78, 498, 332],
                "image": "data\\images\\2350839.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animal is in the field", 2],
            ["where are the trees", 1]
        ],
        "org_questions": [
            ["what is the main color of the ground", -1],
            ["what animal is in the field", 2],
            ["what is the persion doing", -1],
            ["how many horses are there on the field", -1],
            ["where was this photo taken", -1],
            ["what is in the background", -1],
            ["when was the picture taken", -1],
            ["what is the weather like", -1],
            ["where are the trees", 1]
        ],
        "context": [
            "two giraffes standing in a field with other animals.",
            "a dog sitting in the grass next to a bird."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2401435",
                "VG_object_id": "399417",
                "bbox": [431, 35, 498, 117],
                "image": "data\\images\\2401435.jpg"
            },
            {
                "VG_image_id": "2380656",
                "VG_object_id": "544242",
                "bbox": [139, 197, 233, 339],
                "image": "data\\images\\2380656.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's trousers", 1],
            ["when is this photo taken", 1],
            ["How many people are there", 1],
            ["what is in the background", 1],
            ["when was the picture taken", 1],
            ["when was this photo taken", 1]
        ],
        "org_questions": [
            ["what color are the man's trousers", 1],
            ["when is this photo taken", 1],
            ["How many people are there", 1],
            ["what is the man doing", -1],
            ["what is in the background", 1],
            ["where is the man", -1],
            ["What is man holding", -1],
            ["who is in the photo", -1],
            ["what is the boy wearing", -1],
            ["when was the picture taken", 1],
            ["when was this photo taken", 1],
            ["what color is the skateboard", -1]
        ],
        "context": [
            "a young man riding a skateboard on top of a ramp.",
            "a man riding a skateboard on top of a bench."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2372461",
                "VG_object_id": "2040560",
                "bbox": [216, 5, 295, 50],
                "image": "data\\images\\2372461.jpg"
            },
            {
                "VG_image_id": "2380989",
                "VG_object_id": "542772",
                "bbox": [179, 182, 272, 223],
                "image": "data\\images\\2380989.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the clock", 2],
            ["what shape is the clock", 1],
            ["what is on the clock", 1],
            ["what is the number on the clock", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["where is the clock", 2],
            ["how many clocks are in the picture", -1],
            ["what time is it", -1],
            ["what shape is the clock", 1],
            ["what is on the clock", 1],
            ["who is in the photo", -1],
            ["what is the number on the clock", 1],
            ["when was the photo taken", -1],
            ["what is in the background", 1],
            ["what is on the side of the clock", -1]
        ],
        "context": [
            "a pizza sitting on top of a stove top.",
            "a clock tower with a weather vane on top of it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2409019",
                "VG_object_id": "250587",
                "bbox": [250, 4, 499, 354],
                "image": "data\\images\\2409019.jpg"
            },
            {
                "VG_image_id": "2381775",
                "VG_object_id": "701941",
                "bbox": [213, 134, 373, 429],
                "image": "data\\images\\2381775.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 1],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what color are the man's clothes", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what is the man wearing", 1],
            ["what is the man doing", -1],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what color are the man's clothes", 1],
            ["what is on the man's head", -1],
            ["what is the man on", -1],
            ["how many people are there in the picture", 1],
            ["where is the man", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man shaking hands with another man in a blue shirt.",
            "a man and woman playing a video game."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2317475",
                "VG_object_id": "3516693",
                "bbox": [223, 206, 315, 388],
                "image": "data\\images\\2317475.jpg"
            },
            {
                "VG_image_id": "2414143",
                "VG_object_id": "159136",
                "bbox": [229, 99, 400, 320],
                "image": "data\\images\\2414143.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the ground", 2],
            ["what color is the court", 2],
            ["How many people are there", 1],
            ["What color is player's shirt", 1],
            ["how many players are there in the picture", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is player's shirt", 1],
            ["What color is the ground", 2],
            ["what is the player doing", -1],
            ["what is the player wearing", -1],
            ["what is the player holding", -1],
            ["how many players are there in the picture", 1],
            ["what color is the court", 2],
            ["when was the photo taken", -1],
            ["what sport is being played", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "two men in white shirts and white shirts playing tennis.",
            "a man playing tennis on a clay court."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2336832",
                "VG_object_id": "3661896",
                "bbox": [170, 100, 238, 164],
                "image": "data\\images\\2336832.jpg"
            },
            {
                "VG_image_id": "2330779",
                "VG_object_id": "3094010",
                "bbox": [104, 174, 242, 245],
                "image": "data\\images\\2330779.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sofa", 2],
            ["how many people are there", 1],
            ["What is laptop on", 1]
        ],
        "org_questions": [
            ["what color is the computer", -1],
            ["what color is the sofa", 2],
            ["how many people are there", 1],
            ["where is the photo taken", -1],
            ["What is laptop on", 1],
            ["who is sitting on the table", -1],
            ["when was the photo taken", -1],
            ["what are the people doing", -1],
            ["where are the people sitting", -1],
            ["what are the people sitting on", -1]
        ],
        "context": [
            "a woman is using a vacuum cleaner.",
            "three men sitting on a couch with a laptop."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2381988",
                "VG_object_id": "700214",
                "bbox": [181, 171, 307, 275],
                "image": "data\\images\\2381988.jpg"
            },
            {
                "VG_image_id": "2324552",
                "VG_object_id": "2877270",
                "bbox": [280, 124, 328, 328],
                "image": "data\\images\\2324552.jpg"
            }
        ],
        "questions_with_scores": [["What color is the towel", 2]],
        "org_questions": [
            ["What color is the towel", 2],
            ["Where is the towel", -1],
            ["What is next to the towel", -1],
            ["how many towels are there", -1],
            ["What is towel on", -1],
            ["What color is the wall", -1],
            ["what color is the background", -1],
            ["what is hanging on the wall", -1],
            ["where was the photo taken", -1],
            ["what is on the towel", -1]
        ],
        "context": [
            "a bathroom with a sink and a towel rack.",
            "a bathroom with a green sink and a mirror"
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2350521",
                "VG_object_id": "867371",
                "bbox": [17, 336, 78, 497],
                "image": "data\\images\\2350521.jpg"
            },
            {
                "VG_image_id": "2352145",
                "VG_object_id": "1664633",
                "bbox": [1, 265, 302, 498],
                "image": "data\\images\\2352145.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 2],
            ["where was the photo taken", 2],
            ["what is the persion doing", 1],
            ["what is the ground covered with", 1],
            ["where is the grass", 1],
            ["what is growing on the ground", 1],
            ["how is the weather", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what is the weather like", 2],
            ["what is the persion doing", 1],
            ["How many people are there", -1],
            ["what is the ground covered with", 1],
            ["What is in the distance", -1],
            ["where was the photo taken", 2],
            ["where is the grass", 1],
            ["what is growing on the ground", 1],
            ["how is the weather", 1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a person standing next to a motorcycle with an umbrella.",
            "a young man is jumping a skateboard over a hill."
        ]
    },
    {
        "object_category": "keyboard",
        "images": [
            {
                "VG_image_id": "2412207",
                "VG_object_id": "200428",
                "bbox": [141, 228, 314, 282],
                "image": "data\\images\\2412207.jpg"
            },
            {
                "VG_image_id": "2404699",
                "VG_object_id": "339220",
                "bbox": [434, 327, 497, 374],
                "image": "data\\images\\2404699.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the keyboard", 1],
            ["where is the keyboard", 1]
        ],
        "org_questions": [
            ["what color is the keyboard", 1],
            ["where is the keyboard", 1],
            ["How many keyboards are there", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a computer monitor and keyboard on a desk.",
            "a desk with three computers and a monitor on it."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2367750",
                "VG_object_id": "3363345",
                "bbox": [3, 272, 352, 498],
                "image": "data\\images\\2367750.jpg"
            },
            {
                "VG_image_id": "2360166",
                "VG_object_id": "1950020",
                "bbox": [25, 259, 346, 496],
                "image": "data\\images\\2360166.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the person's trousers", 2],
            ["what color are the person's shirt", 1],
            ["what is in the distance", 1],
            ["How many balls are there in the image", 1],
            ["How many people are there in the image", 1],
            ["what is the court made of", 1]
        ],
        "org_questions": [
            ["what color are the person's trousers", 2],
            ["what color are the person's shirt", 1],
            ["what is the main color of the ground", -1],
            ["how many people are there", -1],
            ["what is in the distance", 1],
            ["How many balls are there in the image", 1],
            ["How many people are there in the image", 1],
            ["where was the photo taken", -1],
            ["what is the court made of", 1],
            ["where are the white lines", -1],
            ["what is green", -1]
        ],
        "context": [
            "a man playing tennis on a grass court.",
            "a woman hitting a tennis ball with a tennis racket."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2354934",
                "VG_object_id": "1765325",
                "bbox": [27, 0, 499, 331],
                "image": "data\\images\\2354934.jpg"
            },
            {
                "VG_image_id": "2413141",
                "VG_object_id": "179442",
                "bbox": [0, 0, 499, 331],
                "image": "data\\images\\2413141.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many forks are there in the picture", 2],
            ["what color is the table", 1],
            ["how many black plates are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["how many forks are there in the picture", 2],
            ["how many black plates are there in the picture", 1],
            ["what food is on the plate", -1],
            ["what is the table made of", -1],
            ["What is on the table", -1],
            ["how many people are there", -1],
            ["what is on the plate", -1],
            ["where was this photo taken", -1],
            ["where are the plates", -1],
            ["what type of table is this", -1],
            ["what type of food is this", -1]
        ],
        "context": [
            "a table with a plate of cake and a fork.",
            "a table topped with plates of food and a bowl of chips."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2337414",
                "VG_object_id": "957799",
                "bbox": [234, 201, 390, 331],
                "image": "data\\images\\2337414.jpg"
            },
            {
                "VG_image_id": "2323804",
                "VG_object_id": "3493480",
                "bbox": [142, 203, 257, 294],
                "image": "data\\images\\2323804.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are there", 1],
            ["what is the dog standing on", 1],
            ["Where is the dog", 1],
            ["what is the dog doing", 1],
            ["what is the dog on", 1],
            ["what is the dog looking at", 1],
            ["where is the dog looking", 1],
            ["where is the dog", 1],
            ["how many dogs are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the dog", -1],
            ["how many dogs are there", 1],
            ["what is the dog standing on", 1],
            ["Where is the dog", 1],
            ["what is the dog doing", 1],
            ["what is the dog on", 1],
            ["what kind of animal is in the picture", -1],
            ["what is the dog looking at", 1],
            ["what animal is shown", -1],
            ["where is the dog looking", 1],
            ["what type of animal is shown", -1],
            ["where is the dog", 1],
            ["how many dogs are in the picture", 1],
            ["What is the dog wearing on its head", -1],
            ["what animal is in the picture", -1]
        ],
        "context": [
            "a man laying in bed with two dogs.",
            "a dog standing on a dock next to a man and woman."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2379532",
                "VG_object_id": "3150400",
                "bbox": [1, 159, 380, 361],
                "image": "data\\images\\2379532.jpg"
            },
            {
                "VG_image_id": "2402918",
                "VG_object_id": "1129982",
                "bbox": [316, 210, 400, 287],
                "image": "data\\images\\2402918.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many tables are there in the picture", 1],
            ["what is the leg of table made of", 1],
            ["what is on the table", 1],
            ["what is in the center of the room", 1],
            ["what is in the room", 1]
        ],
        "org_questions": [
            ["how many tables are there in the picture", 1],
            ["what is the leg of table made of", 1],
            ["Where is the table", -1],
            ["what color is the table", 2],
            ["what is on the table", 1],
            ["how many people are there", -1],
            ["what is in the center of the room", 1],
            ["what is under the table", -1],
            ["what is next to the table", -1],
            ["what is in the room", 1],
            ["what is in front of the table", -1]
        ],
        "context": [
            "a classroom with tables and chairs in it.",
            "a bed with a painting on the wall"
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2364016",
                "VG_object_id": "760234",
                "bbox": [5, 320, 375, 499],
                "image": "data\\images\\2364016.jpg"
            },
            {
                "VG_image_id": "2341352",
                "VG_object_id": "2823836",
                "bbox": [6, 289, 62, 394],
                "image": "data\\images\\2341352.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 1],
            ["what is on the rug", 1],
            ["what room is the rug in", 1]
        ],
        "org_questions": [
            ["what color is the rug", -1],
            ["what is on the ground", -1],
            ["what color is the wall", -1],
            ["how many people are there in the photo", 1],
            ["where is the rug", -1],
            ["what is the ground covered with", -1],
            ["what is on the rug", 1],
            ["what is the color of the floor", -1],
            ["how is the floor made", -1],
            ["what type of floor is this", -1],
            ["what material is the floor made of", -1],
            ["what room is the rug in", 1],
            ["what shape is the rug", -1],
            ["what is the floor under the rug made of", -1],
            ["what pattern is on the floor", -1]
        ],
        "context": [
            "a man sitting on a blue bench wearing headphones.",
            "a bed with a book on it and a lamp on the nightstand."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2338174",
                "VG_object_id": "2273356",
                "bbox": [75, 201, 240, 290],
                "image": "data\\images\\2338174.jpg"
            },
            {
                "VG_image_id": "2380549",
                "VG_object_id": "544958",
                "bbox": [49, 252, 155, 308],
                "image": "data\\images\\2380549.jpg"
            }
        ],
        "questions_with_scores": [
            ["what device does the screen belong to", 2],
            ["What is the screen used for", 1],
            ["how many people are there", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["Where is the screen", -1],
            ["What is the screen used for", 1],
            ["what device does the screen belong to", 2],
            ["what is in front of the screen", -1],
            ["how many people are there", 1],
            ["who is in the photo", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a woman leaning on a microwave in a kitchen.",
            "a living room with a fireplace, television, and a fireplace."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2357829",
                "VG_object_id": "808220",
                "bbox": [262, 63, 358, 176],
                "image": "data\\images\\2357829.jpg"
            },
            {
                "VG_image_id": "2407425",
                "VG_object_id": "279686",
                "bbox": [156, 193, 222, 280],
                "image": "data\\images\\2407425.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the boy's shirt", 2],
            ["What sport is the boy doing", 1],
            ["What is the boy holding", 1],
            ["What is man doing", 1]
        ],
        "org_questions": [
            ["What color is the boy's shirt", 2],
            ["What sport is the boy doing", 1],
            ["What is the boy holding", 1],
            ["how many people are the in the picture", -1],
            ["where is the photo taken", -1],
            ["what is the man wearing on his head", -1],
            ["What is man doing", 1],
            ["what is the person wearing", -1],
            ["when was the picture taken", -1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a young boy swinging a baseball bat at a ball."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2396337",
                "VG_object_id": "1198728",
                "bbox": [209, 99, 342, 279],
                "image": "data\\images\\2396337.jpg"
            },
            {
                "VG_image_id": "2375081",
                "VG_object_id": "723760",
                "bbox": [200, 230, 283, 293],
                "image": "data\\images\\2375081.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 1],
            ["what color is the man's pant", 1],
            ["how many people are in the picture", 1],
            ["what color is the shirt", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what color is the man's pant", 1],
            ["how many people are in the picture", 1],
            ["What is person doing", -1],
            ["what is the persion holding", -1],
            ["what is the person wearing", -1],
            ["what color is the shirt", 1],
            ["when was the picture taken", -1],
            ["what color is the grass", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a young boy holding a frisbee in his hand.",
            "a man in a field with a frisbee."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2367238",
                "VG_object_id": "1811284",
                "bbox": [0, 34, 272, 346],
                "image": "data\\images\\2367238.jpg"
            },
            {
                "VG_image_id": "2346825",
                "VG_object_id": "2253575",
                "bbox": [145, 19, 395, 331],
                "image": "data\\images\\2346825.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is the man doing", 1],
            ["what gesture is the man", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["how many people are in the picture", 2],
            ["where is the man", -1],
            ["what is the man holding", -1],
            ["what gesture is the man", 1],
            ["how many bikes are there", -1],
            ["What is person doing", -1],
            ["what is on the man's face", -1],
            ["what is behind the man", -1],
            ["what is the persion sitting on", -1],
            ["what kind of shirt is the man wearing", -1]
        ],
        "context": [
            "a man and a woman sitting at a table with a child.",
            "a man standing at a table with a laptop."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2350099",
                "VG_object_id": "1884669",
                "bbox": [334, 168, 397, 258],
                "image": "data\\images\\2350099.jpg"
            },
            {
                "VG_image_id": "2322857",
                "VG_object_id": "3313417",
                "bbox": [214, 0, 373, 366],
                "image": "data\\images\\2322857.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bottle are in the picture", 1],
            ["How many people are there", 1],
            ["how many bottles", 1],
            ["what is next to the bottle", 1],
            ["how many bottles are there", 1],
            ["what is on the left of the bottle", 1],
            ["who is in the photo", 1],
            ["what is behind the bottle", 1]
        ],
        "org_questions": [
            ["what color is the bottle", -1],
            ["where is the bottle", -1],
            ["how many bottle are in the picture", 1],
            ["Where is the photo taken", -1],
            ["How many people are there", 1],
            ["how many bottles", 1],
            ["what is next to the bottle", 1],
            ["what is in the bottle", -1],
            ["how many bottles are there", 1],
            ["where is the photo taken", -1],
            ["What is the table made of", -1],
            ["what is on the left of the bottle", 1],
            ["who is in the photo", 1],
            ["what is behind the bottle", 1],
            ["what is on the table", -1]
        ],
        "context": [
            "a group of people sitting around a table with wine glasses.",
            "a bottle of drink next to a bottle of carrots."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2376897",
                "VG_object_id": "1926189",
                "bbox": [352, 0, 494, 385],
                "image": "data\\images\\2376897.jpg"
            },
            {
                "VG_image_id": "2346208",
                "VG_object_id": "1800730",
                "bbox": [95, 145, 205, 236],
                "image": "data\\images\\2346208.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["what is the person holding", 1],
            ["what is the weather like", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["What is person looking at", 1],
            ["who is wearing a black shirt", 1]
        ],
        "org_questions": [
            ["what is the person holding", 1],
            ["what is the person doing", 2],
            ["what is the weather like", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["What is person looking at", 1],
            ["how many people are there", -1],
            ["when was this photo taken", -1],
            ["who is wearing a black shirt", 1],
            ["what are the people wearing", -1]
        ],
        "context": [
            "a woman walking down a sidewalk with a bag on her shoulder.",
            "a group of people sitting on a grass covered field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2352929",
                "VG_object_id": "3581078",
                "bbox": [158, 91, 279, 208],
                "image": "data\\images\\2352929.jpg"
            },
            {
                "VG_image_id": "2414989",
                "VG_object_id": "148123",
                "bbox": [236, 52, 375, 272],
                "image": "data\\images\\2414989.jpg"
            }
        ],
        "questions_with_scores": [["what is the woman wearing", 2]],
        "org_questions": [
            ["what is the woman wearing", 2],
            ["what color is the surfboard", -1],
            ["HOw many people are ther", -1],
            ["where is the woman", -1],
            ["what is the woman doing", -1],
            ["what is in the distance", -1],
            ["what gesture is the woman", -1],
            ["who is surfing", -1],
            ["what is the woman standing on", -1],
            ["who is on the surfboard", -1],
            ["what color is the woman's hair", -1]
        ],
        "context": [
            "a woman riding a surfboard on a wave in the ocean.",
            "a woman riding a surfboard on top of a wave."
        ]
    },
    {
        "object_category": "skier",
        "images": [
            {
                "VG_image_id": "2356641",
                "VG_object_id": "3771776",
                "bbox": [218, 105, 308, 252],
                "image": "data\\images\\2356641.jpg"
            },
            {
                "VG_image_id": "2359930",
                "VG_object_id": "2405339",
                "bbox": [141, 52, 313, 309],
                "image": "data\\images\\2359930.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the man's pants", 2],
            ["How many people are there", 2],
            ["what color is the man's shirt", 1],
            ["what time is it", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the color of the man's pants", 2],
            ["what time is it", 1],
            ["How many people are there", 2],
            ["what is in the distance", -1],
            ["what is the gender of the person", -1],
            ["what kind of hat is the skier wearing", -1],
            ["when is the picture taken", -1],
            ["who is skiing", -1],
            ["where is the man", -1],
            ["what are the people doing", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a person skiing on a snowy hill",
            "two people on skis in the snow at night."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2340451",
                "VG_object_id": "947553",
                "bbox": [191, 100, 461, 228],
                "image": "data\\images\\2340451.jpg"
            },
            {
                "VG_image_id": "2377797",
                "VG_object_id": "2014111",
                "bbox": [6, 101, 220, 301],
                "image": "data\\images\\2377797.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are there", 2],
            ["what is the horse doing", 1],
            ["what color is the grass", 1],
            ["what is the ground covered with", 1],
            ["what is behind the horse", 1]
        ],
        "org_questions": [
            ["what is the horse doing", 1],
            ["what is on the horse", -1],
            ["how many horses are there", 2],
            ["what color is the grass", 1],
            ["what is the ground covered with", 1],
            ["what color is the ground", -1],
            ["what kind of animal is this", -1],
            ["what is behind the horse", 1]
        ],
        "context": [
            "a person riding a horse jumping over a post.",
            "a group of horses standing on a cobblestone street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2334403",
                "VG_object_id": "2754013",
                "bbox": [51, 8, 313, 303],
                "image": "data\\images\\2334403.jpg"
            },
            {
                "VG_image_id": "2347494",
                "VG_object_id": "1956027",
                "bbox": [354, 118, 417, 345],
                "image": "data\\images\\2347494.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["where is the man", 2],
            ["what is the man wearing", 1],
            ["how many people are there", 1],
            ["what is the persion on the left wearing on face", 1],
            ["What is man doing", 1],
            ["what is the man holding", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["where is the man", 2],
            ["what is the man wearing", 1],
            ["how many people are there", 1],
            ["what is the race of the man", -1],
            ["what is the persion on the left wearing on face", 1],
            ["What is man doing", 1],
            ["when was the photo taken", -1],
            ["what is the man holding", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a man sitting on the beach with a surfboard.",
            "a group of women standing on a street."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2369582",
                "VG_object_id": "1772335",
                "bbox": [6, 42, 499, 374],
                "image": "data\\images\\2369582.jpg"
            },
            {
                "VG_image_id": "2412279",
                "VG_object_id": "198401",
                "bbox": [80, 161, 231, 401],
                "image": "data\\images\\2412279.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cats are there", 2],
            ["what is the cat sitting on", 1],
            ["what is in front of the cat", 1]
        ],
        "org_questions": [
            ["how many cats are there", 2],
            ["what color is the cat", -1],
            ["what is the cat sitting on", 1],
            ["where are the cats", -1],
            ["What is the cat doing", -1],
            ["what is in front of the cat", 1],
            ["Where is the cat", -1],
            ["what animal is in the picture", -1],
            ["who is in the picture", -1],
            ["what is the cat looking at", -1],
            ["what is behind the cat", -1],
            ["what type of animal is shown", -1]
        ],
        "context": [
            "two cats looking out of a window with a vase of flowers.",
            "a cat sitting on a bed with a white and black blanket."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2403821",
                "VG_object_id": "349137",
                "bbox": [95, 96, 195, 235],
                "image": "data\\images\\2403821.jpg"
            },
            {
                "VG_image_id": "2365578",
                "VG_object_id": "3883494",
                "bbox": [168, 140, 302, 269],
                "image": "data\\images\\2365578.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["What sports is man doing", 2],
            ["what sport is the man playing", 1],
            ["what is the man holding", 1],
            ["What is man doing", 1],
            ["what is the person doing", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what sport is the man playing", 1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["What is man doing", 1],
            ["What sports is man doing", 2],
            ["what is the person doing", 1],
            ["who is wearing a white shirt", -1],
            ["when was the photo taken", -1],
            ["what is on the man's head", 1],
            ["what is the player wearing", -1]
        ],
        "context": [
            "a man in a red shirt and white shorts playing tennis.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2323128",
                "VG_object_id": "3168260",
                "bbox": [2, 216, 496, 301],
                "image": "data\\images\\2323128.jpg"
            },
            {
                "VG_image_id": "2378125",
                "VG_object_id": "2034965",
                "bbox": [3, 5, 494, 434],
                "image": "data\\images\\2378125.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are there on the field", 2],
            ["what color is the field", 1],
            ["what color are the plants", 1],
            ["what animal is in the field", 1],
            ["what are the animals doing", 1],
            ["how many people are there in the picture", 1],
            ["what is the main color of the ground", 1]
        ],
        "org_questions": [
            ["what color is the field", 1],
            ["how many horses are there on the field", 2],
            ["what color are the plants", 1],
            ["what animal is in the field", 1],
            ["what are the animals doing", 1],
            ["how many people are there in the picture", 1],
            ["what is the main color of the ground", 1],
            ["where was this photo taken", -1],
            ["what is in the background", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1]
        ],
        "context": [
            "a man riding a horse drawn carriage down a road.",
            "a herd of sheep grazing on a grassy hill."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2351730",
                "VG_object_id": "1694480",
                "bbox": [151, 120, 262, 326],
                "image": "data\\images\\2351730.jpg"
            },
            {
                "VG_image_id": "2327375",
                "VG_object_id": "3511102",
                "bbox": [110, 105, 248, 301],
                "image": "data\\images\\2327375.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's coat", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what is beside the man", -1],
            ["what color is the man's coat", 1],
            ["What is man wearing on his face", -1],
            ["where is the man", -1],
            ["what is the man wearing", -1],
            ["what color is the background", -1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["who is skiing", -1],
            ["what is the person holding", 1]
        ],
        "context": [
            "a man and woman are standing in the snow on skis.",
            "a man riding a snowboard down a rail."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2383218",
                "VG_object_id": "1322816",
                "bbox": [22, 215, 332, 495],
                "image": "data\\images\\2383218.jpg"
            },
            {
                "VG_image_id": "2366054",
                "VG_object_id": "1717964",
                "bbox": [0, 20, 499, 330],
                "image": "data\\images\\2366054.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is on the table", 1],
            ["what is the table on", 1],
            ["what food is on the plate", 1],
            ["how many glasses are there on the table", 1]
        ],
        "org_questions": [
            ["what is on the table", 1],
            ["how many people are in the picture", 2],
            ["what color is the table ", -1],
            ["what is the table on", 1],
            ["what food is on the plate", 1],
            ["how many glasses are there on the table", 1],
            ["where was the photo taken", -1],
            ["where is the plate", -1],
            ["what is the table color", -1],
            ["what is covering the table", -1]
        ],
        "context": [
            "a table full of plates of food.",
            "a chocolate cake with white frosting and a glass of water."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2385193",
                "VG_object_id": "1299756",
                "bbox": [1, 174, 108, 374],
                "image": "data\\images\\2385193.jpg"
            },
            {
                "VG_image_id": "2373291",
                "VG_object_id": "2123173",
                "bbox": [18, 207, 86, 410],
                "image": "data\\images\\2373291.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["how many people are there", 1],
            ["What is the ground made of", 1]
        ],
        "org_questions": [
            ["what is the girl doing", -1],
            ["what color is the ground", 2],
            ["how many people are there", 1],
            ["what is on the girl's head", -1],
            ["What is the ground made of", 1],
            ["What is the woman holding", -1],
            ["what kind of clothes is the girl wearing", -1],
            ["What color is the person's shirt", -1],
            ["when was the photo taken", -1],
            ["where is the woman", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a woman is painting an elephant on a wooden fence.",
            "two people in a field flying a kite."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2391981",
                "VG_object_id": "1235282",
                "bbox": [112, 136, 412, 399],
                "image": "data\\images\\2391981.jpg"
            },
            {
                "VG_image_id": "2374743",
                "VG_object_id": "2512056",
                "bbox": [75, 195, 324, 498],
                "image": "data\\images\\2374743.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the cat on", 1],
            ["where are the cats", 1],
            ["what is the cat doing", 1],
            ["what is the cat sitting on", 1]
        ],
        "org_questions": [
            ["what color is the cat", -1],
            ["what is the cat on", 1],
            ["how many cats are there", -1],
            ["where are the cats", 1],
            ["what is the cat doing", 1],
            ["what is the cat sitting on", 1],
            ["what type of animal is shown", -1],
            ["what is on the cat's head", -1],
            ["who is in the picture", -1],
            ["what is behind the cat", -1]
        ],
        "context": [
            "a cat laying on top of a laptop computer.",
            "a cat sitting on a train looking out a window."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2317454",
                "VG_object_id": "2738653",
                "bbox": [65, 81, 248, 238],
                "image": "data\\images\\2317454.jpg"
            },
            {
                "VG_image_id": "2395461",
                "VG_object_id": "1205220",
                "bbox": [323, 176, 493, 370],
                "image": "data\\images\\2395461.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["what is on the table", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["what color is the man's shirt", 1],
            ["what is on the man's wrist", 1]
        ],
        "org_questions": [
            ["how many people are in the picture", 1],
            ["what is on the table", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["what is the gender of the person", -1],
            ["where is the man", -1],
            ["what color is the man's shirt", 1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1],
            ["what is on the man's wrist", 1]
        ],
        "context": [
            "a man and a child sitting at a table with pizza.",
            "a man standing in front of a laptop computer."
        ]
    },
    {
        "object_category": "banana",
        "images": [
            {
                "VG_image_id": "2364910",
                "VG_object_id": "637879",
                "bbox": [18, 55, 272, 380],
                "image": "data\\images\\2364910.jpg"
            },
            {
                "VG_image_id": "2400607",
                "VG_object_id": "407467",
                "bbox": [308, 23, 411, 293],
                "image": "data\\images\\2400607.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the banana", 1],
            ["how is the banana", 1]
        ],
        "org_questions": [
            ["what color is the banana", 1],
            ["what is on the top of the banana", -1],
            ["what is beside the banana", -1],
            ["how many people are there", -1],
            ["What fruits are there", -1],
            ["how many bananas are in the picture", -1],
            ["how is the banana", 1],
            ["what kind of fruit is this", -1]
        ],
        "context": [
            "a blue banana with a red ball",
            "a banana with a banana on top of it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2358215",
                "VG_object_id": "2391315",
                "bbox": [13, 15, 209, 293],
                "image": "data\\images\\2358215.jpg"
            },
            {
                "VG_image_id": "2346898",
                "VG_object_id": "2889757",
                "bbox": [296, 123, 441, 294],
                "image": "data\\images\\2346898.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["how many people are there", 1],
            ["what is the woman wearing", 1],
            ["what is in front of the woman", 1],
            ["what is on the woman's face", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", -1],
            ["what is the woman holding", 1],
            ["what color is the wall", -1],
            ["how many people are there", 1],
            ["where is the photo taken", -1],
            ["what is the woman wearing", 1],
            ["what is in front of the woman", 1],
            ["where is the person", -1],
            ["who is in the photo", -1],
            ["what is the woman doing", -1],
            ["what is on the woman's face", 1],
            ["what is the woman sitting on", -1],
            ["What is the color of woman", -1],
            ["How many people are there", 1],
            ["What is woman doing", -1]
        ],
        "context": [
            "a family sitting at a table with a plate of food.",
            "a group of people sitting around a table with food."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2340531",
                "VG_object_id": "2198394",
                "bbox": [0, 444, 283, 499],
                "image": "data\\images\\2340531.jpg"
            },
            {
                "VG_image_id": "2376477",
                "VG_object_id": "3684900",
                "bbox": [0, 263, 396, 498],
                "image": "data\\images\\2376477.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the land", 1],
            ["what are people doing", 1],
            ["where was this picture taken", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what is on the land", -1],
            ["what is in the background", -1],
            ["how many people are there on the land", 1],
            ["where is the land", -1],
            ["what are people doing", 1],
            ["what is the ground covered with", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", 1],
            ["what is green", -1],
            ["what is the land made of", -1],
            ["how many people are there", 1],
            ["what is in the distance", -1],
            ["How many giraffes are there", -1],
            ["where was this taken", -1]
        ],
        "context": [
            "a person flying a kite in the sky.",
            "two boys playing with a red ball in a grassy field."
        ]
    },
    {
        "object_category": "bird",
        "images": [
            {
                "VG_image_id": "2393289",
                "VG_object_id": "1221264",
                "bbox": [67, 38, 387, 94],
                "image": "data\\images\\2393289.jpg"
            },
            {
                "VG_image_id": "2358026",
                "VG_object_id": "3548849",
                "bbox": [63, 48, 153, 115],
                "image": "data\\images\\2358026.jpg"
            }
        ],
        "questions_with_scores": [["What is under the bird", 1]],
        "org_questions": [
            ["What is under the bird", 1],
            ["How many birds are there in the image", -1],
            ["what color is the sky", -1],
            ["what is the bird standing on", -1],
            ["what is the color of the background", -1],
            ["when was this picture taken", -1],
            ["how is the weather", -1],
            ["where are the birds", -1],
            ["what is flying in the sky", -1],
            ["what is flying", -1]
        ],
        "context": [
            "a boat is traveling across the ocean with birds flying around.",
            "a bird flying over a building with a tower."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2338905",
                "VG_object_id": "952833",
                "bbox": [95, 62, 474, 411],
                "image": "data\\images\\2338905.jpg"
            },
            {
                "VG_image_id": "2323078",
                "VG_object_id": "3427755",
                "bbox": [27, 53, 372, 394],
                "image": "data\\images\\2323078.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cow's head", 2],
            ["what color is the cow", 1],
            ["where is the cow", 1],
            ["What color is cow", 1],
            ["what is on the cow", 1]
        ],
        "org_questions": [
            ["what color is the cow", 1],
            ["where is the cow", 1],
            ["how many cows are there", -1],
            ["what is the cow doing", -1],
            ["What color is cow", 1],
            ["what color is the cow's head", 2],
            ["when was this photo taken", -1],
            ["what is the cow looking at", -1],
            ["where was this photo taken", -1],
            ["what is on the cow", 1]
        ],
        "context": [
            "a cow standing next to a metal chain.",
            "a cow standing in a field with a sky background"
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2343903",
                "VG_object_id": "3635282",
                "bbox": [152, 298, 229, 458],
                "image": "data\\images\\2343903.jpg"
            },
            {
                "VG_image_id": "2364641",
                "VG_object_id": "1887878",
                "bbox": [245, 5, 335, 280],
                "image": "data\\images\\2364641.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the person doing", 2],
            ["What color is person's shirt", 2],
            ["Where are people", 1],
            ["what is the person holding", 1],
            ["where is the person", 1],
            ["what is the person in the middle doing", 1],
            ["who is in the photo", 1],
            ["what is the man standing on", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What is the person doing", 2],
            ["What color is person's shirt", 2],
            ["Where are people", 1],
            ["what is on the person's head", -1],
            ["what is the person holding", 1],
            ["where is the person", 1],
            ["what is the person in the middle doing", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", 1],
            ["what is the man standing on", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a man and woman flying a kite on a beach.",
            "a man sitting on a motorcycle on the street."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2361364",
                "VG_object_id": "2134480",
                "bbox": [286, 101, 481, 229],
                "image": "data\\images\\2361364.jpg"
            },
            {
                "VG_image_id": "1160178",
                "VG_object_id": "3534380",
                "bbox": [3, 296, 878, 677],
                "image": "data\\images\\1160178.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bicycles are there in the picture", 2],
            ["where is the bicycle", 2],
            ["what is in the background", 1],
            ["what is the ground covered with", 1],
            ["What is background of image", 1],
            ["where was this picture taken", 1],
            ["what is on the ground", 1],
            ["what is next to the bike", 1]
        ],
        "org_questions": [
            ["how many bicycles are there in the picture", 2],
            ["where is the bicycle", 2],
            ["what is in the background", 1],
            ["What time is it", -1],
            ["what is the ground covered with", 1],
            ["What is the vehicle", -1],
            ["What is background of image", 1],
            ["when was the photo taken", -1],
            ["where was this picture taken", 1],
            ["what is on the ground", 1],
            ["what is next to the bike", 1]
        ],
        "context": [
            "a bike parked in a field of yellow flowers.",
            "a large group of bikes parked on the sidewalk."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2379097",
                "VG_object_id": "1361166",
                "bbox": [19, 341, 336, 498],
                "image": "data\\images\\2379097.jpg"
            },
            {
                "VG_image_id": "2415987",
                "VG_object_id": "3346857",
                "bbox": [281, 206, 484, 373],
                "image": "data\\images\\2415987.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["how many people are there", 1],
            ["how many tennis balls are there on the ground", 1],
            ["how many tennis rackets are there on the ground", 1],
            ["What is the ground made of", 1],
            ["what is the picture taken", 1],
            ["What is on the ground", 1],
            ["what is the land made of", 1],
            ["who is in the photo", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["how many tennis balls are there on the ground", 1],
            ["how many tennis rackets are there on the ground", 1],
            ["What is the ground made of", 1],
            ["what is the picture taken", 1],
            ["What is on the ground", 1],
            ["what is the land made of", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["where was this photo taken", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a tennis racket and tennis balls on a court.",
            "a group of people standing in front of a sign."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2361800",
                "VG_object_id": "2569363",
                "bbox": [57, 109, 134, 188],
                "image": "data\\images\\2361800.jpg"
            },
            {
                "VG_image_id": "2395543",
                "VG_object_id": "1204660",
                "bbox": [326, 252, 390, 300],
                "image": "data\\images\\2395543.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the gesture of the man", 1],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["what is the gesture of the man", 1],
            ["what is the man doing", 1],
            ["where is the man", -1],
            ["how many people are there", 1],
            ["what color is the person's shirt", -1],
            ["what is the gender of the person", -1],
            ["what color is the shirt", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1]
        ],
        "context": [
            "a dog sitting on a sidewalk next to a bar.",
            "a man milking a cow in a barn."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2349738",
                "VG_object_id": "2899399",
                "bbox": [1, 75, 498, 407],
                "image": "data\\images\\2349738.jpg"
            },
            {
                "VG_image_id": "2408203",
                "VG_object_id": "266125",
                "bbox": [0, 106, 407, 227],
                "image": "data\\images\\2408203.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["how many players are there", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["what is on the ground", -1],
            ["what is the man wearing", -1],
            ["what is the ground covered with", -1],
            ["where was the photo taken", -1],
            ["what sport is being played", -1],
            ["where is the grass", -1],
            ["where is this scene", -1],
            ["how many players are there", 1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball",
            "a baseball player standing on a field holding a bat."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2397625",
                "VG_object_id": "432229",
                "bbox": [270, 305, 380, 358],
                "image": "data\\images\\2397625.jpg"
            },
            {
                "VG_image_id": "2373617",
                "VG_object_id": "588369",
                "bbox": [191, 406, 267, 472],
                "image": "data\\images\\2373617.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 2],
            ["what room is it", 2],
            ["Where is the photo taken", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["what color is the floor", 2],
            ["what room is the floor in", -1],
            ["what is on the floor", -1],
            ["how many people are there", -1],
            ["Where is the photo taken", 1],
            ["What is the floor made of", -1],
            ["how many doors are there", -1],
            ["what color is the wall", -1],
            ["what shape is the floor", -1],
            ["what type of floor is this", -1],
            ["where is the floor", -1],
            ["what room is this", 1],
            ["what room is it", 2]
        ],
        "context": [
            "a bathroom with a toilet, sink, and a cabinet.",
            "a room with a bed and a table and pictures on the wall"
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2384611",
                "VG_object_id": "1307137",
                "bbox": [27, 66, 158, 238],
                "image": "data\\images\\2384611.jpg"
            },
            {
                "VG_image_id": "2387039",
                "VG_object_id": "679459",
                "bbox": [335, 20, 500, 331],
                "image": "data\\images\\2387039.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the computer", 1],
            ["how many computers are there", 1],
            ["what is in front of the computer", 1],
            ["what is the main color of the laptop", 1]
        ],
        "org_questions": [
            ["what color is the computer", 1],
            ["how many computers are there", 1],
            ["what is on the computer screen", -1],
            ["where is the laptop", -1],
            ["What is next to the laptop", -1],
            ["What is above the laptop", -1],
            ["how many people are there", -1],
            ["what is on top of the laptop", -1],
            ["what is in front of the computer", 1],
            ["what is on the desk", -1],
            ["where are the computers", -1],
            ["what is the main color of the laptop", 1]
        ],
        "context": [
            "a table with several electronic equipment and electronics on it.",
            "a man sitting at a desk with a computer and a monitor."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2414141",
                "VG_object_id": "159173",
                "bbox": [11, 201, 93, 398],
                "image": "data\\images\\2414141.jpg"
            },
            {
                "VG_image_id": "2357601",
                "VG_object_id": "810698",
                "bbox": [24, 56, 149, 229],
                "image": "data\\images\\2357601.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's shirt", 2],
            ["what is the ground covered with", 1],
            ["what color is the ground", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the boy's shirt", 2],
            ["what is the ground covered with", 1],
            ["what color is the ground", 1],
            ["how many people are there in the picture", 1],
            ["what is the man doing", -1],
            ["what is the child holding", -1],
            ["what is the child on", -1],
            ["what is the child doing", -1],
            ["when was the picture taken", -1],
            ["what is on the man's head", -1],
            ["where is the boy", -1],
            ["who is on the skateboard", -1]
        ],
        "context": [
            "a young boy riding a skateboard on a bridge.",
            "a young boy riding a skateboard down a street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2358030",
                "VG_object_id": "2367994",
                "bbox": [171, 167, 318, 272],
                "image": "data\\images\\2358030.jpg"
            },
            {
                "VG_image_id": "2372080",
                "VG_object_id": "737598",
                "bbox": [65, 90, 249, 437],
                "image": "data\\images\\2372080.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 1],
            ["what is the man sitting on", 1],
            ["what is the ground covered with", 1],
            ["How many people are there", 1],
            ["What is person doing", 1],
            ["what gesture is the man", 1],
            ["what is the man holding", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what is the man wearing", 1],
            ["what is the man sitting on", 1],
            ["what is the ground covered with", 1],
            ["How many people are there", 1],
            ["What is person doing", 1],
            ["what gesture is the man", 1],
            ["what is the man holding", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["where are the people", -1],
            ["what is behind the man", 1]
        ],
        "context": [
            "two men riding horses on a mountain side.",
            "a man is sitting on a bench reading a newspaper."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2348897",
                "VG_object_id": "878939",
                "bbox": [65, 171, 208, 311],
                "image": "data\\images\\2348897.jpg"
            },
            {
                "VG_image_id": "2339212",
                "VG_object_id": "3697320",
                "bbox": [133, 217, 211, 329],
                "image": "data\\images\\2339212.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["where is the chair", 1],
            ["what is the floor made of", 1],
            ["where is the picture taken", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["who is sitting on the chair", -1],
            ["where is the chair", 1],
            ["what shape is the back of the chair", -1],
            ["what is the floor made of", 1],
            ["how many chairs are there in the picture", -1],
            ["what is on the chair", -1],
            ["what is the chair made of", -1],
            ["what is the man doing", -1],
            ["what is in the photo", -1],
            ["where is the picture taken", 1],
            ["how many people are there", 1]
        ],
        "context": [
            "a man sitting on a chair with a can of soda.",
            "a tennis player is on the court playing tennis."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2383556",
                "VG_object_id": "531769",
                "bbox": [2, 174, 236, 500],
                "image": "data\\images\\2383556.jpg"
            },
            {
                "VG_image_id": "2367964",
                "VG_object_id": "1952652",
                "bbox": [47, 8, 317, 498],
                "image": "data\\images\\2367964.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["What color is child's shirt", 2],
            ["what are the people doing", 1],
            ["what is the child holding", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what are the people doing", 1],
            ["What animal is the child with", -1],
            ["What color is child's shirt", 2],
            ["what is the child holding", 1],
            ["what are the child playing", -1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1],
            ["what is the woman wearing", -1]
        ],
        "context": [
            "a woman and a boy are smiling for the camera.",
            "a little girl holding a banana in her hand."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2351893",
                "VG_object_id": "857489",
                "bbox": [139, 154, 252, 238],
                "image": "data\\images\\2351893.jpg"
            },
            {
                "VG_image_id": "2335017",
                "VG_object_id": "3358882",
                "bbox": [97, 275, 184, 381],
                "image": "data\\images\\2335017.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["how many shirts are there", 2],
            ["what color is the shirt", 1],
            ["where is the person", 1],
            ["what gender is the person in the shirt", 1],
            ["what is the persion wearing on his head", 1],
            ["what is the posture of the person in the shirt", 1],
            ["What is the gender of person", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the person doing", 2],
            ["where is the person", 1],
            ["how many shirts are there", 2],
            ["what gender is the person in the shirt", 1],
            ["what is the persion wearing on his head", 1],
            ["what is the posture of the person in the shirt", 1],
            ["What is the gender of person", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a group of chefs preparing food in a kitchen.",
            "a woman sitting on a bench in front of a pipe organ."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2360192",
                "VG_object_id": "1799480",
                "bbox": [226, 37, 296, 97],
                "image": "data\\images\\2360192.jpg"
            },
            {
                "VG_image_id": "2318844",
                "VG_object_id": "1006729",
                "bbox": [213, 79, 290, 173],
                "image": "data\\images\\2318844.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 2],
            ["where is the man", 1],
            ["what is the man wearing on his head", 1],
            ["What is the man looking at", 1],
            ["what is the man holding", 1],
            ["where is the photo taken", 1],
            ["what is the boy wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 2],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["what is the man wearing on his head", 1],
            ["What is the man looking at", 1],
            ["what is the man holding", 1],
            ["where is the photo taken", 1],
            ["when was the photo taken", -1],
            ["what is the boy wearing", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a man riding a wave on top of a surfboard.",
            "a person riding a horse in a gravel area."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2400929",
                "VG_object_id": "1153509",
                "bbox": [137, 66, 209, 129],
                "image": "data\\images\\2400929.jpg"
            },
            {
                "VG_image_id": "2354847",
                "VG_object_id": "2433012",
                "bbox": [139, 291, 184, 392],
                "image": "data\\images\\2354847.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bottle", 2],
            ["what color is the table", 1],
            ["what is in the background", 1],
            ["what is on the side of the bottle", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", -1],
            ["what color is the bottle", 2],
            ["how many bottles are there", -1],
            ["where is the photo taken", -1],
            ["what is the table under the bottle made of", -1],
            ["what is in the background", 1],
            ["what is on the side of the bottle", 1],
            ["where is the bottle", -1],
            ["what is in the middle of the photo", -1]
        ],
        "context": [
            "a person holding scissors over a stack of papers.",
            "a desk with a laptop and a computer on it."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2359470",
                "VG_object_id": "793086",
                "bbox": [180, 142, 224, 174],
                "image": "data\\images\\2359470.jpg"
            },
            {
                "VG_image_id": "2368106",
                "VG_object_id": "3201860",
                "bbox": [237, 44, 316, 147],
                "image": "data\\images\\2368106.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["How many people are there", 1],
            ["how many mans are in the picture", 1],
            ["how many people are there", 1],
            ["what is the persion holding", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", -1],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["How many people are there", 1],
            ["how many mans are in the picture", 1],
            ["who is wearing a white shirt", -1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1],
            ["how many people are there", 1],
            ["what is the pattern of the person's shirt", -1],
            ["who is wearing the shirt", -1],
            ["what is the persion holding", 1],
            ["where is the picture taken", 1],
            ["what is the boy wearing", -1]
        ],
        "context": [
            "a man riding an elephant down a street.",
            "a group of young men riding skateboards on a sidewalk."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2376621",
                "VG_object_id": "3683942",
                "bbox": [7, 387, 370, 498],
                "image": "data\\images\\2376621.jpg"
            },
            {
                "VG_image_id": "2365508",
                "VG_object_id": "633680",
                "bbox": [31, 234, 497, 370],
                "image": "data\\images\\2365508.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many animals are there", 2],
            ["where is picture taken", 2],
            ["What kind of animal is on the land", 2],
            ["What color is the land", 1],
            ["What animal is on the land", 1],
            ["what is the ground covered with", 1],
            ["where is the land", 1],
            ["what is land made of", 1],
            ["what is the ground like", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["What color is the land", 1],
            ["What animal is on the land", 1],
            ["How many animals are there", 2],
            ["where is picture taken", 2],
            ["what is the ground covered with", 1],
            ["where is the land", 1],
            ["what is land made of", 1],
            ["how many trucks are there on the land", -1],
            ["how is the weather", -1],
            ["what is in the ground", -1],
            ["what is the ground like", 1],
            ["what is covering the ground", 1],
            ["What kind of animal is on the land", 2]
        ],
        "context": [
            "a giraffe standing next to a tree in a field.",
            "a group of cows standing in a barn."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2375125",
                "VG_object_id": "584170",
                "bbox": [282, 197, 362, 241],
                "image": "data\\images\\2375125.jpg"
            },
            {
                "VG_image_id": "2370743",
                "VG_object_id": "3856971",
                "bbox": [185, 441, 485, 496],
                "image": "data\\images\\2370743.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            [" what is the floor made of", 1],
            ["what shape is the floor", 1],
            ["what kind of floor", 1],
            ["what kind of flooring is in the picture", 1],
            ["what material is the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["where is the picture taken", -1],
            ["what is on the floor", -1],
            [" what is the floor made of", 1],
            ["what shape is the floor", 1],
            ["what kind of floor", 1],
            ["what kind of flooring is in the picture", 1],
            ["what material is the floor", 1],
            ["where is the rug", -1]
        ],
        "context": [
            "a dog laying on a rug in a living room.",
            "a kitchen with a sink and a stove"
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2332311",
                "VG_object_id": "3072742",
                "bbox": [1, 1, 106, 53],
                "image": "data\\images\\2332311.jpg"
            },
            {
                "VG_image_id": "2397057",
                "VG_object_id": "1193066",
                "bbox": [55, 188, 113, 226],
                "image": "data\\images\\2397057.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are in the picture", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the car", -1],
            ["what is in the background", -1],
            ["how many cars are in the picture", 1],
            ["what time is it", -1],
            ["where is the car", -1],
            ["what is the weather like", -1],
            ["when is the photo taken", -1],
            ["where was this photo taken", -1],
            ["when was this taken", -1],
            ["when was the picture taken", -1],
            ["what is the pattern on the car", -1],
            ["how many people are there", 1],
            ["what is in the distance", -1],
            ["When is photo taken", -1],
            ["what type of vehicle is in the picture", -1]
        ],
        "context": [
            "a bench sitting on the sidewalk in front of a car.",
            "a green truck driving down a street next to a tree."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2408388",
                "VG_object_id": "262691",
                "bbox": [0, 0, 499, 499],
                "image": "data\\images\\2408388.jpg"
            },
            {
                "VG_image_id": "2345422",
                "VG_object_id": "3371493",
                "bbox": [5, 177, 329, 484],
                "image": "data\\images\\2345422.jpg"
            }
        ],
        "questions_with_scores": [["how many cups are there", 2]],
        "org_questions": [
            ["what color is the table", -1],
            ["how many cups are there", 2],
            ["what is on the plate", -1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["what is the color of the plate", -1],
            ["what is on the table", -1],
            ["where was this picture taken", -1],
            ["what is the table color", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a banana split with whipped cream and cherries on a plate.",
            "a person pouring wine into a glass"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2349487",
                "VG_object_id": "2604864",
                "bbox": [19, 72, 138, 305],
                "image": "data\\images\\2349487.jpg"
            },
            {
                "VG_image_id": "2377797",
                "VG_object_id": "2015423",
                "bbox": [403, 90, 468, 245],
                "image": "data\\images\\2377797.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["where is the man", 1],
            ["what is the persion wearing on his face", 1],
            ["What is man holding", 1]
        ],
        "org_questions": [
            ["where is the man", 1],
            ["what is the man doing", -1],
            ["what color is the man's shirt", 2],
            ["how many people are there in the picture", -1],
            ["what is the persion wearing on his face", 1],
            ["What is man holding", 1],
            ["what is the man's posture", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion riding", -1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a man is walking next to an elephant.",
            "a group of horses standing on a cobblestone street."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2413221",
                "VG_object_id": "3057682",
                "bbox": [352, 82, 418, 326],
                "image": "data\\images\\2413221.jpg"
            },
            {
                "VG_image_id": "2365720",
                "VG_object_id": "2545399",
                "bbox": [40, 161, 204, 462],
                "image": "data\\images\\2365720.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color are the boy's shorts", 1],
            ["What is the boy doing", 1],
            ["What is in the background", 1],
            ["what is the boy holding", 1],
            ["What is child doing", 1],
            ["what is the persion standing on", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["What color are the boy's shorts", 1],
            ["What is the boy doing", 1],
            ["What is in the background", 1],
            ["how many people are there", -1],
            ["What is the boy wearing on his head", -1],
            ["where is the man", -1],
            ["what is the boy holding", 1],
            ["What is child doing", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion standing on", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "three children standing next to a train on a track.",
            "a young boy and girl are playing with a fire hydrant."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2345757",
                "VG_object_id": "3622310",
                "bbox": [134, 14, 439, 329],
                "image": "data\\images\\2345757.jpg"
            },
            {
                "VG_image_id": "2372781",
                "VG_object_id": "1827069",
                "bbox": [90, 15, 175, 145],
                "image": "data\\images\\2372781.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there ", 2],
            ["What is the boy holding", 1],
            ["What is the boy wearing on his face", 1],
            ["Where is the boy", 1],
            ["what color is the man's head", 1],
            ["what is in the background", 1],
            ["what is the boy doing", 1],
            ["what is the boy wearing", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["What is the boy holding", 1],
            ["What is the boy wearing on his face", 1],
            ["Where is the boy", 1],
            ["how many children are there ", 2],
            ["what color is the man's head", 1],
            ["what is in the background", 1],
            ["what is the boy doing", 1],
            ["what is the boy wearing", 1],
            ["when was the photo taken", -1],
            ["how many people are in the photo", 1]
        ],
        "context": [
            "a little boy holding an apple and eating an apple.",
            "a birthday cake with candles on it"
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2337635",
                "VG_object_id": "2382699",
                "bbox": [18, 173, 471, 369],
                "image": "data\\images\\2337635.jpg"
            },
            {
                "VG_image_id": "2343101",
                "VG_object_id": "927213",
                "bbox": [138, 54, 361, 374],
                "image": "data\\images\\2343101.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there", 1],
            ["what is in the background", 1],
            ["What is zebra doing", 1]
        ],
        "org_questions": [
            ["what is the zebra doing", -1],
            ["how many zebras are there", 1],
            ["what is in the background", 1],
            ["where is the zebra", -1],
            ["what is the color of the land", -1],
            ["what is the ground covered with", -1],
            ["how many people are there in the picture", -1],
            ["What is zebra doing", 1],
            ["what type of animal is shown", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["what is the zebra standing on", -1]
        ],
        "context": [
            "a group of zebras and giraffes in a zoo enclosure.",
            "a zebra standing in a field of tall grass."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2340709",
                "VG_object_id": "2359519",
                "bbox": [172, 82, 329, 314],
                "image": "data\\images\\2340709.jpg"
            },
            {
                "VG_image_id": "2342429",
                "VG_object_id": "934293",
                "bbox": [390, 9, 492, 230],
                "image": "data\\images\\2342429.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there ", 2],
            ["how many dogs are there", 2],
            ["What is man doing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["how many people are there ", 2],
            ["how many dogs are there", 2],
            ["where is the photo taken", -1],
            ["what is the man wearing", -1],
            ["What is man doing", 1],
            ["what is the man holding", 1],
            ["Where is the man", -1],
            ["who is wearing a black shirt", -1],
            ["when was the photo taken", -1],
            ["what is on the man's face", -1],
            ["what kind of pants is the man wearing", -1]
        ],
        "context": [
            "two men sitting on a bench with a dog.",
            "a man standing in the water with dogs."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2365578",
                "VG_object_id": "3883494",
                "bbox": [168, 140, 302, 269],
                "image": "data\\images\\2365578.jpg"
            },
            {
                "VG_image_id": "2356051",
                "VG_object_id": "2356488",
                "bbox": [133, 85, 184, 180],
                "image": "data\\images\\2356051.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 1],
            ["what is the person doing", 1],
            ["what color is the ground", 1],
            ["what is the man wearing on his head", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the person doing", 1],
            ["what color is the ground", 1],
            ["how many shirts are there", -1],
            ["where is the photo taken", -1],
            ["what is in the distance", -1],
            ["what is the man wearing on his head", 1],
            ["what is the player wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2410712",
                "VG_object_id": "362722",
                "bbox": [197, 148, 324, 328],
                "image": "data\\images\\2410712.jpg"
            },
            {
                "VG_image_id": "2376750",
                "VG_object_id": "570078",
                "bbox": [178, 122, 406, 306],
                "image": "data\\images\\2376750.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are on the elephant", 2],
            ["what is in the background", 1],
            ["who is riding the elephant", 1]
        ],
        "org_questions": [
            ["how many people are on the elephant", 2],
            ["where is the elephant", -1],
            ["what is on the elephant's back", -1],
            ["what are the elephants doing", -1],
            ["what is in the background", 1],
            ["what is in front of the elephants", -1],
            ["where is the nose of the elephant", -1],
            ["what kind of animal is this", -1],
            ["what color is the elephant", -1],
            ["who is riding the elephant", 1],
            ["what are the people riding", -1],
            ["what is the elephant standing on", -1]
        ],
        "context": [
            "a woman riding on the back of an elephant.",
            "a group of people riding on top of an elephant."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2332584",
                "VG_object_id": "969196",
                "bbox": [12, 25, 239, 197],
                "image": "data\\images\\2332584.jpg"
            },
            {
                "VG_image_id": "2404930",
                "VG_object_id": "336477",
                "bbox": [80, 72, 349, 333],
                "image": "data\\images\\2404930.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bus", 2],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what color is the bus", 2],
            ["how is the weather", 1],
            ["what is on the ground", -1],
            ["how many people are there", -1],
            ["where is the bus", -1],
            ["what is the bus doing", -1],
            ["what is the road made of", -1],
            ["what is in the distance", -1],
            ["when was the photo taken", -1],
            ["what type of bus is this", -1],
            ["what is on the side of the bus", -1],
            ["what is behind the bus", -1]
        ],
        "context": [
            "a bus splashing water on a street.",
            "a double decker bus driving down a street."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2344789",
                "VG_object_id": "2654396",
                "bbox": [22, 130, 239, 267],
                "image": "data\\images\\2344789.jpg"
            },
            {
                "VG_image_id": "43",
                "VG_object_id": "1061691",
                "bbox": [273, 176, 554, 416],
                "image": "data\\images\\43.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in front of the television", 2],
            ["what color is the wall behind the television", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the wall behind the television", 1],
            ["where is the television placed on", -1],
            ["what color is the television", -1],
            ["what time is it", -1],
            ["what is the television on", -1],
            ["what is in front of the television", 2],
            ["where is the television", -1],
            ["how many people are there", 1],
            ["when was the picture taken", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "a man and a boy playing a video game",
            "a small television on a counter in a kitchen."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2412759",
                "VG_object_id": "187413",
                "bbox": [159, 140, 459, 331],
                "image": "data\\images\\2412759.jpg"
            },
            {
                "VG_image_id": "2359146",
                "VG_object_id": "3046853",
                "bbox": [6, 205, 499, 327],
                "image": "data\\images\\2359146.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animal is on the floor", 2],
            ["what color is the animal", 1],
            ["what color is the floor", 1],
            ["how many people are there", 1],
            ["how many chairs are there", 1],
            ["where was the photo taken", 1],
            ["what type of floor is this", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what animal is on the floor", 2],
            ["what color is the animal", 1],
            ["what color is the floor", 1],
            ["how many people are there", 1],
            ["what is on the ground", -1],
            ["what is the floor made of", -1],
            ["what is covering the floor", -1],
            ["how many chairs are there", 1],
            ["where was the photo taken", 1],
            ["what type of floor is this", 1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a dog is standing in the kitchen looking at a table.",
            "a woman in a costume is standing next to a horse."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "1160045",
                "VG_object_id": "1600739",
                "bbox": [429, 286, 655, 680],
                "image": "data\\images\\1160045.jpg"
            },
            {
                "VG_image_id": "2366438",
                "VG_object_id": "2348030",
                "bbox": [121, 111, 256, 296],
                "image": "data\\images\\2366438.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's hat", 2],
            ["how many players are there in the photo", 1]
        ],
        "org_questions": [
            ["What color is the man's hat", 2],
            ["What is the man holding", -1],
            ["who is in the picture", -1],
            ["what is on the player's head", -1],
            ["how many players are there in the photo", 1],
            ["what is the player doing", -1],
            ["What is player holding", -1],
            ["where was the photo taken", -1],
            ["what sport is being played", -1],
            ["where is the batter standing", -1],
            ["who is holding the bat", -1]
        ],
        "context": [
            "a pitcher on the mound throwing a baseball.",
            "a baseball player swinging a bat at a ball"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2349002",
                "VG_object_id": "1822449",
                "bbox": [1, 185, 95, 315],
                "image": "data\\images\\2349002.jpg"
            },
            {
                "VG_image_id": "2400203",
                "VG_object_id": "1161662",
                "bbox": [132, 146, 258, 299],
                "image": "data\\images\\2400203.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the person playing", 2],
            ["what color is the person's shirt", 1],
            ["what is the gender of the person", 1],
            ["what color is the background", 1],
            ["what is the persion doing", 1],
            ["what is the person holding", 1],
            ["what color is the ground", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what is the gender of the person", 1],
            ["what color is the background", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the persion doing", 1],
            ["what is the person holding", 1],
            ["where is the person", -1],
            ["what is the persion wearing", -1],
            ["what color is the ground", 1],
            ["what sport is the person playing", 2]
        ],
        "context": [
            "a group of people playing a game of frisbee.",
            "a man playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2376700",
                "VG_object_id": "570398",
                "bbox": [108, 4, 500, 371],
                "image": "data\\images\\2376700.jpg"
            },
            {
                "VG_image_id": "2407378",
                "VG_object_id": "323577",
                "bbox": [175, 120, 254, 343],
                "image": "data\\images\\2407378.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 2],
            ["what is the guy doing", 1],
            ["what is the guy standing on ", 1],
            ["what color is the background", 1],
            ["what sport is the man doing", 1]
        ],
        "org_questions": [
            ["what is the guy doing", 1],
            ["what is the guy standing on ", 1],
            ["how many people are there", -1],
            ["what color is the background", 1],
            ["what is the man wearing on his face", -1],
            ["what is the person holding", 2],
            ["what is the person wearing", -1],
            ["what sport is the man doing", 1],
            ["where is the picture taken", -1],
            ["what is the gender of the person", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man is doing a trick on a skateboard.",
            "two men in shorts and t - shirts are on stage."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "150330",
                "VG_object_id": "1038224",
                "bbox": [2, 327, 1022, 704],
                "image": "data\\images\\150330.jpg"
            },
            {
                "VG_image_id": "2362980",
                "VG_object_id": "2286631",
                "bbox": [0, 257, 498, 334],
                "image": "data\\images\\2362980.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what color is the truck on the land", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["how many trucks are there on the land", -1],
            ["what color is the truck on the land", 1],
            ["where is picture taken", -1],
            ["what is the ground covered with", 1],
            ["where is the land", -1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["how is the weather", -1],
            ["what is on the side of the road", -1],
            ["what kind of vehicle is this", -1]
        ],
        "context": [
            "a bus parked in a parking lot with people standing on it.",
            "a blue truck with a boat on top of it"
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2353128",
                "VG_object_id": "849109",
                "bbox": [196, 285, 316, 346],
                "image": "data\\images\\2353128.jpg"
            },
            {
                "VG_image_id": "2329515",
                "VG_object_id": "974349",
                "bbox": [284, 271, 345, 353],
                "image": "data\\images\\2329515.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["where was the picture taken", 1],
            ["what is in the background", 1],
            ["what is next to the suitcase", 1]
        ],
        "org_questions": [
            ["what color is the bag", -1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["where was the picture taken", 1],
            ["what is in the background", 1],
            ["what is next to the suitcase", 1],
            ["what is the bag made of", -1],
            ["how many bags are there", -1]
        ],
        "context": [
            "a cat laying on top of a suitcase on a bed.",
            "a large amount of luggage sitting on the floor."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2366222",
                "VG_object_id": "1732829",
                "bbox": [47, 0, 240, 375],
                "image": "data\\images\\2366222.jpg"
            },
            {
                "VG_image_id": "1159530",
                "VG_object_id": "1594814",
                "bbox": [447, 177, 884, 633],
                "image": "data\\images\\1159530.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["how many people are there", 2],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is on the man's face", 1],
            ["what is in the man's hand", 1]
        ],
        "org_questions": [
            ["what is the man holding", 2],
            ["what is the man doing", 1],
            ["how many people are there", 2],
            ["What color is the shirt", -1],
            ["when is this picture taken", -1],
            ["where is the man", -1],
            ["what is the man wearing", 1],
            ["who is in the photo", -1],
            ["what is on the man's face", 1],
            ["when was the photo taken", -1],
            ["what is in the man's hand", 1]
        ],
        "context": [
            "two men preparing a turkey in a kitchen.",
            "a man sitting on a couch playing a video game."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2369251",
                "VG_object_id": "743831",
                "bbox": [153, 102, 376, 332],
                "image": "data\\images\\2369251.jpg"
            },
            {
                "VG_image_id": "2331164",
                "VG_object_id": "3026372",
                "bbox": [58, 180, 267, 277],
                "image": "data\\images\\2331164.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many cows are there", 2],
            ["What is near to the cow", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["How many cows are there", 2],
            ["What is near to the cow", 1],
            ["what color are the cows", -1],
            ["what is in the background", 1],
            ["What color is cow", -1],
            ["what color is the ground the cow standing on", -1],
            ["what color is the cow's head", -1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["what are the cows doing", -1],
            ["where are the cows", -1],
            ["what is on the cow's head", -1],
            ["what color is the cow", -1]
        ],
        "context": [
            "a cow standing in the middle of a street.",
            "a couple of cows walking down a street."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2386729",
                "VG_object_id": "517070",
                "bbox": [327, 261, 414, 334],
                "image": "data\\images\\2386729.jpg"
            },
            {
                "VG_image_id": "2368252",
                "VG_object_id": "618839",
                "bbox": [2, 212, 497, 377],
                "image": "data\\images\\2368252.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["what is the floor made of", 1],
            ["what pattern is the floor", 1],
            ["what is on the ground", 1],
            ["what type of floor is this", 1],
            ["what is the pattern on the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is the floor made of", 1],
            ["what pattern is the floor", 1],
            ["how many people are there", -1],
            ["what is on the ground", 1],
            ["what type of floor is this", 1],
            ["what is the pattern on the floor", 1]
        ],
        "context": [
            "a bed with a canopy and a canopy over it.",
            "a white vase sitting on top of a wooden floor."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "1159666",
                "VG_object_id": "1596199",
                "bbox": [158, 476, 238, 595],
                "image": "data\\images\\1159666.jpg"
            },
            {
                "VG_image_id": "2356151",
                "VG_object_id": "2483597",
                "bbox": [201, 148, 393, 371],
                "image": "data\\images\\2356151.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the towel", 1],
            ["who is on the towel", 1],
            ["what color is the towel on the right", 1]
        ],
        "org_questions": [
            ["what is the color of the towel", 1],
            ["what is on the wall", -1],
            ["what color is the wall", -1],
            ["what room is the towel in", -1],
            ["who is on the towel", 1],
            ["what is behind the towel", -1],
            ["what is the towel put on ", -1],
            ["how many towels are there", -1],
            ["where was the photo taken", -1],
            ["where are the towels", -1],
            ["what color is the towel on the right", 1]
        ],
        "context": [
            "a bathroom with a sink, mirror, and shower.",
            "a towel rack with a teddy bear and a towel."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2393248",
                "VG_object_id": "473744",
                "bbox": [184, 245, 403, 444],
                "image": "data\\images\\2393248.jpg"
            },
            {
                "VG_image_id": "2355919",
                "VG_object_id": "1820718",
                "bbox": [110, 198, 442, 330],
                "image": "data\\images\\2355919.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the gender of person", 2],
            ["What color is person's shirt", 1],
            ["What is person holding", 1],
            ["what is the persion holding", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["What is the gender of person", 2],
            ["What color is person's shirt", 1],
            ["What is person holding", 1],
            ["what is the persion holding", 1],
            ["how many people are there", -1],
            ["what is the person wearing", 1]
        ],
        "context": [
            "a man in a blue shirt and tie posing for a picture.",
            "a woman wearing sunglasses holding a yellow umbrella."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2317234",
                "VG_object_id": "3465289",
                "bbox": [81, 293, 288, 459],
                "image": "data\\images\\2317234.jpg"
            },
            {
                "VG_image_id": "150350",
                "VG_object_id": "1566884",
                "bbox": [604, 270, 997, 648],
                "image": "data\\images\\150350.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the shape of the cake", 1],
            ["what color is the table", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["how many cakes are there", -1],
            ["what is the shape of the cake", 1],
            ["what color is the table", 1],
            ["what is on top of the cake", -1],
            ["What is cake on", -1],
            ["what is on the cake", -1],
            ["how many people are there in the picture", 1],
            ["where is the cake", -1],
            ["what is the cake made of", -1],
            ["what kind of cake is this", -1],
            ["what is the cake", -1]
        ],
        "context": [
            "a little girl sitting in front of a chocolate cake.",
            "two cakes sitting on a table with a knife and fork."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2396464",
                "VG_object_id": "444001",
                "bbox": [302, 244, 375, 365],
                "image": "data\\images\\2396464.jpg"
            },
            {
                "VG_image_id": "2391990",
                "VG_object_id": "484332",
                "bbox": [49, 215, 137, 298],
                "image": "data\\images\\2391990.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is the floor made of", 2],
            ["what shape is the chair", 1],
            ["what is on the chair", 1],
            ["how many chairs are there", 1]
        ],
        "org_questions": [
            ["what shape is the chair", 1],
            ["what is the floor the chair on made of", -1],
            ["what is on the chair", 1],
            ["how many chairs are there", 1],
            ["Where is the chair", -1],
            ["what is in the room", -1],
            ["how many people are in the picture", 2],
            ["what is the floor made of", 2]
        ],
        "context": [
            "a man standing in a living room holding a wii controller.",
            "a kitchen with a table and chairs and a table."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2377162",
                "VG_object_id": "2721852",
                "bbox": [39, 98, 374, 245],
                "image": "data\\images\\2377162.jpg"
            },
            {
                "VG_image_id": "2326479",
                "VG_object_id": "2822587",
                "bbox": [48, 194, 385, 326],
                "image": "data\\images\\2326479.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what color is the plane", 1],
            ["how many planes are in the picture", 1],
            ["what is the ground covered with", 1],
            ["what color is the plane's tail", 1]
        ],
        "org_questions": [
            ["what color is the plane", 1],
            ["what color is the ground", 2],
            ["how many planes are in the picture", 1],
            ["when is the picture taken", -1],
            ["where is the plane", -1],
            ["what is the ground covered with", 1],
            ["what color is the plane's tail", 1],
            ["what is the plane doing", -1],
            ["what is in the background", -1],
            ["where was this picture taken", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a blue and white airplane is on the runway.",
            "a large jetliner flying over an airport."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2383387",
                "VG_object_id": "694407",
                "bbox": [158, 290, 323, 481],
                "image": "data\\images\\2383387.jpg"
            },
            {
                "VG_image_id": "2359432",
                "VG_object_id": "2295808",
                "bbox": [2, 39, 63, 210],
                "image": "data\\images\\2359432.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["what is on the chair", 1],
            ["how many people are in the picture", 1]
        ],
        "org_questions": [
            ["what is the chair made of", -1],
            ["what color is the chair", 2],
            ["what color is the ground", -1],
            ["how many chairs are there in the photo", -1],
            ["where is the chair", -1],
            ["what is on the chair", 1],
            ["how many people are in the picture", 1],
            ["what is the floor made of", -1],
            ["what is next to the chair", -1],
            ["what material is the chair made of", -1],
            ["where are the chairs", -1],
            ["what are the chairs made of", -1]
        ],
        "context": [
            "a man sitting at a desk using a laptop.",
            "a room with a row of tables and a laptop on top of it."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2401958",
                "VG_object_id": "1142199",
                "bbox": [385, 169, 457, 215],
                "image": "data\\images\\2401958.jpg"
            },
            {
                "VG_image_id": "2319910",
                "VG_object_id": "997653",
                "bbox": [249, 117, 334, 333],
                "image": "data\\images\\2319910.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["How many people are there", 1],
            ["What is man holding", 1],
            ["where are the people staying", 1],
            ["what is the persion holding", 1],
            ["what kind of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", -1],
            ["How many people are there", 1],
            ["What is man holding", 1],
            ["where is the photo taken", 2],
            ["where are the people staying", 1],
            ["what is the persion holding", 1],
            ["what kind of shirt is the man wearing", 1],
            ["what is on the man's face", -1]
        ],
        "context": [
            "a man holding an umbrella in the rain.",
            "a man and woman smiling at a formal dinner."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2347151",
                "VG_object_id": "892948",
                "bbox": [6, 325, 322, 496],
                "image": "data\\images\\2347151.jpg"
            },
            {
                "VG_image_id": "2357088",
                "VG_object_id": "2199186",
                "bbox": [11, 265, 493, 372],
                "image": "data\\images\\2357088.jpg"
            }
        ],
        "questions_with_scores": [
            ["When is photo taken", 2],
            ["what time is it", 2],
            ["how many cars are there on the street", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["When is photo taken", 2],
            ["How mant people are there", -1],
            ["what is in front of the car", -1],
            ["how is the weather", -1],
            ["what time is it", 2],
            ["how many cars are there on the street", 1],
            ["where was this photo taken", -1],
            ["what is the road made of", -1],
            ["where are the cars", -1],
            ["what is on the ground", -1],
            ["when was the picture taken", 1]
        ],
        "context": [
            "a street sign with a sticker on it",
            "a red stop light at night with cars passing by."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2380218",
                "VG_object_id": "546902",
                "bbox": [357, 45, 424, 237],
                "image": "data\\images\\2380218.jpg"
            },
            {
                "VG_image_id": "2354091",
                "VG_object_id": "1825540",
                "bbox": [67, 4, 136, 181],
                "image": "data\\images\\2354091.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing", 2],
            ["what is on woman's face", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["who is playing the disk", -1],
            ["what is the persion sitting on", -1],
            ["what is on woman's face", 1],
            ["how many people are there", 1],
            ["what is the woman doing", -1],
            ["what is the woman holding", 1],
            ["when was this photo taken", -1],
            ["what kind of pants is the woman wearing", -1],
            ["where was the photo taken", -1],
            ["who is wearing a white shirt", -1],
            ["what is the woman wearing", 2]
        ],
        "context": [
            "a young boy throwing a frisbee in a park.",
            "a dog catching a frisbee in its mouth."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2338494",
                "VG_object_id": "2745330",
                "bbox": [121, 146, 185, 279],
                "image": "data\\images\\2338494.jpg"
            },
            {
                "VG_image_id": "2331746",
                "VG_object_id": "2996237",
                "bbox": [237, 25, 356, 187],
                "image": "data\\images\\2331746.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing", 1],
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["what is the woman holding", 1],
            ["what is on the back of the woman", 1],
            ["what is the woman on", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", 1],
            ["what is the woman doing", 1],
            ["how many people are there", -1],
            ["where is the woman", 1],
            ["what gesture is the woman", -1],
            ["what is the woman holding", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is on the back of the woman", 1],
            ["what is the woman on", 1]
        ],
        "context": [
            "a person riding a horse in a corral.",
            "a man pulling a boat on the beach."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2402573",
                "VG_object_id": "388495",
                "bbox": [73, 51, 310, 233],
                "image": "data\\images\\2402573.jpg"
            },
            {
                "VG_image_id": "2400025",
                "VG_object_id": "1163768",
                "bbox": [58, 214, 330, 458],
                "image": "data\\images\\2400025.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bench", 2],
            ["what is the bench on", 1],
            ["what is behind the bench", 1],
            ["what is on the bench", 1],
            ["what is under the bench", 1]
        ],
        "org_questions": [
            ["what color is the bench", 2],
            ["what is the bench on", 1],
            ["who is on the bench", -1],
            ["when is this photo taken", -1],
            ["where is the bench", -1],
            ["what is behind the bench", 1],
            ["what is the bench made of", -1],
            ["how many benches are there", -1],
            ["what is on the bench", 1],
            ["what is made of wood", -1],
            ["what is in front of the bench", -1],
            ["what is under the bench", 1]
        ],
        "context": [
            "a blue book on a bench",
            "a little boy jumping on a bench in a park."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2366106",
                "VG_object_id": "2168228",
                "bbox": [290, 65, 413, 244],
                "image": "data\\images\\2366106.jpg"
            },
            {
                "VG_image_id": "2319306",
                "VG_object_id": "3442971",
                "bbox": [1, 173, 261, 447],
                "image": "data\\images\\2319306.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 2],
            ["what is on the man's face", 2],
            ["what color is the shirt", 1],
            ["how many faces are there in the picture", 1],
            ["What is the man doing", 1],
            ["what is the person holding", 1],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["what color is the man's shirt", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["where is the picture taken", 2],
            ["how many faces are there in the picture", 1],
            ["What is the man doing", 1],
            ["what is the person holding", 1],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["what color is the man's shirt", 1],
            ["what is on the man's face", 2],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man is using a laptop in a kitchen.",
            "two men sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2408098",
                "VG_object_id": "268277",
                "bbox": [128, 182, 246, 374],
                "image": "data\\images\\2408098.jpg"
            },
            {
                "VG_image_id": "2328459",
                "VG_object_id": "2789367",
                "bbox": [3, 13, 498, 279],
                "image": "data\\images\\2328459.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's hair", 2],
            ["where is the woman", 2],
            ["how many people are there in the picture", 2],
            ["what color is the background", 1],
            ["How many people are there", 1],
            ["what is the woman's posture", 1],
            ["who is in the photo", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's hair", 2],
            ["where is the woman", 2],
            ["what color is the background", 1],
            ["How many people are there", 1],
            ["What is woman holding", -1],
            ["what is the woman wearing", -1],
            ["what is the woman's posture", 1],
            ["who is in the photo", 1],
            ["what is the persion doing", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a restaurant with a menu on the wall.",
            "a woman laying on a bed with a pillow."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2348987",
                "VG_object_id": "1953005",
                "bbox": [179, 25, 226, 86],
                "image": "data\\images\\2348987.jpg"
            },
            {
                "VG_image_id": "2352733",
                "VG_object_id": "852086",
                "bbox": [167, 81, 286, 179],
                "image": "data\\images\\2352733.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the clock", 2],
            ["what is the clock on", 1],
            ["what color is the clock", 1],
            ["where is the clock", 1],
            ["what time is showed on the clock", 1],
            ["what is the clock made of", 1],
            ["what time is it", 1]
        ],
        "org_questions": [
            ["what is the clock on", 1],
            ["what shape is the clock", 2],
            ["what color is the clock", 1],
            ["how many people are there in the picture", -1],
            ["when is this picture taken", -1],
            ["where is the clock", 1],
            ["what time is showed on the clock", 1],
            ["what is the clock made of", 1],
            ["what time is it", 1],
            ["what is in the background", -1]
        ],
        "context": ["a door is open", "an apple with a clock attached to it."]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2382021",
                "VG_object_id": "1332012",
                "bbox": [252, 9, 402, 209],
                "image": "data\\images\\2382021.jpg"
            },
            {
                "VG_image_id": "2350153",
                "VG_object_id": "869797",
                "bbox": [251, 125, 374, 377],
                "image": "data\\images\\2350153.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the man", 2],
            ["what is on the man's head", 2],
            ["what is the man doing", 1],
            ["What is man holding", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["where is the man", 2],
            ["what is on the man's head", 2],
            ["how many people are there", -1],
            ["what color is the man's shirt", -1],
            ["What is man holding", 1],
            ["what is on the man's faces", -1],
            ["who is wearing a black shirt", -1],
            ["what is the persion standing on", 1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a woman blowing out candles on a cake.",
            "a man is preparing a hot dog cart with sausages."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2414989",
                "VG_object_id": "148123",
                "bbox": [236, 52, 375, 272],
                "image": "data\\images\\2414989.jpg"
            },
            {
                "VG_image_id": "2344392",
                "VG_object_id": "3282889",
                "bbox": [262, 22, 494, 331],
                "image": "data\\images\\2344392.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman doing", 2],
            ["What is the woman wearing on her head", 2],
            ["Where is the woman", 1],
            ["what is the woman on", 1],
            ["what is the girl wearing", 1]
        ],
        "org_questions": [
            ["What is woman doing", 2],
            ["Where is the woman", 1],
            ["How many people are there", -1],
            ["What is the woman wearing on her head", 2],
            ["what is the woman holding", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman on", 1],
            ["what is the girl wearing", 1]
        ],
        "context": [
            "a woman riding a surfboard on top of a wave.",
            "a woman is leading a cow with a saddle."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2396353",
                "VG_object_id": "445387",
                "bbox": [325, 9, 455, 125],
                "image": "data\\images\\2396353.jpg"
            },
            {
                "VG_image_id": "2405843",
                "VG_object_id": "371596",
                "bbox": [208, 81, 471, 182],
                "image": "data\\images\\2405843.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the cabinet", 1],
            ["WHat color is the wall", 1]
        ],
        "org_questions": [
            ["what is the color of the cabinet", 1],
            ["what room is the cabinet in", -1],
            ["what is on the wall", -1],
            ["where is the cabinets", -1],
            ["WHat color is the wall", 1],
            ["What is on the cabinet", -1],
            ["how many people are there", -1],
            ["what is in the background", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a group of men standing around a kitchen.",
            "a woman playing a video game with a remote."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2332345",
                "VG_object_id": "3712775",
                "bbox": [110, 113, 246, 331],
                "image": "data\\images\\2332345.jpg"
            },
            {
                "VG_image_id": "2402650",
                "VG_object_id": "387900",
                "bbox": [10, 12, 357, 495],
                "image": "data\\images\\2402650.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many persons are there", 2],
            ["how many people are there in the picture", 2],
            ["what color is the person's shirt", 1],
            ["what is the person doing", 1],
            ["what is the persion wearing", 1],
            ["who is in the photo", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["how many persons are there", 2],
            ["what is the person doing", 1],
            ["what is the persion wearing", 1],
            ["how many people are there in the picture", 2],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "three women are smiling and smiling in a kitchen.",
            "a man is eating a sandwich and smiling."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2348871",
                "VG_object_id": "1978654",
                "bbox": [277, 179, 498, 356],
                "image": "data\\images\\2348871.jpg"
            },
            {
                "VG_image_id": "2357148",
                "VG_object_id": "2187229",
                "bbox": [0, 0, 333, 499],
                "image": "data\\images\\2357148.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the keyboard", 2],
            ["how many people are there in the picture", 1],
            ["how many keyboards are there on the table", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the keyboard", 2],
            ["how many plates are there", -1],
            ["how many people are there in the picture", 1],
            ["how many keyboards are there on the table", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a woman sitting on a couch with a laptop.",
            "a small keyboard sitting next to a small keyboard."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2354627",
                "VG_object_id": "1760649",
                "bbox": [294, 26, 376, 233],
                "image": "data\\images\\2354627.jpg"
            },
            {
                "VG_image_id": "2414804",
                "VG_object_id": "151605",
                "bbox": [315, 256, 391, 374],
                "image": "data\\images\\2414804.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of vehicles is in the picture", 2],
            ["what is the man standing at", 1],
            ["where is the man", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what is the man standing at", 1],
            ["how many people are in the picture ", -1],
            ["where is the man", 1],
            ["what is the man doing", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion holding", 1],
            ["when was this picture taken", -1],
            ["what kind of vehicles is in the picture", 2]
        ],
        "context": [
            "two men standing next to a motorcycle on a metal surface.",
            "a man standing on a boat in a body of water."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2402588",
                "VG_object_id": "388328",
                "bbox": [129, 59, 249, 259],
                "image": "data\\images\\2402588.jpg"
            },
            {
                "VG_image_id": "2374030",
                "VG_object_id": "587456",
                "bbox": [69, 50, 167, 200],
                "image": "data\\images\\2374030.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is woman's shirt", 2],
            ["where is the woman sitting", 2],
            ["Where is the woman", 1]
        ],
        "org_questions": [
            ["What color is woman's shirt", 2],
            ["How many people are there", -1],
            ["Where is the woman", 1],
            ["what is above the woman", -1],
            ["when is the picture taken", -1],
            ["What is woman doing", -1],
            ["whatis the woman holding", -1],
            ["where is the woman sitting", 2],
            ["who is in the picture", -1],
            ["what is the woman sitting on", -1],
            ["what is the woman looking at", -1]
        ],
        "context": [
            "a woman sitting on a stone wall using a laptop.",
            "a man and a woman sitting at a table with computers."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2366605",
                "VG_object_id": "3878151",
                "bbox": [332, 12, 484, 346],
                "image": "data\\images\\2366605.jpg"
            },
            {
                "VG_image_id": "2398348",
                "VG_object_id": "426133",
                "bbox": [85, 162, 244, 355],
                "image": "data\\images\\2398348.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what color is the girls shirt", 1],
            ["what is the child doing", 1]
        ],
        "org_questions": [
            ["what color is the girls shirt", 1],
            ["how many people are in the picture", 2],
            ["where is the girl", -1],
            ["which age group does the girl belong to", -1],
            ["what is on the girl's head", -1],
            ["how old is the girl", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the child doing", 1],
            ["what is the child wearing", -1]
        ],
        "context": [
            "a child in a pink jacket and glasses playing with a broken toilet.",
            "two children playing with a toy car in a living room."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2373421",
                "VG_object_id": "732508",
                "bbox": [4, 38, 495, 365],
                "image": "data\\images\\2373421.jpg"
            },
            {
                "VG_image_id": "2316553",
                "VG_object_id": "2943557",
                "bbox": [63, 390, 373, 498],
                "image": "data\\images\\2316553.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what is behind the train", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is the land made of", -1],
            ["what is on the land", -1],
            ["How many people are there", -1],
            ["What season is it", -1],
            ["what kind of animal is there on the land", -1],
            ["what is the picture taken", -1],
            ["where are the trains", -1],
            ["what is behind the train", 1],
            ["what is the train doing", -1],
            ["what is on the side of the train", -1]
        ],
        "context": [
            "a model train set with a model train on the tracks.",
            "a train is traveling down the tracks near a forest."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2406614",
                "VG_object_id": "368506",
                "bbox": [99, 99, 290, 446],
                "image": "data\\images\\2406614.jpg"
            },
            {
                "VG_image_id": "2404424",
                "VG_object_id": "1115821",
                "bbox": [200, 67, 380, 280],
                "image": "data\\images\\2404424.jpg"
            }
        ],
        "questions_with_scores": [["how many giraffes are there", 2]],
        "org_questions": [
            ["how many giraffes are there", 2],
            ["what is the giraffe doing", -1],
            ["what color is the grass", -1],
            ["where are the giraffe", -1],
            ["what is in the background", -1],
            ["what is the giraffe standing on", -1],
            ["What is the background of image", -1],
            ["when was the photo taken", -1],
            ["what type of animal is shown", -1],
            ["where is the giraffe looking", -1],
            ["what animal is this", -1],
            ["what color is the giraffe", -1]
        ],
        "context": [
            "a giraffe standing in a field next to a tree.",
            "two giraffes standing under a tree"
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2399691",
                "VG_object_id": "414443",
                "bbox": [182, 162, 315, 278],
                "image": "data\\images\\2399691.jpg"
            },
            {
                "VG_image_id": "2394773",
                "VG_object_id": "461220",
                "bbox": [150, 93, 346, 267],
                "image": "data\\images\\2394773.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the laptop", 1],
            ["How many screens are there", 1]
        ],
        "org_questions": [
            ["What is laptop on", -1],
            ["What color is the laptop", 1],
            ["How many screens are there", 1],
            ["Where is the laptop", -1],
            ["what is on the screen of the laptop", -1],
            ["how many people are there in the picture", -1],
            ["what is the main color of the laptop's screen", -1],
            ["what color is the table", -1],
            ["what type of computer is shown", -1],
            ["what is next to the laptop", -1],
            ["what is on the table", -1],
            ["what is the laptop sitting on", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a desk.",
            "a cat is laying on a table with a laptop."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2354535",
                "VG_object_id": "837712",
                "bbox": [0, 2, 499, 357],
                "image": "data\\images\\2354535.jpg"
            },
            {
                "VG_image_id": "2358131",
                "VG_object_id": "805776",
                "bbox": [8, 6, 475, 364],
                "image": "data\\images\\2358131.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sofa", 2],
            ["how many cats are on the sofa", 2],
            ["what color is the pillow on the sofa", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 2],
            ["how many cats are on the sofa", 2],
            ["what is on the sofa", -1],
            ["where is the sofa placed", -1],
            ["what is the pattern on the sofa", -1],
            ["How many people are there", -1],
            ["what color is the pillow on the sofa", 1],
            ["what kind of animal is this", -1],
            ["why is the cat laying down", -1],
            ["where was the photo taken", -1],
            ["what is behind the cat", -1],
            ["what is the cat doing", -1]
        ],
        "context": [
            "a cat sleeping on a couch",
            "two cats sleeping on a pink blanket next to remotes."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2355781",
                "VG_object_id": "826579",
                "bbox": [122, 82, 379, 204],
                "image": "data\\images\\2355781.jpg"
            },
            {
                "VG_image_id": "2322853",
                "VG_object_id": "3314128",
                "bbox": [66, 209, 348, 281],
                "image": "data\\images\\2322853.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 1],
            ["what is beside the train", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the train", 1],
            ["what is beside the train", 1],
            ["what color is the ground", -1],
            ["how many people are there", -1],
            ["where is the train", -1],
            ["what is the ground covered with", 1],
            ["what is on the ground", -1],
            ["what is in the distance", -1],
            ["what is the train doing", -1],
            ["what type of train is shown", -1],
            ["what is the train on", -1]
        ],
        "context": [
            "a train is traveling down the tracks near a forest.",
            "a train is traveling through a lush green forest."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2344779",
                "VG_object_id": "2906577",
                "bbox": [5, 7, 182, 119],
                "image": "data\\images\\2344779.jpg"
            },
            {
                "VG_image_id": "2368621",
                "VG_object_id": "3867911",
                "bbox": [11, 270, 373, 496],
                "image": "data\\images\\2368621.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the blanket", 1],
            ["what is on the blanket", 1],
            ["how many dogs are there on the blanket", 1],
            ["where is under the blanket", 1],
            ["who is in the picture", 1],
            ["what is in the background", 1],
            ["how many people are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the blanket", 1],
            ["what is on the blanket", 1],
            ["when is this photo taken", -1],
            ["how many dogs are there on the blanket", 1],
            ["where is under the blanket", 1],
            ["who is in the picture", 1],
            ["what is in the background", 1],
            ["where is the picture taken", -1],
            ["when was the picture taken", -1],
            ["how many people are in the picture", 1]
        ],
        "context": [
            "a meal in a plastic container on a bed.",
            "a woman sitting on a couch with a dog."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2407096",
                "VG_object_id": "3811062",
                "bbox": [3, 331, 331, 497],
                "image": "data\\images\\2407096.jpg"
            },
            {
                "VG_image_id": "2373600",
                "VG_object_id": "3725950",
                "bbox": [6, 283, 497, 330],
                "image": "data\\images\\2373600.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the land", 1],
            ["what color is the land", 1],
            ["what is the land made of", 1],
            ["how many people are there", 1],
            ["what is in the distance", 1],
            ["what is the ground covered with", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is on the land", 1],
            ["what color is the land", 1],
            ["what is the land made of", 1],
            ["how many people are there", 1],
            ["what is in the distance", 1],
            ["what animals are there", -1],
            ["what is the ground covered with", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a man and a woman sitting on a motorcycle.",
            "a beach with many people flying kites on it."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2316990",
                "VG_object_id": "2889611",
                "bbox": [0, 42, 499, 199],
                "image": "data\\images\\2316990.jpg"
            },
            {
                "VG_image_id": "2386100",
                "VG_object_id": "1289457",
                "bbox": [403, 130, 483, 188],
                "image": "data\\images\\2386100.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["what kind of truck is this", 1]
        ],
        "org_questions": [
            ["what color is the building", 2],
            ["what color is the truck", -1],
            ["what time is it", -1],
            ["how many people are there in the picture", -1],
            ["What is behind the building", -1],
            ["what is the truck doing", -1],
            ["what kind of truck is this", 1],
            ["when was this picture taken", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a truck is parked next to a building.",
            "a fire truck parked in a parking lot."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2371527",
                "VG_object_id": "1916289",
                "bbox": [18, 91, 372, 498],
                "image": "data\\images\\2371527.jpg"
            },
            {
                "VG_image_id": "2396682",
                "VG_object_id": "441535",
                "bbox": [60, 95, 278, 458],
                "image": "data\\images\\2396682.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many giraffes are there", 2],
            ["what color is the giraffes", 1],
            ["what is next to the giraffe", 1]
        ],
        "org_questions": [
            ["how many giraffes are there", 2],
            ["what color is the giraffes", 1],
            ["what is the ground the giraffe standing on made of", -1],
            ["where are the giraffe", -1],
            ["what is the giraffe doing", -1],
            ["what is the giraffe standing on", -1],
            ["what type of animal is shown", -1],
            ["when was the photo taken", -1],
            ["what is behind the giraffes", -1],
            ["where was the photo taken", -1],
            ["what is next to the giraffe", 1]
        ],
        "context": [
            "two giraffes standing in a fenced in area.",
            "a giraffe standing next to a tree in a zoo."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2319527",
                "VG_object_id": "3521825",
                "bbox": [133, 31, 285, 277],
                "image": "data\\images\\2319527.jpg"
            },
            {
                "VG_image_id": "2362687",
                "VG_object_id": "2600466",
                "bbox": [194, 17, 277, 128],
                "image": "data\\images\\2362687.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 2],
            ["what is the woman holding", 2],
            ["what is the woman standing on", 1],
            ["what color is the woman's shirt", 1],
            ["where is the woman", 1],
            ["who is in the photo", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["what is the woman standing on", 1],
            ["what is the woman doing", 2],
            ["how many people are there", -1],
            ["what color is the woman's shirt", 1],
            ["where is the woman", 1],
            ["what kind of clothes is the woman wearing", -1],
            ["what is the woman holding", 2],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the persion wearing", -1],
            ["what is the man on", 1]
        ],
        "context": [
            "a woman riding a surfboard on top of a wave.",
            "a woman riding a horse over a jump."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2362878",
                "VG_object_id": "770336",
                "bbox": [44, 108, 302, 285],
                "image": "data\\images\\2362878.jpg"
            },
            {
                "VG_image_id": "2361772",
                "VG_object_id": "2530613",
                "bbox": [5, 150, 285, 469],
                "image": "data\\images\\2361772.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 2],
            ["what is the main color of the laptop's screen", 2],
            ["How many people are there", 1],
            ["what color is the screen of the laptop", 1]
        ],
        "org_questions": [
            ["What color is the table", 2],
            ["How many people are there", 1],
            ["what is the main color of the laptop's screen", 2],
            ["how many laptops are there", -1],
            ["what color is the screen of the laptop", 1],
            ["what is behind the laptop", -1],
            ["where is the laptop", -1],
            ["what type of computer is this", -1],
            ["what is on top of the laptop", -1],
            ["what is the laptop on", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a desk.",
            "a woman is looking at a laptop screen."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2329561",
                "VG_object_id": "3726810",
                "bbox": [8, 2, 484, 367],
                "image": "data\\images\\2329561.jpg"
            },
            {
                "VG_image_id": "2406989",
                "VG_object_id": "287130",
                "bbox": [0, 251, 498, 374],
                "image": "data\\images\\2406989.jpg"
            }
        ],
        "questions_with_scores": [
            ["where was this photo taken", 2],
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["How many people are there", 1],
            ["what is the table made of", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["where is the food", -1],
            ["How many people are there", 1],
            ["what is the table made of", 1],
            ["how many bowls are there on the table", -1],
            ["what is the food on", -1],
            ["where was this photo taken", 2],
            ["what is in the background", 1],
            ["what type of food is shown", -1]
        ],
        "context": [
            "a person holding a donut in their hand.",
            "a man and a woman sitting at a table with plates of food."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2357367",
                "VG_object_id": "3554463",
                "bbox": [5, 167, 372, 493],
                "image": "data\\images\\2357367.jpg"
            },
            {
                "VG_image_id": "2414233",
                "VG_object_id": "157266",
                "bbox": [10, 104, 483, 323],
                "image": "data\\images\\2414233.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are on the table", 1],
            ["how many people are there", 1],
            ["what is on the plate", 1],
            ["how many plates are there on the table", 1],
            ["how many glasses are there on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["how many plates are on the table", 1],
            ["how many people are there", 1],
            ["what is on the plate", 1],
            ["what is the table made of", -1],
            ["where is the food", -1],
            ["what kind of food is this", -1],
            ["how many plates are there on the table", 1],
            ["how many glasses are there on the table", 1],
            ["where is the table", -1],
            ["what is the plate made of", -1],
            ["what color is the plate", -1]
        ],
        "context": [
            "a table with a plate of food and a pizza",
            "a person giving a thumbs up while sitting at a table with a pizza."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2342412",
                "VG_object_id": "2566208",
                "bbox": [200, 105, 274, 338],
                "image": "data\\images\\2342412.jpg"
            },
            {
                "VG_image_id": "2416742",
                "VG_object_id": "3243592",
                "bbox": [440, 156, 493, 288],
                "image": "data\\images\\2416742.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 2],
            ["how many women are there in the picture", 1],
            ["what  is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the weather like", 2],
            ["what is in the background", -1],
            ["how many women are there in the picture", 1],
            ["Where is the girl", -1],
            ["what color is the woman hair", -1],
            ["what  is the woman holding", 1],
            ["what is the woman wearing", -1],
            ["when was this picture taken", -1],
            ["what are the people doing", -1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "people walking in the rain",
            "a dog is walking past a police car."
        ]
    },
    {
        "object_category": "lamp",
        "images": [
            {
                "VG_image_id": "2332655",
                "VG_object_id": "3483614",
                "bbox": [210, 142, 262, 269],
                "image": "data\\images\\2332655.jpg"
            },
            {
                "VG_image_id": "2395701",
                "VG_object_id": "451722",
                "bbox": [214, 144, 294, 339],
                "image": "data\\images\\2395701.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2]
        ],
        "org_questions": [
            ["where is the lamp", -1],
            ["what color is the wall", -1],
            ["how many people are there in the picture", 2],
            ["when is this picture taken", -1],
            ["What color is lamp", -1],
            ["where are the lamps", -1],
            ["what is hanging from the lamp", -1],
            ["what is behind the lamp", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a man standing in a living room holding a wii remote.",
            "a lamp and a lamp in a room"
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2348946",
                "VG_object_id": "878577",
                "bbox": [46, 15, 302, 290],
                "image": "data\\images\\2348946.jpg"
            },
            {
                "VG_image_id": "2367488",
                "VG_object_id": "2044975",
                "bbox": [168, 17, 447, 374],
                "image": "data\\images\\2367488.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the dog", 2],
            ["what are the dogs doing", 1]
        ],
        "org_questions": [
            ["what color is the dog", -1],
            ["where is the dog", 2],
            ["what are the dogs doing", 1],
            ["what is the breed of the dog", -1],
            ["what is the color of the dog's head", -1],
            ["what kind of animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["how many dogs are there", -1],
            ["what is around the dog's neck", -1]
        ],
        "context": [
            "a dog wearing a purple life jacket riding a surfboard.",
            "a dog sitting in the back of a car."
        ]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2336092",
                "VG_object_id": "2654463",
                "bbox": [40, 77, 172, 157],
                "image": "data\\images\\2336092.jpg"
            },
            {
                "VG_image_id": "2358147",
                "VG_object_id": "1858874",
                "bbox": [208, 61, 363, 279],
                "image": "data\\images\\2358147.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["what time is it", 1],
            ["How many people are there", 1],
            ["when was the picture taken", 1],
            ["what is red", 1]
        ],
        "org_questions": [
            ["what color is the light", -1],
            ["what is in the background", 1],
            ["what time is it", 1],
            ["How many people are there", 1],
            ["where is the photo taken", -1],
            ["what is the shape of the lamp", -1],
            ["what is the light on", -1],
            ["when was the picture taken", 1],
            ["what is red", 1],
            ["where are the traffic lights", -1],
            ["what is on the street", -1]
        ],
        "context": [
            "a group of people riding bikes down a street.",
            "a traffic light with a green light on it."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2328044",
                "VG_object_id": "3736894",
                "bbox": [259, 359, 320, 419],
                "image": "data\\images\\2328044.jpg"
            },
            {
                "VG_image_id": "2350244",
                "VG_object_id": "2191994",
                "bbox": [38, 81, 103, 186],
                "image": "data\\images\\2350244.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["how many people are there in the picture", 2],
            ["what color is the ground", 1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the ground", 1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["what is the man doing", -1],
            ["where is the person", -1],
            ["what is the man wearing", -1],
            ["what is the person holding", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["where was the photo taken", 1],
            ["what is the man riding", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man on a motor bike in front of a blue building.",
            "a man riding a motorcycle in a field."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2382426",
                "VG_object_id": "539182",
                "bbox": [56, 235, 356, 493],
                "image": "data\\images\\2382426.jpg"
            },
            {
                "VG_image_id": "2373618",
                "VG_object_id": "1984292",
                "bbox": [123, 164, 383, 324],
                "image": "data\\images\\2373618.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what is on the plate", 1],
            ["what kind of food is on the plate", 1]
        ],
        "org_questions": [
            ["what is on the plate", 1],
            ["what color is the table", 2],
            ["how many plate is there", -1],
            ["what is the plate made of", -1],
            ["what kind of food is on the plate", 1],
            ["what is on the table", -1],
            ["what shape is the plate", -1],
            ["what is the food on", -1],
            ["what is under the plate", -1],
            ["what is the plate sitting on", -1]
        ],
        "context": [
            "a dessert with a banana split and a cherry on top.",
            "a person is cutting a slice of pizza."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2379536",
                "VG_object_id": "552190",
                "bbox": [2, 132, 169, 500],
                "image": "data\\images\\2379536.jpg"
            },
            {
                "VG_image_id": "2404858",
                "VG_object_id": "1112394",
                "bbox": [348, 109, 438, 273],
                "image": "data\\images\\2404858.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's shirt", 2],
            ["what is the girl doing", 1],
            ["what is the ground covered with", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what color is the girl's shirt", 2],
            ["how many girls are there", -1],
            ["what is the girl wearing on the head", -1],
            ["what is the ground covered with", 1],
            ["what time is it", -1],
            ["where is the girl", -1],
            ["what is the woman holding", 1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1],
            ["what is the little girl wearing", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a little girl feeding a cow in a barn.",
            "a little girl running towards a tent."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2337296",
                "VG_object_id": "3352007",
                "bbox": [6, 20, 454, 374],
                "image": "data\\images\\2337296.jpg"
            },
            {
                "VG_image_id": "2398677",
                "VG_object_id": "1177350",
                "bbox": [104, 145, 358, 441],
                "image": "data\\images\\2398677.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 2],
            ["what is on woman's face", 2],
            ["what color is the woman's shirt", 1],
            ["How many people are there", 1],
            ["what is the woman holding", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 2],
            ["what color is the woman's shirt", 1],
            ["what is on woman's face", 2],
            ["How many people are there", 1],
            ["what is the woman holding", 1],
            ["what is the woman wearing", -1],
            ["where was the photo taken", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a woman wearing sunglasses and a scarf talking on a cell phone.",
            "a woman sitting in a chair reading a book."
        ]
    },
    {
        "object_category": "computer",
        "images": [
            {
                "VG_image_id": "2364900",
                "VG_object_id": "2635465",
                "bbox": [246, 188, 406, 310],
                "image": "data\\images\\2364900.jpg"
            },
            {
                "VG_image_id": "2413614",
                "VG_object_id": "169864",
                "bbox": [17, 0, 500, 300],
                "image": "data\\images\\2413614.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many computers are in the picture", 2],
            ["how many computers are there", 1],
            ["what color is the table under the computer", 1],
            ["what color is the table", 1],
            ["what is next to the laptop", 1],
            ["what is in front of the laptop", 1]
        ],
        "org_questions": [
            ["how many computers are there", 1],
            ["what color is the table under the computer", 1],
            ["where is the computer", -1],
            ["what is on the desk", -1],
            ["how many computers are in the picture", 2],
            ["what color is the table", 1],
            ["what type of computer is this", -1],
            ["what is the laptop sitting on", -1],
            ["what is next to the laptop", 1],
            ["what is under the laptop", -1],
            ["what is in front of the laptop", 1]
        ],
        "context": [
            "a man sitting at a desk with a laptop.",
            "a laptop computer sitting on top of a desk."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2384992",
                "VG_object_id": "1302351",
                "bbox": [1, 296, 500, 498],
                "image": "data\\images\\2384992.jpg"
            },
            {
                "VG_image_id": "2382697",
                "VG_object_id": "1326767",
                "bbox": [1, 216, 497, 331],
                "image": "data\\images\\2382697.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 2],
            ["what color is the chair", 2],
            ["what is on the floor", 1],
            ["what pattern is the floor", 1],
            ["what is the floor made of", 1],
            ["how many people are there on the floor", 1],
            ["what is the ground covered with", 1],
            ["what room is this", 1],
            ["what is covering the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 2],
            ["what is on the floor", 1],
            ["what color is the chair", 2],
            ["how many doors are there", -1],
            ["what pattern is the floor", 1],
            ["what is the floor made of", 1],
            ["how many people are there on the floor", 1],
            ["what is the ground covered with", 1],
            ["where was the photo taken", -1],
            ["what room is this", 1],
            ["what is covering the floor", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a group of people sitting at a table in a large room.",
            "a living room with a couch, desk and chair."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2349256",
                "VG_object_id": "1704407",
                "bbox": [1, 0, 498, 500],
                "image": "data\\images\\2349256.jpg"
            },
            {
                "VG_image_id": "2366594",
                "VG_object_id": "1953238",
                "bbox": [28, 49, 409, 407],
                "image": "data\\images\\2366594.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many forks are there on the table", 1],
            ["how many cakes are there on the table", 1],
            ["what food is on the plate", 1]
        ],
        "org_questions": [
            ["how many forks are there on the table", 1],
            ["how many cakes are there on the table", 1],
            ["what main color is the table", -1],
            ["what food is on the plate", 1],
            ["what is the table made of", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a pickle on a sandwich",
            "a person holding a piece of cake on top of a foil covered table."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2327095",
                "VG_object_id": "2898373",
                "bbox": [288, 50, 440, 232],
                "image": "data\\images\\2327095.jpg"
            },
            {
                "VG_image_id": "713018",
                "VG_object_id": "1076348",
                "bbox": [85, 260, 189, 479],
                "image": "data\\images\\713018.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's trousers", 1],
            ["what time is it", 1],
            ["what are the people wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's trousers", 1],
            ["what time is it", 1],
            ["what is the ground covered with", -1],
            ["how many people are there in the picture", -1],
            ["what is the man wearing on his head", -1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["when was this photo taken", -1],
            ["what is the man holding", -1],
            ["what are the people wearing", 1]
        ],
        "context": [
            "a motorcycle parked on the side of the road.",
            "a group of people at a train station"
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2333885",
                "VG_object_id": "3026257",
                "bbox": [1, 224, 139, 498],
                "image": "data\\images\\2333885.jpg"
            },
            {
                "VG_image_id": "2352929",
                "VG_object_id": "2135779",
                "bbox": [181, 90, 285, 209],
                "image": "data\\images\\2352929.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 2],
            ["where is the picture taken", 2],
            ["what color is the girl's clothes", 1],
            ["What is girl wearing on his head", 1],
            ["where is the girl", 1],
            ["what are the people doing", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 2],
            ["where is the picture taken", 2],
            ["what color is the girl's clothes", 1],
            ["how many people are there", -1],
            ["What is girl wearing on his head", 1],
            ["where is the girl", 1],
            ["what are the people doing", 1],
            ["what is the gender of the person", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a girl is looking at a giraffe in a zoo.",
            "a woman riding a surfboard on a wave in the ocean."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2398888",
                "VG_object_id": "421094",
                "bbox": [173, 195, 267, 365],
                "image": "data\\images\\2398888.jpg"
            },
            {
                "VG_image_id": "2348758",
                "VG_object_id": "2650791",
                "bbox": [63, 143, 148, 361],
                "image": "data\\images\\2348758.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["what is the gesture of the woman", 1],
            ["where is the woman", 1],
            ["what is the woman holding", 1],
            ["what is the woman doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what is the gesture of the woman", 1],
            ["where is the woman", 1],
            ["how many people are there", -1],
            ["what is the woman holding", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["who is wearing glasses", -1],
            ["what is on the woman's face", -1],
            ["what kind of pants is the woman wearing", -1]
        ],
        "context": [
            "a group of people sitting on a bench.",
            "a group of people standing around a counter."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2358415",
                "VG_object_id": "2413927",
                "bbox": [1, 292, 333, 372],
                "image": "data\\images\\2358415.jpg"
            },
            {
                "VG_image_id": "2378588",
                "VG_object_id": "2124323",
                "bbox": [1, 182, 498, 372],
                "image": "data\\images\\2378588.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the photo", 2],
            ["what is on the table", 1],
            ["what color is the plate", 1],
            ["what is covering the table", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 1],
            ["where is the picture taken", -1],
            ["how many plates are there on the table", -1],
            ["what is the table made of", -1],
            ["what color is the plate", 1],
            ["what is covering the table", 1],
            ["how many people are in the photo", 2],
            ["what material is the table made of", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a group of people working in a kitchen.",
            "a slice of pie and a cup of coffee on a table."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2370761",
                "VG_object_id": "601817",
                "bbox": [124, 190, 418, 460],
                "image": "data\\images\\2370761.jpg"
            },
            {
                "VG_image_id": "2383162",
                "VG_object_id": "1323255",
                "bbox": [56, 74, 490, 329],
                "image": "data\\images\\2383162.jpg"
            }
        ],
        "questions_with_scores": [["what is the bench made of", 1]],
        "org_questions": [
            ["what color is the ground", -1],
            ["what is behind the bench", -1],
            ["when is this photo taken", -1],
            ["who is sitting on the bench", -1],
            ["what is the bench made of", 1],
            ["where is the bench", -1],
            ["what is placed on the bench", -1],
            ["how is the weather", -1],
            ["how many benches are there", -1],
            ["what is the bench sitting on", -1],
            ["what is under the bench", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a bench in a park in the middle of a park.",
            "a bench with moss on it in a forest."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2391357",
                "VG_object_id": "3827122",
                "bbox": [14, 40, 486, 318],
                "image": "data\\images\\2391357.jpg"
            },
            {
                "VG_image_id": "2327702",
                "VG_object_id": "3937207",
                "bbox": [2, 74, 331, 353],
                "image": "data\\images\\2327702.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants are there", 2],
            ["how many tusks are there", 2]
        ],
        "org_questions": [
            ["how many elephants are there", 2],
            ["what color are the elephants", -1],
            ["where is the elephant", -1],
            ["what is the elephant doing", -1],
            ["what is in the background", -1],
            ["where is the elephant standing", -1],
            ["when was the photo taken", -1],
            ["what kind of animals are these", -1],
            ["what are the elephants standing on", -1],
            ["where was the photo taken", -1],
            ["how many tusks are there", 2]
        ],
        "context": [
            "a group of elephants standing next to each other.",
            "an elephant standing behind a fence in a zoo."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2350569",
                "VG_object_id": "3593188",
                "bbox": [89, 115, 228, 326],
                "image": "data\\images\\2350569.jpg"
            },
            {
                "VG_image_id": "2386861",
                "VG_object_id": "1281724",
                "bbox": [200, 27, 275, 213],
                "image": "data\\images\\2386861.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["What color is person's shirt", 2],
            ["Where are people", 1],
            ["where is the person", 1],
            ["who is in the photo", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what is the person doing", -1],
            ["what is on the person's head", -1],
            ["how many people are there", 2],
            ["What color is person's shirt", 2],
            ["Where are people", 1],
            ["what is in the background", -1],
            ["where is the person", 1],
            ["What is person doing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the persion holding", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a group of people looking at an elephant at the zoo.",
            "a young man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2398905",
                "VG_object_id": "1174390",
                "bbox": [0, 379, 332, 500],
                "image": "data\\images\\2398905.jpg"
            },
            {
                "VG_image_id": "2411906",
                "VG_object_id": "1078712",
                "bbox": [7, 328, 386, 372],
                "image": "data\\images\\2411906.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["what is the table made of", 1],
            ["what is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["what is the table made of", 1],
            ["what is on the plate", 1],
            ["how many plates are there on the table", -1],
            ["how many plates are there", -1],
            ["where was this photo taken", -1],
            ["what is the persion sitting on", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a woman sitting at a table with a cell phone.",
            "a man holding a baby while sitting at a table."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2342612",
                "VG_object_id": "3513401",
                "bbox": [172, 186, 260, 254],
                "image": "data\\images\\2342612.jpg"
            },
            {
                "VG_image_id": "2317201",
                "VG_object_id": "1022787",
                "bbox": [212, 177, 282, 249],
                "image": "data\\images\\2317201.jpg"
            }
        ],
        "questions_with_scores": [
            ["what time does the clock say", 2],
            ["how many clocks are there", 1]
        ],
        "org_questions": [
            ["what is the clock on", -1],
            ["how many clocks are there", 1],
            ["what time is it", -1],
            ["where is the clock", -1],
            ["what is the color of the wall", -1],
            ["what is the clock made of", -1],
            ["what is behind the clock", -1],
            ["what is the shape of the clock", -1],
            ["what is on the building", -1],
            ["when was the photo taken", -1],
            ["what time does the clock say", 2]
        ],
        "context": [
            "a tall clock tower with a clock on it.",
            "a large clock on the side of a building."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2330220",
                "VG_object_id": "3722251",
                "bbox": [244, 17, 492, 428],
                "image": "data\\images\\2330220.jpg"
            },
            {
                "VG_image_id": "2377308",
                "VG_object_id": "1670553",
                "bbox": [417, 119, 498, 374],
                "image": "data\\images\\2377308.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["where is the man", 1],
            ["where is the picture taken", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["where is the man", 1],
            ["what is on the man's faces", -1],
            ["how many people are there", 2],
            ["what color is the man's shirt", -1],
            ["what is the man holding", -1],
            ["what is man wearing", -1],
            ["where is the picture taken", 1],
            ["who is wearing glasses", -1],
            ["what is behind the man", 1],
            ["who is in the photo", -1],
            ["what is the man wearing on the face", -1]
        ],
        "context": [
            "a man and a woman posing for a picture.",
            "a bus with a tv on the top of it"
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2333362",
                "VG_object_id": "2966962",
                "bbox": [243, 160, 295, 269],
                "image": "data\\images\\2333362.jpg"
            },
            {
                "VG_image_id": "2365247",
                "VG_object_id": "757520",
                "bbox": [84, 231, 256, 416],
                "image": "data\\images\\2365247.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the woman doing", 1],
            ["what is the persion wearing on her face", 1],
            ["Where is the person", 1],
            ["what is the person wearing", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what color is the trousers", 2],
            ["what is the persion wearing on her face", 1],
            ["how many people are there", -1],
            ["Who is wearing those trousers", -1],
            ["Where is the person", 1],
            ["what is in the distance", -1],
            ["how many persons are there in the picture", -1],
            ["what is the person wearing", 1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a woman on skis in the snow.",
            "a woman sitting on a bench in a dark night."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2340709",
                "VG_object_id": "2359519",
                "bbox": [172, 82, 329, 314],
                "image": "data\\images\\2340709.jpg"
            },
            {
                "VG_image_id": "2322889",
                "VG_object_id": "3383234",
                "bbox": [88, 88, 270, 358],
                "image": "data\\images\\2322889.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["how many men are there in the picture", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man wearing", -1],
            ["how many men are there in the picture", 1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["what is the man holding", 1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "two men sitting on a bench with a dog.",
            "a person jumping a skate board in the air"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316986",
                "VG_object_id": "1050983",
                "bbox": [83, 108, 318, 421],
                "image": "data\\images\\2316986.jpg"
            },
            {
                "VG_image_id": "2414959",
                "VG_object_id": "148721",
                "bbox": [9, 3, 496, 491],
                "image": "data\\images\\2414959.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man doing", 2],
            ["What color is man's shirt", 2],
            ["What color is man;s shirt", 1],
            ["when was the picture taken", 1],
            ["what sport is being played", 1],
            ["where is the man", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["What is man doing", 2],
            ["What color is man;s shirt", 1],
            ["how many people are there", -1],
            ["what is the man wearing", -1],
            ["when was the picture taken", 1],
            ["what sport is being played", 1],
            ["where is the man", 1],
            ["who is in the picture", 1],
            ["What color is man's shirt", 2]
        ],
        "context": [
            "a man riding a skateboard on a ramp.",
            "a man in red shirt catching a rugby ball."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2344070",
                "VG_object_id": "2895479",
                "bbox": [2, 20, 497, 372],
                "image": "data\\images\\2344070.jpg"
            },
            {
                "VG_image_id": "2361516",
                "VG_object_id": "1685940",
                "bbox": [4, 4, 498, 374],
                "image": "data\\images\\2361516.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of food is it", 1],
            ["what shape is the plate on the table", 1]
        ],
        "org_questions": [
            ["what is the table made of", -1],
            ["what color is the table", -1],
            ["what kind of food is it", 1],
            ["how many forks are there", -1],
            ["what shape is the plate on the table", 1],
            ["what is on the table", -1],
            ["where is the picture taken", -1],
            ["what is the food sitting on", -1],
            ["where is the plate", -1]
        ],
        "context": [
            "a plate of food on a table",
            "a tray with a variety of donuts and drinks on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2358015",
                "VG_object_id": "806819",
                "bbox": [193, 217, 284, 332],
                "image": "data\\images\\2358015.jpg"
            },
            {
                "VG_image_id": "2412776",
                "VG_object_id": "3214420",
                "bbox": [192, 73, 333, 308],
                "image": "data\\images\\2412776.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["how many women are there", 1],
            ["what is the woman sitting on", 1]
        ],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what color is the woman's shirt", 2],
            ["how many women are there", 1],
            ["what is the woman wearing", -1],
            ["What is woman holding", -1],
            ["what is the woman's posture", -1],
            ["What is the woman holding", -1],
            ["what is the gender of the person in the picture", -1],
            ["who is in the photo", -1],
            ["what is the woman sitting on", 1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "a crowd of people sitting at tables in a room.",
            "a woman sitting on a toilet in a bathroom."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2368161",
                "VG_object_id": "747727",
                "bbox": [174, 97, 284, 189],
                "image": "data\\images\\2368161.jpg"
            },
            {
                "VG_image_id": "2363277",
                "VG_object_id": "2485007",
                "bbox": [2, 126, 499, 240],
                "image": "data\\images\\2363277.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there", 1],
            ["what is behind the zebras", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["how many zebras are there", 1],
            ["What is zebra doing", -1],
            ["where is the zebras", -1],
            ["what is the ground covered with", -1],
            ["what is on the ground", -1],
            ["what is the main color of the grass", -1],
            ["what animals are in the photo", -1],
            ["what pattern is on the zebras", -1],
            ["what is behind the zebras", 1],
            ["what are the zebras eating", -1],
            ["what kind of animals are these", -1],
            ["what are the zebras standing on ", -1],
            ["what is in the distance", -1],
            ["what animals are on the grass", -1]
        ],
        "context": [
            "three zebras are laying down in the grass.",
            "a group of zebras walking across a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2377858",
                "VG_object_id": "3201575",
                "bbox": [4, 1, 221, 373],
                "image": "data\\images\\2377858.jpg"
            },
            {
                "VG_image_id": "2398854",
                "VG_object_id": "421466",
                "bbox": [126, 62, 360, 388],
                "image": "data\\images\\2398854.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["Where is the man", 2],
            ["What color is man's shirt", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["Where is the man", 2],
            ["What color is man's shirt", 1],
            ["what is the man doing", 1],
            ["what are the people wearing", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1]
        ],
        "context": [
            "a man kneeling down next to a toothbrush.",
            "a man and a woman sitting on a bench."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2413652",
                "VG_object_id": "169055",
                "bbox": [6, 181, 496, 276],
                "image": "data\\images\\2413652.jpg"
            },
            {
                "VG_image_id": "2362015",
                "VG_object_id": "3529547",
                "bbox": [1, 170, 484, 372],
                "image": "data\\images\\2362015.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["What are people doing", 1],
            ["what is the man on the land doing", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what kind of animal is on the land", -1],
            ["what is the land made of", -1],
            ["how many people are there", 1],
            ["WHat is on the background of image", -1],
            ["what is in the distance", -1],
            ["how many boxes are there on the ground", -1],
            ["what is on the land", -1],
            ["where is the grass", -1],
            ["how is the weather", -1],
            ["where was this picture taken", -1],
            ["what is covering the ground", -1],
            ["What are people doing", 1],
            ["What color is the ground", -1],
            ["what is the man on the land doing", 1]
        ],
        "context": [
            "two elephants standing next to each other on a dirt field.",
            "two elephants standing in a dirt field with people watching."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2391401",
                "VG_object_id": "489351",
                "bbox": [109, 82, 258, 499],
                "image": "data\\images\\2391401.jpg"
            },
            {
                "VG_image_id": "2362933",
                "VG_object_id": "3747859",
                "bbox": [163, 130, 263, 326],
                "image": "data\\images\\2362933.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing on head", 1],
            ["what is the man wearing on his neck", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", -1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing on head", 1],
            ["how is the weather", -1],
            ["what is the man wearing", -1],
            ["what is the man wearing on his neck", 1],
            ["when was the photo taken", -1],
            ["what is the man holding", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a group of young men walking down a street holding umbrellas.",
            "a man walking with an elephant in the jungle."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2380397",
                "VG_object_id": "1347168",
                "bbox": [278, 320, 330, 401],
                "image": "data\\images\\2380397.jpg"
            },
            {
                "VG_image_id": "2353415",
                "VG_object_id": "1815172",
                "bbox": [1, 415, 371, 498],
                "image": "data\\images\\2353415.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["how many people are in the picture", 1],
            ["what is in the background", 1],
            ["what room is the floor in", 1],
            ["what is the floor made of", 1],
            ["where is the picture taken", 1],
            ["what is on the ground", 1],
            ["where was this photo taken", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["how many people are in the picture", 1],
            ["what is in the background", 1],
            ["what room is the floor in", 1],
            ["what is the floor made of", 1],
            ["where is the picture taken", 1],
            ["what is on the ground", 1],
            ["how many chairs are on the floor", -1],
            ["where was this photo taken", 1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a white refrigerator freezer sitting inside of a kitchen.",
            "a woman swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2373687",
                "VG_object_id": "3201618",
                "bbox": [35, 44, 132, 319],
                "image": "data\\images\\2373687.jpg"
            },
            {
                "VG_image_id": "2403070",
                "VG_object_id": "1128227",
                "bbox": [100, 5, 365, 426],
                "image": "data\\images\\2403070.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the man", 2],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the man on", 1],
            ["what kind of clothes is the man wearing", 1],
            ["what is the man holding", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the man on", 1],
            ["how many people are there", -1],
            ["what is the man wearing on his face", -1],
            ["where is the man", 2],
            ["what kind of clothes is the man wearing", 1],
            ["what is the man holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion on the left wearing", 1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man sitting in a chair in a kitchen.",
            "a man on a motorcycle with a helmet on"
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2416065",
                "VG_object_id": "3195386",
                "bbox": [0, 204, 255, 341],
                "image": "data\\images\\2416065.jpg"
            },
            {
                "VG_image_id": "2317361",
                "VG_object_id": "3480609",
                "bbox": [180, 231, 499, 498],
                "image": "data\\images\\2317361.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sofa", 1],
            ["what color is the ground", 1],
            ["what is the floor under the sofa made of", 1],
            ["what color is the table", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 1],
            ["what color is the ground", 1],
            ["how many people are there", -1],
            ["what is the floor under the sofa made of", 1],
            ["what is in front of the couch", -1],
            ["what color is the table", 1],
            ["what kind of furniture is in the room", -1],
            ["what is on the sofa", -1],
            ["what is in the room", -1],
            ["how many pillows are on the couch", -1]
        ],
        "context": [
            "a living room with two green couches and a lamp.",
            "a living room with a couch, table, and chairs."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2417807",
                "VG_object_id": "2792592",
                "bbox": [2, 53, 388, 498],
                "image": "data\\images\\2417807.jpg"
            },
            {
                "VG_image_id": "107933",
                "VG_object_id": "1073563",
                "bbox": [3, 129, 359, 722],
                "image": "data\\images\\107933.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man wearing", 2],
            ["What is man doing", 2],
            ["what is the man doing", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What is man wearing", 2],
            ["What is man doing", 2],
            ["what is the man doing", 1],
            ["where was the photo taken", 1],
            ["who is in the photo", -1],
            ["what is behind the man", -1],
            ["what is on the man's face", -1]
        ],
        "context": [
            "a man in a suit and tie posing for a picture.",
            "a man and a woman playing a video game."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2414962",
                "VG_object_id": "148674",
                "bbox": [114, 13, 161, 135],
                "image": "data\\images\\2414962.jpg"
            },
            {
                "VG_image_id": "2405710",
                "VG_object_id": "372426",
                "bbox": [337, 30, 498, 152],
                "image": "data\\images\\2405710.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the towel", 1],
            ["where is the towel placed", 1],
            ["what is the towel put on ", 1],
            ["how many people are there", 1],
            ["where are the towels", 1]
        ],
        "org_questions": [
            ["what color is the towel", 1],
            ["where is the towel placed", 1],
            ["who is on the towel", -1],
            ["what is the towel put on ", 1],
            ["where is the picture taken", -1],
            ["what is the towel on", -1],
            ["how many people are there", 1],
            ["how many towels are there", -1],
            ["where are the towels", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man taking a picture of himself in a bathroom mirror.",
            "a cat is sitting on a shelf in a bathroom."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2365984",
                "VG_object_id": "755157",
                "bbox": [88, 37, 467, 247],
                "image": "data\\images\\2365984.jpg"
            },
            {
                "VG_image_id": "2334360",
                "VG_object_id": "3424602",
                "bbox": [185, 0, 476, 114],
                "image": "data\\images\\2334360.jpg"
            }
        ],
        "questions_with_scores": [["what is on the table", 1]],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 1],
            ["how many people are there", -1],
            ["where was the photo taken", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a box of pepperoni pizza on a table.",
            "a bowl of oranges on a table with a cloth"
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2357254",
                "VG_object_id": "1808702",
                "bbox": [322, 79, 399, 220],
                "image": "data\\images\\2357254.jpg"
            },
            {
                "VG_image_id": "2414573",
                "VG_object_id": "155888",
                "bbox": [210, 16, 391, 326],
                "image": "data\\images\\2414573.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the women doing", 2],
            ["what is on the girl's head", 2],
            ["what color is the girl's shirt", 1],
            ["what is the girl wearing", 1],
            ["where is the girl", 1],
            ["what is the ground the girl standing on made of", 1],
            ["what is the girl holding", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", 1],
            ["what is the girl wearing", 1],
            ["where is the girl", 1],
            ["how many girls are in the picture", -1],
            ["what is the ground the girl standing on made of", 1],
            ["what are the women doing", 2],
            ["what is the girl holding", 1],
            ["what is on the girl's head", 2],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a group of people sitting on top of a wooden fence.",
            "a young girl riding a skateboard on a ramp."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2391950",
                "VG_object_id": "1235750",
                "bbox": [217, 147, 309, 298],
                "image": "data\\images\\2391950.jpg"
            },
            {
                "VG_image_id": "2344699",
                "VG_object_id": "2589895",
                "bbox": [293, 216, 369, 342],
                "image": "data\\images\\2344699.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what is the man riding", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what is in the background", -1],
            ["what is the color of the person's pants", -1],
            ["who is riding the bicycle", -1],
            ["what is the ground covered with", -1],
            ["when was the picture taken", -1],
            ["what is the man doing", -1],
            ["where was the photo taken", -1],
            ["what is the man riding", 1]
        ],
        "context": [
            "a man riding a motorcycle down a street.",
            "a man riding a bike down a street next to a truck."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2390105",
                "VG_object_id": "1253870",
                "bbox": [53, 52, 374, 332],
                "image": "data\\images\\2390105.jpg"
            },
            {
                "VG_image_id": "2416645",
                "VG_object_id": "2947332",
                "bbox": [93, 271, 311, 499],
                "image": "data\\images\\2416645.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cows are there", 1],
            ["what is in the background", 1],
            ["how many cows are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the cow's head", -1],
            ["how many cows are there", 1],
            ["what is the ground covered with", -1],
            ["what is the cow doing ", -1],
            ["where is the cow", -1],
            ["What color is cow", -1],
            ["What color is the ground", -1],
            ["what animal is in the picture", -1],
            ["when was this photo taken", -1],
            ["what is in the background", 1],
            ["what type of animal is shown", -1],
            ["what is behind the cows", -1],
            ["how many cows are in the picture", 1],
            ["what color is the grass", -1],
            ["what color is the ground", -1]
        ],
        "context": [
            "a brown and white cow standing on top of a lush green field.",
            "two cows standing on a hill with trees in the background."
        ]
    },
    {
        "object_category": "book",
        "images": [
            {
                "VG_image_id": "2376534",
                "VG_object_id": "1865616",
                "bbox": [23, 409, 89, 465],
                "image": "data\\images\\2376534.jpg"
            },
            {
                "VG_image_id": "2393025",
                "VG_object_id": "475501",
                "bbox": [418, 64, 500, 115],
                "image": "data\\images\\2393025.jpg"
            }
        ],
        "questions_with_scores": [
            ["what main color is the book", 1],
            ["what color is the table", 1],
            ["what color is the book cover", 1]
        ],
        "org_questions": [
            ["what main color is the book", 1],
            ["where is the book", -1],
            ["how many people are there", -1],
            ["how many books are in the picture", -1],
            ["what color is the table", 1],
            ["what color is the book cover", 1],
            ["what is on the table", -1]
        ],
        "context": [
            "a kitchen with a stove, microwave and cabinets.",
            "a laptop computer sitting on top of a wooden table."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2393034",
                "VG_object_id": "3824877",
                "bbox": [0, 35, 500, 285],
                "image": "data\\images\\2393034.jpg"
            },
            {
                "VG_image_id": "2319613",
                "VG_object_id": "3447221",
                "bbox": [187, 163, 413, 303],
                "image": "data\\images\\2319613.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is the bus", 1],
            ["What is the weather like", 1],
            ["what is in the background", 1],
            ["where was this photo taken", 1],
            ["what is behind the bus", 1]
        ],
        "org_questions": [
            ["What color is the bus", 1],
            ["What is the weather like", 1],
            ["How many people are there", 2],
            ["when is this photo taken ", -1],
            ["what is in the background", 1],
            ["where is the bus", -1],
            ["what is the bus doing", -1],
            ["where was this photo taken", 1],
            ["what is on the side of the bus", -1],
            ["what is behind the bus", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a cow walking down a street with people walking around.",
            "a man standing next to a bus in a mirror."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2373416",
                "VG_object_id": "2163980",
                "bbox": [7, 270, 78, 329],
                "image": "data\\images\\2373416.jpg"
            },
            {
                "VG_image_id": "2387096",
                "VG_object_id": "679213",
                "bbox": [227, 66, 296, 154],
                "image": "data\\images\\2387096.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the shirt", 2],
            ["what is the man doing", 2],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what is the color of the shirt", 2],
            ["what is the man doing", 2],
            ["how many people are there", -1],
            ["what is the man wearing on his head", -1],
            ["what time is it", -1],
            ["who is wearing the shirt", -1],
            ["What is the gender of the person", -1],
            ["when was the photo taken", -1],
            ["what is the person wearing", 1],
            ["who is in the photo", -1],
            ["what color is the man's hair", -1]
        ],
        "context": [
            "two men sitting on a couch",
            "a person riding a skateboard on a ramp."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2391058",
                "VG_object_id": "491929",
                "bbox": [220, 91, 327, 499],
                "image": "data\\images\\2391058.jpg"
            },
            {
                "VG_image_id": "2346473",
                "VG_object_id": "2539101",
                "bbox": [67, 106, 130, 394],
                "image": "data\\images\\2346473.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many clocks are on the tower", 2],
            ["what color is the tower", 1],
            ["what time is it", 1]
        ],
        "org_questions": [
            ["what color is the tower", 1],
            ["how many clocks are on the tower", 2],
            ["what time is it", 1],
            ["where is the picture taken", -1],
            ["what is on the top of the tower", -1],
            ["what is the weather like", -1],
            ["where is the clock", -1],
            ["what is the building made of", -1],
            ["when was the picture taken", -1],
            ["what is in front of the building", -1],
            ["what time of day is it", -1],
            ["what is on the building", -1]
        ],
        "context": [
            "a street light with a clock tower in the background.",
            "a clock tower with a man on a bike."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2364983",
                "VG_object_id": "3886701",
                "bbox": [40, 293, 157, 409],
                "image": "data\\images\\2364983.jpg"
            },
            {
                "VG_image_id": "2372312",
                "VG_object_id": "1849073",
                "bbox": [193, 122, 300, 330],
                "image": "data\\images\\2372312.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bicycles are there", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["how many bicycles are there", 1],
            ["how many people are there", 1],
            ["what is in the distance", -1],
            ["what time is it", -1],
            ["what is the ground covered with", -1],
            ["what is the weather", -1],
            ["what is the persion doing", -1],
            ["what is the persion riding on", -1]
        ],
        "context": [
            "a person riding a bike on a city street",
            "a group of people riding bicycles down a street."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2395683",
                "VG_object_id": "451868",
                "bbox": [108, 45, 412, 263],
                "image": "data\\images\\2395683.jpg"
            },
            {
                "VG_image_id": "2383748",
                "VG_object_id": "3847811",
                "bbox": [42, 56, 451, 308],
                "image": "data\\images\\2383748.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the truck", 2],
            ["what is the ground under the truck made of", 1],
            ["what direction is the truck facing to", 1],
            ["what is the ground covered with", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the truck", 2],
            ["what is the ground under the truck made of", 1],
            ["how many trucks are there in the photo", -1],
            ["when is this picture taken", -1],
            ["what direction is the truck facing to", 1],
            ["what is on the truck", -1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["where are the trees", -1],
            ["what type of vehicle is this", -1],
            ["what is the truck doing", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a truck on a dirt road",
            "a classic pickup truck is parked in a parking lot."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2355950",
                "VG_object_id": "825097",
                "bbox": [49, 1, 310, 333],
                "image": "data\\images\\2355950.jpg"
            },
            {
                "VG_image_id": "2347240",
                "VG_object_id": "1672592",
                "bbox": [256, 143, 498, 332],
                "image": "data\\images\\2347240.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["how many cars are there", 1],
            ["how many bikes are there", 1],
            ["what vehicle is beside the men", 1],
            ["where is the man", 1],
            ["What is man doing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["how many cars are there", 1],
            ["how many bikes are there", 1],
            ["what vehicle is beside the men", 1],
            ["where is the man", 1],
            ["What is man doing", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what is the gender of the person", -1],
            ["who is in the picture", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man is standing next to a fire hydrant.",
            "a couple of people sitting on a bench by the water."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2315912",
                "VG_object_id": "3104791",
                "bbox": [198, 92, 300, 264],
                "image": "data\\images\\2315912.jpg"
            },
            {
                "VG_image_id": "2372512",
                "VG_object_id": "3733421",
                "bbox": [345, 81, 498, 331],
                "image": "data\\images\\2372512.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's shirt", 1],
            ["where is the child", 1],
            ["where is the man", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", 1],
            ["what is the child doing", -1],
            ["where is the child", 1],
            ["what is on the child's head", -1],
            ["what is the boy holding", -1],
            ["where is the man", 1],
            ["what is the man doing", -1],
            ["how many people are in the photo", 1],
            ["when was the photo taken", -1],
            ["what is the boy wearing", -1]
        ],
        "context": [
            "a little boy is smiling at the camera.",
            "a young girl standing next to a fire hydrant."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2321556",
                "VG_object_id": "3369731",
                "bbox": [140, 105, 242, 330],
                "image": "data\\images\\2321556.jpg"
            },
            {
                "VG_image_id": "2352763",
                "VG_object_id": "851846",
                "bbox": [17, 28, 428, 398],
                "image": "data\\images\\2352763.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the lady's shirt", 2],
            ["what is the lady wearing on her face", 1],
            ["what is the lady doing", 1]
        ],
        "org_questions": [
            ["what is the lady wearing on her face", 1],
            ["what color is the lady's shirt", 2],
            ["what is the lady doing", 1],
            ["how many people are there", -1],
            ["what is the weather like", -1],
            ["What season is it", -1],
            ["what is the woman holding", -1],
            ["where is the woman", -1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a woman taking a picture of herself with her cell phone.",
            "a woman talking on a cell phone while wearing sunglasses."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2373738",
                "VG_object_id": "1777032",
                "bbox": [139, 73, 422, 490],
                "image": "data\\images\\2373738.jpg"
            },
            {
                "VG_image_id": "2353195",
                "VG_object_id": "848399",
                "bbox": [35, 27, 161, 296],
                "image": "data\\images\\2353195.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["where are the men", 1],
            ["what is the man holding", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["who is in the photo", 1],
            ["what sport is being played", 1],
            ["what is the man wearing on his head", 1],
            ["Where is the man", 1],
            ["what is he doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["how many people are there", -1],
            ["where are the men", 1],
            ["what is the man holding", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what sport is being played", 1],
            ["what is the man wearing on his head", 1],
            ["Where is the man", 1],
            ["what is on the man's feet", -1],
            ["what is he doing", 1]
        ],
        "context": [
            "a baseball player is getting ready to bat.",
            "a man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2376506",
                "VG_object_id": "2485879",
                "bbox": [17, 183, 182, 451],
                "image": "data\\images\\2376506.jpg"
            },
            {
                "VG_image_id": "2337104",
                "VG_object_id": "3251067",
                "bbox": [0, 168, 68, 356],
                "image": "data\\images\\2337104.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["who is in the photo", 1],
            ["what is on the woman's face", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["where is the woman", -1],
            ["what is the woman wearing", 1],
            ["how many people are there", 2],
            ["what is the persion holding", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is on the woman's face", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man and woman sitting on a bench.",
            "a bus parked next to a sidewalk with a bike on the front."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2395728",
                "VG_object_id": "451532",
                "bbox": [128, 150, 227, 321],
                "image": "data\\images\\2395728.jpg"
            },
            {
                "VG_image_id": "2331155",
                "VG_object_id": "3433231",
                "bbox": [49, 22, 461, 325],
                "image": "data\\images\\2331155.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 1],
            ["what color is the ground", 1],
            ["what color are the clothes of the man on the right", 1],
            ["what is the person doing", 1],
            ["what is the boy holding", 1],
            ["What is child doing", 1],
            ["how many boys are there", 1],
            ["where was this picture taken", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 1],
            ["what color is the ground", 1],
            ["what color are the clothes of the man on the right", 1],
            ["what is on the boy's head", -1],
            ["what is the person doing", 1],
            ["what is the boy holding", 1],
            ["What is child doing", 1],
            ["how many boys are there", 1],
            ["where was this picture taken", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a boy flying a kite on a beach.",
            "two boys playing soccer on a field with a ball."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2324660",
                "VG_object_id": "2901995",
                "bbox": [5, 7, 298, 490],
                "image": "data\\images\\2324660.jpg"
            },
            {
                "VG_image_id": "2350898",
                "VG_object_id": "1659941",
                "bbox": [59, 98, 124, 250],
                "image": "data\\images\\2350898.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man standing on", 1],
            ["what is the color of man's pants", 1],
            ["what is the man doing", 1],
            ["How many people are there", 1],
            ["Where is man", 1],
            ["where is the man", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["what is the man standing on", 1],
            ["what is the color of man's pants", 1],
            ["what is the man doing", 1],
            ["How many people are there", 1],
            ["Where is man", 1],
            ["where is the man", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what kind of pants is the man wearing", -1],
            ["what is the persion on the left wearing", 1]
        ],
        "context": [
            "a skateboarder doing a trick at a skate park.",
            "two boys are walking on a dock near a house."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2361652",
                "VG_object_id": "1845136",
                "bbox": [183, 110, 315, 241],
                "image": "data\\images\\2361652.jpg"
            },
            {
                "VG_image_id": "2368515",
                "VG_object_id": "2044554",
                "bbox": [72, 19, 120, 83],
                "image": "data\\images\\2368515.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the board", 2],
            ["where is the board", 2],
            ["where is the photo taken", 1],
            ["what is on the left side of the photo", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the board", 2],
            ["where is the board", 2],
            ["How many people are there", -1],
            ["what direction does the board face", -1],
            ["where is the photo taken", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is on the left side of the photo", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a sign on the side of a mountain with a blue sky.",
            "a black and white photo of a toilet in a shed."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2370723",
                "VG_object_id": "1769644",
                "bbox": [59, 83, 364, 487],
                "image": "data\\images\\2370723.jpg"
            },
            {
                "VG_image_id": "2378181",
                "VG_object_id": "561564",
                "bbox": [28, 94, 384, 397],
                "image": "data\\images\\2378181.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on his head", 2],
            ["what is the man wearing on his neck", 2],
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["What is man doing", 1]
        ],
        "org_questions": [
            ["what is the man wearing on his head", 2],
            ["what is the man wearing on his neck", 2],
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["What is man doing", 1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is behind the man", -1]
        ],
        "context": [
            "a man sitting at a table with a plate of food.",
            "a man wearing a hat and tie with a tie around his neck."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2334559",
                "VG_object_id": "964875",
                "bbox": [239, 154, 416, 331],
                "image": "data\\images\\2334559.jpg"
            },
            {
                "VG_image_id": "2319595",
                "VG_object_id": "1000596",
                "bbox": [183, 0, 363, 278],
                "image": "data\\images\\2319595.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 1],
            ["what is the persion holding", 1],
            ["what is in the man's hand", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the person doing", -1],
            ["what color is the wall", -1],
            ["how many people are there", -1],
            ["where is the man", -1],
            ["what is the persion holding", 1],
            ["how many men are in the picture", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1],
            ["what is in the man's hand", 1]
        ],
        "context": [
            "a man holding a tennis racket in his hand.",
            "a man and woman playing a video game."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2389268",
                "VG_object_id": "1262535",
                "bbox": [17, 14, 483, 483],
                "image": "data\\images\\2389268.jpg"
            },
            {
                "VG_image_id": "2349557",
                "VG_object_id": "2510073",
                "bbox": [144, 76, 422, 261],
                "image": "data\\images\\2349557.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the plate", 2],
            ["How many kinds of food are there", 1],
            ["what is color of the plate", 1],
            ["what kind of food is this", 1]
        ],
        "org_questions": [
            ["What is the food on", -1],
            ["How many kinds of food are there", 1],
            ["what is color of the plate", 1],
            ["what is in the plate", -1],
            ["where is the food placed on", -1],
            ["how many people are there", -1],
            ["what shape is the plate", 2],
            ["what is the plate made of", -1],
            ["what kind of food is this", 1],
            ["what is under the plate", -1],
            ["where is the plate", -1]
        ],
        "context": [
            "a hamburger and fries are sitting on a plate.",
            "a grill with hot dogs and a tinfoil."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2386554",
                "VG_object_id": "1284887",
                "bbox": [2, 19, 497, 497],
                "image": "data\\images\\2386554.jpg"
            },
            {
                "VG_image_id": "2368135",
                "VG_object_id": "2179537",
                "bbox": [0, 16, 498, 459],
                "image": "data\\images\\2368135.jpg"
            }
        ],
        "questions_with_scores": [["what color is the plate", 2]],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", -1],
            ["what color is the plate", 2],
            ["how many forks are there", -1],
            ["what color is the background", -1],
            ["what shape is the table", -1],
            ["what is the table made of", -1],
            ["what is the pizza sitting on", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a slice of pizza on a plate with a drink.",
            "a man cutting a pizza with a pizza cutter."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2336680",
                "VG_object_id": "3485037",
                "bbox": [58, 212, 276, 410],
                "image": "data\\images\\2336680.jpg"
            },
            {
                "VG_image_id": "2328100",
                "VG_object_id": "978810",
                "bbox": [119, 61, 206, 362],
                "image": "data\\images\\2328100.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many clocks are there", 2],
            ["what is on the clock", 2],
            ["what color is the wall", 1],
            ["what is above the clock", 1]
        ],
        "org_questions": [
            ["how many clocks are there", 2],
            ["what is on the clock", 2],
            ["what time is it", -1],
            ["what color is the wall", 1],
            ["what shape is the clock", -1],
            ["where is the clock", -1],
            ["what is the building made of", -1],
            ["when was the photo taken", -1],
            ["what is behind the clock", -1],
            ["what is under the clock", -1],
            ["what is above the clock", 1]
        ],
        "context": [
            "a clock on the side of a building.",
            "a clock on a pole on a street corner."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2375505",
                "VG_object_id": "2689527",
                "bbox": [223, 204, 380, 317],
                "image": "data\\images\\2375505.jpg"
            },
            {
                "VG_image_id": "2412557",
                "VG_object_id": "191790",
                "bbox": [333, 159, 417, 204],
                "image": "data\\images\\2412557.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the man holding", 2],
            ["What color is the man's shirt", 2],
            ["where is the photo taken", 1],
            ["what is the man wearing on the head", 1],
            ["What is the background of image", 1],
            ["where is person in the trouser", 1],
            ["what type of shirt is the man wearing", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["What is the man holding", 2],
            ["How many people are there", -1],
            ["What color is the man's shirt", 2],
            ["where is the photo taken", 1],
            ["who is wearing the trousers", -1],
            ["what is the man wearing on the head", 1],
            ["What is the background of image", 1],
            ["where is person in the trouser", 1],
            ["what type of pants is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what type of shirt is the man wearing", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man holding a tennis racket on a tennis court.",
            "a man and a woman sitting on a bench with a child."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2401126",
                "VG_object_id": "1151460",
                "bbox": [214, 72, 306, 169],
                "image": "data\\images\\2401126.jpg"
            },
            {
                "VG_image_id": "2373568",
                "VG_object_id": "2481141",
                "bbox": [413, 100, 498, 291],
                "image": "data\\images\\2373568.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the color of the man's shorts", 2],
            ["what color is the shirt", 1],
            ["what is the man wearing on head", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what color is the ground", 2],
            ["what is the man wearing on head", 1],
            ["how many people are there", -1],
            ["what is the man doing", -1],
            ["how long is the sleeves of the shirt", -1],
            ["who is wearing the shirt", -1],
            ["what is the land made of ", -1],
            ["when was the picture taken", -1],
            ["what is the player wearing", -1],
            ["what is on the man's head", 1],
            ["what is the color of the man's shorts", 2]
        ],
        "context": [
            "a man swinging a tennis racket on a tennis court.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2330771",
                "VG_object_id": "3343872",
                "bbox": [171, 115, 284, 234],
                "image": "data\\images\\2330771.jpg"
            },
            {
                "VG_image_id": "2323128",
                "VG_object_id": "3173820",
                "bbox": [128, 125, 205, 247],
                "image": "data\\images\\2323128.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man on", 2],
            ["what is the man holding", 1],
            ["what is the man's right hand doing", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is the man on", 2],
            ["what color is the man's shirt", -1],
            ["how many people are there", -1],
            ["how is the weather", -1],
            ["what is the man holding", 1],
            ["what is the man's right hand doing", 1],
            ["what color is the ground", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a man riding a skateboard on a ramp.",
            "a man riding a horse drawn carriage down a road."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2364135",
                "VG_object_id": "1808515",
                "bbox": [1, 3, 350, 485],
                "image": "data\\images\\2364135.jpg"
            },
            {
                "VG_image_id": "2381102",
                "VG_object_id": "707354",
                "bbox": [67, 114, 166, 298],
                "image": "data\\images\\2381102.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's clothes", 2],
            ["how many people are there", 1],
            ["what time is it", 1],
            ["what is the persion doing", 1],
            ["what is the person wearing", 1],
            ["when is the picture taken", 1],
            ["what kind of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", 2],
            ["how many people are there", 1],
            ["what time is it", 1],
            ["what is the persion doing", 1],
            ["WHere is the person", -1],
            ["what is the weather like", -1],
            ["what is the person wearing", 1],
            ["when is the picture taken", 1],
            ["who is in the photo", -1],
            ["what is on the man's head", -1],
            ["what kind of shirt is the man wearing", 1]
        ],
        "context": [
            "a man with a tie and a shirt on",
            "a group of people standing around a wedding ceremony."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2384623",
                "VG_object_id": "1307031",
                "bbox": [126, 204, 205, 307],
                "image": "data\\images\\2384623.jpg"
            },
            {
                "VG_image_id": "2376328",
                "VG_object_id": "573851",
                "bbox": [339, 152, 498, 332],
                "image": "data\\images\\2376328.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 1],
            ["what is the chair made of", 1],
            ["how many chairs are there", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["what is the chair made of", 1],
            ["how many chairs are there", 1],
            ["where is the photo taken", -1],
            ["where is the chair", -1],
            ["how many people are in  the picture", -1],
            ["how many pillows are there on the chair", -1],
            ["what room is this", -1],
            ["what is next to the couch", -1],
            ["what is on the couch", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a living room with a couch, chair, and a clock.",
            "a living room with a table, chairs, and a vase of yellow roses."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2335533",
                "VG_object_id": "962589",
                "bbox": [1, 231, 373, 498],
                "image": "data\\images\\2335533.jpg"
            },
            {
                "VG_image_id": "2361800",
                "VG_object_id": "1678173",
                "bbox": [5, 346, 371, 498],
                "image": "data\\images\\2361800.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are there on the ground", 2],
            ["what is on the ground", 1],
            ["what animal is on the ground", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["how many dogs are there on the ground", 2],
            ["what color is the ground", -1],
            ["what is on the ground", 1],
            ["when is this picture taken", -1],
            ["what animal is on the ground", 1],
            ["what is the weather like", -1],
            ["what is the floor made of", -1],
            ["how is the weather", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a refrigerator sitting on the ground in front of a building.",
            "a dog sitting on a sidewalk next to a bar."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2412747",
                "VG_object_id": "2780866",
                "bbox": [240, 96, 313, 266],
                "image": "data\\images\\2412747.jpg"
            },
            {
                "VG_image_id": "2350239",
                "VG_object_id": "3594593",
                "bbox": [139, 40, 450, 307],
                "image": "data\\images\\2350239.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's clothes", 2],
            ["what is the weather like", 1],
            ["what color is the ground", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", 2],
            ["what is the weather like", 1],
            ["what color is the ground", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's feet", -1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a couple of people sitting on a ski lift.",
            "a woman running with a frisbee in her hand."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2397644",
                "VG_object_id": "1188040",
                "bbox": [3, 176, 479, 332],
                "image": "data\\images\\2397644.jpg"
            },
            {
                "VG_image_id": "2351522",
                "VG_object_id": "2142438",
                "bbox": [2, 112, 492, 373],
                "image": "data\\images\\2351522.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color are the horses", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["What color are the horses", 1],
            ["What color is the water", -1],
            ["how many people are there", 1],
            ["what are the people doing on the beach", -1],
            ["what is standing on the beach", -1],
            ["what is in the sky", -1],
            ["what is the man doing", -1],
            ["where was this photo taken", -1],
            ["what are the horses walking on", -1],
            ["where is the beach", -1],
            ["what is in the background", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "two people riding horses on a beach at sunset.",
            "three people riding horses on a beach with a dog"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2410020",
                "VG_object_id": "228013",
                "bbox": [320, 166, 498, 383],
                "image": "data\\images\\2410020.jpg"
            },
            {
                "VG_image_id": "2319374",
                "VG_object_id": "3315366",
                "bbox": [392, 150, 494, 317],
                "image": "data\\images\\2319374.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what gender is the person in the shirt", 2],
            ["what is hanging on the shirt", 1],
            ["what is the gender of the person wearing the shirt", 1],
            ["who is wearing the shirt", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is hanging on the shirt", 1],
            ["what is the gender of the person wearing the shirt", 1],
            ["How many people are there", -1],
            ["who is wearing the shirt", 1],
            ["how is the weather", -1],
            ["when was this picture taken", -1],
            ["what is the persion holding", 1],
            ["what gender is the person in the shirt", 2]
        ],
        "context": [
            "a man and a woman on a cell phone.",
            "a woman is riding on a train and looking out the window."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2344502",
                "VG_object_id": "2740578",
                "bbox": [8, 146, 499, 326],
                "image": "data\\images\\2344502.jpg"
            },
            {
                "VG_image_id": "2341866",
                "VG_object_id": "2263146",
                "bbox": [0, 298, 373, 498],
                "image": "data\\images\\2341866.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what room is this", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the wall", -1],
            ["what color is the floor", -1],
            ["how many people are there", 1],
            ["what is the floor made of", -1],
            ["what is on the floor", -1],
            ["what room is this", 1],
            ["where are the chairs", -1],
            ["where is the picture taken", 1],
            ["what kind of floor is this", -1],
            ["what is covering the floor", -1],
            ["what is the flooring", -1],
            ["what kind of flooring is this", -1]
        ],
        "context": [
            "a group of people sitting around a table playing wii.",
            "a little boy sitting on a chair eating a doughnut."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2349025",
                "VG_object_id": "3601575",
                "bbox": [213, 291, 291, 331],
                "image": "data\\images\\2349025.jpg"
            },
            {
                "VG_image_id": "2395777",
                "VG_object_id": "3822761",
                "bbox": [2, 305, 108, 355],
                "image": "data\\images\\2395777.jpg"
            }
        ],
        "questions_with_scores": [
            ["what time is the picture taken", 2],
            ["when was this photo taken", 2],
            ["what is the color of the ground", 1],
            ["what is the ground covered with", 1],
            ["what is standing on the ground", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is on the ground", -1],
            ["what is the color of the ground", 1],
            ["what time is the picture taken", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["what is standing on the ground", 1],
            ["how is the weather", -1],
            ["when was this photo taken", 2],
            ["what is in the background", 1]
        ],
        "context": [
            "a wooden bench sitting in front of a bush.",
            "two bears are walking through a grassy field."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2349765",
                "VG_object_id": "2069854",
                "bbox": [71, 120, 470, 258],
                "image": "data\\images\\2349765.jpg"
            },
            {
                "VG_image_id": "2402018",
                "VG_object_id": "3816810",
                "bbox": [63, 169, 174, 229],
                "image": "data\\images\\2402018.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bench", 1],
            ["what color is the ground", 1],
            ["Where is the bench", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the bench", 1],
            ["what color is the ground", 1],
            ["what is the bench made of", -1],
            ["How many people are there", -1],
            ["Where is the bench", 1],
            ["what is the weather like", -1],
            ["what is on the bench", -1],
            ["what is the bench make of", -1],
            ["when was the photo taken", -1],
            ["what is behind the bench", -1],
            ["where was the photo taken", 1],
            ["what is under the bench", -1]
        ],
        "context": [
            "two benches sitting on a sidewalk near the ocean.",
            "a park bench sitting in a park surrounded by flowers."
        ]
    },
    {
        "object_category": "basket",
        "images": [
            {
                "VG_image_id": "2417657",
                "VG_object_id": "3465499",
                "bbox": [198, 216, 310, 311],
                "image": "data\\images\\2417657.jpg"
            },
            {
                "VG_image_id": "2388642",
                "VG_object_id": "671942",
                "bbox": [299, 388, 353, 448],
                "image": "data\\images\\2388642.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is next to the bicycle", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["How many bicycles are there", -1],
            ["What is next to the bicycle", 1],
            ["what shape is the basket", -1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", -1],
            ["what is on the floor", -1],
            ["what is under the bike", -1],
            ["where is the basket", -1],
            ["what is behind the bike", -1],
            ["what is the bike made of", -1]
        ],
        "context": [
            "a man riding a motorcycle down a street.",
            "a cat in a doorway"
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2399443",
                "VG_object_id": "1169683",
                "bbox": [153, 308, 258, 406],
                "image": "data\\images\\2399443.jpg"
            },
            {
                "VG_image_id": "2398814",
                "VG_object_id": "1175472",
                "bbox": [304, 5, 496, 157],
                "image": "data\\images\\2398814.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the dog doing", 1],
            ["where is the dog", 1],
            ["what is the dog standing on", 1],
            ["What color is the background", 1],
            ["what is in front of the dog", 1],
            ["what is the dog wearing", 1],
            ["what animal is in the photo", 1],
            ["what is the dog looking at", 1]
        ],
        "org_questions": [
            ["what is the dog doing", 1],
            ["where is the dog", 1],
            ["what is the dog standing on", 1],
            ["What is the dog wearing on its head", -1],
            ["What color is the background", 1],
            ["what gesture is the dog", -1],
            ["what is in front of the dog", 1],
            ["what is the dog wearing", 1],
            ["what animal is in the photo", 1],
            ["how many dogs are pictured", -1],
            ["what is the dog looking at", 1]
        ],
        "context": [
            "a woman and a dog on the beach.",
            "a cat eating out of a bowl of food."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2356296",
                "VG_object_id": "821799",
                "bbox": [55, 410, 279, 498],
                "image": "data\\images\\2356296.jpg"
            },
            {
                "VG_image_id": "2359849",
                "VG_object_id": "2187898",
                "bbox": [4, 161, 498, 325],
                "image": "data\\images\\2359849.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["WHat is on the floor", 1],
            ["what is on the picture", 1],
            ["what room is this", 1],
            ["what is covering the floor", 1]
        ],
        "org_questions": [
            ["What color is the floor", -1],
            ["WHat is on the floor", 1],
            ["what is the pattern of the floor", -1],
            ["what is the floor made of", -1],
            ["what is on the picture", 1],
            ["what is the color of the wall", -1],
            ["where is the photo taken", 2],
            ["what room is this", 1],
            ["what is covering the floor", 1]
        ],
        "context": [
            "a bathroom with a toilet and a curtain.",
            "a dining table and chairs in a room."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2372150",
                "VG_object_id": "1696966",
                "bbox": [1, 167, 222, 351],
                "image": "data\\images\\2372150.jpg"
            },
            {
                "VG_image_id": "2357079",
                "VG_object_id": "2321122",
                "bbox": [83, 94, 353, 218],
                "image": "data\\images\\2357079.jpg"
            }
        ],
        "questions_with_scores": [["what is the cat looking at", 1]],
        "org_questions": [
            ["what color is the cat", -1],
            ["where is the cat", -1],
            ["what is the cat doing", -1],
            ["how many cats are in the picture", -1],
            ["what is in the background", -1],
            ["what is the cat standing on", -1],
            ["what animal is in the picture", -1],
            ["who is in the photo", -1],
            ["what is the cat looking at", 1],
            ["what is on the cat's head", -1]
        ],
        "context": [
            "a cat is sitting on a bed next to a laptop.",
            "a black and white cat laying on top of a laptop."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2400586",
                "VG_object_id": "1187270",
                "bbox": [82, 86, 438, 256],
                "image": "data\\images\\2400586.jpg"
            },
            {
                "VG_image_id": "2316459",
                "VG_object_id": "1053725",
                "bbox": [398, 279, 484, 371],
                "image": "data\\images\\2316459.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there", 1],
            ["what are the children doing", 1],
            ["Where is child", 1],
            ["What color is child shirt", 1],
            ["what are the people standing on", 1],
            ["where was the photo taken", 1],
            ["where are the people standing", 1]
        ],
        "org_questions": [
            ["how many children are there", 1],
            ["what are the children doing", 1],
            ["Where is child", 1],
            ["what gender is the child", -1],
            ["What color is child shirt", 1],
            ["who is in the photo", -1],
            ["what are the people standing on", 1],
            ["what is the weather", -1],
            ["where was the photo taken", 1],
            ["where are the people standing", 1]
        ],
        "context": [
            "a group of children sitting next to each other.",
            "a man sitting on the beach next to a yellow umbrella."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2363287",
                "VG_object_id": "1908759",
                "bbox": [2, 116, 499, 331],
                "image": "data\\images\\2363287.jpg"
            },
            {
                "VG_image_id": "2397668",
                "VG_object_id": "431845",
                "bbox": [2, 26, 496, 331],
                "image": "data\\images\\2397668.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what color is the plate", 2],
            ["how many pizzas are there", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what color is the plate", 2],
            ["how many pizzas are there", 1],
            ["what shape is the table", -1],
            ["what food is it", -1],
            ["where is the table", -1],
            ["how many people are there in the picture", 1],
            ["what is the table made of", -1],
            ["what is the pizza sitting on", -1],
            ["what is on the table", -1],
            ["what type of food is on the table", -1]
        ],
        "context": [
            "a man sitting at a table with two pizzas.",
            "a pizza with black olives and cheese on a wooden board."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2355456",
                "VG_object_id": "829358",
                "bbox": [40, 396, 182, 499],
                "image": "data\\images\\2355456.jpg"
            },
            {
                "VG_image_id": "2396679",
                "VG_object_id": "441582",
                "bbox": [41, 239, 96, 326],
                "image": "data\\images\\2396679.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the person doing", 1],
            ["what is beside the person", 1],
            ["what is the person in the trousers holding", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["what is the person doing", 1],
            ["what is beside the person", 1],
            ["how many people are there", -1],
            ["where is the person", -1],
            ["what is the person in the trousers holding", 1],
            ["when was this photo taken", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a man sitting next to a baby elephant with a bell around his neck.",
            "a group of people on skis in the snow."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2357938",
                "VG_object_id": "807517",
                "bbox": [2, 111, 477, 330],
                "image": "data\\images\\2357938.jpg"
            },
            {
                "VG_image_id": "2326623",
                "VG_object_id": "3939017",
                "bbox": [2, 32, 498, 370],
                "image": "data\\images\\2326623.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the couch", 1],
            ["how many people are there on the couch", 1],
            ["what color is the man's shirt", 1],
            ["how many pillows are there on the couch", 1],
            ["what is in front of the couch", 1],
            ["what color is the wall", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the couch", 1],
            ["how many people are there on the couch", 1],
            ["what color is the man's shirt", 1],
            ["what is behind the couch", -1],
            ["What is in the back of sofa", -1],
            ["how many pillows are there on the couch", 1],
            ["what is in front of the couch", 1],
            ["what color is the wall", 1],
            ["when was the picture taken", 1],
            ["what type of animal is shown", -1],
            ["where was this picture taken", -1],
            ["what are the people doing", -1]
        ],
        "context": [
            "a man and two children sitting on a couch with a dog.",
            "a man and a dog laying on a couch."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2369151",
                "VG_object_id": "2044611",
                "bbox": [92, 32, 431, 373],
                "image": "data\\images\\2369151.jpg"
            },
            {
                "VG_image_id": "2383082",
                "VG_object_id": "695019",
                "bbox": [274, 18, 439, 374],
                "image": "data\\images\\2383082.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["how many people are there", 1],
            ["what is the man holding", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["how many people are there", 1],
            ["what is the man wearing on head", -1],
            ["Where is the man", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is behind the man", 1],
            ["what is the man standing on", -1],
            ["what is on the man's face", -1]
        ],
        "context": [
            "a man taking a selfie in a bathroom.",
            "a man and a woman playing a video game."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2338196",
                "VG_object_id": "2775940",
                "bbox": [160, 65, 282, 246],
                "image": "data\\images\\2338196.jpg"
            },
            {
                "VG_image_id": "2406157",
                "VG_object_id": "326117",
                "bbox": [134, 38, 297, 329],
                "image": "data\\images\\2406157.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are in the picture", 1],
            ["what is the person wearing", 1],
            ["what is the person on", 1],
            ["how many persons are there", 1],
            ["what is on the man's head", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are in the picture", 1],
            ["what color is the person's shirt", -1],
            ["what is the ground covered with", -1],
            ["what is the person wearing", 1],
            ["what is the person on", 1],
            ["how many persons are there", 1],
            ["when was the photo taken", -1],
            ["what is on the man's head", 1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man riding a horse in a rodeo",
            "a man riding a dirt bike on a beach."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2368193",
                "VG_object_id": "1957148",
                "bbox": [138, 148, 289, 347],
                "image": "data\\images\\2368193.jpg"
            },
            {
                "VG_image_id": "2395805",
                "VG_object_id": "450718",
                "bbox": [294, 154, 351, 204],
                "image": "data\\images\\2395805.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the man holding", 2],
            ["what is the player wearing on the head", 1],
            ["Where is the man", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the man holding", 2],
            ["what is the man doing", -1],
            ["what gender is the person in the shirt", -1],
            ["what is the player wearing on the head", 1],
            ["Where is the man", 1],
            ["what is the posture of the person in the shirt", -1],
            ["What is man doing", -1],
            ["when was the photo taken", -1],
            ["what is the man wearing", 1],
            ["what color is the grass", -1]
        ],
        "context": [
            "a man in a green shirt and white shorts playing frisbee.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2395805",
                "VG_object_id": "450718",
                "bbox": [294, 154, 351, 204],
                "image": "data\\images\\2395805.jpg"
            },
            {
                "VG_image_id": "2389936",
                "VG_object_id": "1255779",
                "bbox": [52, 239, 183, 424],
                "image": "data\\images\\2389936.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is man holding", 2],
            ["What sports is man doing", 2],
            ["what color are the man's pants", 2],
            ["what kind of sport is the person doing", 2],
            ["what is the person in the shirt doing", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person in the shirt doing", 1],
            ["what is in the background", 1],
            ["what gender is the person in the shirt", -1],
            ["where is the person", -1],
            ["what is man holding", 2],
            ["What sports is man doing", 2],
            ["when was the photo taken", -1],
            ["what color are the man's pants", 2],
            ["what kind of sport is the person doing", 2]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a man holding a frisbee in his hand."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2406157",
                "VG_object_id": "326122",
                "bbox": [2, 195, 499, 373],
                "image": "data\\images\\2406157.jpg"
            },
            {
                "VG_image_id": "2376273",
                "VG_object_id": "2221933",
                "bbox": [1, 317, 373, 497],
                "image": "data\\images\\2376273.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what are the people doing on the beach", 2],
            ["what is on the man's head", 1],
            ["what is the main color of the sand", 1]
        ],
        "org_questions": [
            ["what is on the man's head", 1],
            ["what is the man doing", 2],
            ["what is in the distance", -1],
            ["what is standing on the beach", -1],
            ["what are the people doing on the beach", 2],
            ["how many people are there", -1],
            ["where was this picture taken", -1],
            ["what is on the ground", -1],
            ["what is the main color of the sand", 1]
        ],
        "context": [
            "a man riding a dirt bike on a beach.",
            "a man holding a frisbee and giving a thumbs up sign."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2358884",
                "VG_object_id": "1895919",
                "bbox": [377, 3, 432, 193],
                "image": "data\\images\\2358884.jpg"
            },
            {
                "VG_image_id": "2337232",
                "VG_object_id": "2513982",
                "bbox": [3, 56, 81, 226],
                "image": "data\\images\\2337232.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 2],
            ["what is on the right side of the room", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["where is the picture taken", -1],
            ["what is behind the curtain", -1],
            ["what room is it", -1],
            ["what color is the curtain", 2],
            ["what is covering the window", -1],
            ["where are the curtains", -1],
            ["what is on the wall", -1],
            ["what is in front of the window", -1],
            ["what is on the right side of the room", 1]
        ],
        "context": [
            "a hospital room with a bed and a hospital bed.",
            "a bedroom with a bed, lamps, and a lamp."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2352864",
                "VG_object_id": "2652986",
                "bbox": [243, 152, 425, 372],
                "image": "data\\images\\2352864.jpg"
            },
            {
                "VG_image_id": "2358635",
                "VG_object_id": "1920208",
                "bbox": [57, 167, 183, 380],
                "image": "data\\images\\2358635.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl holding", 2],
            ["what is the girl wearing on her head", 2],
            ["what is in the background", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what is the girl holding", 2],
            ["what is the girl wearing on her head", 2],
            ["how many people are there standing around the girl", -1],
            ["where is the girl", -1],
            ["what is in the background", 1],
            ["How old is the girl", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion doing", 1],
            ["what is the girl wearing", -1],
            ["what color is the grass", -1]
        ],
        "context": [
            "a young girl is running with a frisbee in her hand.",
            "a little girl is playing with a kite."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2366683",
                "VG_object_id": "626799",
                "bbox": [160, 112, 276, 232],
                "image": "data\\images\\2366683.jpg"
            },
            {
                "VG_image_id": "2325214",
                "VG_object_id": "986659",
                "bbox": [225, 37, 327, 153],
                "image": "data\\images\\2325214.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the man holding", 2],
            ["What sports is man doing", 1],
            ["What color is man's shoes", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the man doing", -1],
            ["what is the man holding", 2],
            ["How many people are there", -1],
            ["What sports is man doing", 1],
            ["What color is man's shoes", 1],
            ["what is the man wearing on his head", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the player wearing", -1]
        ],
        "context": [
            "a man in a red and white baseball uniform is throwing a ball.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2417800",
                "VG_object_id": "3446437",
                "bbox": [1, 283, 498, 330],
                "image": "data\\images\\2417800.jpg"
            },
            {
                "VG_image_id": "2368973",
                "VG_object_id": "1863336",
                "bbox": [9, 159, 499, 328],
                "image": "data\\images\\2368973.jpg"
            }
        ],
        "questions_with_scores": [
            ["how is the weather", 2],
            ["what is on the ground", 1],
            ["where is the picture taken", 1],
            ["when is this photo taken", 1],
            ["what is the weather like", 1],
            ["what is on the side of the road", 1],
            ["where is the road", 1]
        ],
        "org_questions": [
            ["how is the weather", 2],
            ["what is on the ground", 1],
            ["where is the picture taken", 1],
            ["How many people are there", -1],
            ["when is this photo taken", 1],
            ["what is the land made of", -1],
            ["how many motorcycles are there on the ground", -1],
            ["what is the weather like", 1],
            ["what is on the side of the road", 1],
            ["where is the road", 1],
            ["what color is the ground", -1]
        ],
        "context": [
            "a man in a small airplane on a runway.",
            "a woman walking down a street holding an umbrella."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2320090",
                "VG_object_id": "2706950",
                "bbox": [131, 9, 229, 216],
                "image": "data\\images\\2320090.jpg"
            },
            {
                "VG_image_id": "2396189",
                "VG_object_id": "665191",
                "bbox": [184, 3, 306, 162],
                "image": "data\\images\\2396189.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's shirt", 1],
            ["what is on the boy's head", 1]
        ],
        "org_questions": [
            ["what color is the boy's shirt", 1],
            ["what is on the boy's head", 1],
            ["where is the boy", -1],
            ["how many people are there", -1],
            ["What is child doing", -1],
            ["what is the boy holding", -1],
            ["What is the background of image", -1],
            ["what is the boy wearing", -1],
            ["when was the photo taken", -1],
            ["who is skateboarding", -1],
            ["what is the man doing", -1]
        ],
        "context": [
            "a man riding a skateboard down a rail.",
            "a boy jumping a skateboard over a sidewalk."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2373362",
                "VG_object_id": "732898",
                "bbox": [2, 132, 290, 333],
                "image": "data\\images\\2373362.jpg"
            },
            {
                "VG_image_id": "2370610",
                "VG_object_id": "2187588",
                "bbox": [2, 188, 495, 331],
                "image": "data\\images\\2370610.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the man's pant", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the man's pant", 1],
            ["what is in the distance", -1],
            ["how many people are there in the picture", 1],
            ["what sport is the person doing", -1],
            ["what is the ground covered with", -1],
            ["who is on the court", -1],
            ["what is the man doing", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["where is the person", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a man swinging a tennis racket at a ball.",
            "a young boy playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2364026",
                "VG_object_id": "2282969",
                "bbox": [209, 112, 306, 164],
                "image": "data\\images\\2364026.jpg"
            },
            {
                "VG_image_id": "2378098",
                "VG_object_id": "3458837",
                "bbox": [158, 94, 219, 171],
                "image": "data\\images\\2378098.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on his head", 2],
            ["how many people are there", 2],
            ["what is the ground covered with", 1],
            ["what is the man holding", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what is the man wearing on his head", 2],
            ["what is the ground covered with", 1],
            ["how many people are there", 2],
            ["what gender is the person in the shirt", -1],
            ["what is the color of the shirt", -1],
            ["what is the man holding", 1],
            ["what is the man doing", 1],
            ["when was this photo taken", -1],
            ["what is the person wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a baseball pitcher is throwing a baseball on a field.",
            "a man riding a motorcycle down a street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2317454",
                "VG_object_id": "3126929",
                "bbox": [27, 0, 252, 239],
                "image": "data\\images\\2317454.jpg"
            },
            {
                "VG_image_id": "2319521",
                "VG_object_id": "2894594",
                "bbox": [209, 2, 489, 350],
                "image": "data\\images\\2319521.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is beside the man", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is beside the man", 1],
            ["what shape is the man's collar", -1],
            ["how many men are there", -1],
            ["What color is man's shirt", -1],
            ["Where is the man", -1],
            ["when was this picture taken", -1],
            ["who is wearing glasses", -1],
            ["what is on the man's face", -1],
            ["what is the man looking at", -1]
        ],
        "context": [
            "a man and a child sitting at a table with pizza.",
            "a man in a suit and tie with a dog."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2414619",
                "VG_object_id": "155142",
                "bbox": [277, 80, 433, 374],
                "image": "data\\images\\2414619.jpg"
            },
            {
                "VG_image_id": "2338309",
                "VG_object_id": "954604",
                "bbox": [210, 1, 480, 337],
                "image": "data\\images\\2338309.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 1],
            ["how many people are there", -1],
            ["what is the woman wearing on head", -1],
            ["where is the woman", -1],
            ["what is the woman holding", 1],
            ["what is the gender of the person on the left", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", -1],
            ["where was the photo taken", -1],
            ["what gender is the person", -1]
        ],
        "context": [
            "a woman playing a video game in a living room.",
            "a woman sitting at a table with a glass of wine."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2335126",
                "VG_object_id": "2635098",
                "bbox": [228, 63, 345, 215],
                "image": "data\\images\\2335126.jpg"
            },
            {
                "VG_image_id": "2406063",
                "VG_object_id": "1104123",
                "bbox": [200, 118, 297, 170],
                "image": "data\\images\\2406063.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what color is the man's shirt", 1],
            ["what color is the surfboard", 1],
            ["what is on the man's feet", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["how many people are in the picture", 2],
            ["what color is the surfboard", 1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["What is the weather like", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is on the surfboard", -1],
            ["what is on the man's feet", 1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a man standing on a surfboard in the water.",
            "a man riding a surfboard on top of a wave."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2365093",
                "VG_object_id": "636533",
                "bbox": [0, 368, 373, 499],
                "image": "data\\images\\2365093.jpg"
            },
            {
                "VG_image_id": "2357770",
                "VG_object_id": "808871",
                "bbox": [46, 249, 212, 499],
                "image": "data\\images\\2357770.jpg"
            }
        ],
        "questions_with_scores": [["what color is the floor", 1]],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is on the floor", -1],
            ["what is on the wall", -1],
            ["how many people are there", -1],
            ["Where is the photo taken", -1],
            ["What is the floor made of", -1],
            ["what color is the wall", -1],
            ["how many people are there on the floor", -1],
            ["what shape is the floor", -1],
            ["what room is this", -1],
            ["what pattern is on the floor", -1],
            ["what kind of flooring is in the room", -1]
        ],
        "context": [
            "a bathroom with a toilet, sink and a trash can.",
            "a bathroom with a green tub and a toilet"
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2357187",
                "VG_object_id": "1786229",
                "bbox": [87, 221, 233, 341],
                "image": "data\\images\\2357187.jpg"
            },
            {
                "VG_image_id": "2406898",
                "VG_object_id": "368008",
                "bbox": [33, 148, 499, 356],
                "image": "data\\images\\2406898.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are there", 2],
            ["what color are the horses", 2],
            ["what color is the ground", 1],
            ["what color is the horse on the right", 1],
            ["what is horse doing", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["where are the horses", 1]
        ],
        "org_questions": [
            ["how many horses are there", 2],
            ["what color is the ground", 1],
            ["what color is the horse on the right", 1],
            ["what is horse doing", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["how many people are there in the picture", -1],
            ["what color are the horses", 2],
            ["what type of animals are shown", -1],
            ["when was the photo taken", -1],
            ["where are the horses", 1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a white horse grazing in a field with mountains in the background.",
            "a group of horses standing on top of a sandy beach."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2392175",
                "VG_object_id": "482426",
                "bbox": [0, 3, 499, 374],
                "image": "data\\images\\2392175.jpg"
            },
            {
                "VG_image_id": "2361759",
                "VG_object_id": "3753472",
                "bbox": [0, 2, 474, 332],
                "image": "data\\images\\2361759.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 2],
            ["what color is the wall", 2]
        ],
        "org_questions": [
            ["what color is the floor", 2],
            ["what color is the wall", 2],
            ["what color is the cabinet", -1],
            ["how many people are there in the picture", -1],
            ["what is the floor made of", -1],
            ["what type of room is this", -1],
            ["what is in the kitchen", -1],
            ["where was this picture taken", -1],
            ["what is on the floor", -1]
        ],
        "context": [
            "a kitchen with a stove, microwave and sink.",
            "a kitchen with a stove, microwave and a stove."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2379189",
                "VG_object_id": "1360126",
                "bbox": [52, 43, 197, 270],
                "image": "data\\images\\2379189.jpg"
            },
            {
                "VG_image_id": "2364758",
                "VG_object_id": "639194",
                "bbox": [239, 137, 301, 192],
                "image": "data\\images\\2364758.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 2],
            ["what color is the board", 1]
        ],
        "org_questions": [
            ["what color is the wall", 2],
            ["what color is the board", 1],
            ["who is on the board", -1],
            ["where is the photo taken", -1],
            ["what direction is the board heading to", -1],
            ["what is the board made of", -1],
            ["how many people are there", -1],
            ["when was the picture taken", -1],
            ["what is on the wall", -1],
            ["what does the sign say", -1]
        ],
        "context": [
            "a sign on a pole next to a brick wall.",
            "a large stuffed animal sitting on a bench in front of a store."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2344518",
                "VG_object_id": "915213",
                "bbox": [117, 277, 180, 355],
                "image": "data\\images\\2344518.jpg"
            },
            {
                "VG_image_id": "2366605",
                "VG_object_id": "1962887",
                "bbox": [425, 0, 499, 68],
                "image": "data\\images\\2366605.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["What color is the bag", 1],
            ["What is man doing", 1],
            ["where is the picture taken", 1],
            ["where is the bag", 1],
            ["what is the persion doing", 1],
            ["what is green", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is the bag", 1],
            ["What is man doing", 1],
            ["where is the picture taken", 1],
            ["where is the bag", 1],
            ["what is the persion doing", 1],
            ["what is blue", -1],
            ["what is green", 1]
        ],
        "context": [
            "a man and a woman eating food at a table.",
            "a child in a pink jacket and glasses playing with a broken toilet."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2378736",
                "VG_object_id": "1365547",
                "bbox": [0, 242, 417, 499],
                "image": "data\\images\\2378736.jpg"
            },
            {
                "VG_image_id": "2374536",
                "VG_object_id": "1805992",
                "bbox": [0, 260, 499, 332],
                "image": "data\\images\\2374536.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man doing", 2],
            ["What is man holding", 1],
            ["how many horses are there on the field", 1],
            ["What sports are people playing ", 1]
        ],
        "org_questions": [
            ["What is man doing", 2],
            ["What is the background of photo", -1],
            ["What is man holding", 1],
            ["how many horses are there on the field", 1],
            ["What color is the ground", -1],
            ["What sports are people playing ", 1],
            ["WHat is in the background", -1],
            ["where is the grass", -1],
            ["where was this picture taken", -1],
            ["what is covering the ground", -1],
            ["what is green", -1]
        ],
        "context": [
            "a man holding a kite in a field.",
            "a woman riding a horse over a hurdle."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2344727",
                "VG_object_id": "913645",
                "bbox": [146, 184, 355, 301],
                "image": "data\\images\\2344727.jpg"
            },
            {
                "VG_image_id": "2350677",
                "VG_object_id": "2694442",
                "bbox": [183, 203, 381, 260],
                "image": "data\\images\\2350677.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cakes are there on the table", 1],
            ["What is cake on", 1],
            ["What is on the cake", 1],
            ["how many people are there", 1],
            ["how many plates are there", 1],
            ["where is the cake", 1],
            ["what is next to the cake", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["how many cakes are there on the table", 1],
            ["what is the shape of the cake", -1],
            ["What is cake on", 1],
            ["What is on the cake", 1],
            ["how many people are there", 1],
            ["what color is the table the cake is placed on", -1],
            ["where are the cakes", -1],
            ["what kind of cake is this", -1],
            ["what is on the table", -1],
            ["how many plates are there", 1],
            ["where is the cake", 1],
            ["what pattern is on the cake", -1],
            ["what is next to the cake", 1],
            ["what is blue", -1]
        ],
        "context": [
            "a man and two children sitting at a table with a cake.",
            "a woman standing in front of a table with a cake on it."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2407568",
                "VG_object_id": "277315",
                "bbox": [0, 210, 475, 331],
                "image": "data\\images\\2407568.jpg"
            },
            {
                "VG_image_id": "2412759",
                "VG_object_id": "187427",
                "bbox": [383, 209, 499, 329],
                "image": "data\\images\\2412759.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the rug", 2],
            ["what is on the rug", 2],
            ["what room is the rug in", 1],
            ["what is the floor under the rug made of", 1],
            ["what type of flooring is shown", 1],
            ["what is the floor color", 1],
            ["what type of flooring is in the picture", 1],
            ["what is the floor made of", 1]
        ],
        "org_questions": [
            ["what color is the rug", 2],
            ["what is on the rug", 2],
            ["what room is the rug in", 1],
            ["what shape is the rug", -1],
            ["how many people are there in the picture", -1],
            ["what is the floor under the rug made of", 1],
            ["where is the rug", -1],
            ["what type of flooring is shown", 1],
            ["what is covering the floor", -1],
            ["what is the floor color", 1],
            ["what type of flooring is in the picture", 1],
            ["what is the floor made of", 1]
        ],
        "context": [
            "a large wooden bed with a wooden frame.",
            "a dog is standing in the kitchen looking at a table."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2350608",
                "VG_object_id": "2503125",
                "bbox": [319, 87, 419, 264],
                "image": "data\\images\\2350608.jpg"
            },
            {
                "VG_image_id": "2394406",
                "VG_object_id": "666894",
                "bbox": [154, 100, 317, 445],
                "image": "data\\images\\2394406.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is on the man's feet", 1],
            ["what is this person doing", 1],
            ["What is man doing", 1],
            ["what gesture is the man", 1]
        ],
        "org_questions": [
            ["what color is the man's jacket", -1],
            ["where is the man", -1],
            ["how many people are there", -1],
            ["what is on the man head", -1],
            ["what transportation is the man taking", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what sport is this", -1],
            ["what is on the man's feet", 1],
            ["what is this person doing", 1],
            ["What is man doing", 1],
            ["what is the man wearing on head", -1],
            ["what gesture is the man", 1]
        ],
        "context": [
            "a person riding a snowboard down a snow covered slope.",
            "a man walking in the snow carrying skis."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2395805",
                "VG_object_id": "450745",
                "bbox": [291, 199, 375, 257],
                "image": "data\\images\\2395805.jpg"
            },
            {
                "VG_image_id": "2354255",
                "VG_object_id": "840050",
                "bbox": [158, 392, 253, 497],
                "image": "data\\images\\2354255.jpg"
            }
        ],
        "questions_with_scores": [
            ["what gender is the person in the trousers", 2],
            ["what are the people doing", 1],
            ["what color is the shirt of the person in the trousers", 1],
            ["what is the person in the trousers holding in hand", 1],
            ["who is wearing the trousers", 1],
            ["what is on the man's head", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what are the people doing", 1],
            ["what color is the shirt of the person in the trousers", 1],
            ["what is the person in the trousers holding in hand", 1],
            ["who is wearing the trousers", 1],
            ["where is the man", -1],
            ["what is on the man's head", 1],
            ["how many people are there in the photo", -1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["what is the man wearing", 1],
            ["what color is the grass", -1],
            ["what gender is the person in the trousers", 2]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a woman and a child playing frisbee in a park."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2368297",
                "VG_object_id": "1787980",
                "bbox": [108, 374, 239, 487],
                "image": "data\\images\\2368297.jpg"
            },
            {
                "VG_image_id": "2322588",
                "VG_object_id": "3340124",
                "bbox": [25, 287, 202, 372],
                "image": "data\\images\\2322588.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is above the cabinet", 1],
            ["what color is the ground", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", -1],
            ["what is above the cabinet", 1],
            ["how many people are there", 2],
            ["where is the cabinet", -1],
            ["what color is the ground", 1],
            ["what are the cabinets made of", -1],
            ["what is on the floor", -1],
            ["what is the table made of", -1],
            ["what is the floor made of", -1]
        ],
        "context": [
            "a library with a lot of clocks on the wall",
            "an older woman holding a broccoli head in her hand."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2367541",
                "VG_object_id": "3873523",
                "bbox": [83, 4, 422, 112],
                "image": "data\\images\\2367541.jpg"
            },
            {
                "VG_image_id": "2368479",
                "VG_object_id": "2635942",
                "bbox": [5, 6, 485, 179],
                "image": "data\\images\\2368479.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sky", 1],
            ["what is the ground covered with", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is in the distance", -1],
            ["what color is the sky", 1],
            ["how many planes are there in the picture", -1],
            ["What time is it", -1],
            ["what is the ground covered with", 1],
            ["how is the weather", -1],
            ["when was this picture taken", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a bus driving down a street next to a truck.",
            "a red train on a track"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2384552",
                "VG_object_id": "526026",
                "bbox": [160, 89, 362, 248],
                "image": "data\\images\\2384552.jpg"
            },
            {
                "VG_image_id": "2362728",
                "VG_object_id": "771750",
                "bbox": [135, 96, 266, 319],
                "image": "data\\images\\2362728.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["how many people are there", -1],
            ["what is the weather like", -1],
            ["what is the man holding", -1],
            ["when was the picture taken", -1],
            ["what is the persion standing on", 1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a man riding a surfboard on a wave in the ocean.",
            "a man is playing frisbee on the beach."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2401945",
                "VG_object_id": "1142332",
                "bbox": [363, 55, 492, 296],
                "image": "data\\images\\2401945.jpg"
            },
            {
                "VG_image_id": "2356048",
                "VG_object_id": "3564453",
                "bbox": [21, 113, 342, 425],
                "image": "data\\images\\2356048.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what color is the man's shirt", 1],
            ["What is man doing", 1],
            ["what sport is being played", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what is on the man's head", 2],
            ["how many people are there", -1],
            ["what color is the man's shirt", 1],
            ["what is the weather like", -1],
            ["Where is the man", -1],
            ["What is man doing", 1],
            ["when was the photo taken", -1],
            ["who is playing", -1],
            ["what sport is being played", 1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a man jumping in the air to hit a tennis ball.",
            "a group of men standing on top of a field."
        ]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2390711",
                "VG_object_id": "1247852",
                "bbox": [58, 183, 121, 280],
                "image": "data\\images\\2390711.jpg"
            },
            {
                "VG_image_id": "2395393",
                "VG_object_id": "3823218",
                "bbox": [428, 172, 486, 302],
                "image": "data\\images\\2395393.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the light", 2],
            ["what is the traffic light on", 1]
        ],
        "org_questions": [
            ["what color is the light", 2],
            ["where is the light", -1],
            ["how many people are there", -1],
            ["what is the weather like", -1],
            ["what is the traffic light on", 1],
            ["What is the status of lamp", -1],
            ["what is this a picture of", -1],
            ["how many traffic lights are there", -1]
        ],
        "context": [
            "a street sign and a traffic light in a city.",
            "a street sign is hanging from a pole."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2379686",
                "VG_object_id": "1355238",
                "bbox": [277, 90, 339, 199],
                "image": "data\\images\\2379686.jpg"
            },
            {
                "VG_image_id": "2388213",
                "VG_object_id": "3268468",
                "bbox": [89, 61, 234, 303],
                "image": "data\\images\\2388213.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["what is beside the woman", 1],
            ["How many people are there", 1],
            ["what color is the person's trousers", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["what is beside the woman", 1],
            ["How many people are there", 1],
            ["what color is the person's trousers", 1],
            ["where is the woman", -1],
            ["what is the woman wearing on her face", -1],
            ["What is the background of image", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion holding", 1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "a woman and child are feeding cows in a pen.",
            "a woman sitting on a step next to two bikes."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2393352",
                "VG_object_id": "473106",
                "bbox": [21, 184, 160, 374],
                "image": "data\\images\\2393352.jpg"
            },
            {
                "VG_image_id": "2347403",
                "VG_object_id": "1991080",
                "bbox": [131, 99, 404, 315],
                "image": "data\\images\\2347403.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["who is in the picture", 2],
            ["What is person doing", 1],
            ["what is the persion holding", 1],
            ["what is the persion wearing", 1],
            ["who is wearing a hat", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What is person doing", 1],
            ["what is the persion wearing on her face", -1],
            ["what is the persion holding", 1],
            ["what is the persion wearing", 1],
            ["when was this photo taken", -1],
            ["who is wearing a hat", 1],
            ["who is in the picture", 2]
        ],
        "context": [
            "a boy sitting on a rock playing a trumpet.",
            "a group of people playing frisbee in a field."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2334998",
                "VG_object_id": "3218491",
                "bbox": [2, 129, 494, 331],
                "image": "data\\images\\2334998.jpg"
            },
            {
                "VG_image_id": "2371417",
                "VG_object_id": "597776",
                "bbox": [1, 242, 499, 334],
                "image": "data\\images\\2371417.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person on the street doing", 2],
            ["what is the person doing", 2],
            ["what is the person wearing", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the person on the street doing", 2],
            ["what is the person on the street wearing", -1],
            ["what color is the line on the street", -1],
            ["how many people are there", 1],
            ["When is the photo taken", -1],
            ["What is the weather like", -1],
            ["what color is the ground", -1],
            ["where was this picture taken", -1],
            ["what is the ground covered with", -1],
            ["where are the white lines", -1],
            ["what is the person doing", 2],
            ["what is the person wearing", 2]
        ],
        "context": [
            "a man walking down a street holding a sign.",
            "a person riding a skateboard down a street."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2324753",
                "VG_object_id": "2941104",
                "bbox": [142, 153, 215, 238],
                "image": "data\\images\\2324753.jpg"
            },
            {
                "VG_image_id": "2353835",
                "VG_object_id": "842827",
                "bbox": [320, 125, 422, 370],
                "image": "data\\images\\2353835.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the person doing", 1],
            ["what is the man wearing", 1],
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["where is the person", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what color is the man's pants", -1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the person doing", 1],
            ["What color is the man's pair of trousers", -1],
            ["when was the photo taken", -1],
            ["what is the man on the left wearing on the head", -1],
            ["what is the man wearing", 1],
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["where is the person", 1],
            ["what is in the distance", 1]
        ],
        "context": [
            "a group of people in a boat on a lake.",
            "a group of people standing around a food truck."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2385247",
                "VG_object_id": "1299212",
                "bbox": [273, 108, 427, 480],
                "image": "data\\images\\2385247.jpg"
            },
            {
                "VG_image_id": "2371276",
                "VG_object_id": "3522992",
                "bbox": [120, 72, 258, 426],
                "image": "data\\images\\2371276.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman's posture", 1],
            ["what color is the woman's shirt", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1]
        ],
        "org_questions": [
            ["what is the woman's posture", 1],
            ["what color is the woman's shirt", 1],
            ["what is the woman doing", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the woman wearing", 1],
            ["where is the woman", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a group of people flying kites in the sky.",
            "a woman in a pink dress is waiting on a bench."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2364987",
                "VG_object_id": "2578237",
                "bbox": [116, 39, 289, 339],
                "image": "data\\images\\2364987.jpg"
            },
            {
                "VG_image_id": "2346042",
                "VG_object_id": "902989",
                "bbox": [264, 92, 427, 244],
                "image": "data\\images\\2346042.jpg"
            }
        ],
        "questions_with_scores": [["What is cow doing", 1]],
        "org_questions": [
            ["what color is the ground", -1],
            ["how many cows are there", -1],
            ["What is cow doing", 1],
            ["what type of animal is shown", -1],
            ["what is on the cow's head", -1],
            ["when was the photo taken", -1],
            ["what is in the distance", -1],
            ["what color is the grass", -1],
            ["what animal is in the picture", -1]
        ],
        "context": [
            "a cow standing on a beach next to a boat.",
            "a group of cows walking down a dirt road."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2352607",
                "VG_object_id": "2118347",
                "bbox": [49, 182, 496, 358],
                "image": "data\\images\\2352607.jpg"
            },
            {
                "VG_image_id": "2376457",
                "VG_object_id": "2120568",
                "bbox": [21, 273, 440, 368],
                "image": "data\\images\\2376457.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["What is color of table", 1]
        ],
        "org_questions": [
            ["what is the table made of", -1],
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["How many forks are there", -1],
            ["What is color of table", 1],
            ["where was the photo taken", -1],
            ["what is in the background", -1],
            ["where are the people", -1]
        ],
        "context": [
            "a table with two laptops and a person sitting at it",
            "a woman is smiling next to a microwave."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2363990",
                "VG_object_id": "760390",
                "bbox": [88, 133, 177, 265],
                "image": "data\\images\\2363990.jpg"
            },
            {
                "VG_image_id": "2370474",
                "VG_object_id": "1829814",
                "bbox": [190, 138, 452, 259],
                "image": "data\\images\\2370474.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the chair", 1],
            ["how many chairs are there", 1],
            ["where was this photo taken", 1],
            ["Where is the chair", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["where is the chair", 1],
            ["how many chairs are there", 1],
            ["what is the floor made of", -1],
            ["how many people are in  the picture", -1],
            ["how many pillows are there on the chair", -1],
            ["what is on the chair", -1],
            ["where was this photo taken", 1],
            ["what are the chairs made of", -1],
            ["Where is the chair", 1],
            ["How many people are there", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a bathroom with a sink, a rug and a rug.",
            "a kitchen with a center island and a refrigerator."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2349887",
                "VG_object_id": "1843294",
                "bbox": [83, 84, 270, 240],
                "image": "data\\images\\2349887.jpg"
            },
            {
                "VG_image_id": "2414972",
                "VG_object_id": "148471",
                "bbox": [314, 109, 405, 208],
                "image": "data\\images\\2414972.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 2],
            ["what color is the shirt", 2],
            ["What sports is man doing", 1],
            ["what is the person holding", 1],
            ["what is the man doing", 1],
            ["who is wearing a white shirt", 1],
            ["what is on the man's head", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What color is the man's shirt", 2],
            ["What sports is man doing", 1],
            ["what gender is the person in the shirt", -1],
            ["what is the person holding", 1],
            ["what is the man doing", 1],
            ["what color is the shirt", 2],
            ["when was the photo taken", -1],
            ["who is wearing a white shirt", 1],
            ["what is on the man's head", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a man riding a skateboard on a sidewalk.",
            "a group of kids standing on a tennis court."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2411176",
                "VG_object_id": "315784",
                "bbox": [28, 253, 100, 306],
                "image": "data\\images\\2411176.jpg"
            },
            {
                "VG_image_id": "2361729",
                "VG_object_id": "779963",
                "bbox": [309, 212, 464, 299],
                "image": "data\\images\\2361729.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 1],
            ["what is the box placed on", 1],
            ["what is beside the box", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the box", -1],
            ["how many draws are there", -1],
            ["how many people are in the background", -1],
            ["where is the picture taken", 1],
            ["what is the box made of", -1],
            ["when is this photo taken", -1],
            ["how many people are there", -1],
            ["what is the box placed on", 1],
            ["what is beside the box", 1],
            ["who is in the picture", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a train is traveling down the tracks in the countryside.",
            "a red suitcase with snow on it"
        ]
    },
    {
        "object_category": "basket",
        "images": [
            {
                "VG_image_id": "2389196",
                "VG_object_id": "1263179",
                "bbox": [33, 161, 454, 300],
                "image": "data\\images\\2389196.jpg"
            },
            {
                "VG_image_id": "2388573",
                "VG_object_id": "508582",
                "bbox": [62, 114, 176, 183],
                "image": "data\\images\\2388573.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the basket", 2],
            ["what color is the table the basket is placed on", 1],
            ["what type of food is shown", 1]
        ],
        "org_questions": [
            ["what color is the basket", 2],
            ["what color is the table the basket is placed on", 1],
            ["how many people are there", -1],
            ["where is the basket", -1],
            ["what is in the basket", -1],
            ["What is the background of image", -1],
            ["what is inside the basket", -1],
            ["what type of food is shown", 1],
            ["what is the food on", -1],
            ["where is the picture taken", -1],
            ["what is on the plate", -1]
        ],
        "context": [
            "a sandwich and onion rings are on a table.",
            "a breakfast of breakfast on a patio table."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2403953",
                "VG_object_id": "379370",
                "bbox": [220, 47, 347, 183],
                "image": "data\\images\\2403953.jpg"
            },
            {
                "VG_image_id": "2409740",
                "VG_object_id": "1707860",
                "bbox": [320, 39, 392, 219],
                "image": "data\\images\\2409740.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what are on the guy's feet", 2],
            ["what is the man wearing", 1],
            ["what is the person holding", 1],
            ["what is the guy doing", 1],
            ["what is the man riding on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is the man wearing", 1],
            ["how many people are in the picture", -1],
            ["what are on the guy's feet", 2],
            ["what is the person holding", 1],
            ["what is the guy doing", 1],
            ["when was this picture taken", -1],
            ["where is the man", -1],
            ["who is in the picture", -1],
            ["what is the man riding on", 1]
        ],
        "context": [
            "a person on a skateboard doing a trick in a skate park.",
            "two men riding bicycles down a street with surfboards."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2390857",
                "VG_object_id": "1246383",
                "bbox": [0, 440, 375, 498],
                "image": "data\\images\\2390857.jpg"
            },
            {
                "VG_image_id": "2358792",
                "VG_object_id": "3362250",
                "bbox": [1, 34, 498, 372],
                "image": "data\\images\\2358792.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the table", 1],
            ["where is the table", 1],
            ["what is on the table", 1],
            ["how many plates are there on the table", 1],
            ["what color is the food on the plate", 1],
            ["what shape is the table", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is the table made of ", -1],
            ["how many people are there in the picture", 2],
            ["where is the table", 1],
            ["what is on the table", 1],
            ["how many plates are there on the table", 1],
            ["what color is the food on the plate", 1],
            ["how many tables are there", -1],
            ["what shape is the table", 1],
            ["what kind of table is this", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a bride and groom cutting a cake",
            "a table with a pizza, a tray of food and a container of orange juice."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2380371",
                "VG_object_id": "1347482",
                "bbox": [335, 99, 437, 169],
                "image": "data\\images\\2380371.jpg"
            },
            {
                "VG_image_id": "2322527",
                "VG_object_id": "3383426",
                "bbox": [280, 275, 347, 336],
                "image": "data\\images\\2322527.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is person doing", 1],
            ["what is the man wearing on head", 1],
            ["What is in the background of image", 1],
            ["What is the background of image", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["what direction is the man looking in", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what color is the ground", -1],
            ["How many people are there", -1],
            ["What is person doing", 1],
            ["what is the man wearing on head", 1],
            ["What is in the background of image", 1],
            ["What is the background of image", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["what direction is the man looking in", 1],
            ["What is man wearing on his face", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man riding a skateboard down a hill.",
            "a train is reflected in a mirror on a train track."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2396261",
                "VG_object_id": "665087",
                "bbox": [167, 126, 331, 338],
                "image": "data\\images\\2396261.jpg"
            },
            {
                "VG_image_id": "2327096",
                "VG_object_id": "981130",
                "bbox": [1, 50, 331, 329],
                "image": "data\\images\\2327096.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 2],
            ["what is on the left of the car", 2]
        ],
        "org_questions": [
            ["what color is the car", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", -1],
            ["what is the weather like", -1],
            ["what is on the left of the car", 2],
            ["when was the picture taken", -1],
            ["what is in the background", -1],
            ["where was the photo taken", -1],
            ["what is on the ground", -1],
            ["what is in front of the car", -1]
        ],
        "context": [
            "a man is standing next to an elephant.",
            "a woman sitting on a bench talking on a cell phone."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2318121",
                "VG_object_id": "1013878",
                "bbox": [56, 122, 148, 199],
                "image": "data\\images\\2318121.jpg"
            },
            {
                "VG_image_id": "2336002",
                "VG_object_id": "2057579",
                "bbox": [2, 259, 203, 331],
                "image": "data\\images\\2336002.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is this photo taken", 1],
            ["where is the table", 1],
            ["what is next to the table", 1],
            ["what is on the table", 1],
            ["how many people are there", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["how many chairs are there beside the table", -1],
            ["where is this photo taken", 1],
            ["what is the table made of", -1],
            ["where is the table", 1],
            ["what is next to the table", 1],
            ["what is on the table", 1],
            ["how many people are there", 1],
            ["what material is the table made of", -1],
            ["how many chairs are there in the picture", -1],
            ["how many plates are there", -1],
            ["how many people are there in the picture", 1],
            ["what is the table color", -1]
        ],
        "context": [
            "a bed with a book shelf and a bookcase.",
            "an older woman and a boy playing a video game."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2395539",
                "VG_object_id": "3823005",
                "bbox": [9, 159, 395, 372],
                "image": "data\\images\\2395539.jpg"
            },
            {
                "VG_image_id": "2319306",
                "VG_object_id": "3442971",
                "bbox": [1, 173, 261, 447],
                "image": "data\\images\\2319306.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["What is man doing", 1],
            ["where is the person", 1],
            ["what is the man doing", 1],
            ["where is the picture taken", 1],
            ["how many faces are there in the picture", 1],
            ["What is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["how many people are there", 1],
            ["What is man doing", 1],
            ["where is the person", 1],
            ["what is the man wearing", -1],
            ["what is the man doing", 1],
            ["what is on the man's head", -1],
            ["what color is the jacket", -1],
            ["what is the color of the shirt", -1],
            ["what color is the shirt of the man in the foreground", -1],
            ["what is the persion wearing", -1],
            ["what color is the shirt", -1],
            ["where is the picture taken", 1],
            ["how many faces are there in the picture", 1],
            ["What is the man doing", 1]
        ],
        "context": [
            "a man sitting at a table with a cup of coffee.",
            "two men sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2393701",
                "VG_object_id": "667546",
                "bbox": [250, 60, 363, 132],
                "image": "data\\images\\2393701.jpg"
            },
            {
                "VG_image_id": "2403127",
                "VG_object_id": "657526",
                "bbox": [123, 152, 180, 210],
                "image": "data\\images\\2403127.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the clock", 1],
            ["what is the clock made of", 1],
            ["what time does the clock say", 1],
            ["what time is it", 1],
            ["what time is on the clock", 1]
        ],
        "org_questions": [
            ["what color is the clock", 1],
            ["what is the clock made of", 1],
            ["how many clocks are there", -1],
            ["when is this picture taken", -1],
            ["what is the shape of the clock", -1],
            ["where is the clock", -1],
            ["what is on the clock", -1],
            ["what time does the clock say", 1],
            ["what time is it", 1],
            ["what time is on the clock", 1],
            ["how many clocks", -1],
            ["what shape is the clock", -1]
        ],
        "context": [
            "a clock on the side of a building with a sky background",
            "a colorful clock tower in a parking lot."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2402312",
                "VG_object_id": "390706",
                "bbox": [145, 90, 351, 289],
                "image": "data\\images\\2402312.jpg"
            },
            {
                "VG_image_id": "2335833",
                "VG_object_id": "961847",
                "bbox": [5, 172, 493, 359],
                "image": "data\\images\\2335833.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of food is it", 1],
            ["how many people are there", 1],
            ["what color is the table", 1],
            ["what shape is plate the food placed on", 1],
            ["what is the food on", 1],
            ["What is the background of image", 1],
            ["what shape is the plate", 1],
            ["what is beside the food", 1],
            ["who is in the picture", 1],
            ["what is on top of the food", 1]
        ],
        "org_questions": [
            ["what kind of food is it", 1],
            ["how many people are there", 1],
            ["what color is the table", 1],
            ["what shape is plate the food placed on", 1],
            ["what is the food on", 1],
            ["What is the background of image", 1],
            ["what shape is the plate", 1],
            ["what is beside the food", 1],
            ["where was the photo taken", -1],
            ["who is in the picture", 1],
            ["where is the food", -1],
            ["what is on top of the food", 1]
        ],
        "context": [
            "a bowl of food on a table next to a menu.",
            "a woman making sandwiches in a glass case."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2405584",
                "VG_object_id": "330729",
                "bbox": [223, 83, 394, 191],
                "image": "data\\images\\2405584.jpg"
            },
            {
                "VG_image_id": "2415366",
                "VG_object_id": "3287893",
                "bbox": [46, 15, 323, 186],
                "image": "data\\images\\2415366.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what color is the table the screen placed on", 1]
        ],
        "org_questions": [
            ["what is on the screen", -1],
            ["what color is the table", 2],
            ["how many screens are there", -1],
            ["where is the screen", -1],
            ["what color is the table the screen placed on", 1],
            ["what is the main color of the computer", -1],
            ["What color is the screen", -1],
            ["what color is the keyboard", -1],
            ["who is in the photo", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a wooden desk.",
            "a computer monitor with snowflakes on it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2369025",
                "VG_object_id": "744275",
                "bbox": [27, 34, 280, 343],
                "image": "data\\images\\2369025.jpg"
            },
            {
                "VG_image_id": "2343697",
                "VG_object_id": "2050890",
                "bbox": [12, 92, 294, 479],
                "image": "data\\images\\2343697.jpg"
            }
        ],
        "questions_with_scores": [
            ["what gesture is the man", 2],
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["who is wearing a bag", -1],
            ["where is the man", -1],
            ["what gesture is the man", 2],
            ["when was the photo taken", -1],
            ["what is the person holding", 1],
            ["where was this photo taken", -1],
            ["who is in the photo", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "two men playing frisbee on the beach.",
            "a man holding a surfboard on the beach."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2324963",
                "VG_object_id": "3502007",
                "bbox": [269, 174, 467, 305],
                "image": "data\\images\\2324963.jpg"
            },
            {
                "VG_image_id": "2318688",
                "VG_object_id": "1008112",
                "bbox": [24, 190, 359, 413],
                "image": "data\\images\\2318688.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the horses", 2],
            ["what color is the carriage", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color are the horses", 2],
            ["what color is the ground", -1],
            ["what color is the carriage", 1],
            ["how many horses are there", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", 1],
            ["what is the horse standing on", -1],
            ["what is on the horse's head", -1],
            ["what are the horses doing", -1],
            ["who is in the picture", -1],
            ["what is behind the horses", -1]
        ],
        "context": [
            "a man driving a red carriage with two horses.",
            "two black horses pulling a carriage with feathers on them."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2360238",
                "VG_object_id": "1718602",
                "bbox": [2, 70, 499, 371],
                "image": "data\\images\\2360238.jpg"
            },
            {
                "VG_image_id": "2325993",
                "VG_object_id": "2900842",
                "bbox": [2, 90, 499, 374],
                "image": "data\\images\\2325993.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the plate", 2],
            ["what shape is the plate", 2],
            ["What color is the table", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["What color is the plate", 2],
            ["how many people are there", -1],
            ["What food is on the plate", -1],
            ["what is the table made of", -1],
            ["what is beside the plate", -1],
            ["how many forks are there on the plate", -1],
            ["what shape is the plate", 2],
            ["where was the photo taken", -1],
            ["what is the food on", -1],
            ["where is the plate", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a plate with a sandwich and chips on it.",
            "a plate of donuts with fruit on it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2331294",
                "VG_object_id": "3261367",
                "bbox": [72, 2, 257, 332],
                "image": "data\\images\\2331294.jpg"
            },
            {
                "VG_image_id": "2348487",
                "VG_object_id": "3605255",
                "bbox": [200, 1, 346, 331],
                "image": "data\\images\\2348487.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["how many people are there in the picture", 2],
            ["what is the ground covered with", 1],
            ["What is man doing", 1],
            ["what is the man doing", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", -1],
            ["what is the man holding", 2],
            ["how many people are there in the picture", 2],
            ["what is the ground covered with", 1],
            ["What is man doing", 1],
            ["how many bikes are there", -1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is on the man's head", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man holding a bottle of wine and a woman holding a bottle.",
            "a man standing on a picnic table eating a sandwich."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2327909",
                "VG_object_id": "3331657",
                "bbox": [178, 220, 218, 275],
                "image": "data\\images\\2327909.jpg"
            },
            {
                "VG_image_id": "2354557",
                "VG_object_id": "837539",
                "bbox": [218, 193, 276, 298],
                "image": "data\\images\\2354557.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plant", 2],
            ["what color is the ground", 2],
            ["where is the plant", 1],
            ["what is the land made of ", 1],
            ["what is the plant sitting on", 1],
            ["how is the weather", 1],
            ["what is in the background of the picture", 1]
        ],
        "org_questions": [
            ["what color is the plant", 2],
            ["where is the plant", 1],
            ["what color is the ground", 2],
            ["how many people are there", -1],
            ["what is the land made of ", 1],
            ["what is the plant sitting on", 1],
            ["when was the picture taken", -1],
            ["how is the weather", 1],
            ["what is in the background of the picture", 1],
            ["what is in the distance", -1]
        ],
        "context": [
            "three people sitting on a motorcycle in front of a building.",
            "two people cross country skiing in the snow."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2339687",
                "VG_object_id": "2336205",
                "bbox": [118, 75, 277, 320],
                "image": "data\\images\\2339687.jpg"
            },
            {
                "VG_image_id": "2381120",
                "VG_object_id": "707215",
                "bbox": [230, 108, 314, 248],
                "image": "data\\images\\2381120.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of sport is the player doing", 2],
            ["what color is the player's clothes", 2],
            ["how many players are there in the picture", 1],
            ["what is in the background", 1],
            ["what is the player holding", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what kind of sport is the player doing", 2],
            ["what color is the player's clothes", 2],
            ["how many players are there in the picture", 1],
            ["what is in the background", 1],
            ["what is the player wearing", -1],
            ["what is the player holding", 1],
            ["what is the player doing", -1],
            ["who is in the photo", -1],
            ["what sport is being played", 1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a boy in a green jacket catching a soccer ball.",
            "a group of men standing on top of a field."
        ]
    },
    {
        "object_category": "toilet",
        "images": [
            {
                "VG_image_id": "2403076",
                "VG_object_id": "383895",
                "bbox": [46, 196, 202, 483],
                "image": "data\\images\\2403076.jpg"
            },
            {
                "VG_image_id": "2342507",
                "VG_object_id": "3206442",
                "bbox": [6, 41, 374, 463],
                "image": "data\\images\\2342507.jpg"
            }
        ],
        "questions_with_scores": [["what is the color of the floor", 1]],
        "org_questions": [
            ["what is the color of the floor", 1],
            ["what is on the floor", -1],
            ["what is on the wall", -1],
            ["where is the photo taken", -1],
            ["what is the floor made of", -1],
            ["what is the toilet on", -1],
            ["what is beside the toilet", -1],
            ["how many toilets are there", -1],
            ["what room is this", -1],
            ["where was this picture taken", -1],
            ["what is behind the toilet", -1]
        ],
        "context": [
            "a white toilet sitting in a bathroom next to a picture.",
            "a toilet with a dirty seat and a trash can in a bathroom."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2403997",
                "VG_object_id": "347275",
                "bbox": [24, 12, 479, 383],
                "image": "data\\images\\2403997.jpg"
            },
            {
                "VG_image_id": "2329280",
                "VG_object_id": "974676",
                "bbox": [39, 0, 499, 197],
                "image": "data\\images\\2329280.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 2],
            ["what is beside the car", 1],
            ["what is the pattern on the car", 1],
            ["how many cars are there", 1]
        ],
        "org_questions": [
            ["what color is the car", 2],
            ["what is beside the car", 1],
            ["what is the pattern on the car", 1],
            ["how many cars are there", 1],
            ["what time is it", -1],
            ["which part of the car can we see in the picture", -1],
            ["what is on the side of the car", -1],
            ["where was the picture taken", -1],
            ["what type of car is this", -1],
            ["when was the picture taken", -1],
            ["where is the car car", -1]
        ],
        "context": [
            "a truck with a skull and crossbones painted on it.",
            "a teddy bear laying on the ground next to a taxi."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2346824",
                "VG_object_id": "1823303",
                "bbox": [218, 14, 467, 397],
                "image": "data\\images\\2346824.jpg"
            },
            {
                "VG_image_id": "2374768",
                "VG_object_id": "2369332",
                "bbox": [261, 213, 343, 310],
                "image": "data\\images\\2374768.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man wearing on the head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man wearing on the head", 1],
            ["how many people are there", -1],
            ["what is the man doing", -1],
            ["What is the background of image", -1],
            ["what is the player holding", -1],
            ["what is the player wearing", -1],
            ["how many players are there in the photo", -1],
            ["who is playing tennis", -1],
            ["what sport is being played", -1],
            ["where is the man", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man swinging a tennis racket on a tennis court.",
            "a man playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2390601",
                "VG_object_id": "1248742",
                "bbox": [91, 7, 406, 227],
                "image": "data\\images\\2390601.jpg"
            },
            {
                "VG_image_id": "2322805",
                "VG_object_id": "3464516",
                "bbox": [47, 226, 305, 452],
                "image": "data\\images\\2322805.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many cakes are there", 1],
            ["How many plates are there", 1]
        ],
        "org_questions": [
            ["How many cakes are there", 1],
            ["What color is the plate", -1],
            ["How many plates are there", 1],
            ["where is the cake", -1],
            ["what color is the table under the cake", -1],
            ["how many people are there in the picture", -1],
            ["what is on top of the cake", -1],
            ["what is the cake made of", -1],
            ["what is the cake sitting on", -1],
            ["what kind of cake is this", -1]
        ],
        "context": [
            "a slice of chocolate cake on a plate",
            "a chocolate cake with chocolate frosting and flowers."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2406621",
                "VG_object_id": "292925",
                "bbox": [241, 212, 303, 327],
                "image": "data\\images\\2406621.jpg"
            },
            {
                "VG_image_id": "2381917",
                "VG_object_id": "700809",
                "bbox": [121, 238, 232, 348],
                "image": "data\\images\\2381917.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the person doing", 2],
            ["where is the man", 2],
            ["what color is the trouser", 1],
            ["what is the ground covered with", 1],
            ["what is the main color of the pants", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 1],
            ["what sport is the person doing", 2],
            ["what is the gender of the person wearing the trousers", -1],
            ["how many horses are there in the picture", -1],
            ["where is the man", 2],
            ["what is the ground covered with", 1],
            ["when was the photo taken", -1],
            ["how many people are in the photo", -1],
            ["what is the main color of the pants", 1]
        ],
        "context": [
            "a person playing tennis on a tennis court.",
            "a young boy wearing a red shirt and gray pants."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2336823",
                "VG_object_id": "2429243",
                "bbox": [213, 213, 330, 356],
                "image": "data\\images\\2336823.jpg"
            },
            {
                "VG_image_id": "2319172",
                "VG_object_id": "1004125",
                "bbox": [153, 297, 252, 374],
                "image": "data\\images\\2319172.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's trouser", 2],
            ["What color is man's shirt", 2],
            ["Where is man ", 1],
            ["when is this picture taken", 1],
            ["what type of pants is the man wearing", 1],
            ["what color are the pants", 1],
            ["what color is the shirt", 1]
        ],
        "org_questions": [
            ["What color is man's trouser", 2],
            ["How many people are there", -1],
            ["What color is man's shirt", 2],
            ["Where is man ", 1],
            ["when is this picture taken", 1],
            ["what type of pants is the man wearing", 1],
            ["what is the man wearing", -1],
            ["what color are the pants", 1],
            ["what color is the shirt", 1]
        ],
        "context": [
            "a woman riding a skateboard down a street.",
            "two men playing a video game in a room."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2369634",
                "VG_object_id": "2317681",
                "bbox": [125, 8, 363, 331],
                "image": "data\\images\\2369634.jpg"
            },
            {
                "VG_image_id": "2360524",
                "VG_object_id": "3760778",
                "bbox": [274, 110, 362, 287],
                "image": "data\\images\\2360524.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's clothes", 1],
            ["what is the gesture of the child", 1],
            ["where is the child", 1],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1],
            ["what is the child on", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the child's clothes", 1],
            ["what is the gesture of the child", 1],
            ["where is the child", 1],
            ["what is the persion wearing on his head", -1],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1],
            ["what is the child on", 1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a little girl sitting in a cart",
            "a man holding a surfboard on the beach."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2334622",
                "VG_object_id": "3188833",
                "bbox": [42, 30, 467, 330],
                "image": "data\\images\\2334622.jpg"
            },
            {
                "VG_image_id": "2331077",
                "VG_object_id": "2811117",
                "bbox": [59, 75, 237, 462],
                "image": "data\\images\\2331077.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is the girl doing", 1],
            ["what is the persion holding", 1],
            ["what color is the ground", 1],
            ["What is child doing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["how many people are in the picture", 2],
            ["what is the girl doing", 1],
            ["what is the persion holding", 1],
            ["where is the picture taken", -1],
            ["what color is the ground", 1],
            ["What is child doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a young boy and girl sitting on the grass",
            "a little girl standing on a sidewalk holding a stuffed animal."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2356661",
                "VG_object_id": "1741780",
                "bbox": [142, 90, 375, 233],
                "image": "data\\images\\2356661.jpg"
            },
            {
                "VG_image_id": "2346339",
                "VG_object_id": "900378",
                "bbox": [13, 2, 499, 281],
                "image": "data\\images\\2346339.jpg"
            }
        ],
        "questions_with_scores": [
            ["when was the picture taken", 2],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the color of the train", -1],
            ["how many trains are there", -1],
            ["where is the photo taken", -1],
            ["what is on the side of the train", -1],
            ["what is the land made of", -1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what kind of vehicle is this", -1],
            ["when was the picture taken", 2],
            ["what is the train on", -1]
        ],
        "context": [
            "a woman sitting on a bench in a train station.",
            "a train that is sitting on the tracks."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2409182",
                "VG_object_id": "247232",
                "bbox": [0, 0, 499, 499],
                "image": "data\\images\\2409182.jpg"
            },
            {
                "VG_image_id": "2334758",
                "VG_object_id": "2598952",
                "bbox": [0, 2, 493, 373],
                "image": "data\\images\\2334758.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["what is the color of the floor", 1],
            ["how many sinks are there", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", -1],
            ["what is on the table", 1],
            ["what is the color of the floor", 1],
            ["how many people are there", -1],
            ["how many sinks are there", 1],
            ["what is on the wall", -1],
            ["What color is the wall", -1],
            ["where was this taken", -1],
            ["what room is this", -1],
            ["who is in the kitchen", -1],
            ["what is the floor made of", -1],
            ["what color is the counter", -1],
            ["what is the color of the table", -1],
            ["what is on the counter", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a kitchen with a wooden table and chairs.",
            "a kitchen with a table and chairs and a table"
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2338731",
                "VG_object_id": "2954737",
                "bbox": [278, 19, 382, 156],
                "image": "data\\images\\2338731.jpg"
            },
            {
                "VG_image_id": "2368845",
                "VG_object_id": "615686",
                "bbox": [41, 9, 195, 243],
                "image": "data\\images\\2368845.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["what gender is the person who holds the racket", 1],
            ["how many women are there in the picture", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what color is the woman's shirt", -1],
            ["what gender is the person who holds the racket", 1],
            ["how many women are there in the picture", 1],
            ["where is the woman", -1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["what is on the player's head", -1],
            ["what gender is the player", -1]
        ],
        "context": [
            "a man holding a tennis racket in his right hand.",
            "two pictures of a woman playing tennis."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2322816",
                "VG_object_id": "3202662",
                "bbox": [156, 172, 499, 329],
                "image": "data\\images\\2322816.jpg"
            },
            {
                "VG_image_id": "2364803",
                "VG_object_id": "1802497",
                "bbox": [336, 223, 498, 332],
                "image": "data\\images\\2364803.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sofa ", 1],
            ["How many people are there", 1],
            ["What is in front of sofa", 1],
            ["what is next to the couch", 1]
        ],
        "org_questions": [
            ["what color is the sofa ", 1],
            ["what is on the sofa", -1],
            ["How many people are there", 1],
            ["what is the floor under the sofa made of", -1],
            ["What is in front of sofa", 1],
            ["how many cats are there on the sofa", -1],
            ["what color is the rug under the sofa", -1],
            ["what room is this", -1],
            ["where is the picture taken", -1],
            ["what is next to the couch", 1],
            ["what is the couch made of", -1]
        ],
        "context": [
            "a living room with a couch, coffee table, television and a couch.",
            "a living room with a painting on the wall"
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2347307",
                "VG_object_id": "3612036",
                "bbox": [51, 98, 268, 366],
                "image": "data\\images\\2347307.jpg"
            },
            {
                "VG_image_id": "2369026",
                "VG_object_id": "2975566",
                "bbox": [94, 41, 273, 483],
                "image": "data\\images\\2369026.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the woman wearing on her head", 1],
            ["where was this photo taken", 1],
            ["what is the woman wearing on her head", 1]
        ],
        "org_questions": [
            ["what color is the woman's hair", -1],
            ["what is the woman holding", -1],
            ["where is the person", -1],
            ["What is the woman wearing on her head", 1],
            ["What is the woman doing", -1],
            ["what is the woman wearing", -1],
            ["how many people are shown", -1],
            ["who is in the photo", -1],
            ["where was this photo taken", 1],
            ["what is in the woman's hand", -1],
            ["what is the woman wearing on her head", 1],
            ["How many people are there", -1],
            ["Where is the woman", -1],
            ["what is the woman doing", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a bride and groom standing next to a wedding cake.",
            "a bride and groom cutting a wedding cake."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2403037",
                "VG_object_id": "1128566",
                "bbox": [286, 91, 387, 169],
                "image": "data\\images\\2403037.jpg"
            },
            {
                "VG_image_id": "2351246",
                "VG_object_id": "862185",
                "bbox": [37, 147, 163, 333],
                "image": "data\\images\\2351246.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["How many people are there", 1],
            ["what shape is the shirt's collar", 1],
            ["What are people doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["What time is it", -1],
            ["what is the person holding", 1],
            ["how many children are there in the picture", -1],
            ["when was the photo taken", -1],
            ["How many people are there", 1],
            ["what shape is the shirt's collar", 1],
            ["What are people doing", 1],
            ["what color is the person's shirt", -1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a woman sitting in a wheel chair next to two dogs.",
            "three women holding bananas in their hands."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2399254",
                "VG_object_id": "417875",
                "bbox": [1, 259, 497, 373],
                "image": "data\\images\\2399254.jpg"
            },
            {
                "VG_image_id": "2404236",
                "VG_object_id": "345052",
                "bbox": [67, 257, 499, 319],
                "image": "data\\images\\2404236.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is on the ground", 1],
            ["what is the ground covered with", 1],
            ["how many trees are there on the land", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the ground", 2],
            ["what is the weather like", -1],
            ["what is on the ground", 1],
            ["How many people are there", -1],
            ["what is the ground covered with", 1],
            ["how many trees are there on the land", 1],
            ["where is the picture taken", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "two cats sitting on a bench in a park.",
            "a jeep is parked on a snowy hill."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2400659",
                "VG_object_id": "406870",
                "bbox": [133, 45, 235, 402],
                "image": "data\\images\\2400659.jpg"
            },
            {
                "VG_image_id": "2344423",
                "VG_object_id": "915700",
                "bbox": [229, 123, 420, 343],
                "image": "data\\images\\2344423.jpg"
            }
        ],
        "questions_with_scores": [["how many giraffe are there", 1]],
        "org_questions": [
            ["what is the giraffe standing on", -1],
            ["how many giraffe are there", 1],
            ["WHat is giraffe doing", -1],
            ["what is on the side of the giraffe", -1],
            ["what is in the background", -1],
            ["what is in front of the giraffe", -1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["where are the giraffes", -1],
            ["what is the giraffe eating", -1],
            ["what is on the giraffe's head", -1]
        ],
        "context": [
            "a giraffe standing in a wooden enclosure.",
            "a group of giraffes and zebras in a field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "713197",
                "VG_object_id": "1909968",
                "bbox": [161, 441, 592, 737],
                "image": "data\\images\\713197.jpg"
            },
            {
                "VG_image_id": "2380079",
                "VG_object_id": "711369",
                "bbox": [109, 219, 296, 403],
                "image": "data\\images\\2380079.jpg"
            }
        ],
        "questions_with_scores": [
            ["what gender is the person in the shirt", 2],
            ["What is the gender of person", 2],
            ["what color is the shirt", 1],
            ["how many people are there", 1],
            ["What is the background of image", 1],
            ["who is in the photo", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what gender is the person in the shirt", 2],
            ["how many people are there", 1],
            ["how long is the sleeves of the shirt", -1],
            ["What is the background of image", 1],
            ["What is the gender of person", 2],
            ["who is in the photo", 1],
            ["where is the picture taken", 1],
            ["where is the person", -1]
        ],
        "context": [
            "a man sitting at a desk eating a piece of food.",
            "a woman and a girl are cooking in a kitchen."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2410863",
                "VG_object_id": "361892",
                "bbox": [176, 280, 325, 349],
                "image": "data\\images\\2410863.jpg"
            },
            {
                "VG_image_id": "2354590",
                "VG_object_id": "837338",
                "bbox": [118, 87, 340, 393],
                "image": "data\\images\\2354590.jpg"
            }
        ],
        "questions_with_scores": [
            ["what main color is the cake", 1],
            ["what color is the table the cake is placed on", 1],
            ["what shape is the plate", 1]
        ],
        "org_questions": [
            ["what main color is the cake", 1],
            ["what color is the table the cake is placed on", 1],
            ["how many people are there", -1],
            ["what shape is the plate", 1],
            ["What is on the cake", -1],
            ["how many candles are there on the cake", -1],
            ["where is the cake", -1],
            ["what is the cake made of", -1],
            ["what kind of cake is this", -1]
        ],
        "context": [
            "a little girl sitting on a couch with a cake.",
            "a cake with a train on it"
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "713188",
                "VG_object_id": "1580138",
                "bbox": [276, 208, 404, 380],
                "image": "data\\images\\713188.jpg"
            },
            {
                "VG_image_id": "2361381",
                "VG_object_id": "782400",
                "bbox": [125, 111, 344, 299],
                "image": "data\\images\\2361381.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["where is the woman", 1],
            ["what is the gesture of the woman", 1],
            ["how many people are there", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding on her hand", 1],
            ["what is the girl wearing", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 1],
            ["where is the woman", 1],
            ["what is the gesture of the woman", 1],
            ["how many people are there", 1],
            ["what color is the woman's shirt", -1],
            ["what is the woman doing", 1],
            ["what is the woman holding on her hand", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the girl wearing", 1]
        ],
        "context": [
            "a woman and her dog on a surfboard.",
            "a woman sitting in the snow with a snowboard."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2368923",
                "VG_object_id": "1667922",
                "bbox": [12, 171, 374, 497],
                "image": "data\\images\\2368923.jpg"
            },
            {
                "VG_image_id": "2417012",
                "VG_object_id": "2818145",
                "bbox": [0, 23, 373, 425],
                "image": "data\\images\\2417012.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 1],
            ["what color is the table", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["what color is the plate", 1],
            ["what is on the plate", -1],
            ["how many plates are there", -1],
            ["where is the plate", -1],
            ["what is in the background", -1],
            ["what food is on the plate", -1],
            ["what color is the table", 1],
            ["what shape is the plate", -1],
            ["what is the plate made of", -1],
            ["what is the food on", -1],
            ["what is under the plate", -1],
            ["what is next to the plate", 1]
        ],
        "context": [
            "a plate with a pepperoni pizza and a fork.",
            "a pizza on a plate with a fork and knife."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2375918",
                "VG_object_id": "578090",
                "bbox": [0, 274, 499, 374],
                "image": "data\\images\\2375918.jpg"
            },
            {
                "VG_image_id": "2399898",
                "VG_object_id": "661255",
                "bbox": [98, 276, 288, 329],
                "image": "data\\images\\2399898.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["what color is the ground", 1],
            ["what is the land made of", 1]
        ],
        "org_questions": [
            ["what is on the ground", 1],
            ["what color is the ground", 1],
            ["when is this picture taken", -1],
            ["how many people are there in the picture", -1],
            ["where is the picture taken", -1],
            ["what is the land made of", 1],
            ["how is the weather", -1],
            ["what is in the background", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a herd of elephants walking across a dirt field.",
            "a horse standing next to a horse in a field."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2359066",
                "VG_object_id": "1839027",
                "bbox": [80, 49, 302, 434],
                "image": "data\\images\\2359066.jpg"
            },
            {
                "VG_image_id": "2315548",
                "VG_object_id": "2871133",
                "bbox": [49, 2, 316, 202],
                "image": "data\\images\\2315548.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's pants", 1],
            ["what color is the ground", 1],
            ["what is the man doing", 1],
            ["what is the person wearing", 1],
            ["what is the persion riding", 1],
            ["where is the boy", 1]
        ],
        "org_questions": [
            ["what color is the man's pants", 1],
            ["what color is the ground", 1],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the person wearing", 1],
            ["what is the persion riding", 1],
            ["where is the boy", 1],
            ["who is in the photo", -1],
            ["what type of shoes is the man wearing", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a man hitting a tennis ball with a racquet.",
            "a person wearing green and white shoes and a skateboard"
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2347494",
                "VG_object_id": "1785683",
                "bbox": [205, 104, 343, 328],
                "image": "data\\images\\2347494.jpg"
            },
            {
                "VG_image_id": "2329340",
                "VG_object_id": "3167499",
                "bbox": [292, 176, 349, 313],
                "image": "data\\images\\2329340.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the woman's skirt", 1],
            ["what are the women doing", 1],
            ["Where is the person", 1],
            ["what is the woman wearing", 1],
            ["What is woman doing", 1],
            ["what color is the background", 1],
            ["what kind of shoes is the girl wearing", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what color is the woman's skirt", 1],
            ["what are the women doing", 1],
            ["Where is the person", 1],
            ["what is the woman holding", -1],
            ["what is the woman wearing", 1],
            ["What is woman doing", 1],
            ["what color is the background", 1],
            ["what gender is the person in the picture", -1],
            ["when was this photo taken", -1],
            ["what sport is being played", -1],
            ["what kind of shoes is the girl wearing", 1]
        ],
        "context": [
            "a group of women standing on a street.",
            "a couple of people playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2318432",
                "VG_object_id": "1010822",
                "bbox": [44, 226, 304, 458],
                "image": "data\\images\\2318432.jpg"
            },
            {
                "VG_image_id": "2387989",
                "VG_object_id": "676103",
                "bbox": [25, 291, 105, 374],
                "image": "data\\images\\2387989.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall behind the table", 2],
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["where is the lamp", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what color is the wall behind the table", 2],
            ["what is on the table", 1],
            ["what is the table made of", -1],
            ["What is the shape of table", -1],
            ["where is the lamp", 1]
        ],
        "context": [
            "a pug dog sitting on a nightstand next to a bed.",
            "a bedroom with a large painting on the wall."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2349381",
                "VG_object_id": "2548970",
                "bbox": [2, 20, 499, 311],
                "image": "data\\images\\2349381.jpg"
            },
            {
                "VG_image_id": "2380218",
                "VG_object_id": "546916",
                "bbox": [0, 202, 498, 332],
                "image": "data\\images\\2380218.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are people doing on the field", 1],
            ["how many women are there in the picture", 1],
            ["what is in the background", 1],
            ["what is the man doing", 1],
            ["what color are the trousers of the person on the right", 1]
        ],
        "org_questions": [
            ["what are people doing on the field", 1],
            ["how many women are there in the picture", 1],
            ["what is in the background", 1],
            ["what is the color of the grass", -1],
            ["what is the man doing", 1],
            ["what color are the trousers of the person on the right", 1],
            ["what color is the ground", -1],
            ["where was this photo taken", -1],
            ["what is the weather like", -1],
            ["where is the grass", -1],
            ["what are the people standing on", -1]
        ],
        "context": [
            "a group of people playing a game of soccer.",
            "a young boy throwing a frisbee in a park."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2319302",
                "VG_object_id": "1003084",
                "bbox": [72, 245, 258, 334],
                "image": "data\\images\\2319302.jpg"
            },
            {
                "VG_image_id": "2412371",
                "VG_object_id": "3122290",
                "bbox": [219, 124, 381, 208],
                "image": "data\\images\\2412371.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plate is there", 2],
            ["what color is the table", 1],
            ["what color is the table the plate on", 1],
            ["how many kinds of food are there on the plate", 1],
            ["what kind of food is on the table", 1]
        ],
        "org_questions": [
            ["what is on the table", -1],
            ["what color is the table", 1],
            ["how many plate is there", 2],
            ["what is the plate made of", -1],
            ["what is the table made of", -1],
            ["what is the main color of the plate", -1],
            ["what color is the table the plate on", 1],
            ["how many kinds of food are there on the plate", 1],
            ["where is the food", -1],
            ["what kind of food is on the table", 1],
            ["what is next to the plate", -1]
        ],
        "context": [
            "a breakfast in the balcony",
            "a table with many plates of food on it"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2347112",
                "VG_object_id": "2289937",
                "bbox": [7, 355, 360, 489],
                "image": "data\\images\\2347112.jpg"
            },
            {
                "VG_image_id": "2382339",
                "VG_object_id": "1329432",
                "bbox": [3, 342, 500, 499],
                "image": "data\\images\\2382339.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["how many keyboards are there on the table", -1],
            ["where is the table", -1],
            ["how many people are there in the picture", -1],
            ["what is the table made out of", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a vase filled with flowers on top of a table.",
            "three glasses of wine sit on a table."
        ]
    },
    {
        "object_category": "basket",
        "images": [
            {
                "VG_image_id": "2371688",
                "VG_object_id": "2287422",
                "bbox": [435, 165, 487, 217],
                "image": "data\\images\\2371688.jpg"
            },
            {
                "VG_image_id": "2325863",
                "VG_object_id": "984641",
                "bbox": [0, 145, 193, 268],
                "image": "data\\images\\2325863.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the basket", 1],
            ["what is inside the basket", 1],
            ["what is in the background", 1],
            ["what is in front of the fence", 1]
        ],
        "org_questions": [
            ["where is the basket", 1],
            ["what is inside the basket", 1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what shape is the basket", -1],
            ["how many baskets are there", -1],
            ["when was the picture taken", -1],
            ["what is in front of the fence", 1],
            ["what color are the leaves", -1]
        ],
        "context": [
            "a group of people standing on a tennis court.",
            "three giraffes standing next to a tree in a zoo."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2338494",
                "VG_object_id": "2774953",
                "bbox": [1, 259, 498, 330],
                "image": "data\\images\\2338494.jpg"
            },
            {
                "VG_image_id": "2328043",
                "VG_object_id": "3370896",
                "bbox": [4, 158, 454, 384],
                "image": "data\\images\\2328043.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what kinds of animals are there", 2],
            ["what color is the ground", 1],
            ["what is in the distance", 1],
            ["what is on the ground", 1],
            ["what animal is standing on the ground", 1],
            ["what is the ground covered with", 1],
            ["WHat animal are there", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is in the distance", 1],
            ["what is on the ground", 1],
            ["how many people are there", 2],
            ["what animal is standing on the ground", 1],
            ["what is the ground covered with", 1],
            ["WHat animal are there", 1],
            ["where was the photo taken", 1],
            ["what is the weather like", -1],
            ["what kinds of animals are there", 2]
        ],
        "context": [
            "a person riding a horse in a corral.",
            "a chicken standing on a wooden fence next to a body of water."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2318039",
                "VG_object_id": "2849081",
                "bbox": [31, 35, 496, 331],
                "image": "data\\images\\2318039.jpg"
            },
            {
                "VG_image_id": "2360009",
                "VG_object_id": "1906304",
                "bbox": [196, 167, 413, 318],
                "image": "data\\images\\2360009.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there", 2],
            ["what color is the background", 1],
            ["how many trees are there in the picture", 1]
        ],
        "org_questions": [
            ["how many zebras are there", 2],
            ["what color is the background", 1],
            ["how many trees are there in the picture", 1],
            ["What is zebra doing", -1],
            ["what is the ground covered with", -1],
            ["what is in the distance", -1],
            ["what color is the grass", -1],
            ["when was the photo taken", -1],
            ["what kind of animal is in the picture", -1],
            ["where are the zebras", -1],
            ["what is the zebra standing on", -1]
        ],
        "context": [
            "a zebra standing in the grass.",
            "a couple of zebras standing in a field."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2403695",
                "VG_object_id": "350573",
                "bbox": [217, 149, 499, 355],
                "image": "data\\images\\2403695.jpg"
            },
            {
                "VG_image_id": "2356811",
                "VG_object_id": "2446198",
                "bbox": [279, 86, 387, 348],
                "image": "data\\images\\2356811.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the photo taken", 2],
            ["what color is the dog", 1],
            ["what is the gesture of the dog", 1],
            ["what is the dog doing", 1],
            ["what is the dog looking at", 1],
            ["what is the dog in", 1]
        ],
        "org_questions": [
            ["what color is the dog", 1],
            ["what is the gesture of the dog", 1],
            ["what color is the ground the dog standing on", -1],
            ["how many dogs are there", -1],
            ["Where is the photo taken", 2],
            ["what is the dog doing", 1],
            ["what is the dog wearing", -1],
            ["what is the dog looking at", 1],
            ["what is on the dog's head", -1],
            ["where is the dog looking", -1],
            ["what is the dog in", 1]
        ],
        "context": [
            "a dog laying on the ground with a suitcase.",
            "a woman playing frisbee with a dog."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2388838",
                "VG_object_id": "670296",
                "bbox": [217, 154, 313, 300],
                "image": "data\\images\\2388838.jpg"
            },
            {
                "VG_image_id": "2353463",
                "VG_object_id": "1659935",
                "bbox": [43, 249, 318, 331],
                "image": "data\\images\\2353463.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many people are there", 2],
            ["what is on the table", 1],
            ["how many people are there in the picture", 1],
            ["what is on top of the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what is on the table", 1],
            ["how many people are there", 2],
            ["what is the table made of", -1],
            ["how many glasses are there on the table", -1],
            ["How many tables are there", -1],
            ["how many people are there in the picture", 1],
            ["where was this photo taken", -1],
            ["where is the table", -1],
            ["what is on top of the table", 1]
        ],
        "context": [
            "a group of people sitting at a table",
            "a living room filled with lots of books."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2392332",
                "VG_object_id": "1231335",
                "bbox": [29, 56, 151, 367],
                "image": "data\\images\\2392332.jpg"
            },
            {
                "VG_image_id": "2317308",
                "VG_object_id": "2891087",
                "bbox": [182, 102, 245, 289],
                "image": "data\\images\\2317308.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the man's hat", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the man's hat", 1],
            ["how many people are there", -1],
            ["what sport is the man playing", -1],
            ["where is the man", -1],
            ["what is the ground covered with", -1],
            ["how is the weather", -1],
            ["what is on the person's head", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the man holding", -1],
            ["what are the people doing", -1]
        ],
        "context": [
            "two people on skis on a snowy slope",
            "two people on skis in the snow."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2388782",
                "VG_object_id": "670750",
                "bbox": [47, 389, 308, 500],
                "image": "data\\images\\2388782.jpg"
            },
            {
                "VG_image_id": "2372381",
                "VG_object_id": "2106603",
                "bbox": [128, 0, 497, 281],
                "image": "data\\images\\2372381.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the board", 1],
            ["what is the background", 1],
            ["what kind of food is on the table", 1]
        ],
        "org_questions": [
            ["what color is the board", 1],
            ["what is on the board", -1],
            ["how many people are there in the picture", 2],
            ["where is the board", -1],
            ["what is the background", 1],
            ["what is the table made of", -1],
            ["what kind of food is on the table", 1],
            ["what is the food on", -1]
        ],
        "context": [
            "a little girl sitting at a table with a pizza.",
            "a pizza with cheese and tomato sauce on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2410754",
                "VG_object_id": "321491",
                "bbox": [163, 71, 307, 373],
                "image": "data\\images\\2410754.jpg"
            },
            {
                "VG_image_id": "2332323",
                "VG_object_id": "2857127",
                "bbox": [328, 185, 403, 326],
                "image": "data\\images\\2332323.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["where is the woman", 1],
            ["how many people are there in the photo", 1],
            ["What is woman doing", 1],
            ["what is the woman wearing", 1],
            ["what gesture is the woman", 1],
            ["what is the woman holding", 1],
            ["who is in the picture", 1],
            ["what are the people doing", 1],
            ["what is the woman doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["where is the woman", 1],
            ["how many people are there in the photo", 1],
            ["What is woman doing", 1],
            ["what is the woman wearing", 1],
            ["what gesture is the woman", 1],
            ["what is the woman holding", 1],
            ["what is the woman's posture", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", 1],
            ["what are the people doing", 1],
            ["what is the woman doing", 1]
        ],
        "context": [
            "a woman holding a kite in a park.",
            "a boy riding a bike in the water."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2320154",
                "VG_object_id": "995706",
                "bbox": [11, 346, 333, 469],
                "image": "data\\images\\2320154.jpg"
            },
            {
                "VG_image_id": "2334516",
                "VG_object_id": "2317193",
                "bbox": [1, 260, 498, 372],
                "image": "data\\images\\2334516.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sky", 2],
            ["what is on the street", 1],
            ["What is next to the street", 1],
            ["how many people are there on the street", 1]
        ],
        "org_questions": [
            ["what color is the street", -1],
            ["what is on the street", 1],
            ["how many cars are there on the street", -1],
            ["What is next to the street", 1],
            ["how many people are there on the street", 1],
            ["where was this photo taken", -1],
            ["what is the road made of", -1],
            ["when was the photo taken", -1],
            ["what color is the sky", 2]
        ],
        "context": [
            "a tall clock tower in the middle of a city.",
            "a street with a traffic light and trees."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2343203",
                "VG_object_id": "926312",
                "bbox": [1, 206, 366, 499],
                "image": "data\\images\\2343203.jpg"
            },
            {
                "VG_image_id": "2377743",
                "VG_object_id": "1897299",
                "bbox": [0, 285, 499, 372],
                "image": "data\\images\\2377743.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the table", 2],
            ["how many people are there in the picture", 1],
            ["how many plates are there on the table", 1]
        ],
        "org_questions": [
            ["What color is the table", -1],
            ["What is on the table", -1],
            ["how many people are there in the picture", 1],
            ["what shape is the table", 2],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["how many plates are there on the table", 1],
            ["where was the picture taken", -1],
            ["what is under the table", -1],
            ["what is the table color", -1],
            ["what are the plates made of", -1],
            ["where are the plates", -1]
        ],
        "context": [
            "a table with plates and cups of coffee and a vase of flowers.",
            "a group of young girls eating pizza at a table."
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2330700",
                "VG_object_id": "3120472",
                "bbox": [70, 208, 235, 326],
                "image": "data\\images\\2330700.jpg"
            },
            {
                "VG_image_id": "2351077",
                "VG_object_id": "2260460",
                "bbox": [88, 118, 298, 223],
                "image": "data\\images\\2351077.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the tray", 2],
            ["what shape is the tray", 1]
        ],
        "org_questions": [
            ["What color is the tray", 2],
            ["What is tray on", -1],
            ["how many trays are there in the picture", -1],
            ["what shape is the tray", 1],
            ["What food is on the plate", -1],
            ["what is on the tray", -1],
            ["where is the food", -1],
            ["what is next to the pizza", -1]
        ],
        "context": [
            "a young boy holding a plate of pizza.",
            "a table with a plate of food and a bowl of salad."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2406037",
                "VG_object_id": "1104329",
                "bbox": [293, 53, 375, 234],
                "image": "data\\images\\2406037.jpg"
            },
            {
                "VG_image_id": "2416290",
                "VG_object_id": "3418459",
                "bbox": [26, 52, 189, 322],
                "image": "data\\images\\2416290.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["How many people are there", 1],
            ["What color is person's shirt", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man holding", -1],
            ["where is the man", 1],
            ["How many people are there", 1],
            ["What color is person's shirt", 1],
            ["what is on the person's head", -1],
            ["Where are people", -1],
            ["where is the person", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a man holding two glasses of ice cream",
            "a man making a pizza in a kitchen."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2375128",
                "VG_object_id": "2219953",
                "bbox": [2, 2, 496, 192],
                "image": "data\\images\\2375128.jpg"
            },
            {
                "VG_image_id": "107992",
                "VG_object_id": "1073796",
                "bbox": [2, 181, 628, 303],
                "image": "data\\images\\107992.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of animal is on the ground", 2],
            ["how many horses are there on the ground", 2],
            ["how many horses are there in the picture", 2],
            ["what color is the sky", 1]
        ],
        "org_questions": [
            ["what color is the sky", 1],
            ["what kind of animal is on the ground", 2],
            ["how many horses are there on the ground", 2],
            ["what time is it", -1],
            ["what is the ground covered with", -1],
            ["what is in the distance", -1],
            ["where was the photo taken", -1],
            ["how is the weather", -1],
            ["where are the trees", -1],
            ["when was the picture taken", -1],
            ["how many horses are there in the picture", 2]
        ],
        "context": [
            "two sheep grazing in a field of grass.",
            "two horses standing in a field with a mountain in the background."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2404050",
                "VG_object_id": "1119230",
                "bbox": [81, 6, 413, 310],
                "image": "data\\images\\2404050.jpg"
            },
            {
                "VG_image_id": "2318139",
                "VG_object_id": "1013721",
                "bbox": [93, 229, 214, 500],
                "image": "data\\images\\2318139.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what is the girl sitting on", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", -1],
            ["what is the girl doing", -1],
            ["what color is the girl's hair", -1],
            ["Where is the girl", -1],
            ["what is the persion holding", -1],
            ["how many people are there", 1],
            ["what is the gender of the person in the photo", -1],
            ["who is in the photo", -1],
            ["what is the girl wearing", -1],
            ["what is the girl sitting on", 1],
            ["what is the girl wearing on the head", -1],
            ["what is the ground covered with", 1],
            ["what is the little girl holding", -1]
        ],
        "context": [
            "a little girl playing with a computer keyboard.",
            "a group of people sitting at a table eating food."
        ]
    },
    {
        "object_category": "ocean",
        "images": [
            {
                "VG_image_id": "2393538",
                "VG_object_id": "471781",
                "bbox": [1, 2, 499, 280],
                "image": "data\\images\\2393538.jpg"
            },
            {
                "VG_image_id": "2404563",
                "VG_object_id": "340993",
                "bbox": [2, 5, 499, 326],
                "image": "data\\images\\2404563.jpg"
            }
        ],
        "questions_with_scores": [["WHat color is the board", 1]],
        "org_questions": [
            ["WHat color is the board", 1],
            ["how many people are there", -1],
            ["how many bird are there on the beach", -1],
            ["where was this picture taken", -1],
            ["who is in the picture", -1],
            ["how is the water", -1],
            ["what is in the water", -1],
            ["where is the water", -1]
        ],
        "context": [
            "a woman holding a surfboard in the ocean.",
            "a man riding a surfboard on top of a wave."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2380596",
                "VG_object_id": "1344866",
                "bbox": [44, 68, 459, 370],
                "image": "data\\images\\2380596.jpg"
            },
            {
                "VG_image_id": "2401074",
                "VG_object_id": "1151909",
                "bbox": [1, 206, 345, 374],
                "image": "data\\images\\2401074.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the bed besides pillows", 1],
            ["how many pillows are there on the bed", 1]
        ],
        "org_questions": [
            ["what color is the bed", -1],
            ["what color is the floor under the bed", -1],
            ["what is the floor under the bed made of", -1],
            ["how many people are there", -1],
            ["what is on the bed besides pillows", 1],
            ["how many pillows are there on the bed", 1],
            ["what is on the bed", -1],
            ["where was the picture taken", -1],
            ["what room is this", -1],
            ["where is the pillow", -1],
            ["what is next to the bed", -1]
        ],
        "context": [
            "a hospital bed with a white sheet on it.",
            "a hotel room with two beds and a picture on the wall."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2363849",
                "VG_object_id": "2653302",
                "bbox": [13, 117, 281, 427],
                "image": "data\\images\\2363849.jpg"
            },
            {
                "VG_image_id": "2340999",
                "VG_object_id": "945084",
                "bbox": [286, 48, 378, 311],
                "image": "data\\images\\2340999.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the man doing", 2],
            ["how many children are there in the picture", 2],
            ["What is the next to  the man", 2],
            ["What is the object near the man", 1],
            ["what color are the boy's trousers", 1],
            ["What is man wearing on his head", 1],
            ["what is the person on", 1],
            ["where is the man standing", 1]
        ],
        "org_questions": [
            ["What is the man doing", 2],
            ["What is the object near the man", 1],
            ["how many children are there in the picture", 2],
            ["what color are the boy's trousers", 1],
            ["What is man wearing on his head", 1],
            ["what is the boy wearing", -1],
            ["what is the person on", 1],
            ["when was the picture taken", -1],
            ["where was the photo taken", -1],
            ["who is in the photo", -1],
            ["where is the man standing", 1],
            ["What is the next to  the man", 2]
        ],
        "context": [
            "two boys are playing baseball on a sidewalk.",
            "a man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2380732",
                "VG_object_id": "1343074",
                "bbox": [123, 175, 496, 328],
                "image": "data\\images\\2380732.jpg"
            },
            {
                "VG_image_id": "2416742",
                "VG_object_id": "2795400",
                "bbox": [0, 207, 480, 340],
                "image": "data\\images\\2416742.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there in the street", 2],
            ["how many people are there in the street", 1]
        ],
        "org_questions": [
            ["what color is the street", -1],
            ["how many cars are there in the street", 2],
            ["how many people are there in the street", 1],
            ["what time is it", -1],
            ["what is the weather like", -1],
            ["When is the photo taken", -1],
            ["where was this picture taken", -1],
            ["what is the road made of", -1],
            ["where is the car", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a bus driving down a street next to a car.",
            "a dog is walking past a police car."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2399796",
                "VG_object_id": "1166192",
                "bbox": [262, 73, 386, 360],
                "image": "data\\images\\2399796.jpg"
            },
            {
                "VG_image_id": "2346804",
                "VG_object_id": "896384",
                "bbox": [122, 58, 360, 321],
                "image": "data\\images\\2346804.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 2],
            ["where is the man", 2],
            ["How many people are there", 1],
            ["What is the man holding", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what is the man wearing", 2],
            ["where is the man", 2],
            ["How many people are there", 1],
            ["What is the man holding", 1],
            ["What is the man wearing on his head", -1],
            ["when was the photo taken", -1],
            ["what is the man standing on", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a group of men standing around a machine.",
            "a man in a jacket talking on a cell phone."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2322280",
                "VG_object_id": "3259194",
                "bbox": [196, 115, 373, 412],
                "image": "data\\images\\2322280.jpg"
            },
            {
                "VG_image_id": "2380566",
                "VG_object_id": "710059",
                "bbox": [317, 108, 445, 280],
                "image": "data\\images\\2380566.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's trousers", 2],
            ["what sport is the man playing", 2],
            ["where is the man", 1],
            ["what is the ground covered with", 1],
            ["what is the man holding", 1],
            ["WHat is man doing", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's trousers", 2],
            ["how many people are there", -1],
            ["what sport is the man playing", 2],
            ["where is the man", 1],
            ["what is the ground covered with", 1],
            ["how is the weather", -1],
            ["what is the man holding", 1],
            ["WHat is man doing", 1],
            ["when was the photo taken", -1],
            ["what kind of shoes is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a man swinging a tennis racket at a ball",
            "a man is kicking a ball on a basketball court."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2388198",
                "VG_object_id": "674882",
                "bbox": [86, 69, 457, 317],
                "image": "data\\images\\2388198.jpg"
            },
            {
                "VG_image_id": "2353280",
                "VG_object_id": "847609",
                "bbox": [1, 0, 500, 305],
                "image": "data\\images\\2353280.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cats are there in the picture", 2],
            ["how many cats are there", 1],
            ["what color are the cats", 1],
            ["what color is the background", 1],
            ["where is the cat", 1],
            ["what is the cat doing", 1],
            ["what is the cat lying on", 1],
            ["what is the main color of the cat", 1]
        ],
        "org_questions": [
            ["how many cats are there", 1],
            ["what color are the cats", 1],
            ["what color is the background", 1],
            ["where is the cat", 1],
            ["what is the cat doing", 1],
            ["what is the cat lying on", 1],
            ["what is the main color of the cat", 1],
            ["when was the photo taken", -1],
            ["what animal is in the picture", -1],
            ["who is in the photo", -1],
            ["what is on the cat", -1],
            ["how many cats are there in the picture", 2]
        ],
        "context": [
            "two cats sleeping on a bench in the grass.",
            "a cat laying inside of a black bag."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2333006",
                "VG_object_id": "3156943",
                "bbox": [21, 171, 402, 330],
                "image": "data\\images\\2333006.jpg"
            },
            {
                "VG_image_id": "2360248",
                "VG_object_id": "2368903",
                "bbox": [63, 240, 411, 399],
                "image": "data\\images\\2360248.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 2],
            ["what color is the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what color is the wall", 2],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the pattern of the floor", -1],
            ["what is the floor made of", -1],
            ["what room is this", -1],
            ["what shape is the floor", -1],
            ["what kind of floor is this", -1],
            ["what is covering the floor", -1],
            ["where is the toilet", -1]
        ],
        "context": [
            "a toilet in a bathroom with a green wall.",
            "a bathroom with a toilet and a roll of toilet paper."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2376700",
                "VG_object_id": "570406",
                "bbox": [210, 36, 498, 319],
                "image": "data\\images\\2376700.jpg"
            },
            {
                "VG_image_id": "2316459",
                "VG_object_id": "1053722",
                "bbox": [330, 294, 426, 404],
                "image": "data\\images\\2316459.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["how many persons are there in the picture", 2],
            ["what is the gesture of the man", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what is the gesture of the man", 1],
            ["what is the man doing", 2],
            ["how many persons are there in the picture", 2],
            ["what gender is the person in the shirt", -1],
            ["what is the person wearing on neck", -1],
            ["when was this picture taken", -1],
            ["who is in the picture", -1],
            ["what is the persion wearing", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man is doing a trick on a skateboard.",
            "a man sitting on the beach next to a yellow umbrella."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2405725",
                "VG_object_id": "372336",
                "bbox": [264, 218, 374, 371],
                "image": "data\\images\\2405725.jpg"
            },
            {
                "VG_image_id": "2392061",
                "VG_object_id": "1234372",
                "bbox": [171, 133, 314, 327],
                "image": "data\\images\\2392061.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is in the background", 1],
            ["how many people are in the picture", -1],
            ["what is the man doing", -1],
            ["where is the photo taken", -1],
            ["how many shirts are there", -1],
            ["what is the main color of the shirt", -1],
            ["what is on the man's head", -1],
            ["who is in the photo", -1],
            ["what is the player holding", -1],
            ["what is the player wearing", -1],
            ["what sport is the man playing", -1],
            ["what is in the distance", 1],
            ["where is the person", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man holding a tennis racquet on a tennis court.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2330572",
                "VG_object_id": "2820824",
                "bbox": [7, 25, 220, 332],
                "image": "data\\images\\2330572.jpg"
            },
            {
                "VG_image_id": "2354465",
                "VG_object_id": "1951359",
                "bbox": [271, 167, 332, 262],
                "image": "data\\images\\2354465.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["What is man doing", 1],
            ["what is the man riding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["how is the weather", -1],
            ["what is the man wearing around his neck", -1],
            ["What is man doing", 1],
            ["what is the man riding", 1],
            ["who is wearing a blue shirt", -1],
            ["when was the photo taken", -1],
            ["what is on the man's head", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man and two children on a boat.",
            "two people riding a horse drawn carriage down a street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2354997",
                "VG_object_id": "2171645",
                "bbox": [157, 49, 263, 292],
                "image": "data\\images\\2354997.jpg"
            },
            {
                "VG_image_id": "2402432",
                "VG_object_id": "389587",
                "bbox": [115, 54, 283, 345],
                "image": "data\\images\\2402432.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is person doing", 2],
            ["who is in the photo", 2],
            ["What is gender of this person", 2],
            ["Wha tis gender of this person", 1],
            ["what is the persion wearing", 1],
            ["what is the gender of the person on the right", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["Wha tis gender of this person", 1],
            ["What is person doing", 2],
            ["what is the persion wearing on his head", -1],
            ["what is the persion wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 2],
            ["what is the gender of the person on the right", 1],
            ["What is gender of this person", 2]
        ],
        "context": [
            "a woman is standing in a bathroom with a man in a purple suit.",
            "a man riding a scooter on a street."
        ]
    },
    {
        "object_category": "banana",
        "images": [
            {
                "VG_image_id": "2317664",
                "VG_object_id": "1018660",
                "bbox": [155, 125, 255, 189],
                "image": "data\\images\\2317664.jpg"
            },
            {
                "VG_image_id": "2357277",
                "VG_object_id": "813304",
                "bbox": [197, 220, 286, 268],
                "image": "data\\images\\2357277.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bananas", 1],
            ["how many bananas are there in the picture", 1]
        ],
        "org_questions": [
            ["where is the bananas", 1],
            ["what is beside the bananas", -1],
            ["how many people are there", -1],
            ["What fruits are there", -1],
            ["how many bananas are there in the picture", 1],
            ["what is the yellow yellow fruit", -1],
            ["what is yellow", -1]
        ],
        "context": [
            "a bowl of apples and bananas on a table.",
            "a display of peppers and peppers at a market."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2342084",
                "VG_object_id": "2700908",
                "bbox": [65, 43, 169, 120],
                "image": "data\\images\\2342084.jpg"
            },
            {
                "VG_image_id": "2377906",
                "VG_object_id": "715887",
                "bbox": [167, 144, 269, 256],
                "image": "data\\images\\2377906.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what color is the background", 1],
            ["What is man doing", 1],
            ["what is on the man's head", 1],
            ["what kind of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what color is the background", 1],
            ["How many people are there", -1],
            ["What is man doing", 1],
            ["what is the gender of the person", -1],
            ["when was the photo taken", -1],
            ["what is on the man's head", 1],
            ["what kind of shirt is the man wearing", 1],
            ["where is the person", -1]
        ],
        "context": [
            "a small child feeding a cat food from a bowl.",
            "a man walking with a skateboard in his hand."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2403262",
                "VG_object_id": "1126480",
                "bbox": [105, 10, 217, 281],
                "image": "data\\images\\2403262.jpg"
            },
            {
                "VG_image_id": "2417311",
                "VG_object_id": "2900566",
                "bbox": [178, 100, 485, 372],
                "image": "data\\images\\2417311.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what sport is it", 2],
            ["what is the man standing on", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what sport is it", 2],
            ["how many people are there", -1],
            ["what is the floor made of", -1],
            ["what animal is the man with", -1],
            ["where is the man", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", 1],
            ["what is the man doing", 1],
            ["what is the boy holding", -1]
        ],
        "context": [
            "a young man riding a skateboard down a street.",
            "a man throwing a frisbee in a park."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2351998",
                "VG_object_id": "1738474",
                "bbox": [127, 77, 407, 257],
                "image": "data\\images\\2351998.jpg"
            },
            {
                "VG_image_id": "2380671",
                "VG_object_id": "1343829",
                "bbox": [20, 255, 370, 488],
                "image": "data\\images\\2380671.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the shape of the pizza", 1],
            ["What color is the table", 1]
        ],
        "org_questions": [
            ["What is the shape of the pizza", 1],
            ["What color is the table", 1],
            ["How many plates are there", -1],
            ["what is the plate sitting on", -1],
            ["what is the table made of", -1],
            ["where is the plate", -1],
            ["what is on the plate", -1],
            ["what is next to the pizza", -1],
            ["what is the pizza on", -1],
            ["what is the color of the plate", -1],
            ["what shape is the plate", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a pizza on a pan on a table",
            "a slice of pizza on a plate next to a beer."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2318915",
                "VG_object_id": "1006104",
                "bbox": [238, 59, 397, 169],
                "image": "data\\images\\2318915.jpg"
            },
            {
                "VG_image_id": "2415256",
                "VG_object_id": "143085",
                "bbox": [271, 96, 325, 330],
                "image": "data\\images\\2415256.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 1],
            ["what color is the wall", 1],
            ["how many curtains are there", 1],
            ["what room is it", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 1],
            ["what color is the wall", 1],
            ["how many curtains are there", 1],
            ["when is this picture taken", -1],
            ["what is in the background", -1],
            ["where is the curtain", -1],
            ["what is the pattern on the curtain", -1],
            ["what room is it", 1],
            ["what is hanging on the wall", -1],
            ["what is covering the window", -1],
            ["when was the photo taken", -1],
            ["what is on the window", -1]
        ],
        "context": [
            "a living room with a couch, coffee table and a fireplace.",
            "a bed in a wooden frame in a room."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2321685",
                "VG_object_id": "3583467",
                "bbox": [245, 151, 345, 280],
                "image": "data\\images\\2321685.jpg"
            },
            {
                "VG_image_id": "2346786",
                "VG_object_id": "896547",
                "bbox": [129, 239, 185, 325],
                "image": "data\\images\\2346786.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the skateboard", 2],
            ["what color of the man's shirt", 2],
            ["how many skateborads are there", 2],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what is the color of the skateboard", 2],
            ["what color of the man's shirt", 2],
            ["How many bicycles are there", -1],
            ["how many people are there in the picture", 1],
            ["when was the photo taken", -1],
            ["what is the boy doing", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", -1],
            ["where was the photo taken", -1],
            ["how many skateborads are there", 2]
        ],
        "context": [
            "a man riding a skateboard on a ramp.",
            "a man riding a skateboard up the side of a ramp."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2356419",
                "VG_object_id": "3772302",
                "bbox": [97, 204, 234, 374],
                "image": "data\\images\\2356419.jpg"
            },
            {
                "VG_image_id": "2372335",
                "VG_object_id": "2105087",
                "bbox": [236, 211, 451, 332],
                "image": "data\\images\\2372335.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 2],
            ["what color is the person's shirt", 1],
            ["what is the gender of the person", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what is the gender of the person", 1],
            ["what is the person holding", 2],
            ["How many people are there", -1],
            ["what is the persion wearing", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a young boy sitting at a table with a donut.",
            "a woman sitting at a table with a wine glass."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2412776",
                "VG_object_id": "187088",
                "bbox": [2, 1, 499, 349],
                "image": "data\\images\\2412776.jpg"
            },
            {
                "VG_image_id": "2364254",
                "VG_object_id": "3740059",
                "bbox": [0, 1, 497, 370],
                "image": "data\\images\\2364254.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is on the toilet", 2],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["What is on the toilet", 2],
            ["What is the texture of wall", -1],
            ["how many sinks are in the bathroom", -1],
            ["what color is the floor", -1],
            ["what color is floor", -1],
            ["who is in the photo", 1],
            ["what room is this", -1],
            ["where was this picture taken", -1],
            ["what is the wall made of", -1],
            ["what is covering the wall", -1]
        ],
        "context": [
            "a woman sitting on a toilet in a bathroom.",
            "a bathroom stall with a toilet and graffiti on the wall."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2368973",
                "VG_object_id": "2081694",
                "bbox": [95, 110, 189, 144],
                "image": "data\\images\\2368973.jpg"
            },
            {
                "VG_image_id": "2415157",
                "VG_object_id": "144837",
                "bbox": [112, 47, 372, 202],
                "image": "data\\images\\2415157.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there under the umbrella", 2],
            ["what color is the umbrella", 1],
            ["when is this picture taken", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 1],
            ["how many people are there under the umbrella", 2],
            ["when is this picture taken", 1],
            ["what pattern is on the umbrella", -1],
            ["what is the person doing", -1],
            ["what is in the background", -1],
            ["where is the umbrella", -1],
            ["how is the weather", -1],
            ["what is the umbrella made of", -1],
            ["who is holding the umbrella", -1],
            ["what are the people holding", -1]
        ],
        "context": [
            "a woman walking down a street holding an umbrella.",
            "two women walking under an umbrella in the rain."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2350291",
                "VG_object_id": "2624041",
                "bbox": [5, 73, 94, 349],
                "image": "data\\images\\2350291.jpg"
            },
            {
                "VG_image_id": "2352948",
                "VG_object_id": "1903023",
                "bbox": [51, 37, 341, 324],
                "image": "data\\images\\2352948.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the boy doing", 2],
            ["where is the boy", 1],
            ["how many children are there in the picture", 1],
            ["how many people are shown", 1]
        ],
        "org_questions": [
            ["where is the boy", 1],
            ["how many children are there in the picture", 1],
            ["what is the boy wearing", -1],
            ["what is the boy doing", 2],
            ["who is in the photo", -1],
            ["how many people are shown", 1]
        ],
        "context": [
            "a man sitting in a chair holding a little girl.",
            "a man riding a skateboard down the side of a ramp."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2364837",
                "VG_object_id": "2116697",
                "bbox": [414, 122, 499, 347],
                "image": "data\\images\\2364837.jpg"
            },
            {
                "VG_image_id": "2341434",
                "VG_object_id": "2673057",
                "bbox": [163, 132, 220, 332],
                "image": "data\\images\\2341434.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the man's pant", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the man's pant", 1],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["where is the person", -1],
            ["what is in front of the person", -1],
            ["what is the man holding", -1],
            ["how many persons are there", -1],
            ["when was this photo taken", -1],
            ["what are the people wearing", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man walking down a street with a newspaper dispenser.",
            "a group of people standing around an elephant."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2348470",
                "VG_object_id": "881847",
                "bbox": [2, 352, 333, 499],
                "image": "data\\images\\2348470.jpg"
            },
            {
                "VG_image_id": "2329937",
                "VG_object_id": "2783343",
                "bbox": [225, 275, 409, 372],
                "image": "data\\images\\2329937.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the floor", 1],
            ["what is the color of the wall", 1]
        ],
        "org_questions": [
            ["what is the color of the floor", 1],
            ["what is on the floor", -1],
            ["what is the color of the wall", 1],
            ["How many people are there", -1],
            ["where is the floor", -1],
            ["what is the floor made of", -1],
            ["what room is this", -1],
            ["what shape is the floor", -1],
            ["what is the flooring", -1],
            ["what type of flooring is shown", -1]
        ],
        "context": [
            "a person taking a picture of a bathroom with a toilet.",
            "a bathroom with a toilet, sink, and shower."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2352811",
                "VG_object_id": "2659228",
                "bbox": [61, 71, 121, 156],
                "image": "data\\images\\2352811.jpg"
            },
            {
                "VG_image_id": "2405266",
                "VG_object_id": "654775",
                "bbox": [238, 307, 289, 374],
                "image": "data\\images\\2405266.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the towel", 1],
            ["How many towels are there", 1],
            ["how many towels are there", 1],
            ["what is behind the towel", 1]
        ],
        "org_questions": [
            ["what color is the towel", -1],
            ["where is the towel", 1],
            ["what room is the towel in", -1],
            ["How many towels are there", 1],
            ["what is hanging on the wall", -1],
            ["what room is it", -1],
            ["where is the picture taken", -1],
            ["how many towels are there", 1],
            ["what is behind the towel", 1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a shower stall with a glass door.",
            "a person taking a picture of a bathroom with a camera."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2382780",
                "VG_object_id": "1326180",
                "bbox": [4, 182, 202, 339],
                "image": "data\\images\\2382780.jpg"
            },
            {
                "VG_image_id": "2390470",
                "VG_object_id": "1249892",
                "bbox": [162, 274, 294, 329],
                "image": "data\\images\\2390470.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the bowl", 1],
            ["what is the bowl made of", 1],
            ["what color is the bowl", 1],
            ["How many plates are there", 1],
            ["what color is it", 1],
            ["what color is the food in the bowl", 1]
        ],
        "org_questions": [
            ["what is in the bowl", 1],
            ["what is the bowl made of", 1],
            ["what color is the bowl", 1],
            ["How many plates are there", 1],
            ["where is the bowl", -1],
            ["what is the bowl placed on", -1],
            ["what color is it", 1],
            ["what color is the food in the bowl", 1],
            ["what is on the table", -1],
            ["what color is the table", -1],
            ["what color is the plate", -1],
            ["what is on the plate", -1]
        ],
        "context": [
            "a table with plates of food and bowls of food.",
            "a group of people standing around a table filled with food."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2393258",
                "VG_object_id": "1221574",
                "bbox": [318, 369, 402, 468],
                "image": "data\\images\\2393258.jpg"
            },
            {
                "VG_image_id": "2386608",
                "VG_object_id": "1284398",
                "bbox": [100, 14, 373, 190],
                "image": "data\\images\\2386608.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the ground covered with", 1],
            ["what is the bicycle leaning on", 1]
        ],
        "org_questions": [
            ["where is the bike", -1],
            ["what color is the ground", -1],
            ["how many bikes are there", -1],
            ["what is the ground covered with", 1],
            ["who is riding the bicycle", -1],
            ["what is the bicycle leaning on", 1],
            ["when was the picture taken", -1],
            ["what is the bike doing", -1],
            ["what is on the bike", -1],
            ["what is on the side of the bike", -1]
        ],
        "context": [
            "a yellow school bus parked on the side of the road",
            "a bike is parked next to a bench."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2322285",
                "VG_object_id": "3051106",
                "bbox": [281, 124, 417, 374],
                "image": "data\\images\\2322285.jpg"
            },
            {
                "VG_image_id": "2367211",
                "VG_object_id": "2311801",
                "bbox": [222, 31, 384, 373],
                "image": "data\\images\\2367211.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of lady's shirt", 2],
            ["what is the lady doing", 2],
            ["how many people are there", 1],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["what is in the woman's hand", 1]
        ],
        "org_questions": [
            ["what is the color of lady's shirt", 2],
            ["what is the lady doing", 2],
            ["how many people are there", 1],
            ["where is the woman", -1],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["how many ladies are there", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is on the woman's head", -1],
            ["what is in the woman's hand", 1]
        ],
        "context": [
            "a woman playing a video game in a living room.",
            "a woman in a gray dress eating an apple."
        ]
    },
    {
        "object_category": "bird",
        "images": [
            {
                "VG_image_id": "2362480",
                "VG_object_id": "2577546",
                "bbox": [0, 26, 447, 366],
                "image": "data\\images\\2362480.jpg"
            },
            {
                "VG_image_id": "2330500",
                "VG_object_id": "2753058",
                "bbox": [177, 97, 468, 195],
                "image": "data\\images\\2330500.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many birds are there", 2],
            ["what is the bird doing", 1],
            ["where is the bird looking", 1]
        ],
        "org_questions": [
            ["how many birds are there", 2],
            ["what is the bird doing", 1],
            ["when is this picture taken", -1],
            ["what is the bird on", -1],
            ["what kind of animal is this", -1],
            ["where was the photo taken", -1],
            ["where is the bird looking", 1],
            ["what kind of bird is this", -1],
            ["where are the birds", -1]
        ],
        "context": [
            "a couple of birds that are standing on the sand",
            "a bird drinking from a bird bath in the sun."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2382379",
                "VG_object_id": "1329033",
                "bbox": [1, 2, 499, 374],
                "image": "data\\images\\2382379.jpg"
            },
            {
                "VG_image_id": "2379889",
                "VG_object_id": "1353153",
                "bbox": [4, 156, 374, 368],
                "image": "data\\images\\2379889.jpg"
            }
        ],
        "questions_with_scores": [["how many forks are there on the table", 1]],
        "org_questions": [
            ["What is on the table", -1],
            ["What is on the plate", -1],
            ["how many forks are there on the table", 1],
            ["where is the table", -1],
            ["what food is on the plate", -1],
            ["what is the food sitting on", -1],
            ["where was the photo taken", -1],
            ["what type of food is shown", -1],
            ["what is in the bowl", -1]
        ],
        "context": [
            "a bowl of food with a spoon in it.",
            "a table with a sandwich, condiments, and a drink."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2345757",
                "VG_object_id": "3622310",
                "bbox": [134, 14, 439, 329],
                "image": "data\\images\\2345757.jpg"
            },
            {
                "VG_image_id": "2397600",
                "VG_object_id": "1188448",
                "bbox": [4, 28, 339, 306],
                "image": "data\\images\\2397600.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["where is the photo taken", 2],
            ["What is male child holding", 1],
            ["what is in the background", 1],
            ["Where is the man", 1],
            ["who is wearing glasses", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What is male child holding", 1],
            ["What color is child's shirt", -1],
            ["where is the photo taken", 2],
            ["what is the boy doing", -1],
            ["what is the boy wearing", -1],
            ["what is in the background", 1],
            ["Where is the man", 1],
            ["who is wearing glasses", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a little boy holding an apple and eating an apple.",
            "three young men standing together holding a beer."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2374055",
                "VG_object_id": "2396762",
                "bbox": [108, 106, 222, 296],
                "image": "data\\images\\2374055.jpg"
            },
            {
                "VG_image_id": "2411652",
                "VG_object_id": "308704",
                "bbox": [170, 26, 280, 314],
                "image": "data\\images\\2411652.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the clock on the tower", 1],
            ["What time is it", 1]
        ],
        "org_questions": [
            ["what is the main color of the tower", -1],
            ["what color is the clock on the tower", 1],
            ["what color is the sky", -1],
            ["What time is it", 1],
            ["how many towers are there", -1],
            ["where is the clock", -1],
            ["what is on the top of the tower", -1],
            ["what is above the clock", -1],
            ["what is on the building", -1],
            ["what is at the top of the tower", -1],
            ["what is on top of the clock", -1]
        ],
        "context": [
            "a clock tower on top of a building.",
            "a large clock tower with a clock on it's side."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2342084",
                "VG_object_id": "2700908",
                "bbox": [65, 43, 169, 120],
                "image": "data\\images\\2342084.jpg"
            },
            {
                "VG_image_id": "2342104",
                "VG_object_id": "937262",
                "bbox": [209, 71, 306, 202],
                "image": "data\\images\\2342104.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["how many people are there", 2],
            ["what is the person holding", 1],
            ["what kind of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", -1],
            ["what is the person holding", 1],
            ["what is the pattern of the boy's shirt", -1],
            ["how many people are there", 2],
            ["when was the photo taken", -1],
            ["what kind of shirt is the man wearing", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a small child feeding a cat food from a bowl.",
            "a man and a woman standing next to a motorcycle."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2367238",
                "VG_object_id": "2122260",
                "bbox": [121, 170, 209, 353],
                "image": "data\\images\\2367238.jpg"
            },
            {
                "VG_image_id": "2400843",
                "VG_object_id": "1154444",
                "bbox": [200, 0, 301, 133],
                "image": "data\\images\\2400843.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bottle", 1],
            ["what color is the table", 1],
            ["How many bottles are there", 1]
        ],
        "org_questions": [
            ["what color is the bottle", 1],
            ["what is the table made of", -1],
            ["what color is the table", 1],
            ["what shape is the glass beside the bottle", -1],
            ["How many bottles are there", 1],
            ["where is the bottle", -1],
            ["what is on the side of the bottle", -1],
            ["what is the bottle on", -1],
            ["where was the photo taken", -1],
            ["what is the bottle of", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a man and a woman sitting at a table with a child.",
            "a plate of food with a knife and fork on a table."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2338204",
                "VG_object_id": "2674122",
                "bbox": [5, 2, 337, 332],
                "image": "data\\images\\2338204.jpg"
            },
            {
                "VG_image_id": "2377138",
                "VG_object_id": "2092871",
                "bbox": [140, 44, 381, 374],
                "image": "data\\images\\2377138.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what shape is the man's collar", 1],
            ["how many men are there", 1],
            ["where is the photo taken", 1],
            ["what is the persion holding", 1],
            ["what is the persion on the left doing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what shape is the man's collar", 1],
            ["how many men are there", 1],
            ["what is the man wearing", -1],
            ["where is the photo taken", 1],
            ["who is in the picture", -1],
            ["what is the persion holding", 1],
            ["what is the persion on the left doing", 1]
        ],
        "context": [
            "a man and a woman laying on a bench.",
            "a man wearing a yellow shirt"
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2322992",
                "VG_object_id": "2894499",
                "bbox": [279, 121, 409, 289],
                "image": "data\\images\\2322992.jpg"
            },
            {
                "VG_image_id": "2408614",
                "VG_object_id": "1092486",
                "bbox": [369, 0, 498, 188],
                "image": "data\\images\\2408614.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's clothes", 1],
            ["what is the person holding", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["how many people are there in the picture", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the person's clothes", 1],
            ["what is the person holding", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["what time is it", -1],
            ["how many people are there in the picture", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a woman throwing a frisbee on a golf course.",
            "a group of children playing soccer in a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2375294",
                "VG_object_id": "1908951",
                "bbox": [91, 100, 371, 330],
                "image": "data\\images\\2375294.jpg"
            },
            {
                "VG_image_id": "2320772",
                "VG_object_id": "1049274",
                "bbox": [242, 19, 387, 149],
                "image": "data\\images\\2320772.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's hat", 2],
            ["how many people are there", 2],
            ["what is the man doing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what color is the man's hat", 2],
            ["what is the color of the man's shirt", -1],
            ["where is the man", -1],
            ["how many people are there", 2],
            ["who is wearing glasses", -1],
            ["when is this picture taken", -1],
            ["what is the man doing", 1],
            ["what is on the man's head", -1],
            ["what is behind the man", 1]
        ],
        "context": [
            "a young man wearing a baseball cap in a kitchen.",
            "a group of men preparing food in a restaurant."
        ]
    },
    {
        "object_category": "skier",
        "images": [
            {
                "VG_image_id": "2409783",
                "VG_object_id": "233613",
                "bbox": [232, 124, 352, 249],
                "image": "data\\images\\2409783.jpg"
            },
            {
                "VG_image_id": "2366493",
                "VG_object_id": "1827672",
                "bbox": [41, 122, 461, 325],
                "image": "data\\images\\2366493.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many skiers are there", 2],
            ["what is the gesture of the skier", 1]
        ],
        "org_questions": [
            ["how many skiers are there", 2],
            ["what is the gesture of the skier", 1],
            ["when is the picture taken", -1],
            ["where is the person", -1],
            ["what color are the man's clothes", -1],
            ["what time is it", -1],
            ["what kind of hat is the skier wearing", -1],
            ["who is in the picture", -1],
            ["what are the people doing", -1],
            ["what is on the ground", -1],
            ["what sport is this", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a skier is skiing down a snowy hill.",
            "a group of people standing on top of a snow covered slope."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2318879",
                "VG_object_id": "3270053",
                "bbox": [1, 66, 496, 373],
                "image": "data\\images\\2318879.jpg"
            },
            {
                "VG_image_id": "2352524",
                "VG_object_id": "3582989",
                "bbox": [2, 145, 498, 369],
                "image": "data\\images\\2352524.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are there in the picture", 2],
            ["how many bulls are there in the picture", 2],
            ["what kind of animal is there", 1],
            ["how many horses are there on the ground", 1],
            ["how many bulls are there on the ground", 1],
            ["what kind of animals are there in the background", 1]
        ],
        "org_questions": [
            ["what kind of animal is there", 1],
            ["how many horses are there on the ground", 1],
            ["how many bulls are there on the ground", 1],
            ["what are the animals doing", -1],
            ["what is in the distance", -1],
            ["what kind of animals are there in the background", 1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["where was this photo taken", -1],
            ["how many horses are there in the picture", 2],
            ["how many bulls are there in the picture", 2]
        ],
        "context": [
            "a herd of cattle standing on a dirt field.",
            "a group of horses standing in a field."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2341175",
                "VG_object_id": "3656689",
                "bbox": [285, 97, 435, 132],
                "image": "data\\images\\2341175.jpg"
            },
            {
                "VG_image_id": "2348059",
                "VG_object_id": "2855379",
                "bbox": [376, 213, 448, 320],
                "image": "data\\images\\2348059.jpg"
            }
        ],
        "questions_with_scores": [["how many pillows are there", 1]],
        "org_questions": [
            ["how many pillows are there", 1],
            ["what color are the pillows", -1],
            ["what shape are the pillows", -1],
            ["where is the pillow placed on", -1],
            ["what is the pillow placed on", -1],
            ["where is the pillow", -1],
            ["How many people are there in the couch", -1],
            ["what is next to the couch", -1],
            ["what is in the room", -1],
            ["what is in front of the couch", -1],
            ["where are the pillows", -1]
        ],
        "context": [
            "a living room with a couch, chair, table and mirror.",
            "a living room with a couch, chairs, and a television."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2368848",
                "VG_object_id": "3477392",
                "bbox": [22, 143, 192, 348],
                "image": "data\\images\\2368848.jpg"
            },
            {
                "VG_image_id": "2367595",
                "VG_object_id": "2531100",
                "bbox": [279, 207, 437, 271],
                "image": "data\\images\\2367595.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog's head", 1],
            ["how many dogs are there", 1],
            ["what is the woman doing", 1],
            ["where is the dog", 1],
            ["what is beside the dog", 1],
            ["what is the dog sitting on", 1]
        ],
        "org_questions": [
            ["what color is the dog's head", 1],
            ["how many dogs are there", 1],
            ["what is the woman doing", 1],
            ["where is the dog", 1],
            ["what is the dog wearing", -1],
            ["what is beside the dog", 1],
            ["who is in the photo", -1],
            ["what is the dog sitting on", 1],
            ["what is on the dog", -1]
        ],
        "context": [
            "a dog getting a drink from a water fountain.",
            "a woman sitting on a bench reading a book."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2396826",
                "VG_object_id": "1194731",
                "bbox": [134, 123, 202, 473],
                "image": "data\\images\\2396826.jpg"
            },
            {
                "VG_image_id": "2400929",
                "VG_object_id": "1153509",
                "bbox": [137, 66, 209, 129],
                "image": "data\\images\\2400929.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bottle", 1],
            ["where is the bottle", 1],
            ["how many bottles are there", 1],
            ["what is in the bottle", 1]
        ],
        "org_questions": [
            ["what color is the bottle", 1],
            ["where is the bottle", 1],
            ["how many bottles are there", 1],
            ["what is in the bottle", 1],
            ["where is the photo taken", -1]
        ],
        "context": [
            "three colorful vases sitting on a carpet.",
            "a person holding scissors over a stack of papers."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2383193",
                "VG_object_id": "1323061",
                "bbox": [1, 278, 500, 499],
                "image": "data\\images\\2383193.jpg"
            },
            {
                "VG_image_id": "2364398",
                "VG_object_id": "2736684",
                "bbox": [152, 12, 365, 332],
                "image": "data\\images\\2364398.jpg"
            }
        ],
        "questions_with_scores": [
            ["When is the picture taken", 1],
            ["What is in the middle of photo", 1],
            ["what time is it", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["When is the picture taken", 1],
            ["How many people are on the street", -1],
            ["what color is the ground", -1],
            ["What is in the middle of photo", 1],
            ["What is the weather like", -1],
            ["what time is it", 1],
            ["where was this photo taken", -1],
            ["what is on the ground", -1],
            ["what is on the side of the road", -1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "a truck is driving down the street in the city.",
            "a city street with a traffic light and a building."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2386196",
                "VG_object_id": "683303",
                "bbox": [334, 95, 471, 175],
                "image": "data\\images\\2386196.jpg"
            },
            {
                "VG_image_id": "2357079",
                "VG_object_id": "2387678",
                "bbox": [114, 26, 318, 155],
                "image": "data\\images\\2357079.jpg"
            }
        ],
        "questions_with_scores": [["what is in front of the screen", 1]],
        "org_questions": [
            ["what color is the screen", -1],
            ["what is in front of the screen", 1],
            ["how many screens are there", -1],
            ["What is on the laptop", -1],
            ["how many people are there", -1],
            ["what is the main color on the screen", -1],
            ["where is the laptop", -1],
            ["what is in the background", -1],
            ["what is the main color of the computer", -1]
        ],
        "context": [
            "a desk with three computers and a computer monitor.",
            "a black and white cat laying on top of a laptop."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2365648",
                "VG_object_id": "2575086",
                "bbox": [3, 39, 499, 373],
                "image": "data\\images\\2365648.jpg"
            },
            {
                "VG_image_id": "2412776",
                "VG_object_id": "3214420",
                "bbox": [192, 73, 333, 308],
                "image": "data\\images\\2412776.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding", 1],
            ["what is the woman's posture", 1],
            ["what is the woman wearing", 1]
        ],
        "org_questions": [
            ["how many people are in the picture", 2],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing on head", -1],
            ["what is the woman holding", 1],
            ["what is the woman's posture", 1],
            ["what is the woman wearing", 1],
            ["who is in the photo", -1],
            ["where was this picture taken", -1],
            ["where is the woman looking", -1]
        ],
        "context": [
            "three women standing next to each other holding wine glasses.",
            "a woman sitting on a toilet in a bathroom."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2417534",
                "VG_object_id": "3003389",
                "bbox": [150, 199, 277, 498],
                "image": "data\\images\\2417534.jpg"
            },
            {
                "VG_image_id": "2341628",
                "VG_object_id": "3184401",
                "bbox": [104, 79, 222, 244],
                "image": "data\\images\\2341628.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what sport is the woman playing", 2],
            ["what color is the woman's shirt", 1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["what is the woman doing", 1],
            ["what is above the woman", 1],
            ["what is the man standing on", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what color is the background", 1],
            ["what is the woman holding", 2],
            ["how many people are there", 1],
            ["where is the woman", -1],
            ["what sport is the woman playing", 2],
            ["what is the woman doing", 1],
            ["what is above the woman", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "context": [
            "a woman standing next to a ladder with an apple tree in the background.",
            "a man riding a horse in a field with people watching."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2359466",
                "VG_object_id": "793151",
                "bbox": [215, 68, 298, 174],
                "image": "data\\images\\2359466.jpg"
            },
            {
                "VG_image_id": "2381317",
                "VG_object_id": "1337480",
                "bbox": [210, 111, 306, 158],
                "image": "data\\images\\2381317.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the man playing", 2],
            ["what color is the background", 1],
            ["what is the man doing", 1],
            ["What is man holding", 1],
            ["how is the photo", 1]
        ],
        "org_questions": [
            ["what color are the man's trousers", -1],
            ["what color is the background", 1],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["who is wearing the trousers", -1],
            ["What is man holding", 1],
            ["when was this photo taken", -1],
            ["how is the photo", 1],
            ["what is on the man's head", -1],
            ["what sport is the man playing", 2]
        ],
        "context": [
            "a snowboarder is doing a trick in the air.",
            "a man riding a motorcycle in the air."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2354629",
                "VG_object_id": "1881513",
                "bbox": [100, 147, 209, 246],
                "image": "data\\images\\2354629.jpg"
            },
            {
                "VG_image_id": "2385575",
                "VG_object_id": "1295202",
                "bbox": [218, 48, 442, 309],
                "image": "data\\images\\2385575.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's clothes", 2],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color are the man's clothes", 2],
            ["what is the man holding", 1],
            ["when is this photo taken", -1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", -1],
            ["what is the man doing", 1],
            ["what color is the background", -1],
            ["where are the people", -1],
            ["what is the man wearing", -1],
            ["where is the picture taken", -1],
            ["what is the man eating", -1]
        ],
        "context": [
            "a group of people sitting around a table eating food.",
            "a group of people standing around a table with plates of food."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2416801",
                "VG_object_id": "1057205",
                "bbox": [176, 124, 495, 365],
                "image": "data\\images\\2416801.jpg"
            },
            {
                "VG_image_id": "2355236",
                "VG_object_id": "831526",
                "bbox": [204, 47, 499, 368],
                "image": "data\\images\\2355236.jpg"
            }
        ],
        "questions_with_scores": [["what color is the quilt", 1]],
        "org_questions": [
            ["what color is the quilt", 1],
            ["what color is the wall", -1],
            ["how many pillows are there on the bed", -1],
            ["what is the floor made of", -1],
            ["where is the lamp", -1],
            ["what is on the bed", -1],
            ["how many people are there on the bed", -1],
            ["what room is this", -1],
            ["what is behind the bed", -1],
            ["where was the photo taken", -1],
            ["what is in the room", -1],
            ["what is next to the bed", -1]
        ],
        "context": [
            "a bed sitting in a bedroom next to a lamp.",
            "a bedroom with a bed, nightstand, and a nightstand."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2376285",
                "VG_object_id": "2566855",
                "bbox": [80, 79, 390, 499],
                "image": "data\\images\\2376285.jpg"
            },
            {
                "VG_image_id": "2377501",
                "VG_object_id": "2285176",
                "bbox": [70, 21, 208, 197],
                "image": "data\\images\\2377501.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is guy doing", 2],
            ["how many people are there", 2],
            ["What color is the guy's shirt", 1],
            ["what is the man carrying", 1],
            ["what is the man doing", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["What color is the guy's shirt", 1],
            ["What is guy doing", 2],
            ["how many people are there", 2],
            ["what is the man carrying", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is on the man's head", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "three men sitting on a couch playing a video game.",
            "a man and a woman preparing food in a kitchen."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2318432",
                "VG_object_id": "1010848",
                "bbox": [3, 407, 374, 498],
                "image": "data\\images\\2318432.jpg"
            },
            {
                "VG_image_id": "2401366",
                "VG_object_id": "1148484",
                "bbox": [9, 285, 493, 339],
                "image": "data\\images\\2401366.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the floor", 2],
            ["what color is the ground", 2],
            ["what room is the floor in", 1],
            ["what is the floor made of", 1],
            ["where is the picture taken", 1],
            ["how is the floor made", 1],
            ["what is covering the floor", 1],
            ["what material is the floor made of", 1]
        ],
        "org_questions": [
            ["what is on the floor", 2],
            ["what color is the ground", 2],
            ["what room is the floor in", 1],
            ["what is the floor made of", 1],
            ["where is the picture taken", 1],
            ["what color is the wall", -1],
            ["what shape is the floor", -1],
            ["how is the floor made", 1],
            ["what is under the floor", -1],
            ["what is covering the floor", 1],
            ["what material is the floor made of", 1]
        ],
        "context": [
            "a pug dog sitting on a nightstand next to a bed.",
            "a man standing in a bathroom next to a urinal."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2356721",
                "VG_object_id": "817876",
                "bbox": [0, 1, 377, 499],
                "image": "data\\images\\2356721.jpg"
            },
            {
                "VG_image_id": "2383203",
                "VG_object_id": "1323003",
                "bbox": [0, 1, 373, 500],
                "image": "data\\images\\2383203.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 1],
            ["what color is the floor", 1]
        ],
        "org_questions": [
            ["what color is the wall", 1],
            ["what color is the floor", 1],
            ["how many toilets are there", -1],
            ["what is on the wall", -1],
            ["What is wall made of", -1],
            ["what is the patten of the wall", -1],
            ["where was this photo taken", -1],
            ["what room is this", -1],
            ["what is in the bathroom", -1],
            ["where is this scene", -1]
        ],
        "context": [
            "a bathroom with a window and a toilet.",
            "a bathroom with a sink, mirror, and toilet."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2413430",
                "VG_object_id": "173692",
                "bbox": [70, 220, 314, 499],
                "image": "data\\images\\2413430.jpg"
            },
            {
                "VG_image_id": "2357082",
                "VG_object_id": "2610567",
                "bbox": [25, 31, 347, 331],
                "image": "data\\images\\2357082.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many giraffes are there", 1],
            ["what is in front of the giraffe", 1],
            ["what is the giraffe eating", 1]
        ],
        "org_questions": [
            ["how many giraffes are there", 1],
            ["where are the giraffes", -1],
            ["what are the giraffes doing", -1],
            ["what is the giraffe standing on", -1],
            ["what is in front of the giraffe", 1],
            ["when was the photo taken", -1],
            ["what kind of animals are there", -1],
            ["where was this picture taken", -1],
            ["what is the giraffe eating", 1]
        ],
        "context": [
            "two giraffes standing next to each other near a rock wall.",
            "a group of giraffes eating hay from a feeder."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2319642",
                "VG_object_id": "2976029",
                "bbox": [81, 29, 449, 316],
                "image": "data\\images\\2319642.jpg"
            },
            {
                "VG_image_id": "2368860",
                "VG_object_id": "2050530",
                "bbox": [205, 125, 336, 307],
                "image": "data\\images\\2368860.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["What is the man holding", 2],
            ["How many people are there", 1],
            ["What is player holding", 1],
            ["what is the player doing", 1],
            ["how many players are there in the picture", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is man's shirt", 2],
            ["What is player holding", 1],
            ["what is the player doing", 1],
            ["what is on the player's head", -1],
            ["how many players are there in the picture", 1],
            ["What color is the ground", -1],
            ["What is the man holding", 2],
            ["where was the photo taken", -1],
            ["what sport is being played", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man wearing a red shirt",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2334987",
                "VG_object_id": "2183322",
                "bbox": [192, 204, 244, 271],
                "image": "data\\images\\2334987.jpg"
            },
            {
                "VG_image_id": "2373610",
                "VG_object_id": "588437",
                "bbox": [113, 262, 174, 364],
                "image": "data\\images\\2373610.jpg"
            }
        ],
        "questions_with_scores": [["what is the man wearing on head", 1]],
        "org_questions": [
            ["how many people are there", -1],
            ["where is this photo taken", -1],
            ["what color is the ground", -1],
            ["what is the man doing", -1],
            ["what is the gound made of", -1],
            ["what is the man wearing on head", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1],
            ["what color is the skier's jacket", -1]
        ],
        "context": [
            "a group of people skiing down a snow covered slope.",
            "a man on skis standing on a snowy slope."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2345295",
                "VG_object_id": "2809828",
                "bbox": [118, 55, 273, 330],
                "image": "data\\images\\2345295.jpg"
            },
            {
                "VG_image_id": "2323299",
                "VG_object_id": "2728064",
                "bbox": [159, 72, 215, 323],
                "image": "data\\images\\2323299.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the child's head", 1],
            ["what is the child doing", 1],
            ["how many children are there in the picture", 1],
            ["where is the person", 1],
            ["what is the boy holding", 1],
            ["when was the photo taken", 1],
            ["what sport is being played", 1],
            ["who is in the photo", 1],
            ["where was the photo taken", 1],
            ["how many children are there ", 1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", -1],
            ["what is on the child's head", 1],
            ["what is the child doing", 1],
            ["how many children are there in the picture", 1],
            ["where is the person", 1],
            ["what is the boy holding", 1],
            ["what is the boy wearing", -1],
            ["when was the photo taken", 1],
            ["what sport is being played", 1],
            ["who is in the photo", 1],
            ["where was the photo taken", 1],
            ["how many children are there ", 1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["when was the picture taken", 1]
        ],
        "context": [
            "a man holding a bat on a baseball field.",
            "three young children holding tennis rackets and smiling."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2408180",
                "VG_object_id": "323121",
                "bbox": [110, 111, 496, 366],
                "image": "data\\images\\2408180.jpg"
            },
            {
                "VG_image_id": "2362256",
                "VG_object_id": "3751213",
                "bbox": [20, 154, 324, 474],
                "image": "data\\images\\2362256.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are there", 2],
            ["what is on the horse's eyes", 2],
            ["what is the horse standing on", 1],
            ["where is  the horse", 1],
            ["how many people are there in the picture", 1],
            ["when was the picture taken", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["how many horses are there", 2],
            ["what is on the horse's eyes", 2],
            ["what is the horse standing on", 1],
            ["what color are the horses", -1],
            ["where is  the horse", 1],
            ["how many people are there in the picture", 1],
            ["when was the picture taken", 1],
            ["what type of animal is shown", -1],
            ["what are the horses doing", -1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "two white horses pulling a carriage down a street.",
            "a white horse with a black mane and a man in a white shirt."
        ]
    },
    {
        "object_category": "skier",
        "images": [
            {
                "VG_image_id": "2369347",
                "VG_object_id": "612088",
                "bbox": [137, 17, 303, 215],
                "image": "data\\images\\2369347.jpg"
            },
            {
                "VG_image_id": "2411396",
                "VG_object_id": "312534",
                "bbox": [222, 20, 334, 274],
                "image": "data\\images\\2411396.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sky", 2],
            ["when is the picture taken", 2],
            ["what color is the skier's jacket", 1],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["what color is the sky", 2],
            ["what color is the skier's jacket", 1],
            ["when is the picture taken", 2],
            ["how many skiers are there", -1],
            ["what is the gender of the person", -1],
            ["where is the person", -1],
            ["what color is the background", 1],
            ["what is on the person's head", -1],
            ["what are the people doing", -1],
            ["who is in the photo", -1],
            ["what is the person holding", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a skier in the air after a jump.",
            "a man on skis on a rail"
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2327095",
                "VG_object_id": "2952170",
                "bbox": [97, 47, 329, 268],
                "image": "data\\images\\2327095.jpg"
            },
            {
                "VG_image_id": "2391950",
                "VG_object_id": "1235740",
                "bbox": [27, 192, 213, 435],
                "image": "data\\images\\2391950.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["what color is the ground", 1],
            ["What is on the motorcycle", 1],
            ["who is riding the motorcycle", 1],
            ["what is the man on", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what are the people doing", 2],
            ["what color is the ground", 1],
            ["how many dogs are there in the picture", -1],
            ["where is the motorcycle", -1],
            ["What is on the motorcycle", 1],
            ["how many people are there in the picture", -1],
            ["who is riding the motorcycle", 1],
            ["what is the man on", 1],
            ["what is the man doing", 1]
        ],
        "context": [
            "a motorcycle parked on the side of the road.",
            "a man riding a motorcycle down a street."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2351850",
                "VG_object_id": "857797",
                "bbox": [308, 247, 442, 351],
                "image": "data\\images\\2351850.jpg"
            },
            {
                "VG_image_id": "2374292",
                "VG_object_id": "1978736",
                "bbox": [246, 297, 329, 449],
                "image": "data\\images\\2374292.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the color of the trousers", 2],
            ["where is the man", 2],
            ["how many people are there in the picture", 1],
            ["what is the persion holding", 1],
            ["what type of pants is the man wearing", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is the color of the trousers", 2],
            ["what gender is the person in the trousers", -1],
            ["how many people are there in the picture", 1],
            ["what is the weather like", -1],
            ["what is the persion holding", 1],
            ["what is the gender of the person", -1],
            ["what type of pants is the man wearing", 1],
            ["where are the people", 1],
            ["what are the people wearing", -1],
            ["what is the man wearing", -1],
            ["where is the man", 2]
        ],
        "context": [
            "three people sitting on a couch in a living room.",
            "a man holding a kite in a park."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2385343",
                "VG_object_id": "1298022",
                "bbox": [73, 166, 135, 263],
                "image": "data\\images\\2385343.jpg"
            },
            {
                "VG_image_id": "2353503",
                "VG_object_id": "2185087",
                "bbox": [206, 285, 250, 390],
                "image": "data\\images\\2353503.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the boy doing", 1],
            ["what color is the boy's shirt", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what is the boy doing", 1],
            ["what color is the boy's shirt", 1],
            ["what color is the boy's hair", -1],
            ["how many people are there", -1],
            ["What is man wearing on his head", -1],
            ["where is the man", -1],
            ["what is the boy holding", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "two boys on skateboards at a skate park.",
            "a man flying a kite in a field."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2349447",
                "VG_object_id": "1669891",
                "bbox": [53, 9, 418, 280],
                "image": "data\\images\\2349447.jpg"
            },
            {
                "VG_image_id": "2359892",
                "VG_object_id": "2011256",
                "bbox": [378, 43, 498, 201],
                "image": "data\\images\\2359892.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the screen", 1],
            ["what is the main color of the screen", 1],
            ["how large is the screen", 1],
            ["what color is the table", 1],
            ["how many people are shown", 1]
        ],
        "org_questions": [
            ["what is on the screen", 1],
            ["what is the screen made of", -1],
            ["what is the main color of the screen", 1],
            ["how large is the screen", 1],
            ["what is next to the computer", -1],
            ["what is in front of the screen", -1],
            ["what color is the table", 1],
            ["when was the picture taken", -1],
            ["how many people are shown", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man standing in front of a podium holding a wii controller.",
            "a man playing a video game on a computer."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2412789",
                "VG_object_id": "3183697",
                "bbox": [248, 18, 459, 312],
                "image": "data\\images\\2412789.jpg"
            },
            {
                "VG_image_id": "2363311",
                "VG_object_id": "766072",
                "bbox": [33, 91, 500, 288],
                "image": "data\\images\\2363311.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the truck", 2],
            ["What color is the ground", 2],
            ["Where is the truck", 1],
            ["what is the floor made of", 1],
            ["what is at the back of the truck", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["What color is the truck", 2],
            ["Where is the truck", 1],
            ["What color is the ground", 2],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the floor made of", 1],
            ["what is at the back of the truck", 1],
            ["what type of vehicle is this", -1],
            ["when was this photo taken", -1],
            ["where was this photo taken", 1],
            ["what is on the road", -1]
        ],
        "context": [
            "a truck is parked on a dirt road.",
            "a food truck parked in a parking lot."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2334077",
                "VG_object_id": "966118",
                "bbox": [88, 184, 189, 309],
                "image": "data\\images\\2334077.jpg"
            },
            {
                "VG_image_id": "2359976",
                "VG_object_id": "1662577",
                "bbox": [108, 241, 231, 379],
                "image": "data\\images\\2359976.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the jacket", 2],
            ["who is wearing the trousers", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what color is the jacket", 2],
            ["what time is it", -1],
            ["what is the person in the trouser doing", -1],
            ["Where is man ", -1],
            ["who is wearing the trousers", 1],
            ["what is the weather like", -1],
            ["what kind of pants is the person wearing", -1],
            ["what is on the ground", -1],
            ["what are the people wearing", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man on skis jumping in the air",
            "a woman wearing a pink tutu and a pink hat."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2380218",
                "VG_object_id": "546898",
                "bbox": [191, 72, 248, 304],
                "image": "data\\images\\2380218.jpg"
            },
            {
                "VG_image_id": "2391283",
                "VG_object_id": "1242016",
                "bbox": [186, 159, 246, 318],
                "image": "data\\images\\2391283.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's shirt", 2],
            ["what color are the boy's pants", 2],
            ["what is the boy playing", 1],
            ["what is on the left boy's head", 1]
        ],
        "org_questions": [
            ["what color is the boy's shirt", 2],
            ["what color are the boy's pants", 2],
            ["what is the boy playing", 1],
            ["how many people are there", -1],
            ["what is on the left boy's head", 1],
            ["what is in the background", -1],
            ["what is the boy wearing", -1],
            ["what is the boy holding", -1],
            ["when was the photo taken", -1],
            ["who is wearing a blue shirt", -1],
            ["what is the boy on the left doing", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a young boy throwing a frisbee in a park.",
            "a man and a boy on skateboards in a park."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2355262",
                "VG_object_id": "2752734",
                "bbox": [97, 16, 349, 234],
                "image": "data\\images\\2355262.jpg"
            },
            {
                "VG_image_id": "2330928",
                "VG_object_id": "3718085",
                "bbox": [147, 114, 373, 497],
                "image": "data\\images\\2330928.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["how many people are there in the photo", 2],
            ["what is the man wearing", 1],
            ["where is the photo taken", 1],
            ["what is the man doing", 1],
            ["what is behind the man", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man wearing", 1],
            ["how many people are there in the photo", 2],
            ["where is the photo taken", 1],
            ["what is the man doing", 1],
            ["what is the man wearing on his head", -1],
            ["what is behind the man", 1],
            ["who is in the picture", -1],
            ["what is the man standing on", 1]
        ],
        "context": [
            "a man doing a trick on a skateboard.",
            "two men in suits are posing for a picture."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2359144",
                "VG_object_id": "1879281",
                "bbox": [86, 47, 335, 392],
                "image": "data\\images\\2359144.jpg"
            },
            {
                "VG_image_id": "2356239",
                "VG_object_id": "2499768",
                "bbox": [18, 139, 122, 326],
                "image": "data\\images\\2356239.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what is the woman wearing", -1],
            ["what color is the woman's hair", -1],
            ["how many people are there", 1],
            ["what is the woman sitting on", -1],
            ["what is the woman holding", -1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["what are the people doing", -1]
        ],
        "context": [
            "an older woman sitting at a table with a plate of food.",
            "a group of people sitting around a table."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2376328",
                "VG_object_id": "573864",
                "bbox": [278, 232, 404, 331],
                "image": "data\\images\\2376328.jpg"
            },
            {
                "VG_image_id": "2386449",
                "VG_object_id": "682107",
                "bbox": [57, 249, 163, 323],
                "image": "data\\images\\2386449.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are there in the picture", 2],
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["what pattern is the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is on the floor", 1],
            ["when is this photo taken", -1],
            ["where is the floor", -1],
            ["what pattern is the floor", 1],
            ["what room is it", -1],
            ["what is the floor made of", -1],
            ["what kind of flooring is this", -1],
            ["what is covering the floor", -1],
            ["how many dogs are there in the picture", 2]
        ],
        "context": [
            "a living room with a table, chairs, and a vase of yellow roses.",
            "a dog laying on the floor next to a teddy bear."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2361309",
                "VG_object_id": "782889",
                "bbox": [1, 215, 500, 394],
                "image": "data\\images\\2361309.jpg"
            },
            {
                "VG_image_id": "2394588",
                "VG_object_id": "1211363",
                "bbox": [4, 217, 324, 499],
                "image": "data\\images\\2394588.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of animal is on the land", 2],
            ["how many animals are there on the land", 2],
            ["what kind of animal is there", 2],
            ["What is on the ground", 1],
            ["what animal is on the ground", 1],
            ["what type of animal is shown", 1]
        ],
        "org_questions": [
            ["what kind of animal is on the land", 2],
            ["what color is the land", -1],
            ["how many animals are there on the land", 2],
            ["What is on the ground", 1],
            ["what is the weather like", -1],
            ["what animal is on the ground", 1],
            ["what is the animal doing", -1],
            ["where was this taken", -1],
            ["what type of animal is shown", 1],
            ["what kind of animal is there", 2]
        ],
        "context": [
            "a group of giraffes standing under a tree.",
            "a polar bear walking on a concrete surface."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2339328",
                "VG_object_id": "3155561",
                "bbox": [4, 23, 497, 277],
                "image": "data\\images\\2339328.jpg"
            },
            {
                "VG_image_id": "2358193",
                "VG_object_id": "2552603",
                "bbox": [12, 15, 483, 298],
                "image": "data\\images\\2358193.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many planes are in the picture", 2],
            ["what color is the airplane", 1],
            ["when is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the airplane", 1],
            ["how many planes are in the picture", 2],
            ["when is the picture taken", 1],
            ["where is the plane", -1],
            ["what is the airplane doing", -1],
            ["what is in the background", -1],
            ["where was this picture taken", -1],
            ["what is on the ceiling", -1],
            ["what is in the air", -1],
            ["how many people are in the picture", -1]
        ],
        "context": [
            "a propeller on a plane",
            "a plane that is sitting in a hanger."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2323364",
                "VG_object_id": "2989410",
                "bbox": [199, 1, 447, 128],
                "image": "data\\images\\2323364.jpg"
            },
            {
                "VG_image_id": "2330113",
                "VG_object_id": "3053042",
                "bbox": [174, 106, 338, 195],
                "image": "data\\images\\2330113.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the umbrella", 2],
            ["what pattern is on the umbrella", 1]
        ],
        "org_questions": [
            ["what are the people doing", -1],
            ["what color is the umbrella", 2],
            ["How many umbrellas are there", -1],
            ["what pattern is on the umbrella", 1],
            ["what time is it", -1],
            ["what is behind the umbrella", -1],
            ["where is the umbrella", -1],
            ["what is the building made of", -1],
            ["what is the weather like", -1],
            ["what are the people holding", -1],
            ["where are the people", -1]
        ],
        "context": [
            "a group of people standing around each other.",
            "a group of people standing on a sidewalk holding umbrellas."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2374816",
                "VG_object_id": "1744548",
                "bbox": [27, 162, 91, 217],
                "image": "data\\images\\2374816.jpg"
            },
            {
                "VG_image_id": "2330573",
                "VG_object_id": "2705237",
                "bbox": [260, 112, 326, 178],
                "image": "data\\images\\2330573.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["where is the person", 2],
            ["what is the gender of the person", 1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["where is the person", 2],
            ["how many people are there", -1],
            ["what is the gender of the person", 1],
            ["what is the persion doing", 1],
            ["what is the weather like", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a young child is looking at a polar bear in a cage.",
            "a man standing in a canoe in a corn field."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2415849",
                "VG_object_id": "3376915",
                "bbox": [2, 232, 498, 349],
                "image": "data\\images\\2415849.jpg"
            },
            {
                "VG_image_id": "2325530",
                "VG_object_id": "3505362",
                "bbox": [0, 241, 496, 329],
                "image": "data\\images\\2325530.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the field", 2],
            ["what color is the plane", 1],
            ["what is the runway made of", 1]
        ],
        "org_questions": [
            ["what is the color of the field", 2],
            ["what is the color of the grass", -1],
            ["what color is the plane", 1],
            ["how many people are there", -1],
            ["what kind of animals are there in the background", -1],
            ["what is in the distance", -1],
            ["where was this photo taken", -1],
            ["how is the weather", -1],
            ["what is the runway made of", 1],
            ["where is the grass", -1]
        ],
        "context": [
            "a large jetliner sitting on top of an airport runway.",
            "a military plane on the runway of an airport."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2320583",
                "VG_object_id": "3102437",
                "bbox": [4, 202, 330, 440],
                "image": "data\\images\\2320583.jpg"
            },
            {
                "VG_image_id": "2404236",
                "VG_object_id": "345051",
                "bbox": [321, 155, 415, 244],
                "image": "data\\images\\2404236.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what color is the car", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the car", 1],
            ["what is the ground covered with", 1],
            ["where is the car", -1],
            ["how many cars are there", -1],
            ["what shape is the car", -1],
            ["when is the photo taken", -1],
            ["how is the weather", -1],
            ["what is on the car", -1],
            ["who is in the photo", -1],
            ["what type of vehicle is this", -1],
            ["what is in the background", 1],
            ["what is in front of the car", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a car with luggage on top of it",
            "a jeep is parked on a snowy hill."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2370883",
                "VG_object_id": "1859139",
                "bbox": [121, 84, 256, 312],
                "image": "data\\images\\2370883.jpg"
            },
            {
                "VG_image_id": "2358390",
                "VG_object_id": "2190390",
                "bbox": [133, 187, 206, 452],
                "image": "data\\images\\2358390.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 1],
            ["where is the woman", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 1],
            ["where is the woman", 1],
            ["how many dogs are in the picture", -1],
            ["what color is the shirt", -1],
            ["what is the woman wearing on her head", -1],
            ["what is the weather like", -1],
            ["what is the gender of the person on the right", -1],
            ["what is the man on the left doing", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a woman with a suitcase standing next to a subway train.",
            "a man and woman pose for a picture in front of a clock."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2355884",
                "VG_object_id": "3773772",
                "bbox": [55, 34, 472, 315],
                "image": "data\\images\\2355884.jpg"
            },
            {
                "VG_image_id": "2387141",
                "VG_object_id": "515026",
                "bbox": [1, 139, 295, 499],
                "image": "data\\images\\2387141.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 1],
            ["what is on the building", 1],
            ["what kind of building is this", 1]
        ],
        "org_questions": [
            ["what color is the building", 1],
            ["how is the weather", -1],
            ["what is in the sky", -1],
            ["how many people are in the picture", -1],
            ["what is the building made of", -1],
            ["Where is the building", -1],
            ["what is on the building", 1],
            ["what time is it", -1],
            ["when was this picture taken", -1],
            ["what kind of building is this", 1]
        ],
        "context": [
            "the building is a landmark.",
            "a tall clock tower with a clock on it."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2366183",
                "VG_object_id": "2741293",
                "bbox": [92, 27, 471, 368],
                "image": "data\\images\\2366183.jpg"
            },
            {
                "VG_image_id": "2371527",
                "VG_object_id": "1916289",
                "bbox": [18, 91, 372, 498],
                "image": "data\\images\\2371527.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many giraffes are there", 2],
            ["what is in front of the giraffe", 1]
        ],
        "org_questions": [
            ["what is in front of the giraffe", 1],
            ["how many giraffes are there", 2],
            ["what is in the background", -1],
            ["what is the giraffe doing", -1],
            ["what is the giraffe standing on", -1],
            ["what is on the side of the giraffe", -1],
            ["what is behind the giraffe", -1],
            ["when was the photo taken", -1],
            ["where was this picture taken", -1],
            ["what kind of animals are these", -1],
            ["where are the giraffes", -1]
        ],
        "context": [
            "a giraffe standing next to a brick building.",
            "two giraffes standing in a fenced in area."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "1593163",
                "VG_object_id": "1616029",
                "bbox": [5, 187, 182, 329],
                "image": "data\\images\\1593163.jpg"
            },
            {
                "VG_image_id": "2365474",
                "VG_object_id": "633981",
                "bbox": [114, 133, 180, 208],
                "image": "data\\images\\2365474.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the laptop", 1],
            ["what is the laptop on", 1],
            ["what is the main color on the screen", 1],
            ["where is the computer", 1],
            ["what is next to the laptop", 1]
        ],
        "org_questions": [
            ["what color is the laptop", 1],
            ["what is the laptop on", 1],
            ["How many screens are there", -1],
            ["what is the main color on the screen", 1],
            ["what color is the wall", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["where is the computer", 1],
            ["what is next to the laptop", 1]
        ],
        "context": [
            "a laptop computer sitting on top of a wooden desk.",
            "a chair with a laptop on it next to a bed."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2414109",
                "VG_object_id": "159772",
                "bbox": [118, 38, 392, 394],
                "image": "data\\images\\2414109.jpg"
            },
            {
                "VG_image_id": "2331516",
                "VG_object_id": "2756072",
                "bbox": [201, 245, 302, 430],
                "image": "data\\images\\2331516.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the skier's shirt", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what color is the skier's shirt", 1],
            ["what time is it", -1],
            ["what is the man doing", -1],
            ["what is the ground covered with", -1],
            ["Where is the man", -1],
            ["What is man holding", -1],
            ["when was this photo taken", -1],
            ["where was this photo taken", -1],
            ["what is the persion riding", -1],
            ["who is riding the skateboard", -1]
        ],
        "context": [
            "a man riding a skateboard on a ledge.",
            "a boy skateboarding in front of a statue."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "1593169",
                "VG_object_id": "1616112",
                "bbox": [353, 177, 809, 765],
                "image": "data\\images\\1593169.jpg"
            },
            {
                "VG_image_id": "2323704",
                "VG_object_id": "3260117",
                "bbox": [74, 82, 313, 312],
                "image": "data\\images\\2323704.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["what color is the woman's hair", 1],
            ["what is the woman wearing", 1],
            ["what are the people doing", 1],
            ["what are the people holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what color is the woman's hair", 1],
            ["How many people are there", -1],
            ["where is the woman", -1],
            ["what is the woman doing", -1],
            ["what gesture is the woman", -1],
            ["what is the woman wearing", 1],
            ["where are the people", -1],
            ["what are the people doing", 1],
            ["what are the people holding", 1]
        ],
        "context": [
            "person, person, and actor at the bar.",
            "a couple of people standing next to each other."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2393034",
                "VG_object_id": "1223583",
                "bbox": [346, 99, 489, 150],
                "image": "data\\images\\2393034.jpg"
            },
            {
                "VG_image_id": "2357503",
                "VG_object_id": "811619",
                "bbox": [54, 3, 283, 191],
                "image": "data\\images\\2357503.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the umbrella", 2],
            ["what is behind the umbrella", 1],
            ["what is the weather like", 1],
            ["what is the pattern on the umbrella", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 2],
            ["what is behind the umbrella", 1],
            ["what is the weather like", 1],
            ["How many umbrellas are there", -1],
            ["what time is it", -1],
            ["what is the pattern on the umbrella", 1],
            ["where is the umbrella", -1],
            ["what is under the umbrella", -1],
            ["what are the people doing", -1],
            ["when was this photo taken", -1],
            ["where was this photo taken", -1],
            ["how is the weather", 1]
        ],
        "context": [
            "a cow walking down a street with people walking around.",
            "a man in a colorful outfit holding an umbrella."
        ]
    },
    {
        "object_category": "ocean",
        "images": [
            {
                "VG_image_id": "2341976",
                "VG_object_id": "2079323",
                "bbox": [0, 95, 499, 260],
                "image": "data\\images\\2341976.jpg"
            },
            {
                "VG_image_id": "2399708",
                "VG_object_id": "3255748",
                "bbox": [2, 124, 495, 314],
                "image": "data\\images\\2399708.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ocean", 1],
            ["what is the person holding", 1],
            ["what is the person doing", 1],
            ["what is the gender of the person", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what color is the ocean", 1],
            ["what is the person holding", 1],
            ["what is the person doing", 1],
            ["how many people are there", -1],
            ["what is the gender of the person", 1],
            ["what is in the distance", -1],
            ["when was the photo taken", -1],
            ["where was this picture taken", -1],
            ["how is the weather", -1],
            ["who is in the picture", 1]
        ],
        "context": [
            "a woman riding a horse on the beach.",
            "a man holding a surfboard on a beach."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2352825",
                "VG_object_id": "851560",
                "bbox": [0, 38, 373, 496],
                "image": "data\\images\\2352825.jpg"
            },
            {
                "VG_image_id": "2329097",
                "VG_object_id": "3795826",
                "bbox": [5, 167, 498, 330],
                "image": "data\\images\\2329097.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the wall", 2],
            ["what is the persion doing", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what color is the wall", 2],
            ["what is on the bed", -1],
            ["what is the persion doing", 1],
            ["how many pillows are there on the bed", -1],
            ["where was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion wearing", 1],
            ["where are the pillows", -1]
        ],
        "context": [
            "a man laying on a bed with a book.",
            "a boy and girl sitting on a bed."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2346499",
                "VG_object_id": "2387725",
                "bbox": [52, 76, 214, 357],
                "image": "data\\images\\2346499.jpg"
            },
            {
                "VG_image_id": "2370852",
                "VG_object_id": "2077915",
                "bbox": [68, 91, 246, 325],
                "image": "data\\images\\2370852.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the woman doing", 1],
            ["what color is the woman's skirt", 1],
            ["where is the woman", 1],
            ["what is the woman holding", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["how many people are there", 2],
            ["what color is the woman's skirt", 1],
            ["what time is it", -1],
            ["where is the woman", 1],
            ["what is the woman holding", 1],
            ["what gesture is the woman", -1],
            ["who is in the photo", -1],
            ["what is the woman wearing", -1],
            ["where was the photo taken", 1],
            ["what is the persion on the right doing", -1]
        ],
        "context": [
            "a group of women playing a video game.",
            "a woman is brushing her teeth in the bathroom."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2390989",
                "VG_object_id": "1245181",
                "bbox": [52, 374, 330, 500],
                "image": "data\\images\\2390989.jpg"
            },
            {
                "VG_image_id": "2408986",
                "VG_object_id": "251201",
                "bbox": [63, 392, 312, 498],
                "image": "data\\images\\2408986.jpg"
            }
        ],
        "questions_with_scores": [["what color is the wall", 1]],
        "org_questions": [
            ["what is the ground covered with", -1],
            ["what color is the ground", -1],
            ["what color is the wall", 1],
            ["where is the floor", -1],
            ["what is on the wall", -1],
            ["what is the floor made of", -1],
            ["what shape is the floor", -1],
            ["what room is this", -1],
            ["what kind of floor is this", -1],
            ["what is covering the floor", -1]
        ],
        "context": [
            "a bathroom with a toilet, sink, and a bathtub.",
            "a bathroom with a toilet and a mirror."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2361594",
                "VG_object_id": "2300640",
                "bbox": [292, 32, 386, 202],
                "image": "data\\images\\2361594.jpg"
            },
            {
                "VG_image_id": "2344508",
                "VG_object_id": "1043473",
                "bbox": [111, 213, 210, 333],
                "image": "data\\images\\2344508.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is color of the man's pants", 2],
            ["what is the man wearing", 2],
            ["what is the man doing", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what is color of the man's pants", 2],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["what is the gender of the person on the right", -1],
            ["who is in the photo", -1],
            ["what is the man holding", -1],
            ["what is the man wearing", 2]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a man in a suit and tie standing next to another man."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2344655",
                "VG_object_id": "914137",
                "bbox": [2, 226, 69, 305],
                "image": "data\\images\\2344655.jpg"
            },
            {
                "VG_image_id": "2322573",
                "VG_object_id": "3548774",
                "bbox": [15, 228, 204, 373],
                "image": "data\\images\\2322573.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many laptops are there", 2],
            ["where is the photo taken", 1],
            ["what color are the walls", 1]
        ],
        "org_questions": [
            ["how many laptops are there", 2],
            ["where is the laptop", -1],
            ["what is on the laptop's screen", -1],
            ["where is the photo taken", 1],
            ["what is the main color of the laptop's screen", -1],
            ["how many people are there", -1],
            ["what is in front of the laptop", -1],
            ["what is on the laptop", -1],
            ["what color are the walls", 1],
            ["what is on the man's lap", -1],
            ["what is the color of the laptop", -1]
        ],
        "context": [
            "a woman and a man sitting on a bed.",
            "three men sitting on a couch with their laptops."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2337913",
                "VG_object_id": "2410023",
                "bbox": [115, 39, 432, 286],
                "image": "data\\images\\2337913.jpg"
            },
            {
                "VG_image_id": "2366701",
                "VG_object_id": "753754",
                "bbox": [51, 102, 499, 265],
                "image": "data\\images\\2366701.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the train", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["What color is the train", 1],
            ["What color is the ground", -1],
            ["Where is the train", -1],
            ["How many trains are there", -1],
            ["what is in the background", 1],
            ["where is the photo taken", -1],
            ["what color is the sky", -1],
            ["how many people are there", -1],
            ["what is the train doing", -1],
            ["what type of train is this", -1],
            ["what is this a picture of", -1],
            ["what is the train on", -1]
        ],
        "context": [
            "a red and white train traveling down train tracks.",
            "a train is parked at a train station."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2382693",
                "VG_object_id": "3831089",
                "bbox": [15, 330, 146, 479],
                "image": "data\\images\\2382693.jpg"
            },
            {
                "VG_image_id": "2395674",
                "VG_object_id": "452049",
                "bbox": [220, 66, 383, 263],
                "image": "data\\images\\2395674.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the person's shirt", 2],
            ["What is in the background", 2],
            ["what sport are the people doing", 1],
            ["where is the person", 1],
            ["what is the persion doing", 1],
            ["What is in the background of image", 1],
            ["who is wearing a white shirt", 1]
        ],
        "org_questions": [
            ["What color is the person's shirt", 2],
            ["What is in the background", 2],
            ["What is the gender of person", -1],
            ["what sport are the people doing", 1],
            ["where is the person", 1],
            ["what is the persion doing", 1],
            ["What is in the background of image", 1],
            ["when was the photo taken", -1],
            ["who is wearing a white shirt", 1]
        ],
        "context": [
            "a woman playing tennis on a court",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2333306",
                "VG_object_id": "2872367",
                "bbox": [356, 47, 491, 294],
                "image": "data\\images\\2333306.jpg"
            },
            {
                "VG_image_id": "2417657",
                "VG_object_id": "3262617",
                "bbox": [2, 31, 234, 328],
                "image": "data\\images\\2417657.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["what is the man riding", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what sport is the man playing", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["what is the man riding", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["where is the man", -1],
            ["what sport is the man playing", 1],
            ["what is the man wearing on his face", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man doing a trick on a skateboard.",
            "a man riding a motorcycle down a street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2338978",
                "VG_object_id": "3446069",
                "bbox": [50, 119, 185, 292],
                "image": "data\\images\\2338978.jpg"
            },
            {
                "VG_image_id": "2347540",
                "VG_object_id": "889816",
                "bbox": [69, 55, 374, 396],
                "image": "data\\images\\2347540.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the man playing", 1],
            ["what is on the man's head", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["where is the person", 1],
            ["what is the man wearing", 1],
            ["where was this photo taken", 1],
            ["what is the number of people", 1],
            ["where is the game being played", 1]
        ],
        "org_questions": [
            ["what sport is the man playing", 1],
            ["what is the man standing on", -1],
            ["what is on the man's head", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["where is the person", 1],
            ["what is the man doing", -1],
            ["what is the man wearing", 1],
            ["who is in the picture", -1],
            ["where was this photo taken", 1],
            ["what is the number of people", 1],
            ["where is the game being played", 1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "two men jumping to catch a frisbee in a field."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2384313",
                "VG_object_id": "527435",
                "bbox": [31, 1, 373, 183],
                "image": "data\\images\\2384313.jpg"
            },
            {
                "VG_image_id": "2348210",
                "VG_object_id": "884041",
                "bbox": [82, 16, 188, 118],
                "image": "data\\images\\2348210.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the plant", 1],
            ["what color is the wall", 1],
            ["what is on the wall", 1],
            ["where is the picture taken", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["where is the plant", 1],
            ["what is in the picture", -1],
            ["how many people are there", -1],
            ["what color is the wall", 1],
            ["what is on the wall", 1],
            ["when was the photo taken", -1],
            ["what is the wall made of", -1],
            ["where is the picture taken", 1],
            ["what color are the leaves", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a bathroom with a toilet and a window.",
            "a red and white bicycle with flowers in a basket on the front."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2349617",
                "VG_object_id": "873342",
                "bbox": [35, 12, 278, 165],
                "image": "data\\images\\2349617.jpg"
            },
            {
                "VG_image_id": "2362111",
                "VG_object_id": "3320721",
                "bbox": [23, 56, 256, 196],
                "image": "data\\images\\2362111.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the screen", 1],
            ["where is the screen", 1],
            ["what is on the screen", 1]
        ],
        "org_questions": [
            ["what color is the screen", 1],
            ["where is the screen", 1],
            ["how many people are there", 2],
            ["what device does the screen belong to", -1],
            ["what is the television made of", -1],
            ["what is the tv doing", -1],
            ["what is on the screen", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a cat sitting on a tv stand watching a television.",
            "a man standing on a stage holding a remote."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2356021",
                "VG_object_id": "2792401",
                "bbox": [310, 3, 495, 249],
                "image": "data\\images\\2356021.jpg"
            },
            {
                "VG_image_id": "2349490",
                "VG_object_id": "874430",
                "bbox": [220, 0, 464, 220],
                "image": "data\\images\\2349490.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the first floor of the building", 2],
            ["what is in front of the building", 1],
            ["how many people are there", 1],
            ["what is the weather like", 1],
            ["what is on the side of the building", 1]
        ],
        "org_questions": [
            ["what color is the first floor of the building", 2],
            ["what is in front of the building", 1],
            ["how many people are there", 1],
            ["Where is the building", -1],
            ["what is the weather like", 1],
            ["what is the building made of", -1],
            ["What is behind the building", -1],
            ["when was the picture taken", -1],
            ["where was the picture taken", -1],
            ["what is on the wall", -1],
            ["what is on the side of the building", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a woman in a costume stands in front of a statue of liberty.",
            "a horse drawn carriage is pulling a carriage."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2374345",
                "VG_object_id": "727461",
                "bbox": [22, 156, 143, 273],
                "image": "data\\images\\2374345.jpg"
            },
            {
                "VG_image_id": "2359950",
                "VG_object_id": "1887180",
                "bbox": [10, 182, 90, 317],
                "image": "data\\images\\2359950.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["whatis the woman holding", 1],
            ["what is the woman sitting on", 1],
            ["what is the woman's posture", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what color is the woman's shirt", 2],
            ["where is the woman", 1],
            ["How many people are there", -1],
            ["whatis the woman holding", 1],
            ["what is the woman sitting on", 1],
            ["what is the woman wearing", -1],
            ["what is the woman's posture", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person holding", 1]
        ],
        "context": [
            "a man standing in a boat filled with vegetables.",
            "a man riding a bike down a road next to a lake."
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2415302",
                "VG_object_id": "3289001",
                "bbox": [0, 27, 448, 356],
                "image": "data\\images\\2415302.jpg"
            },
            {
                "VG_image_id": "2336736",
                "VG_object_id": "3107260",
                "bbox": [72, 129, 425, 366],
                "image": "data\\images\\2336736.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many motorcycles are there", 2],
            ["What color is the motorcycle", 2],
            ["What color is the ground", 1],
            ["where is the motorcycle", 1],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["how many people are there in the picture", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["How many motorcycles are there", 2],
            ["What color is the motorcycle", 2],
            ["What color is the ground", 1],
            ["where is the motorcycle", 1],
            ["What is on the motorcycle", -1],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["how many people are there in the picture", 1],
            ["when was the picture taken", -1],
            ["what kind of vehicle is this", -1],
            ["where was the photo taken", 1],
            ["what is parked on the ground", -1]
        ],
        "context": [
            "a blue motorcycle parked on the grass.",
            "a motorcycle parked on the side of a street."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2370484",
                "VG_object_id": "3858418",
                "bbox": [401, 39, 491, 268],
                "image": "data\\images\\2370484.jpg"
            },
            {
                "VG_image_id": "2344011",
                "VG_object_id": "2243205",
                "bbox": [356, 43, 498, 138],
                "image": "data\\images\\2344011.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 1],
            ["what kind of vehicle", 1],
            ["how many trees are there in the picture", 1],
            ["how many people are there", 1],
            ["What the color of building", 1]
        ],
        "org_questions": [
            ["what color is the building", 1],
            ["what kind of vehicle", 1],
            ["how many trees are there in the picture", 1],
            ["what time is it", -1],
            ["what is the building made of", -1],
            ["what color is the sky", -1],
            ["how many people are there", 1],
            ["What the color of building", 1],
            ["where are the windows", -1],
            ["what is on the side of the building", -1],
            ["when was this picture taken", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a red train is parked on the tracks.",
            "two red double decker buses parked on a brick road."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2376159",
                "VG_object_id": "1784574",
                "bbox": [120, 177, 182, 242],
                "image": "data\\images\\2376159.jpg"
            },
            {
                "VG_image_id": "2365833",
                "VG_object_id": "1804258",
                "bbox": [139, 184, 191, 250],
                "image": "data\\images\\2365833.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man on", 2],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["where is the ground made of", 1],
            ["what sport is it", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is the man on", 2],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what is the weather like", -1],
            ["where is the ground made of", 1],
            ["what sport is it", 1],
            ["when was this photo taken", -1],
            ["what is on the person's head", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", 1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a snowboarder is going down a steep hill.",
            "a group of people riding motorcycles down a dirt road."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2384231",
                "VG_object_id": "692304",
                "bbox": [62, 101, 173, 250],
                "image": "data\\images\\2384231.jpg"
            },
            {
                "VG_image_id": "2398313",
                "VG_object_id": "1180855",
                "bbox": [411, 22, 475, 64],
                "image": "data\\images\\2398313.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["where is the person", 2],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what is the persion doing", 1],
            ["what kind of shirt is the woman wearing", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["where is the photo taken", 2],
            ["what is the gender of the person", -1],
            ["what color is the background", 1],
            ["what is the persion doing", 1],
            ["where is the person", 2],
            ["who is wearing a white shirt", -1],
            ["what kind of shirt is the woman wearing", 1],
            ["when was the picture taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a woman playing tennis on a tennis court.",
            "a pizza sitting on a table next to a menu."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2400101",
                "VG_object_id": "1162997",
                "bbox": [3, 167, 323, 419],
                "image": "data\\images\\2400101.jpg"
            },
            {
                "VG_image_id": "2340678",
                "VG_object_id": "2311208",
                "bbox": [110, 97, 269, 330],
                "image": "data\\images\\2340678.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the man's shoes", 1],
            ["what is on the man head", 1],
            ["what is the man holding", 1],
            ["what sport is this", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what color is the man's shirt", 1],
            ["what color is the man's shoes", 1],
            ["how many people are there", -1],
            ["what is on the man head", 1],
            ["where is the man", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1],
            ["what sport is this", 1],
            ["what is in the air", -1]
        ],
        "context": [
            "a man jumping in the air to catch a frisbee.",
            "a man jumping up to hit a tennis ball."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2330764",
                "VG_object_id": "1046016",
                "bbox": [257, 5, 388, 82],
                "image": "data\\images\\2330764.jpg"
            },
            {
                "VG_image_id": "2373583",
                "VG_object_id": "588462",
                "bbox": [242, 159, 341, 203],
                "image": "data\\images\\2373583.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["what color is the umbrella", 1],
            ["how is the weather", 1],
            ["what is in the background", 1],
            ["what is the weather like", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 1],
            ["how many people are there in the photo", 2],
            ["where is the umbrella", -1],
            ["what is the man doing", -1],
            ["what is the pattern on the umbrella", -1],
            ["what is the persion holding", -1],
            ["what is on the umbrella", -1],
            ["how many umbrellas are in the picture", -1],
            ["how is the weather", 1],
            ["what is the umbrella made of", -1],
            ["what is in the background", 1],
            ["what is the weather like", 1]
        ],
        "context": [
            "a man walking a dog with an umbrella",
            "a man throwing a frisbee at a frisbee golf course."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2362021",
                "VG_object_id": "3332896",
                "bbox": [200, 88, 265, 147],
                "image": "data\\images\\2362021.jpg"
            },
            {
                "VG_image_id": "2329135",
                "VG_object_id": "3256211",
                "bbox": [264, 40, 338, 122],
                "image": "data\\images\\2329135.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on his head", 2],
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many shirts are there", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many shirts are there", 1],
            ["Who is wearing a blue shirt", -1],
            ["what is the gender of the person", -1],
            ["how many people are there in the picture", 1],
            ["what is the man wearing on his head", 2],
            ["when was the photo taken", -1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a couple of people riding on the back of an elephant.",
            "a man riding a skateboard on a ramp."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2414105",
                "VG_object_id": "159835",
                "bbox": [308, 169, 409, 275],
                "image": "data\\images\\2414105.jpg"
            },
            {
                "VG_image_id": "2359248",
                "VG_object_id": "2483711",
                "bbox": [209, 207, 261, 349],
                "image": "data\\images\\2359248.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["what is in the background", 2],
            ["how many people are there", 1],
            ["what color is the trouser", 1],
            ["what is the ground covered with", 1],
            ["what kind of pants is the man wearing", 1],
            ["where are the people", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what are the people doing", 2],
            ["what is in the background", 2],
            ["how many people are there", 1],
            ["what is the man wearing on his face", -1],
            ["what color is the trouser", 1],
            ["what is the ground covered with", 1],
            ["when was the picture taken", -1],
            ["who is wearing a white shirt", -1],
            ["what kind of pants is the man wearing", 1],
            ["where are the people", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a man riding a skateboard down a sidewalk."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2368808",
                "VG_object_id": "615794",
                "bbox": [102, 55, 422, 296],
                "image": "data\\images\\2368808.jpg"
            },
            {
                "VG_image_id": "2345381",
                "VG_object_id": "908587",
                "bbox": [87, 79, 461, 255],
                "image": "data\\images\\2345381.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the truck", 2],
            ["how many people are there", 1],
            ["what time is it", 1],
            ["when is this photo taken", 1]
        ],
        "org_questions": [
            ["what color is the truck", 2],
            ["how many people are there", 1],
            ["what time is it", 1],
            ["what animal is in the picture", -1],
            ["what is on the ground", -1],
            ["Where is the truck", -1],
            ["how is the weather", -1],
            ["when is this photo taken", 1],
            ["what are the people doing", -1],
            ["what type of vehicle is this", -1],
            ["where was this photo taken", -1],
            ["what is on the side of the truck", -1]
        ],
        "context": [
            "a woman standing next to a moving truck.",
            "a group of people standing outside of a food truck."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2331332",
                "VG_object_id": "3184489",
                "bbox": [29, 238, 354, 498],
                "image": "data\\images\\2331332.jpg"
            },
            {
                "VG_image_id": "2340127",
                "VG_object_id": "2660062",
                "bbox": [67, 94, 431, 259],
                "image": "data\\images\\2340127.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the animal", 2],
            ["What animal is on the land", 1],
            ["what is in the distance", 1],
            ["what are the animals", 1],
            ["What is the animal in the image", 1],
            ["what kind of animal is it", 1],
            ["where was the photo taken", 1],
            ["who is in the photo", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["What color is the animal", 2],
            ["What animal is on the land", 1],
            ["How many animals are there", -1],
            ["what is the animal doing", -1],
            ["what is in the distance", 1],
            ["what are the animals", 1],
            ["What is the animal in the image", 1],
            ["what kind of animal is it", 1],
            ["where was the photo taken", 1],
            ["who is in the photo", 1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a man sitting on a bed next to a cow.",
            "a horse laying in the grass with a bush in the background."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2387355",
                "VG_object_id": "513917",
                "bbox": [67, 1, 313, 379],
                "image": "data\\images\\2387355.jpg"
            },
            {
                "VG_image_id": "2392061",
                "VG_object_id": "1234365",
                "bbox": [42, 43, 320, 439],
                "image": "data\\images\\2392061.jpg"
            }
        ],
        "questions_with_scores": [
            ["What sports is man doing", 2],
            ["What color is man's shirt", 2],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is in the man's hand", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["What sports is man doing", 2],
            ["What color is man's shirt", 2],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["how is the weather", -1],
            ["what is the man wearing", -1],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is in the man's hand", 1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a man jumping in the air on a skateboard.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2321268",
                "VG_object_id": "3341791",
                "bbox": [144, 223, 242, 470],
                "image": "data\\images\\2321268.jpg"
            },
            {
                "VG_image_id": "2365348",
                "VG_object_id": "634854",
                "bbox": [189, 175, 259, 312],
                "image": "data\\images\\2365348.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["What is color of image", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what is the persion wearing on the left", 1]
        ],
        "org_questions": [
            ["What is color of image", 1],
            ["What is man doing", -1],
            ["how many people are there", 1],
            ["what is the gender of the person", -1],
            ["what is the person wearing", -1],
            ["what color are the man's trousers", -1],
            ["what color is the background", 1],
            ["what is the man standing on", -1],
            ["what is the man holding", 2],
            ["what is the persion wearing on the left", 1]
        ],
        "context": [
            "a man and a woman standing in a room.",
            "a man standing in a field with a accordion."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2354357",
                "VG_object_id": "839193",
                "bbox": [63, 66, 179, 428],
                "image": "data\\images\\2354357.jpg"
            },
            {
                "VG_image_id": "2374678",
                "VG_object_id": "1919324",
                "bbox": [136, 83, 327, 375],
                "image": "data\\images\\2374678.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["how many men are in the picture", 1],
            ["what is the man wearing on his face", 1],
            ["what color is the background", 1],
            ["what is the persion riding", 1],
            ["what kind of pants is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["how many men are in the picture", 1],
            ["what is the man wearing on his face", 1],
            ["what color is the background", 1],
            ["where is the man", -1],
            ["what is the persion riding", 1],
            ["what is the man wearing on head", -1],
            ["when was the photo taken", -1],
            ["what kind of pants is the man wearing", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man walking a dog on a leash next to a tree.",
            "two men pose for a picture on a motorcycle."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2360044",
                "VG_object_id": "2304990",
                "bbox": [0, 239, 145, 498],
                "image": "data\\images\\2360044.jpg"
            },
            {
                "VG_image_id": "2355527",
                "VG_object_id": "1979212",
                "bbox": [116, 91, 296, 373],
                "image": "data\\images\\2355527.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the girl's shirt", 1],
            ["What is the girl doing", 1],
            ["where is the girl", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["What color is the girl's shirt", 1],
            ["What is the girl doing", 1],
            ["what is the girl wearing on the head", -1],
            ["where is the girl", 1],
            ["what is the woman holding", 1],
            ["how many people are there", -1],
            ["what gender is the person", -1],
            ["who is in the photo", -1],
            ["what is the woman standing on", -1],
            ["what is the woman wearing", -1]
        ],
        "context": [
            "a group of people standing around a tv.",
            "a woman playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "vegetable",
        "images": [
            {
                "VG_image_id": "2359527",
                "VG_object_id": "2692914",
                "bbox": [243, 52, 308, 114],
                "image": "data\\images\\2359527.jpg"
            },
            {
                "VG_image_id": "2390527",
                "VG_object_id": "1249500",
                "bbox": [270, 17, 355, 62],
                "image": "data\\images\\2390527.jpg"
            }
        ],
        "questions_with_scores": [
            ["What vegetable is it", 1],
            ["what is the food on", 1],
            ["what kind of vegetable is it", 1],
            ["what is the green vegetable on the plate", 1],
            ["what kind of food is on the plate", 1]
        ],
        "org_questions": [
            ["how many plates are there", -1],
            ["what color is the table", -1],
            ["What vegetable is it", 1],
            ["where is the vegetables placed on", -1],
            ["what is the food on", 1],
            ["what kind of vegetable is it", 1],
            ["what is the green vegetable on the plate", 1],
            ["what kind of food is on the plate", 1],
            ["what is on the plate", -1],
            ["what is green", -1],
            ["what kind of meat is on the plate", -1]
        ],
        "context": [
            "a plate of food on a table",
            "a sandwich cut in half on a plate with fries."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2376267",
                "VG_object_id": "574447",
                "bbox": [262, 321, 344, 374],
                "image": "data\\images\\2376267.jpg"
            },
            {
                "VG_image_id": "2402485",
                "VG_object_id": "389256",
                "bbox": [231, 236, 328, 328],
                "image": "data\\images\\2402485.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["What color is person's shirt", 2],
            ["what is the ground covered with", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the ground", 2],
            ["how many people are there", -1],
            ["What is person doing", -1],
            ["what is the ground covered with", 1],
            ["What color is person's shirt", 2],
            ["what is the persion wearing", 1],
            ["what is the main color of the pants", -1],
            ["what color are the pants", -1]
        ],
        "context": [
            "a man and a little boy playing with frisbees.",
            "a woman playing a video game in a living room."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2403836",
                "VG_object_id": "348958",
                "bbox": [151, 212, 329, 357],
                "image": "data\\images\\2403836.jpg"
            },
            {
                "VG_image_id": "2411937",
                "VG_object_id": "206501",
                "bbox": [104, 104, 273, 195],
                "image": "data\\images\\2411937.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the ground covered with", 1],
            ["what is the dog doing", 1],
            ["what is the color of the background", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["how many animals are there", -1],
            ["What is the dog wearing on its head", -1],
            ["what is the dog doing", 1],
            ["what is the color of the background", 1],
            ["what gesture is the dog", -1],
            ["what is the dog wearing", -1],
            ["when was this photo taken", -1],
            ["what type of animal is shown", -1],
            ["where is the dog", -1],
            ["what is the dog looking at", -1],
            ["what is around the dog's neck", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a dog and a horse standing next to each other.",
            "a dog laying in the grass with a frisbee."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2372272",
                "VG_object_id": "592806",
                "bbox": [0, 163, 423, 374],
                "image": "data\\images\\2372272.jpg"
            },
            {
                "VG_image_id": "2345499",
                "VG_object_id": "907734",
                "bbox": [2, 347, 373, 500],
                "image": "data\\images\\2345499.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what color is the ground", 2],
            ["what is on the table", 1],
            ["what is the main color of the rug", 1]
        ],
        "org_questions": [
            ["what is on the table", 1],
            ["what color is the table", 2],
            ["what color is the ground", 2],
            ["how many people are there in the photo", -1],
            ["Which room of a house is it", -1],
            ["what is the floor made of", -1],
            ["what is the main color of the rug", 1],
            ["WHat is on the rug", -1],
            ["where is the carpet", -1],
            ["what type of flooring is shown", -1],
            ["what is covering the floor", -1],
            ["what kind of floor is in the room", -1]
        ],
        "context": [
            "a living room with a rug, chairs, and a table.",
            "a living room with a couch, coffee table, and a window."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2352751",
                "VG_object_id": "1736898",
                "bbox": [258, 132, 331, 427],
                "image": "data\\images\\2352751.jpg"
            },
            {
                "VG_image_id": "2380076",
                "VG_object_id": "548002",
                "bbox": [31, 94, 225, 386],
                "image": "data\\images\\2380076.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the tie", 2],
            ["what is the pattern of the tie", 2],
            ["what color is the man's shirt", 1]
        ],
        "org_questions": [
            ["what color is the tie", 2],
            ["what is the pattern of the tie", 2],
            ["how many people are there", -1],
            ["where is the man", -1],
            ["what is the gender of the person", -1],
            ["what color is the man's shirt", 1],
            ["who is wearing a tie", -1],
            ["what is around the man's neck", -1],
            ["what is on the tie", -1]
        ],
        "context": [
            "a man in a blue shirt and tie sitting on a couch.",
            "a man wearing a black tie with a white shirt and a sweater."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2403373",
                "VG_object_id": "382039",
                "bbox": [0, 1, 328, 498],
                "image": "data\\images\\2403373.jpg"
            },
            {
                "VG_image_id": "2319343",
                "VG_object_id": "2841078",
                "bbox": [1, 0, 372, 499],
                "image": "data\\images\\2319343.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the wall", 1],
            ["what is on the wall", 1],
            ["what is the pattern of the wall", 1]
        ],
        "org_questions": [
            ["what is the color of the wall", 1],
            ["what is on the wall", 1],
            ["how many toilets are there", -1],
            ["what is the pattern of the wall", 1],
            ["What is wall made of", -1],
            ["how may toilets are there", -1],
            ["where was this photo taken", -1],
            ["what room is this", -1],
            ["what is in the bathroom", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a bathroom with a bathtub, toilet and bathtub.",
            "a bathroom with a toilet and a sink"
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2389709",
                "VG_object_id": "1258119",
                "bbox": [332, 161, 431, 236],
                "image": "data\\images\\2389709.jpg"
            },
            {
                "VG_image_id": "2393524",
                "VG_object_id": "471894",
                "bbox": [136, 23, 228, 124],
                "image": "data\\images\\2393524.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the container", 1],
            ["how many people are there", 1],
            ["where was the photo taken", 1],
            ["what is in the top of the photo", 1],
            ["what is in the bottom right corner", 1]
        ],
        "org_questions": [
            ["what color is the container", 1],
            ["where is the container", -1],
            ["what is beside the container", -1],
            ["how many people are there", 1],
            ["what is the container on", -1],
            ["what is on the counter", -1],
            ["What is in the bowl", -1],
            ["when was the picture taken", -1],
            ["where was the photo taken", 1],
            ["what is in the top of the photo", 1],
            ["what is in the bottom right corner", 1]
        ],
        "context": [
            "a woman selling food on a street cart.",
            "a display case filled with chocolate chip cookies and chocolate chips."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2359142",
                "VG_object_id": "3539914",
                "bbox": [5, 219, 484, 328],
                "image": "data\\images\\2359142.jpg"
            },
            {
                "VG_image_id": "2364664",
                "VG_object_id": "2804836",
                "bbox": [0, 105, 499, 373],
                "image": "data\\images\\2364664.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the grass", 1],
            ["what is the animal on the field", 1],
            ["how many cows are there on the field", 1]
        ],
        "org_questions": [
            ["what animal is in the field", -1],
            ["what color is the grass", 1],
            ["what is on the ground", -1],
            ["How many people are there", -1],
            ["what is the animal on the field", 1],
            ["what is beside the field", -1],
            ["how many cows are there on the field", 1],
            ["where was the photo taken", -1],
            ["what is the fence made of", -1],
            ["where is the fence", -1]
        ],
        "context": [
            "two cows are grazing in a field near a tree.",
            "a brown horse with a mask on its head"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2396778",
                "VG_object_id": "440582",
                "bbox": [22, 240, 328, 479],
                "image": "data\\images\\2396778.jpg"
            },
            {
                "VG_image_id": "2361997",
                "VG_object_id": "3752355",
                "bbox": [0, 356, 416, 498],
                "image": "data\\images\\2361997.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many pizzas are there", 2]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["how many pizzas are there", 2],
            ["what food is on the plate", -1],
            ["what is the table made of", -1],
            ["what is on the table", -1],
            ["what is on the plate", -1],
            ["where was this photo taken", -1],
            ["what shape is the pizza", -1],
            ["what is the pizza sitting on", -1],
            ["where is the pizza", -1],
            ["what kind of pizza is on the table", -1]
        ],
        "context": [
            "a woman sitting at a table with two pizzas.",
            "a woman sitting at a table with a pizza."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2350198",
                "VG_object_id": "2750857",
                "bbox": [279, 265, 396, 331],
                "image": "data\\images\\2350198.jpg"
            },
            {
                "VG_image_id": "2396949",
                "VG_object_id": "438995",
                "bbox": [251, 290, 494, 375],
                "image": "data\\images\\2396949.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many stopcocks are there in the picture", 1],
            ["what color is the table under the sink", 1]
        ],
        "org_questions": [
            ["what color is the sink", -1],
            ["how many stopcocks are there in the picture", 1],
            ["what shape is the sink", -1],
            ["where is the sink", -1],
            ["what is above the sink", -1],
            ["what is beside the sink", -1],
            ["what color is the table under the sink", 1],
            ["what room is this", -1],
            ["what is the faucet made of", -1],
            ["what is on the counter", -1],
            ["what is next to the sink", -1],
            ["how many sinks are there", -1]
        ],
        "context": [
            "a bathroom with a sink, mirror, and a mirror.",
            "a bathroom with a sink, mirror and toilet."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2354723",
                "VG_object_id": "3776371",
                "bbox": [0, 183, 496, 330],
                "image": "data\\images\\2354723.jpg"
            },
            {
                "VG_image_id": "2340314",
                "VG_object_id": "3225329",
                "bbox": [0, 281, 331, 498],
                "image": "data\\images\\2340314.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 2],
            ["what is in the background", 2],
            ["what is on the land", 1],
            ["where is the picture taken", 1],
            ["what kind of animal is on the land", 1],
            ["what is the land made of", 1],
            ["What kind of animal is there", 1],
            ["what is the weather like", 1]
        ],
        "org_questions": [
            ["what color is the land", 2],
            ["what is on the land", 1],
            ["where is the picture taken", 1],
            ["How many people are there", -1],
            ["what kind of animal is on the land", 1],
            ["what is the land made of", 1],
            ["What kind of animal is there", 1],
            ["when was the photo taken", -1],
            ["what is the weather like", 1],
            ["what is in the background", 2]
        ],
        "context": [
            "a herd of cows laying on top of a lush green field.",
            "a boat on the beach with a cloudy sky in the background."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2322911",
                "VG_object_id": "3011434",
                "bbox": [250, 197, 312, 287],
                "image": "data\\images\\2322911.jpg"
            },
            {
                "VG_image_id": "2413467",
                "VG_object_id": "173043",
                "bbox": [5, 196, 139, 477],
                "image": "data\\images\\2413467.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cabinet", 2],
            ["how many drawers does the cabinet have", 2],
            ["how many drawers are there", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", 2],
            ["how many drawers does the cabinet have", 2],
            ["What is on the cabinet", -1],
            ["what color is the floor under the cabinet", -1],
            ["what is the color of the floor", -1],
            ["what is the floor made of", -1],
            ["where is the picture taken", -1],
            ["how many drawers are there", 1]
        ],
        "context": [
            "a kitchen with a sink, stove, and cabinets.",
            "a white cabinet with a silver handle and a white cabinet"
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2387343",
                "VG_object_id": "1276832",
                "bbox": [43, 55, 496, 365],
                "image": "data\\images\\2387343.jpg"
            },
            {
                "VG_image_id": "2363170",
                "VG_object_id": "2579390",
                "bbox": [134, 271, 319, 397],
                "image": "data\\images\\2363170.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many pots are there", 1],
            ["what is the main color of the food", 1],
            ["what type of food is on the plate", 1]
        ],
        "org_questions": [
            ["What is on the plate", -1],
            ["What color is the table", -1],
            ["How many pots are there", 1],
            ["where is the food placed on", -1],
            ["what is the container made of", -1],
            ["what color is the plate under the food", -1],
            ["what is the main color of the food", 1],
            ["what shape is the plate", -1],
            ["what type of food is on the plate", 1],
            ["what is the plate on", -1],
            ["what is next to the plate", -1],
            ["where is the plate", -1]
        ],
        "context": [
            "a plate of food with a cup of coffee.",
            "a table with plates of food and drinks on it."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2398840",
                "VG_object_id": "1175112",
                "bbox": [121, 94, 236, 471],
                "image": "data\\images\\2398840.jpg"
            },
            {
                "VG_image_id": "713266",
                "VG_object_id": "1581363",
                "bbox": [483, 209, 706, 406],
                "image": "data\\images\\713266.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 1],
            ["what color are the boy's clothes", 1],
            ["where is the boy", 1],
            ["what is the boy holding", 1],
            ["what is in the background", 1],
            ["what is the child doing", 1],
            ["how many children are there ", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 1],
            ["what color are the boy's clothes", 1],
            ["where is the boy", 1],
            ["What is the boy wearing on his head", -1],
            ["what is the boy holding", 1],
            ["what is in the background", 1],
            ["what is the child doing", 1],
            ["how many children are there ", 1],
            ["what is the gender of the person in the picture", -1],
            ["when was the photo taken", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a young boy holding an umbrella on a street.",
            "a group of people riding in a raft on a river."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2332640",
                "VG_object_id": "3711269",
                "bbox": [0, 0, 496, 369],
                "image": "data\\images\\2332640.jpg"
            },
            {
                "VG_image_id": "2394602",
                "VG_object_id": "462705",
                "bbox": [177, 230, 499, 375],
                "image": "data\\images\\2394602.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 1],
            ["What is on the table", 1],
            ["How many people are there", 1],
            ["how many chairs are there beside the table", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["What is on the table", 1],
            ["How many people are there", 1],
            ["what is the table made of", -1],
            ["how many chairs are there beside the table", 1],
            ["what is on the plate", -1],
            ["where was this photo taken", -1],
            ["what is the table sitting on", -1],
            ["how is the table made", -1]
        ],
        "context": [
            "a hamburger and fries are on a plate.",
            "a man in a red shirt"
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2316902",
                "VG_object_id": "2993993",
                "bbox": [4, 205, 497, 331],
                "image": "data\\images\\2316902.jpg"
            },
            {
                "VG_image_id": "2359055",
                "VG_object_id": "2200297",
                "bbox": [9, 204, 329, 498],
                "image": "data\\images\\2359055.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of trousers is the man wearing", 1],
            ["what color are the man's trousers", 1],
            ["how many people are there on the ground", 1],
            ["what is on the side of the street", 1],
            ["What is next to the street", 1]
        ],
        "org_questions": [
            ["what kind of trousers is the man wearing", 1],
            ["what color are the man's trousers", 1],
            ["how many people are there on the ground", 1],
            ["what time is it", -1],
            ["what is on the side of the street", 1],
            ["When is photo taken", -1],
            ["What is next to the street", 1],
            ["where was this picture taken", -1],
            ["what is the man doing", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a boy riding a skateboard down a street."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2343823",
                "VG_object_id": "2197968",
                "bbox": [92, 48, 422, 336],
                "image": "data\\images\\2343823.jpg"
            },
            {
                "VG_image_id": "2334992",
                "VG_object_id": "3096387",
                "bbox": [275, 109, 454, 254],
                "image": "data\\images\\2334992.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many zebras are there", 2],
            ["what color is the grass", 1],
            ["what is the main color of the background", 1]
        ],
        "org_questions": [
            ["How many zebras are there", 2],
            ["where is the zebra", -1],
            ["what color is the grass", 1],
            ["What is zebra doing", -1],
            ["what is in the distance", -1],
            ["what is the main color of the background", 1],
            ["when was the picture taken", -1],
            ["what kind of animal is in the picture", -1],
            ["what are the zebras standing on", -1],
            ["what is behind the zebra", -1],
            ["where are the zebras standing", -1]
        ],
        "context": [
            "a zebra standing in tall grass next to a tree.",
            "a group of zebras grazing in a field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2369858",
                "VG_object_id": "3861304",
                "bbox": [135, 51, 306, 179],
                "image": "data\\images\\2369858.jpg"
            },
            {
                "VG_image_id": "2328330",
                "VG_object_id": "2980151",
                "bbox": [221, 121, 285, 231],
                "image": "data\\images\\2328330.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["Where is the man", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["how many people are in the picture", -1],
            ["what is the man doing", 1],
            ["what is the gender of the person", -1],
            ["Where is the man", 1],
            ["how many shirts are in the picture", -1],
            ["how many people are there", -1],
            ["what is on the man's head", -1],
            ["when was the photo taken", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man is shaving a sheep on the ground.",
            "a man is playing frisbee on the beach."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2354514",
                "VG_object_id": "1750852",
                "bbox": [401, 47, 499, 138],
                "image": "data\\images\\2354514.jpg"
            },
            {
                "VG_image_id": "2393155",
                "VG_object_id": "1222452",
                "bbox": [110, 167, 380, 374],
                "image": "data\\images\\2393155.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["where was this picture taken", 2],
            ["what color is the seat", 1],
            ["where is the seat", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the seat", 1],
            ["where is the seat", 1],
            ["how many people are there in the photo", 2],
            ["how many seats are there", -1],
            ["when was the photo taken", -1],
            ["how is the weather", -1],
            ["who is in the photo", 1],
            ["where was this picture taken", 2]
        ],
        "context": [
            "two tennis players talking to each other on a tennis court.",
            "a bus with a bus seat and a sign on the window."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2382762",
                "VG_object_id": "1326341",
                "bbox": [2, 229, 500, 280],
                "image": "data\\images\\2382762.jpg"
            },
            {
                "VG_image_id": "2394299",
                "VG_object_id": "465376",
                "bbox": [3, 360, 368, 499],
                "image": "data\\images\\2394299.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are on the field", 2],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["how many people are on the field", 2],
            ["what are the people doing", -1],
            ["what is in the background", -1],
            ["what is the color of the grass", -1],
            ["what are the people doing on the field", -1],
            ["How many trains are there", -1],
            ["where was this photo taken", 1],
            ["what are the people standing on", -1],
            ["where is the grass", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a group of people playing soccer in a field.",
            "a man flying a kite in a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2386634",
                "VG_object_id": "517566",
                "bbox": [209, 87, 363, 311],
                "image": "data\\images\\2386634.jpg"
            },
            {
                "VG_image_id": "2398823",
                "VG_object_id": "1175306",
                "bbox": [174, 189, 327, 331],
                "image": "data\\images\\2398823.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of animal is it", 2],
            ["what color is the man's shirt", 1],
            ["what animal is the man with", 1],
            ["what are the people doing", 1],
            ["What is man doing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what animal is the man with", 1],
            ["How many people are there", -1],
            ["what are the people doing", 1],
            ["where is the photo taken", -1],
            ["where is the person", -1],
            ["What is man doing", 1],
            ["what is on the man's head", -1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is the man standing on", -1],
            ["what is the man holding", 1],
            ["what kind of animal is it", 2]
        ],
        "context": [
            "a man is playing with two elephants in the wild.",
            "a man is petting a giraffe at a zoo."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2339296",
                "VG_object_id": "951875",
                "bbox": [237, 106, 418, 213],
                "image": "data\\images\\2339296.jpg"
            },
            {
                "VG_image_id": "2351395",
                "VG_object_id": "3412795",
                "bbox": [73, 202, 140, 341],
                "image": "data\\images\\2351395.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person in the picture doing", 1],
            ["what color is the person's shirt", 1],
            ["who is in the photo", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", -1],
            ["what is the person in the picture doing", 1],
            ["what color is the person's shirt", 1],
            ["how many people are there", -1],
            ["where is the cabinet", -1],
            ["what is in the background", -1],
            ["what is on the wall", -1],
            ["who is in the photo", 1],
            ["what is the persion wearing", -1],
            ["where is the picture taken", -1],
            ["what is the persion doing", 1]
        ],
        "context": [
            "a woman playing a game with a wii controller.",
            "a man is standing in a kitchen with a chocolate cake."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2362696",
                "VG_object_id": "771996",
                "bbox": [345, 98, 405, 142],
                "image": "data\\images\\2362696.jpg"
            },
            {
                "VG_image_id": "2413915",
                "VG_object_id": "163597",
                "bbox": [289, 206, 372, 251],
                "image": "data\\images\\2413915.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cows are there in the picture", 2],
            ["what are the cows doing", 1]
        ],
        "org_questions": [
            ["how many cows are there in the picture", 2],
            ["what is in the background", -1],
            ["what color is the cow", -1],
            ["where is the cow", -1],
            ["What is near to the cow", -1],
            ["what color is the ground the cow standing on", -1],
            ["what color is the cow's head", -1],
            ["What color is the ground", -1],
            ["what kind of animal is in the picture", -1],
            ["when was this picture taken", -1],
            ["what are the cows doing", 1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a dog is sitting in a field with a cart and a dog.",
            "a woman sitting on a hill looking at a cow laying on the grass."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2391708",
                "VG_object_id": "1238273",
                "bbox": [321, 26, 436, 128],
                "image": "data\\images\\2391708.jpg"
            },
            {
                "VG_image_id": "2383001",
                "VG_object_id": "535702",
                "bbox": [24, 150, 87, 207],
                "image": "data\\images\\2383001.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the ground the elephants standing on made of", 1],
            ["where is the elephant", 1],
            ["what are the elephants standing on", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["how many elephants are there", -1],
            ["what is the ground the elephants standing on made of", 1],
            ["what color is the ground the elephants standing on", -1],
            ["What is elephant doing", -1],
            ["where is the elephant", 1],
            ["what is in the distance", -1],
            ["what are the elephants standing on", 1],
            ["what is on the elephants", -1],
            ["when was this picture taken", -1],
            ["what animal is in the photo", -1],
            ["what is the animal", -1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a white truck driving down a road next to a herd of elephants.",
            "a group of people standing around a watering hole."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2331332",
                "VG_object_id": "3319901",
                "bbox": [50, 246, 351, 486],
                "image": "data\\images\\2331332.jpg"
            },
            {
                "VG_image_id": "2332874",
                "VG_object_id": "3207340",
                "bbox": [181, 91, 356, 172],
                "image": "data\\images\\2332874.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the cow", 2],
            ["How many cows are there", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["How many cows are there", 1],
            ["What is cow doing", -1],
            ["What color is the cow", 2],
            ["where is the cow", -1],
            ["what is the weather like", -1],
            ["what is in the distance", 1],
            ["what is the cow doing ", -1],
            ["when was the picture taken", -1],
            ["what kind of animal is in the picture", -1],
            ["what is on the ground", -1],
            ["what animal is shown", -1]
        ],
        "context": [
            "a man sitting on a bed next to a cow.",
            "a man and two cows standing next to a pond."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2355493",
                "VG_object_id": "2625190",
                "bbox": [79, 260, 217, 320],
                "image": "data\\images\\2355493.jpg"
            },
            {
                "VG_image_id": "2345826",
                "VG_object_id": "2292749",
                "bbox": [179, 92, 299, 166],
                "image": "data\\images\\2345826.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there", 2],
            ["what time is it", 2],
            ["what color is the sky", 1],
            ["which part of the car can we see in the picture", 1],
            ["when was the photo taken", 1],
            ["what is parked on the street", 1],
            ["what is on the side of the road", 1]
        ],
        "org_questions": [
            ["how many cars are there", 2],
            ["what time is it", 2],
            ["what is in front of the car", -1],
            ["what color is the sky", 1],
            ["what is the weather like", -1],
            ["what is the pattern on the car", -1],
            ["which part of the car can we see in the picture", 1],
            ["What is the background of photo", -1],
            ["when was the photo taken", 1],
            ["what is parked on the street", 1],
            ["what is on the side of the road", 1]
        ],
        "context": [
            "a truck is driving down the street in front of a large building.",
            "a row of parked bicycles on a city street."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2368594",
                "VG_object_id": "1750550",
                "bbox": [123, 153, 338, 245],
                "image": "data\\images\\2368594.jpg"
            },
            {
                "VG_image_id": "2391401",
                "VG_object_id": "489364",
                "bbox": [1, 404, 79, 499],
                "image": "data\\images\\2391401.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the person holding", 1],
            ["who is wearing the trousers", 1],
            ["what is on the ground", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["what is the person holding", 1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what gender is the person in the trouser", -1],
            ["who is wearing the trousers", 1],
            ["when is the picture taken", -1],
            ["what is on the ground", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a baby sitting on a skateboard next to a skateboard.",
            "a group of young men walking down a street holding umbrellas."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2347108",
                "VG_object_id": "2007893",
                "bbox": [58, 45, 194, 373],
                "image": "data\\images\\2347108.jpg"
            },
            {
                "VG_image_id": "2364741",
                "VG_object_id": "3888081",
                "bbox": [132, 48, 301, 381],
                "image": "data\\images\\2364741.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 2],
            ["How many people are there", 1],
            ["how is the weather", 1],
            ["where is the man", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what color is the man's shirt", 2],
            ["what is the man holding", 2],
            ["How many people are there", 1],
            ["what is the man wearing on the head", -1],
            ["how is the weather", 1],
            ["where is the man", 1],
            ["what kind of coat is the man wearing", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what is on the man's face", 1]
        ],
        "context": [
            "a man standing on a bridge holding an umbrella.",
            "a man and a woman standing next to a motorcycle."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2353788",
                "VG_object_id": "3576887",
                "bbox": [104, 188, 211, 376],
                "image": "data\\images\\2353788.jpg"
            },
            {
                "VG_image_id": "2361873",
                "VG_object_id": "2793418",
                "bbox": [48, 201, 100, 313],
                "image": "data\\images\\2361873.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["where is the woman", 1],
            ["what is the woman holding", 1],
            ["what gesture is the woman", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what are the people doing", -1],
            ["where is the woman", 1],
            ["what is the woman holding", 1],
            ["what gesture is the woman", 1],
            ["What is woman doing", -1],
            ["what time of day is it", -1],
            ["who is in the photo", -1],
            ["what are the people standing on", -1],
            ["what are the people wearing", -1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a girl walking in a line of buses.",
            "a crowd of people standing around a plane."
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2317251",
                "VG_object_id": "3215981",
                "bbox": [87, 257, 450, 329],
                "image": "data\\images\\2317251.jpg"
            },
            {
                "VG_image_id": "2340314",
                "VG_object_id": "2745682",
                "bbox": [49, 276, 274, 442],
                "image": "data\\images\\2340314.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boat", 1],
            ["what is the boat on", 1],
            ["how many people are on the boat", 1],
            ["where is the boat", 1],
            ["what is on the boat", 1],
            ["What is the color of boat", 1],
            ["what kind of boat is this", 1],
            ["what is behind the boat", 1]
        ],
        "org_questions": [
            ["what color is the boat", 1],
            ["what is the boat on", 1],
            ["how many people are on the boat", 1],
            ["where is the boat", 1],
            ["what is the boat doing", -1],
            ["what is on the boat", 1],
            ["how many boats are there", -1],
            ["What is the color of boat", 1],
            ["when was the photo taken", -1],
            ["what kind of boat is this", 1],
            ["what is the boat made of", -1],
            ["what is behind the boat", 1]
        ],
        "context": [
            "a group of people standing on a raft.",
            "a boat on the beach with a cloudy sky in the background."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2383530",
                "VG_object_id": "532050",
                "bbox": [256, 112, 336, 227],
                "image": "data\\images\\2383530.jpg"
            },
            {
                "VG_image_id": "2374597",
                "VG_object_id": "725861",
                "bbox": [160, 99, 271, 205],
                "image": "data\\images\\2374597.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is boy doing", 2],
            ["What color is the boy's shirt", 1],
            ["WHat color is boy's trouser", 1],
            ["What is man holding", 1],
            ["What sports is man doing", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["What color is the boy's shirt", 1],
            ["WHat color is boy's trouser", 1],
            ["What is boy doing", 2],
            ["where is the person", -1],
            ["What is man holding", 1],
            ["What sports is man doing", 1],
            ["how many people are in the photo", 1],
            ["when was this photo taken", -1],
            ["what type of shirt is the boy wearing", -1]
        ],
        "context": [
            "a young boy swinging a bat at a ball.",
            "a boy is doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2351623",
                "VG_object_id": "2416653",
                "bbox": [268, 104, 320, 182],
                "image": "data\\images\\2351623.jpg"
            },
            {
                "VG_image_id": "2416293",
                "VG_object_id": "3053914",
                "bbox": [121, 97, 312, 295],
                "image": "data\\images\\2416293.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color are the man's pants", 1],
            ["what is the color of the background", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color are the man's pants", 1],
            ["how many people are there", -1],
            ["What is man doing", -1],
            ["where is the person", -1],
            ["what is the color of the background", 1],
            ["who is in the water", -1],
            ["what is the man standing on", -1],
            ["what is the man wearing", -1],
            ["what is the man riding", -1],
            ["who is surfing", -1]
        ],
        "context": [
            "a man riding a surfboard on top of a wave.",
            "a man riding a surfboard on a wave in the ocean."
        ]
    },
    {
        "object_category": "banana",
        "images": [
            {
                "VG_image_id": "2372166",
                "VG_object_id": "2512703",
                "bbox": [116, 27, 350, 115],
                "image": "data\\images\\2372166.jpg"
            },
            {
                "VG_image_id": "2371490",
                "VG_object_id": "1879227",
                "bbox": [279, 203, 374, 323],
                "image": "data\\images\\2371490.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the bananas", 2],
            ["where are the bananas", 2],
            ["what fruit is in the picture besides banana", 1],
            ["what is the banana sitting on", 1],
            ["where is the banana placing", 1],
            ["what is in the background", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color are the bananas", 2],
            ["where are the bananas", 2],
            ["how many bananas are there", -1],
            ["what fruit is in the picture besides banana", 1],
            ["what is the banana sitting on", 1],
            ["where is the banana placing", 1],
            ["when was the picture taken", -1],
            ["what is in the photo", -1],
            ["what kind of fruit is in the basket", -1],
            ["what is in the background", 1],
            ["how many people are in the photo", 1]
        ],
        "context": [
            "a woman with a tray of food on her head.",
            "a man standing in a grocery store holding a bunch of oranges."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2330113",
                "VG_object_id": "2864392",
                "bbox": [298, 166, 445, 487],
                "image": "data\\images\\2330113.jpg"
            },
            {
                "VG_image_id": "2333848",
                "VG_object_id": "2765708",
                "bbox": [270, 65, 389, 300],
                "image": "data\\images\\2333848.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 2],
            ["what is the person wearing", 2],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what are the people doing", 1],
            ["what type of pants is the man wearing", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what is the weather like", 2],
            ["what is the man wearing on the head", -1],
            ["where is the photo taken", 1],
            ["what are the people doing", 1],
            ["what is the man holding", -1],
            ["who is wearing a hat", -1],
            ["what type of pants is the man wearing", 1],
            ["when was the picture taken", -1],
            ["what is the person wearing", 2],
            ["who is in the photo", -1]
        ],
        "context": [
            "a group of people standing on a sidewalk holding umbrellas.",
            "a man sitting on a bench with a skateboard."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2417044",
                "VG_object_id": "3135465",
                "bbox": [102, 261, 469, 363],
                "image": "data\\images\\2417044.jpg"
            },
            {
                "VG_image_id": "2366183",
                "VG_object_id": "2393173",
                "bbox": [0, 330, 500, 386],
                "image": "data\\images\\2366183.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of animal is on the land", 1],
            ["how many animals are on the land", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["what kind of animal is on the land", 1],
            ["how many animals are on the land", 1],
            ["what color is the ground", -1],
            ["what is in the distance", 1],
            ["where is the land", -1],
            ["what is the ground covered with", -1],
            ["what is on the ground", -1],
            ["how is the weather", -1],
            ["what is green", -1],
            ["what is the weather like", -1],
            ["how is the grass", -1],
            ["what is the ground color", -1]
        ],
        "context": [
            "two bears laying on the ground in the woods.",
            "a giraffe standing next to a brick building."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2364016",
                "VG_object_id": "760234",
                "bbox": [5, 320, 375, 499],
                "image": "data\\images\\2364016.jpg"
            },
            {
                "VG_image_id": "2365460",
                "VG_object_id": "1712909",
                "bbox": [22, 373, 497, 499],
                "image": "data\\images\\2365460.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the rug", 1],
            ["what is on the ground", 1],
            ["what color is the wall", 1],
            ["how many people are there in the photo", 1],
            ["what is on the rug", 1],
            ["what is the color of the floor", 1]
        ],
        "org_questions": [
            ["what color is the rug", 1],
            ["what is on the ground", 1],
            ["what color is the wall", 1],
            ["how many people are there in the photo", 1],
            ["where is the rug", -1],
            ["what is the ground covered with", -1],
            ["what is on the rug", 1],
            ["what is the color of the floor", 1],
            ["how is the floor made", -1],
            ["what type of floor is this", -1],
            ["what material is the floor made of", -1]
        ],
        "context": [
            "a man sitting on a blue bench wearing headphones.",
            "a bathroom with a bathtub and a toilet."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2374685",
                "VG_object_id": "1713174",
                "bbox": [232, 140, 462, 311],
                "image": "data\\images\\2374685.jpg"
            },
            {
                "VG_image_id": "2355789",
                "VG_object_id": "1041819",
                "bbox": [69, 81, 311, 281],
                "image": "data\\images\\2355789.jpg"
            }
        ],
        "questions_with_scores": [["What color is the bus", 1]],
        "org_questions": [
            ["What color is the bus", 1],
            ["How many decks does the bus have", -1],
            ["What color is the sky in the background", -1],
            ["When is the picture taken", -1],
            ["what is in the background", -1],
            ["where is the bus", -1],
            ["how many people are there in front of the bus", -1],
            ["when is this photo taken ", -1],
            ["what is the bus doing", -1],
            ["who is driving the bus", -1],
            ["what type of bus is this", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a double decker bus driving down a street.",
            "a bus driving down a street next to a tree."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2393388",
                "VG_object_id": "3824739",
                "bbox": [25, 250, 173, 411],
                "image": "data\\images\\2393388.jpg"
            },
            {
                "VG_image_id": "2390334",
                "VG_object_id": "1251414",
                "bbox": [145, 264, 196, 375],
                "image": "data\\images\\2390334.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["what are the people doing", 1],
            ["Where is man ", 1],
            ["what in the background", 1],
            ["what is on the man's head", 1],
            ["who is in the picture", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing on the head", 1],
            ["what is in the background", 1],
            ["What is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the trouser", -1],
            ["how many people are in the picture", 1],
            ["what are the people doing", 1],
            ["Where is man ", 1],
            ["what in the background", 1],
            ["what type of pants is the man wearing", -1],
            ["what is on the man's head", 1],
            ["who is in the picture", 1],
            ["what is the man wearing", -1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing on the head", 1],
            ["who is wearing the trousers", -1],
            ["what is in the background", 1],
            ["What is the man doing", 1]
        ],
        "context": [
            "a man wearing a white shirt and black pants.",
            "a group of people posing for a picture."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2334207",
                "VG_object_id": "2874227",
                "bbox": [133, 445, 183, 496],
                "image": "data\\images\\2334207.jpg"
            },
            {
                "VG_image_id": "2330113",
                "VG_object_id": "3461891",
                "bbox": [104, 172, 159, 369],
                "image": "data\\images\\2330113.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 2],
            ["what color is the woman's clothes", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["what is the weather like", 2],
            ["what is the woman wearing", -1],
            ["what color is the woman's clothes", 1],
            ["what is the persion sitting on", 1],
            ["what gesture is the woman", -1],
            ["What is woman wearing in the face", -1],
            ["What is the woman wearing on her head", -1],
            ["when was this picture taken", -1],
            ["how many people are shown", -1],
            ["where are the people", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a clock tower with a clock on top of it.",
            "a group of people standing on a sidewalk holding umbrellas."
        ]
    },
    {
        "object_category": "keyboard",
        "images": [
            {
                "VG_image_id": "2352607",
                "VG_object_id": "2322478",
                "bbox": [71, 249, 196, 324],
                "image": "data\\images\\2352607.jpg"
            },
            {
                "VG_image_id": "2339271",
                "VG_object_id": "951905",
                "bbox": [135, 263, 360, 362],
                "image": "data\\images\\2339271.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many screens are there in the picture", 2],
            ["how many people are there", 1],
            ["what color is the table", 1],
            ["how many people are there in the picture", 1],
            ["what is the desk made of", 1]
        ],
        "org_questions": [
            ["what color is the keyboard", -1],
            ["where is the keyboard", -1],
            ["what is on the table", -1],
            ["how many people are there", 1],
            ["how many screens are there in the picture", 2],
            ["what color is the table", 1],
            ["How many keyboards are there", -1],
            ["how many people are there in the picture", 1],
            ["what is the desk made of", 1],
            ["what is under the computer", -1],
            ["what is in front of the computer", -1]
        ],
        "context": [
            "a table with two laptops and a person sitting at it",
            "a computer monitor sitting on top of a glass desk."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2401818",
                "VG_object_id": "395638",
                "bbox": [57, 85, 245, 205],
                "image": "data\\images\\2401818.jpg"
            },
            {
                "VG_image_id": "2414707",
                "VG_object_id": "153467",
                "bbox": [70, 8, 202, 172],
                "image": "data\\images\\2414707.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["how many people are there in the photo", 2],
            ["what is the persion on the left wearing", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what is the man holding", 2],
            ["how many people are there in the photo", 2],
            ["what is the color of the man's shirt", -1],
            ["where is the man", -1],
            ["how long is the man's hair", -1],
            ["what is the man wearing around his neck", -1],
            ["what is the man doing", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion on the left wearing", 1],
            ["what is the man on", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a man in a wet suit surfing in the ocean.",
            "a man riding a surfboard with dogs on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2417251",
                "VG_object_id": "3038758",
                "bbox": [153, 136, 321, 274],
                "image": "data\\images\\2417251.jpg"
            },
            {
                "VG_image_id": "2383082",
                "VG_object_id": "695018",
                "bbox": [16, 102, 235, 374],
                "image": "data\\images\\2383082.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman doing", 2],
            ["How many people are there", 1],
            ["what is the woman holding", 1],
            ["what is the persion sitting on", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is woman's shirt", -1],
            ["What is woman doing", 2],
            ["what is the woman wearing on the head", -1],
            ["Where is the woman", -1],
            ["what is the woman holding", 1],
            ["what is the woman wearing", -1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1],
            ["where was the photo taken", -1],
            ["what is the persion doing", 1]
        ],
        "context": [
            "a woman sitting on a bed giving a thumbs up.",
            "a man and a woman playing a video game."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2367867",
                "VG_object_id": "2929492",
                "bbox": [1, 322, 374, 497],
                "image": "data\\images\\2367867.jpg"
            },
            {
                "VG_image_id": "2364867",
                "VG_object_id": "758474",
                "bbox": [1, 15, 500, 375],
                "image": "data\\images\\2364867.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the table", 2],
            ["how many plates are there on the table", 1],
            ["how many plates are there", 1]
        ],
        "org_questions": [
            ["what is the color of the table", 2],
            ["how many plates are there on the table", 1],
            ["what is on the table", -1],
            ["how many plates are there", 1],
            ["what is the pizza sitting on", -1],
            ["where was this photo taken", -1],
            ["what is under the pizza", -1]
        ],
        "context": [
            "a young man sitting at a table with a pizza.",
            "a pizza on a plate on a table."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2362015",
                "VG_object_id": "3529547",
                "bbox": [1, 170, 484, 372],
                "image": "data\\images\\2362015.jpg"
            },
            {
                "VG_image_id": "2408372",
                "VG_object_id": "263074",
                "bbox": [308, 291, 449, 338],
                "image": "data\\images\\2408372.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["how many elephants are there in the picture", 2],
            ["what color is the ground", 1],
            ["how many elephants are there on the ground", 1],
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["what is the persion doing", 1],
            ["where was this photo taken", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["how many elephants are there on the ground", 1],
            ["how many people are there in the picture", 2],
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["what is the persion doing", 1],
            ["where was this photo taken", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["what is the man standing on", 1],
            ["how many elephants are there in the picture", 2]
        ],
        "context": [
            "two elephants standing in a dirt field with people watching.",
            "a young boy riding a skateboard down a street."
        ]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2395510",
                "VG_object_id": "453618",
                "bbox": [110, 82, 171, 154],
                "image": "data\\images\\2395510.jpg"
            },
            {
                "VG_image_id": "2390711",
                "VG_object_id": "1247852",
                "bbox": [58, 183, 121, 280],
                "image": "data\\images\\2390711.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 1],
            ["how many traffic lights are there", 1]
        ],
        "org_questions": [
            ["what color is the light", -1],
            ["what is the weather like", 1],
            ["how many people are there", -1],
            ["what room is the light in", -1],
            ["what is in the distance", -1],
            ["where is the light", -1],
            ["what is the traffic light on", -1],
            ["when was the picture taken", -1],
            ["how many traffic lights are there", 1],
            ["what color are the traffic lights", -1],
            ["What is the status of lamp", -1],
            ["what is this a picture of", -1]
        ],
        "context": [
            "a traffic light hanging from a pole with a brick building in the background.",
            "a street sign and a traffic light in a city."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2404728",
                "VG_object_id": "2906626",
                "bbox": [6, 92, 158, 293],
                "image": "data\\images\\2404728.jpg"
            },
            {
                "VG_image_id": "2399018",
                "VG_object_id": "3819577",
                "bbox": [129, 2, 496, 373],
                "image": "data\\images\\2399018.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there in the picture", 2],
            ["how many people are in the picture", 2],
            ["what color is the clothes of the child", 1],
            ["what is the child holding", 1]
        ],
        "org_questions": [
            ["what color is the clothes of the child", 1],
            ["how many children are there in the picture", 2],
            ["what is the child holding", 1],
            ["Where is the man", -1],
            ["What is man doing", -1],
            ["what is the boy wearing", -1],
            ["what is the boy doing", -1],
            ["What color is child's hair", -1],
            ["what is the boy sitting on", -1],
            ["how many people are in the picture", 2],
            ["what is the little boy doing", -1]
        ],
        "context": [
            "three children sitting at a table with plastic cups.",
            "a young boy playing a piano with stuffed animals."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2414628",
                "VG_object_id": "155026",
                "bbox": [84, 194, 286, 257],
                "image": "data\\images\\2414628.jpg"
            },
            {
                "VG_image_id": "1159766",
                "VG_object_id": "1597430",
                "bbox": [67, 134, 193, 212],
                "image": "data\\images\\1159766.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the umbrella", 1],
            ["What is on the the towel", 1],
            ["who is on the towel", 1],
            ["how many people are shown", 1]
        ],
        "org_questions": [
            ["Where is the towel", -1],
            ["What color is the umbrella", 1],
            ["What is on the the towel", 1],
            ["who is on the towel", 1],
            ["where is the picture taken", -1],
            ["how many people are shown", 1],
            ["when was the picture taken", -1],
            ["where are the people", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a man laying on a beach under an umbrella.",
            "a man sitting on the beach under an umbrella."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2345799",
                "VG_object_id": "904682",
                "bbox": [61, 102, 374, 500],
                "image": "data\\images\\2345799.jpg"
            },
            {
                "VG_image_id": "2351889",
                "VG_object_id": "2535183",
                "bbox": [27, 0, 418, 372],
                "image": "data\\images\\2351889.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 1],
            ["what is the man holding", 1],
            ["How many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what is the man wearing", 1],
            ["where is the man", -1],
            ["what is the man holding", 1],
            ["How many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the man doing", 1],
            ["who is in the photo", -1],
            ["what is in the background", -1],
            ["what is the man standing on", -1],
            ["what is on the man's face", -1]
        ],
        "context": [
            "a man in a suit holding two bottles of wine.",
            "a man in a bathroom with his hands on his hips."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2414659",
                "VG_object_id": "154402",
                "bbox": [246, 149, 335, 341],
                "image": "data\\images\\2414659.jpg"
            },
            {
                "VG_image_id": "2317578",
                "VG_object_id": "3790944",
                "bbox": [172, 192, 235, 324],
                "image": "data\\images\\2317578.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", 2],
            ["where is the person", 1],
            ["what color is the person's pants", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", 2],
            ["what is the person doing", -1],
            ["how many shirts are there", -1],
            ["where is the person", 1],
            ["what color is the person's pants", 1],
            ["what is the person wearing", 1]
        ],
        "context": [
            "a man wearing a pink glove on his left hand.",
            "a woman drinking from a glass while standing next to a table."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2372262",
                "VG_object_id": "592843",
                "bbox": [135, 108, 234, 316],
                "image": "data\\images\\2372262.jpg"
            },
            {
                "VG_image_id": "2390283",
                "VG_object_id": "1251937",
                "bbox": [228, 73, 390, 255],
                "image": "data\\images\\2390283.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what color is the wall in the distance", 2],
            ["what color is the wall behind the shirt", 1],
            ["what color is the woman's hair", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what color is the wall behind the shirt", 1],
            ["how many people are there in the photo", -1],
            ["What is woman doing", -1],
            ["what is the gender of the person", -1],
            ["where is the person", -1],
            ["How many people are there", -1],
            ["when was the photo taken", -1],
            ["what is the player wearing", -1],
            ["what color is the woman's hair", 1],
            ["what color is the wall in the distance", 2]
        ],
        "context": [
            "a woman holding a tennis racket on a tennis court.",
            "a woman swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2353472",
                "VG_object_id": "1883097",
                "bbox": [2, 1, 345, 331],
                "image": "data\\images\\2353472.jpg"
            },
            {
                "VG_image_id": "2360141",
                "VG_object_id": "1994317",
                "bbox": [82, 68, 199, 262],
                "image": "data\\images\\2360141.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl holding", 2],
            ["what color is the wall", 1],
            ["how many people are there", 1],
            ["What is girl doing", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["what is the girl holding", 2],
            ["what color is the wall", 1],
            ["how many people are there", 1],
            ["What is girl doing", 1],
            ["what is in the background", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["where was this photo taken", 1]
        ],
        "context": [
            "a little girl brushing her teeth with a toothbrush.",
            "a man and a woman sitting at a table with pizzas."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2376294",
                "VG_object_id": "2602491",
                "bbox": [91, 223, 473, 371],
                "image": "data\\images\\2376294.jpg"
            },
            {
                "VG_image_id": "2358787",
                "VG_object_id": "2282350",
                "bbox": [257, 145, 427, 270],
                "image": "data\\images\\2358787.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the ground", 1]
        ],
        "org_questions": [
            ["what color is the sofa", -1],
            ["what is on the sofa", -1],
            ["what color is the ground", 1],
            ["how many pillows are there on the couch", -1],
            ["what is the floor under the sofa made of", -1],
            ["what is in front of the couch", -1],
            ["where is the picture taken", -1],
            ["how many people are there", 2],
            ["what is next to the couch", -1]
        ],
        "context": [
            "a living room with a couch, chairs, and a television.",
            "two people sitting on a couch in a living room."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2368222",
                "VG_object_id": "619009",
                "bbox": [135, 129, 272, 302],
                "image": "data\\images\\2368222.jpg"
            },
            {
                "VG_image_id": "2375774",
                "VG_object_id": "579475",
                "bbox": [213, 41, 315, 264],
                "image": "data\\images\\2375774.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man standing on", 2],
            ["what is behind the man", 2],
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["What is man doing", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what is the man standing on", 2],
            ["what color is the man's shirt", 1],
            ["what is the man doing", 1],
            ["HOw many people are there", -1],
            ["where is the man", 1],
            ["what is the man wearing", -1],
            ["what color is the ground", -1],
            ["What is man doing", 1],
            ["when was the photo taken", -1],
            ["what is behind the man", 2],
            ["what is on the man's face", 1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a man standing on top of an elephant.",
            "a man standing next to a bicycle with a basket of bananas on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2402588",
                "VG_object_id": "388328",
                "bbox": [129, 59, 249, 259],
                "image": "data\\images\\2402588.jpg"
            },
            {
                "VG_image_id": "2397708",
                "VG_object_id": "431425",
                "bbox": [160, 101, 308, 332],
                "image": "data\\images\\2397708.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the woman sitting", 2],
            ["how is the woman's hair", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's top", -1],
            ["where is the woman sitting", 2],
            ["how many people are there", 1],
            ["what is the woman wearing on her face", -1],
            ["what is the woman doing", -1],
            ["what is the woman wearing", -1],
            ["where is the woman ", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["how is the woman's hair", 2],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a woman sitting on a stone wall using a laptop.",
            "a man and woman sitting on a bench."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2320360",
                "VG_object_id": "3174357",
                "bbox": [13, 391, 349, 498],
                "image": "data\\images\\2320360.jpg"
            },
            {
                "VG_image_id": "2317004",
                "VG_object_id": "2951234",
                "bbox": [2, 82, 498, 373],
                "image": "data\\images\\2317004.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the ground", 1],
            ["what is the ground covered with", 1],
            ["what is the picture taken", 1],
            ["What is on the ground", 1],
            ["what is the land made of", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", 2],
            ["what is the picture taken", 1],
            ["What is on the ground", 1],
            ["how many tennis rackets are there on the ground", -1],
            ["what is the land made of", 1],
            ["when was the picture taken", -1],
            ["where was this picture taken", 1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a skateboard with stickers on it",
            "a man and two children playing with a frisbee."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2346588",
                "VG_object_id": "2073561",
                "bbox": [168, 125, 228, 355],
                "image": "data\\images\\2346588.jpg"
            },
            {
                "VG_image_id": "2317591",
                "VG_object_id": "3701260",
                "bbox": [39, 8, 330, 497],
                "image": "data\\images\\2317591.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the lady doing", 1],
            ["what is the lady wearing", 1],
            ["what is on the lady's face", 1],
            ["what is the woman holding", 1],
            ["what is the woman doing in the picture", 1]
        ],
        "org_questions": [
            ["what is the lady doing", 1],
            ["what is the lady wearing", 1],
            ["what is on the lady's face", 1],
            ["How many people are there", -1],
            ["where is the lady", -1],
            ["what is the woman holding", 1],
            ["what is on the lady's head", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman doing in the picture", 1]
        ],
        "context": [
            "a woman in a pink dress standing in front of a bed.",
            "a woman brushing her teeth with a toothbrush."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2376730",
                "VG_object_id": "570152",
                "bbox": [223, 337, 325, 375],
                "image": "data\\images\\2376730.jpg"
            },
            {
                "VG_image_id": "2383053",
                "VG_object_id": "1324050",
                "bbox": [126, 379, 219, 499],
                "image": "data\\images\\2383053.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the man holding", 2]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["what is the man holding", 2],
            ["how many people are there", -1],
            ["What is person doing", -1],
            ["what is the man doing", -1],
            ["what color is the person's shirt", -1],
            ["when was the picture taken", -1],
            ["what type of pants is the man wearing", -1],
            ["what is the boy wearing", -1]
        ],
        "context": [
            "a man holding a surfboard in front of a tree.",
            "a young boy in a baseball uniform holding a baseball glove."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2354157",
                "VG_object_id": "2462937",
                "bbox": [160, 242, 274, 364],
                "image": "data\\images\\2354157.jpg"
            },
            {
                "VG_image_id": "2398677",
                "VG_object_id": "1177349",
                "bbox": [97, 209, 344, 450],
                "image": "data\\images\\2398677.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many chairs are there", 1],
            ["what is the persion doing", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["How many chairs are there", 1],
            ["what is the persion doing", 1],
            ["what are the people sitting on", -1],
            ["where are the people sitting", -1],
            ["where are the people", 1]
        ],
        "context": [
            "a group of people sitting around a table.",
            "a woman sitting in a chair reading a book."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2377616",
                "VG_object_id": "2564820",
                "bbox": [308, 77, 496, 330],
                "image": "data\\images\\2377616.jpg"
            },
            {
                "VG_image_id": "2404728",
                "VG_object_id": "3814799",
                "bbox": [268, 52, 463, 254],
                "image": "data\\images\\2404728.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the girl", 2],
            ["what is the girl holding", 1],
            ["what color is the girl's clothes", 1],
            ["how many people are there", 1],
            ["what is the color of the girl's hair", 1]
        ],
        "org_questions": [
            ["what is the girl holding", 1],
            ["what color is the girl's clothes", 1],
            ["where is the girl", 2],
            ["how many people are there", 1],
            ["what are the people doing", -1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["what is the persion eating", -1],
            ["what is the color of the girl's hair", 1]
        ],
        "context": [
            "a woman feeding a giraffe a leaf from a leaf.",
            "three children sitting at a table with plastic cups."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2323956",
                "VG_object_id": "2920518",
                "bbox": [86, 110, 181, 267],
                "image": "data\\images\\2323956.jpg"
            },
            {
                "VG_image_id": "2315988",
                "VG_object_id": "3426615",
                "bbox": [31, 135, 341, 439],
                "image": "data\\images\\2315988.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the guy's shirt", 2],
            ["what sport is the guy doing", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["What is guy doing", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["what is the man carrying", 1],
            ["what is the man wearing on his head", 1]
        ],
        "org_questions": [
            ["what color is the guy's shirt", 2],
            ["what sport is the guy doing", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["What is guy doing", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["what is the man carrying", 1],
            ["when was the photo taken", -1],
            ["where was this photo taken", -1],
            ["what is the man wearing on his head", 1],
            ["where is the player", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a man playing tennis on a court"
        ]
    },
    {
        "object_category": "lamp",
        "images": [
            {
                "VG_image_id": "2384975",
                "VG_object_id": "1302585",
                "bbox": [231, 47, 297, 164],
                "image": "data\\images\\2384975.jpg"
            },
            {
                "VG_image_id": "2385835",
                "VG_object_id": "1292483",
                "bbox": [3, 3, 85, 74],
                "image": "data\\images\\2385835.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the lamp standing on", 1],
            ["where are the lights", 1]
        ],
        "org_questions": [
            ["where is the photo taken", -1],
            ["where is the light", -1],
            ["how many lamps are there in the picture", -1],
            ["what color are the lamps", -1],
            ["when is this picture taken", -1],
            ["what is under the lamp", -1],
            ["Where is the lamp standing on", 1],
            ["what is on the wall", -1],
            ["what is on top of the lamp", -1],
            ["what is hanging from the wall", -1],
            ["where are the lights", 1]
        ],
        "context": [
            "a bedroom with a dresser and a lamp.",
            "a refrigerator with a lot of beer bottles on it."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2360408",
                "VG_object_id": "2587975",
                "bbox": [53, 27, 285, 475],
                "image": "data\\images\\2360408.jpg"
            },
            {
                "VG_image_id": "2325358",
                "VG_object_id": "3147814",
                "bbox": [95, 16, 275, 498],
                "image": "data\\images\\2325358.jpg"
            }
        ],
        "questions_with_scores": [["what time is on the clock", 1]],
        "org_questions": [
            ["what color is the tower", -1],
            ["what time is on the clock", 1],
            ["where is the picture taken", -1],
            ["what is the tower made of", -1],
            ["what is the weather like", -1],
            ["when is this picture taken", -1],
            ["what time is it", -1],
            ["how many clocks are there", -1],
            ["what is on top of the building", -1],
            ["where is the clock", -1],
            ["what is on the tower", -1]
        ],
        "context": [
            "a clock tower with a clock on the front.",
            "a tall clock tower with a clock on it's face."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2351529",
                "VG_object_id": "3282704",
                "bbox": [67, 32, 189, 243],
                "image": "data\\images\\2351529.jpg"
            },
            {
                "VG_image_id": "2345524",
                "VG_object_id": "2579958",
                "bbox": [232, 30, 494, 310],
                "image": "data\\images\\2345524.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's hair", 2],
            ["where is the woman", 1],
            ["what is the woman's posture", 1],
            ["how many people are there", 1],
            ["What is woman doing", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["where is the woman", 1],
            ["what is the woman's posture", 1],
            ["what color is the woman's hair", 2],
            ["how many people are there", 1],
            ["What is the gender of the person", -1],
            ["What is woman doing", 1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is the woman holding", 1]
        ],
        "context": [
            "a group of people sitting on a couch playing a video game.",
            "a woman standing on a bed while a man is on a laptop."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2363735",
                "VG_object_id": "1827452",
                "bbox": [21, 33, 452, 333],
                "image": "data\\images\\2363735.jpg"
            },
            {
                "VG_image_id": "2370585",
                "VG_object_id": "3857920",
                "bbox": [6, 58, 464, 291],
                "image": "data\\images\\2370585.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many motorcycles are there", 1],
            ["what is the ground covered with", 1],
            ["what is next to the motorcycle", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many motorcycles are there", 1],
            ["what is the ground covered with", 1],
            ["what is next to the motorcycle", 1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["what is in the background", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a motorcycle parked on the sidewalk outside a building.",
            "person and his motorcycle are racing."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2398158",
                "VG_object_id": "1182116",
                "bbox": [242, 168, 315, 375],
                "image": "data\\images\\2398158.jpg"
            },
            {
                "VG_image_id": "2397113",
                "VG_object_id": "437059",
                "bbox": [182, 56, 353, 500],
                "image": "data\\images\\2397113.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["what color is the woman's shirt", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["what color is the background", 1],
            ["what is the woman carrying", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what is the woman doing", 1],
            ["where is the woman", -1],
            ["how many people are there in the photo", 2],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["what is above the woman", -1],
            ["what color is the background", 1],
            ["when was this photo taken", -1],
            ["who is in the picture", -1],
            ["when was the picture taken", -1],
            ["what is the woman carrying", 1]
        ],
        "context": [
            "a woman standing next to a small airplane.",
            "a woman talking on a cell phone while standing next to a building."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2405399",
                "VG_object_id": "1108148",
                "bbox": [175, 115, 258, 248],
                "image": "data\\images\\2405399.jpg"
            },
            {
                "VG_image_id": "2339187",
                "VG_object_id": "3411595",
                "bbox": [197, 172, 291, 343],
                "image": "data\\images\\2339187.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is carrying the bag", 2],
            ["where is the picture taken", 2],
            ["what color is the bag", 1],
            ["what is the persion doing", 1],
            ["Where is the bag", 1],
            ["How many people are there", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what color is the bag", 1],
            ["who is carrying the bag", 2],
            ["what is the persion doing", 1],
            ["Where is the bag", 1],
            ["what is the bag made of", -1],
            ["How many people are there", 1],
            ["what is the person wearing", 1],
            ["where is the picture taken", 2]
        ],
        "context": [
            "two men standing on a beach holding a surfboard.",
            "a woman taking a picture of a bathroom with a large mirror."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2410379",
                "VG_object_id": "219026",
                "bbox": [226, 65, 288, 192],
                "image": "data\\images\\2410379.jpg"
            },
            {
                "VG_image_id": "2417101",
                "VG_object_id": "3398725",
                "bbox": [68, 178, 173, 277],
                "image": "data\\images\\2417101.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 2],
            ["what sport is it", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["What is man wearing on his head", 1],
            ["where is the person", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what sport is it", 1],
            ["what is the man holding", 2],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the gender of the person", -1],
            ["What is man wearing on his head", 1],
            ["where is the person", 1],
            ["when was the picture taken", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man in a red shirt and white shorts holding a tennis racket.",
            "a man in a yellow shirt catching a frisbee."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2374073",
                "VG_object_id": "729017",
                "bbox": [12, 111, 67, 202],
                "image": "data\\images\\2374073.jpg"
            },
            {
                "VG_image_id": "2334876",
                "VG_object_id": "2504754",
                "bbox": [81, 14, 496, 331],
                "image": "data\\images\\2334876.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the girl", 2],
            ["what is the woman holding", 2],
            ["How many people are there", 1],
            ["What color is girl's shirt", 1],
            ["What is girl doing", 1],
            ["what is in front of the girl", 1],
            ["what is the woman wearing", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["Where is the girl", 2],
            ["what is on the girl's head", -1],
            ["What color is girl's shirt", 1],
            ["What is girl doing", 1],
            ["what is the woman holding", 2],
            ["what is in front of the girl", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["what is the woman wearing", 1],
            ["what is the persion standing on", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man carrying a surfboard on top of a beach.",
            "a woman in a bikini is standing in the water with a surfboard."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2368222",
                "VG_object_id": "619009",
                "bbox": [135, 129, 272, 302],
                "image": "data\\images\\2368222.jpg"
            },
            {
                "VG_image_id": "2357475",
                "VG_object_id": "2232840",
                "bbox": [42, 2, 250, 498],
                "image": "data\\images\\2357475.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["how many people are there", 2],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is behind the man", 1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man standing on top of an elephant.",
            "a man in a red pair of underwear holding a banana."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2319642",
                "VG_object_id": "3228260",
                "bbox": [128, 60, 284, 175],
                "image": "data\\images\\2319642.jpg"
            },
            {
                "VG_image_id": "2344833",
                "VG_object_id": "912713",
                "bbox": [321, 168, 427, 241],
                "image": "data\\images\\2344833.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 2],
            ["What is man doing", 2],
            ["What is the man wearing on his head", 1],
            ["where is this picture taken", 1],
            ["what are the man doing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 2],
            ["What is man doing", 2],
            ["What is the man wearing on his head", 1],
            ["how many people are there", -1],
            ["where is this picture taken", 1],
            ["what gender is the person", -1],
            ["what is on the man's neck", -1],
            ["what are the man doing", 1],
            ["when was the photo taken", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man wearing a red shirt",
            "a man sitting on a bench next to two giraffes."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2356299",
                "VG_object_id": "1942780",
                "bbox": [213, 157, 264, 249],
                "image": "data\\images\\2356299.jpg"
            },
            {
                "VG_image_id": "2373037",
                "VG_object_id": "2606140",
                "bbox": [187, 63, 408, 323],
                "image": "data\\images\\2373037.jpg"
            }
        ],
        "questions_with_scores": [
            ["when is this picture taken", 2],
            ["when was the photo taken", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the guy doing", -1],
            ["when is this picture taken", 2],
            ["how many people are there", 1],
            ["What color is the ground", -1],
            ["what is the man wearing", -1],
            ["what is the person holding", -1],
            ["what sport is the guy doing", -1],
            ["where are the people", -1],
            ["who is in the picture", -1],
            ["when was the photo taken", 2],
            ["what are they doing", -1]
        ],
        "context": [
            "a group of people at a skate park.",
            "a man doing a trick on a skateboard at a skate park."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316263",
                "VG_object_id": "2854928",
                "bbox": [79, 120, 244, 270],
                "image": "data\\images\\2316263.jpg"
            },
            {
                "VG_image_id": "2319229",
                "VG_object_id": "2714398",
                "bbox": [257, 129, 331, 236],
                "image": "data\\images\\2319229.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the man holding", 2],
            ["what is the man doing", 1],
            ["what color is the man;s shirt", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man;s shirt", 1],
            ["how many people are there", 2],
            ["where is the person", -1],
            ["what is in the distance", -1],
            ["what is the weather like", -1],
            ["what is the man holding", 2],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man holding a pole",
            "a man riding a surfboard on top of a wave."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2381842",
                "VG_object_id": "1333671",
                "bbox": [213, 188, 337, 237],
                "image": "data\\images\\2381842.jpg"
            },
            {
                "VG_image_id": "2349866",
                "VG_object_id": "3596624",
                "bbox": [219, 34, 427, 89],
                "image": "data\\images\\2349866.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the shelf", 1],
            ["what is in front of the shelf", 1]
        ],
        "org_questions": [
            ["What color is the shelf", 1],
            ["What is on the shelf", -1],
            ["how many people are in the picture", -1],
            ["where is the shelf", -1],
            ["what is in front of the shelf", 1],
            ["what is the color of the wall", -1],
            ["what room is this", -1],
            ["who is in the photo", -1],
            ["where was the photo taken", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a bathroom with black and white tiles and a sink.",
            "a toilet with a wooden seat in a bathroom."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2375447",
                "VG_object_id": "1895991",
                "bbox": [30, 59, 499, 291],
                "image": "data\\images\\2375447.jpg"
            },
            {
                "VG_image_id": "2411587",
                "VG_object_id": "309682",
                "bbox": [263, 106, 451, 285],
                "image": "data\\images\\2411587.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are sitting on the bench", 2],
            ["what is the bench made of", 1],
            ["what is on the bench", 1],
            ["what is the persion wearing on her head", 1],
            ["What is in the background of image", 1],
            ["when was the picture taken", 1],
            ["what is behind the bench", 1]
        ],
        "org_questions": [
            ["what is the bench made of", 1],
            ["what color is the bench", -1],
            ["what is on the bench", 1],
            ["where was the photo taken", -1],
            ["how many people are sitting on the bench", 2],
            ["what is the persion wearing on her head", 1],
            ["What is in the background of image", 1],
            ["when was the picture taken", 1],
            ["where are the benches", -1],
            ["what is behind the bench", 1]
        ],
        "context": [
            "a person sitting on a bench with a blanket on it.",
            "a bench in the middle of a street at night."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2403273",
                "VG_object_id": "382561",
                "bbox": [1, 2, 357, 490],
                "image": "data\\images\\2403273.jpg"
            },
            {
                "VG_image_id": "2403505",
                "VG_object_id": "1124196",
                "bbox": [3, 1, 374, 497],
                "image": "data\\images\\2403505.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the kitchen", 2],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["how many people are there in the kitchen", 2],
            ["what is on the man's head", 1],
            ["What color is the table", -1],
            ["what is the gender of the person", -1],
            ["who is in the kitchen", -1],
            ["how many sinks are there", -1],
            ["where was this taken", -1],
            ["what are the people doing", -1],
            ["what room is this", -1],
            ["where are the people", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man in a white shirt",
            "a man in a black shirt is making a smoothie."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2317636",
                "VG_object_id": "1018917",
                "bbox": [47, 90, 245, 309],
                "image": "data\\images\\2317636.jpg"
            },
            {
                "VG_image_id": "2380687",
                "VG_object_id": "1343637",
                "bbox": [7, 112, 205, 232],
                "image": "data\\images\\2380687.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the dog", 1],
            ["what is in front of the dog", 1]
        ],
        "org_questions": [
            ["what is the dog doing", -1],
            ["where is the dog", 1],
            ["what is in front of the dog", 1],
            ["how many dogs are there in the picture", -1],
            ["what is the dog wearing", -1],
            ["what gesture is the dog", -1],
            ["what kind of animal is shown", -1],
            ["when was the photo taken", -1],
            ["what is around the dog's neck", -1],
            ["where is the dog looking", -1]
        ],
        "context": [
            "a dog running in the water with a ball in its mouth.",
            "a man riding a skateboard next to a dog."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2345396",
                "VG_object_id": "908406",
                "bbox": [246, 104, 330, 307],
                "image": "data\\images\\2345396.jpg"
            },
            {
                "VG_image_id": "2400924",
                "VG_object_id": "1153552",
                "bbox": [100, 199, 214, 438],
                "image": "data\\images\\2400924.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what sport is the man playing", 1],
            ["what color is the man shirt", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what is the man holding", -1],
            ["how many people are there", 1],
            ["what is the main color of the floor", -1],
            ["Where is the man", -1],
            ["what sport is the man playing", 1],
            ["what color is the man shirt", 1],
            ["when was the photo taken", -1],
            ["who is playing tennis", -1],
            ["what is the man standing on", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "two men standing on a tennis court holding tennis rackets.",
            "a man standing on a tennis court holding a racquet."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2319647",
                "VG_object_id": "3010427",
                "bbox": [1, 241, 498, 368],
                "image": "data\\images\\2319647.jpg"
            },
            {
                "VG_image_id": "2342554",
                "VG_object_id": "2253253",
                "bbox": [7, 170, 495, 326],
                "image": "data\\images\\2342554.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the land", 2],
            ["what is in the distance", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what animals are there", 1],
            ["where was this taken", 1]
        ],
        "org_questions": [
            ["what is color of the ground", -1],
            ["what is on the land", 2],
            ["what is in the distance", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what animals are there", 1],
            ["when was the photo taken", -1],
            ["where was this taken", 1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a horse drawn carriage in front of a city square.",
            "a large jetliner sitting on top of an airport runway."
        ]
    },
    {
        "object_category": "keyboard",
        "images": [
            {
                "VG_image_id": "2346538",
                "VG_object_id": "2447792",
                "bbox": [129, 240, 203, 297],
                "image": "data\\images\\2346538.jpg"
            },
            {
                "VG_image_id": "2327811",
                "VG_object_id": "3071268",
                "bbox": [328, 274, 381, 314],
                "image": "data\\images\\2327811.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the table", 1],
            ["what color is the screen", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["how many laptop are there", -1],
            ["what are the people doing", -1],
            ["what color is the table", 1],
            ["where is the computer", -1],
            ["how many screens are on the table", -1],
            ["what color is the screen", 1],
            ["how many people are there in the picture", 2],
            ["when was this photo taken", -1],
            ["what is on the man's hand", -1],
            ["who is in the photo", -1],
            ["where was this photo taken", 1]
        ],
        "context": [
            "a man sitting at a desk working on a laptop.",
            "a large room full of people with laptops."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2393970",
                "VG_object_id": "468165",
                "bbox": [63, 102, 134, 311],
                "image": "data\\images\\2393970.jpg"
            },
            {
                "VG_image_id": "2346244",
                "VG_object_id": "1888813",
                "bbox": [222, 355, 258, 424],
                "image": "data\\images\\2346244.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is the man on", 1],
            ["where is the photo taken", 1],
            ["when was the picture taken", 1],
            ["what kind of pants is the man wearing", 1],
            ["How many people are there", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color are the man's pants", -1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is the man wearing around his neck", -1],
            ["what is the man on", 1],
            ["what is the man wearing", -1],
            ["where is the photo taken", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", 1],
            ["what kind of pants is the man wearing", 1],
            ["what color is the man's shirt", -1],
            ["How many people are there", 1],
            ["when was the photo taken", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a woman holding a tennis racquet on a tennis court.",
            "a lighthouse is lit up at night."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2362489",
                "VG_object_id": "3890378",
                "bbox": [228, 68, 409, 364],
                "image": "data\\images\\2362489.jpg"
            },
            {
                "VG_image_id": "2324244",
                "VG_object_id": "3158196",
                "bbox": [62, 137, 175, 259],
                "image": "data\\images\\2324244.jpg"
            }
        ],
        "questions_with_scores": [
            ["WHat color is player's shirt", 2],
            ["What color is player's hat", 2],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["WHat color is player's shirt", 2],
            ["What color is player's hat", 2],
            ["how many players are there in the photo", -1],
            ["what sport is the person playing", -1],
            ["what is on the player's head", -1],
            ["what is in the background", -1],
            ["what is the player doing", -1],
            ["what is the man holding", 1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a baseball player throwing a ball on a field."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2406358",
                "VG_object_id": "652487",
                "bbox": [53, 170, 380, 280],
                "image": "data\\images\\2406358.jpg"
            },
            {
                "VG_image_id": "2416645",
                "VG_object_id": "2947332",
                "bbox": [93, 271, 311, 499],
                "image": "data\\images\\2416645.jpg"
            }
        ],
        "questions_with_scores": [["how many cows are in the picture", 2]],
        "org_questions": [
            ["how many cows are in the picture", 2],
            ["what color is the grass", -1],
            ["what color is the ground", -1],
            ["what type of animal is shown", -1],
            ["when was the picture taken", -1],
            ["what are the cows standing on", -1],
            ["what is behind the cows", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a herd of cattle standing on a dirt path.",
            "two cows standing on a hill with trees in the background."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2382253",
                "VG_object_id": "1329931",
                "bbox": [189, 221, 500, 330],
                "image": "data\\images\\2382253.jpg"
            },
            {
                "VG_image_id": "2403280",
                "VG_object_id": "354923",
                "bbox": [49, 401, 290, 499],
                "image": "data\\images\\2403280.jpg"
            }
        ],
        "questions_with_scores": [["what color is the floor", 2]],
        "org_questions": [
            ["what color is the floor", 2],
            ["what color is the wall", -1],
            ["what color is the toilet on the floor", -1],
            ["where is the photo taken", -1],
            ["What is on the floor", -1],
            ["what room is this", -1]
        ],
        "context": [
            "a toilet with a control panel on the back of it.",
            "a bathroom with a toilet and a wooden seat."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2372211",
                "VG_object_id": "593258",
                "bbox": [139, 205, 242, 371],
                "image": "data\\images\\2372211.jpg"
            },
            {
                "VG_image_id": "2393078",
                "VG_object_id": "1223128",
                "bbox": [276, 105, 337, 194],
                "image": "data\\images\\2393078.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trouser", 2],
            ["what color are the pants of the boy in the background", 2],
            ["what is the persion wearing on the head", 1],
            ["What is the background of image", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 2],
            ["where is the person", -1],
            ["what is the persion wearing on the head", 1],
            ["who is wearing the trousers", -1],
            ["What is the man doing", -1],
            ["what is the persion in the shirt holding", -1],
            ["What is the background of image", 1],
            ["when was the photo taken", -1],
            ["what type of pants is the boy wearing", -1],
            ["what is the boy wearing", -1],
            ["what color are the pants of the boy in the background", 2]
        ],
        "context": [
            "a man in green pants and a white shirt riding a skateboard.",
            "a man riding a skateboard on top of a ramp."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2367289",
                "VG_object_id": "623489",
                "bbox": [249, 252, 498, 364],
                "image": "data\\images\\2367289.jpg"
            },
            {
                "VG_image_id": "2342087",
                "VG_object_id": "937477",
                "bbox": [33, 220, 237, 282],
                "image": "data\\images\\2342087.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["what is under the table", 1]
        ],
        "org_questions": [
            ["what is on the table", 1],
            ["where is the table", -1],
            ["how many forks are there", -1],
            ["what color is the background", -1],
            ["how many people are there in the picture", -1],
            ["what main color is the table", -1],
            ["how many glasses are there on the table", -1],
            ["what material is the table made of", -1],
            ["what is under the table", 1],
            ["what is the table made out of", -1]
        ],
        "context": [
            "a living room with a couch, coffee table and a couch.",
            "a train car with a table and chairs"
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2391398",
                "VG_object_id": "3827089",
                "bbox": [27, 243, 317, 373],
                "image": "data\\images\\2391398.jpg"
            },
            {
                "VG_image_id": "2372331",
                "VG_object_id": "592484",
                "bbox": [2, 160, 328, 431],
                "image": "data\\images\\2372331.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the distance", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what is the persion doing", -1],
            ["what is in the distance", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a bench sitting in the grass near a body of water.",
            "a cat playing with a toy on the floor"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2372354",
                "VG_object_id": "2606161",
                "bbox": [129, 331, 282, 497],
                "image": "data\\images\\2372354.jpg"
            },
            {
                "VG_image_id": "2322573",
                "VG_object_id": "2991556",
                "bbox": [276, 211, 498, 373],
                "image": "data\\images\\2322573.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's clothes", 2],
            ["How many people are there", 1],
            ["What color is the wall", 1],
            ["what is the man doing", 1],
            ["what type of shirt is the man wearing", 1],
            ["what is on the man's neck", 1],
            ["what is the man wearing", 1],
            ["what is around the man's neck", 1]
        ],
        "org_questions": [
            ["What color is the man's clothes", 2],
            ["How many people are there", 1],
            ["What color is the wall", 1],
            ["where is the photo taken", -1],
            ["who is wearing the shirt", -1],
            ["what is the man doing", 1],
            ["where is the man", -1],
            ["what type of shirt is the man wearing", 1],
            ["what is on the man's neck", 1],
            ["what is the man wearing", 1],
            ["what is around the man's neck", 1]
        ],
        "context": [
            "a man with a beard and mustache standing in front of a clock.",
            "three men sitting on a couch with their laptops."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2414959",
                "VG_object_id": "148706",
                "bbox": [198, 101, 449, 393],
                "image": "data\\images\\2414959.jpg"
            },
            {
                "VG_image_id": "2365578",
                "VG_object_id": "3883494",
                "bbox": [168, 140, 302, 269],
                "image": "data\\images\\2365578.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["who is wearing the shirt", 1],
            ["where is the man", 1],
            ["what is the person holding", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 2],
            ["how many people are there", -1],
            ["when is this picture taken", -1],
            ["who is wearing the shirt", 1],
            ["where is the man", 1],
            ["what is the person holding", 1],
            ["what is the boy wearing", -1],
            ["when was the photo taken", -1],
            ["what color is the grass", -1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a man in red shirt catching a rugby ball.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2398606",
                "VG_object_id": "662316",
                "bbox": [1, 1, 500, 85],
                "image": "data\\images\\2398606.jpg"
            },
            {
                "VG_image_id": "2320278",
                "VG_object_id": "994543",
                "bbox": [2, 393, 497, 442],
                "image": "data\\images\\2320278.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 2],
            ["what is the table sitting on", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is on the table", 2],
            ["what color is the table", -1],
            ["How many screens are there", -1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["What is the shape of table", -1],
            ["what is the table sitting on", 1],
            ["how is the table made", -1],
            ["what kind of table is this", -1],
            ["what is in the background", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a piece of cake with chocolate frosting and chocolate writing on it.",
            "two small figurines of two praying on a shelf."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2363786",
                "VG_object_id": "3743665",
                "bbox": [4, 188, 484, 331],
                "image": "data\\images\\2363786.jpg"
            },
            {
                "VG_image_id": "2397798",
                "VG_object_id": "430571",
                "bbox": [100, 238, 449, 332],
                "image": "data\\images\\2397798.jpg"
            }
        ],
        "questions_with_scores": [["what room is the floor in", 1]],
        "org_questions": [
            ["what is the floor made of", -1],
            ["what is on the floor", -1],
            ["what room is the floor in", 1],
            ["how many people are there on the floor", -1],
            ["WHere is the window", -1],
            ["what color is the ground", -1],
            ["what is on the ground", -1],
            ["what type of flooring is shown", -1],
            ["what is in the room", -1],
            ["what is covering the floor", -1],
            ["what kind of flooring is on the floor", -1]
        ],
        "context": [
            "a hotel room with a bed, television, and a desk.",
            "a living room with a couch, coffee table, coffee table and a couch."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2322816",
                "VG_object_id": "3513599",
                "bbox": [62, 142, 125, 194],
                "image": "data\\images\\2322816.jpg"
            },
            {
                "VG_image_id": "2322283",
                "VG_object_id": "992502",
                "bbox": [337, 175, 419, 231],
                "image": "data\\images\\2322283.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the pillow", 2],
            ["what is the pillow placed on", 2],
            ["Where is the pillow", 1],
            ["where is the pillow placed on", 1],
            ["where are the pillows", 1]
        ],
        "org_questions": [
            ["what is the color of the pillow", 2],
            ["what is the pillow on", -1],
            ["how many pillows are there", -1],
            ["Where is the pillow", 1],
            ["where is the pillow placed on", 1],
            ["what is the pillow placed on", 2],
            ["where are the pillows", 1]
        ],
        "context": [
            "a living room with a couch, coffee table, television and a couch.",
            "a bedroom with a bed and a window."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2349098",
                "VG_object_id": "1925540",
                "bbox": [257, 146, 356, 221],
                "image": "data\\images\\2349098.jpg"
            },
            {
                "VG_image_id": "2364304",
                "VG_object_id": "641945",
                "bbox": [231, 152, 307, 199],
                "image": "data\\images\\2364304.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the car", 2],
            ["what type of vehicle is shown", 2],
            ["where is the car", 1],
            ["what color is the car", 1],
            ["How many cars are there", 1],
            ["how is the weather", 1],
            ["what is red", 1],
            ["what is the color of the sky", 1]
        ],
        "org_questions": [
            ["where is the car", 1],
            ["what color is the car", 1],
            ["How many cars are there", 1],
            ["what time is it", -1],
            ["what shape is the car", 2],
            ["how is the weather", 1],
            ["when is the photo taken", -1],
            ["what type of vehicle is shown", 2],
            ["what is red", 1],
            ["when was the picture taken", -1],
            ["what is the color of the sky", 1]
        ],
        "context": [
            "a man pushing a baby in a stroller on a sidewalk.",
            "a train traveling through a field next to a field."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2377110",
                "VG_object_id": "1949419",
                "bbox": [35, 265, 496, 364],
                "image": "data\\images\\2377110.jpg"
            },
            {
                "VG_image_id": "2377743",
                "VG_object_id": "1897299",
                "bbox": [0, 285, 499, 372],
                "image": "data\\images\\2377743.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["how many cups are there", 1],
            ["where is the table", 1],
            ["where was the photo taken", 1],
            ["what is in the room", 1],
            ["what is the food on the tray", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 1],
            ["what color is the wall", -1],
            ["how many cups are there", 1],
            ["what shape is the table", -1],
            ["where is the table", 1],
            ["what is the table made of", -1],
            ["where was the photo taken", 1],
            ["what is in the room", 1],
            ["how many bottles are there on the table", -1],
            ["what is the food on the tray", 1],
            ["what pattern is on the table", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a table with a lamp and a lamp on it",
            "a group of young girls eating pizza at a table."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2345987",
                "VG_object_id": "1947844",
                "bbox": [57, 114, 203, 234],
                "image": "data\\images\\2345987.jpg"
            },
            {
                "VG_image_id": "2375465",
                "VG_object_id": "722160",
                "bbox": [173, 140, 351, 255],
                "image": "data\\images\\2375465.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the what color is the person's shirt", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", 1],
            ["what color is the ground", 1],
            ["what is the land made of", 1]
        ],
        "org_questions": [
            ["what color is the what color is the person's shirt", 1],
            ["what is the ground covered with", 1],
            ["what is the person holding", -1],
            ["how many people are there", 1],
            ["what is the person doing", -1],
            ["what color is the ground", 1],
            ["how many horses are there on the photo", -1],
            ["what is the land made of", 1],
            ["where are the horses", -1],
            ["what is on the horse's head", -1],
            ["what animal is shown", -1],
            ["who is on the horse", -1]
        ],
        "context": [
            "a man riding a horse while holding a polo stick.",
            "two horses walking on a beach."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2319330",
                "VG_object_id": "3299688",
                "bbox": [71, 263, 269, 498],
                "image": "data\\images\\2319330.jpg"
            },
            {
                "VG_image_id": "2368047",
                "VG_object_id": "748624",
                "bbox": [237, 192, 332, 260],
                "image": "data\\images\\2368047.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is shirt", 2],
            ["What is the gender of the person", 2],
            ["What color is the hair of person", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["What color is shirt", 2],
            ["What is the gender of the person", 2],
            ["What color is the hair of person", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["what is the person wearing", -1],
            ["who is in the picture", 1]
        ],
        "context": [
            "a young girl holding a wii remote in her hand.",
            "a man with his arms crossed in front of a fire place."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2327643",
                "VG_object_id": "979796",
                "bbox": [65, 251, 140, 302],
                "image": "data\\images\\2327643.jpg"
            },
            {
                "VG_image_id": "2413726",
                "VG_object_id": "167518",
                "bbox": [49, 19, 325, 228],
                "image": "data\\images\\2413726.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the umbrella", 1],
            ["what is in the distance", 1],
            ["what pattern is on the umbrella", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 1],
            ["what is in the distance", 1],
            ["how many people are there", -1],
            ["what pattern is on the umbrella", 1],
            ["where is the umbrella", -1],
            ["who is holding the umbrella", -1],
            ["what is the ground covered with", 1],
            ["how is the weather", -1],
            ["what is the persion doing", -1],
            ["what are the people holding", -1]
        ],
        "context": [
            "a person sitting on a rock looking out at the ocean.",
            "an elderly woman holding an umbrella in her hand."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2374077",
                "VG_object_id": "2387249",
                "bbox": [18, 4, 182, 256],
                "image": "data\\images\\2374077.jpg"
            },
            {
                "VG_image_id": "2322285",
                "VG_object_id": "3051106",
                "bbox": [281, 124, 417, 374],
                "image": "data\\images\\2322285.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the lady's shirt", 2],
            ["what is the lady doing", 1],
            ["where is the lady", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1],
            ["how many people are there in the picture", 1],
            ["where was the photo taken", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what is the lady doing", 1],
            ["what color is the lady's shirt", 2],
            ["where is the lady", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1],
            ["what is the woman wearing", -1],
            ["how many people are there in the picture", 1],
            ["who is in the photo", -1],
            ["where was the photo taken", 1],
            ["what is on the woman's head", -1],
            ["what are the people doing", 1]
        ],
        "context": [
            "two women in a kitchen cooking food together.",
            "a woman playing a video game in a living room."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2375255",
                "VG_object_id": "723051",
                "bbox": [114, 123, 207, 264],
                "image": "data\\images\\2375255.jpg"
            },
            {
                "VG_image_id": "2350040",
                "VG_object_id": "1738594",
                "bbox": [366, 93, 471, 236],
                "image": "data\\images\\2350040.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's top", 1],
            ["what is the woman doing", 1],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1],
            ["what is the woman looking at", 1]
        ],
        "org_questions": [
            ["what color is the woman's top", 1],
            ["what is the woman doing", 1],
            ["where is the person", 1],
            ["how many people are there", 1],
            ["what is in the background", -1],
            ["what is the woman holding", 1],
            ["what is the color of the woman's hair", -1],
            ["who is in the photo", -1],
            ["what is on the woman's head", -1],
            ["what is the girl wearing", -1],
            ["what is the woman looking at", 1]
        ],
        "context": [
            "a woman sitting on a bouncy castle while holding a cell phone.",
            "a bathroom with three urinals and a woman in a blue dress."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2319372",
                "VG_object_id": "1002543",
                "bbox": [0, 178, 129, 334],
                "image": "data\\images\\2319372.jpg"
            },
            {
                "VG_image_id": "2326560",
                "VG_object_id": "982051",
                "bbox": [0, 1, 297, 378],
                "image": "data\\images\\2326560.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's hair", 2],
            ["how many children are there in the picture", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what color is the boy's shirt", 1],
            ["what is the child doing", 1],
            ["what is the boy holding", 1]
        ],
        "org_questions": [
            ["what color is the child's hair", 2],
            ["how many children are there in the picture", 1],
            ["what color is the table", -1],
            ["what is in the background", 1],
            ["where is the person", -1],
            ["what is the boy wearing", -1],
            ["how many people are there", 1],
            ["what color is the boy's shirt", 1],
            ["what is the child doing", 1],
            ["who is in the picture", -1],
            ["what is the boy holding", 1],
            ["what is the child eating", -1]
        ],
        "context": [
            "a group of children sitting at a table eating pizza.",
            "a young boy is eating a piece of pizza."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2344303",
                "VG_object_id": "916409",
                "bbox": [35, 279, 499, 373],
                "image": "data\\images\\2344303.jpg"
            },
            {
                "VG_image_id": "2350315",
                "VG_object_id": "2387851",
                "bbox": [25, 213, 383, 396],
                "image": "data\\images\\2350315.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 1],
            ["what is on the floor", 1],
            ["where is the photo taken", 1],
            ["what pattern is the floor", 1],
            ["what is the ground covered with", 1],
            ["what is the main color of the floor", 1],
            ["what type of floor is this", 1]
        ],
        "org_questions": [
            ["what is the floor made of", 1],
            ["what is on the floor", 1],
            ["where is the photo taken", 1],
            ["what color is the wall", -1],
            ["what pattern is the floor", 1],
            ["what is the ground covered with", 1],
            ["what is the main color of the floor", 1],
            ["what shape is the floor", -1],
            ["what type of floor is this", 1]
        ],
        "context": [
            "a bunk bed with a ladder and a ladder.",
            "a bench sitting on a stone walkway next to a bush."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "498012",
                "VG_object_id": "1039250",
                "bbox": [485, 2, 987, 248],
                "image": "data\\images\\498012.jpg"
            },
            {
                "VG_image_id": "2381775",
                "VG_object_id": "701926",
                "bbox": [216, 119, 341, 433],
                "image": "data\\images\\2381775.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 1],
            ["where is the man", 1],
            ["How many people are there", 1],
            ["What is man doing", 1],
            ["what color is the table", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man holding", 1],
            ["where is the man", 1],
            ["How many people are there", 1],
            ["What is man doing", 1],
            ["what color is the table", 1],
            ["what is the man doing", 1],
            ["who is in the photo", -1],
            ["what is the man wearing", 1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a picnic table with a person sitting at it",
            "a man and woman playing a video game."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2399783",
                "VG_object_id": "1166372",
                "bbox": [187, 42, 408, 227],
                "image": "data\\images\\2399783.jpg"
            },
            {
                "VG_image_id": "2373496",
                "VG_object_id": "588689",
                "bbox": [64, 165, 259, 356],
                "image": "data\\images\\2373496.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the TV", 1],
            ["what color is the background", 1],
            ["where is the TV", 1],
            ["what is in front of the tv", 1],
            ["what is the tv sitting on", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the TV", 1],
            ["what color is the background", 1],
            ["where is the TV", 1],
            ["what time is it", -1],
            ["how many people are there in the picture", -1],
            ["what is on the television", -1],
            ["what is in front of the tv", 1],
            ["when was the picture taken", -1],
            ["what is the tv sitting on", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a television that is sitting on a stand.",
            "a television sitting on the grass near a curb."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2341999",
                "VG_object_id": "3425646",
                "bbox": [106, 15, 369, 462],
                "image": "data\\images\\2341999.jpg"
            },
            {
                "VG_image_id": "2356532",
                "VG_object_id": "2347572",
                "bbox": [261, 24, 394, 345],
                "image": "data\\images\\2356532.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bottle", 1],
            ["how many bottles are there", 1],
            ["What is in the bottle", 1]
        ],
        "org_questions": [
            ["what color is the bottle", 1],
            ["what is on the side of the bottle", -1],
            ["how many bottles are there", 1],
            ["where is the photo taken", -1],
            ["What is the table made of", -1],
            ["What is in the bottle", 1],
            ["what is in front of the bottle", -1],
            ["where is the bottle", -1],
            ["what is next to the jar", -1],
            ["what is in the background", -1],
            ["what is in the photo", -1],
            ["where was the picture taken", -1],
            ["what is beside the bottle", -1]
        ],
        "context": [
            "a glass bottle on a table",
            "a hamburger with fries and ketchup on a cutting board."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2360911",
                "VG_object_id": "2299140",
                "bbox": [6, 0, 490, 373],
                "image": "data\\images\\2360911.jpg"
            },
            {
                "VG_image_id": "2339451",
                "VG_object_id": "2176309",
                "bbox": [5, 290, 327, 370],
                "image": "data\\images\\2339451.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the sofa", 1],
            ["what color is the sofa", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is on the sofa", 1],
            ["what color is the sofa", 1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["where was this photo taken", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a teddy bear sitting on a couch in a room.",
            "a living room with a table, chairs, and a clock."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2316492",
                "VG_object_id": "2721486",
                "bbox": [2, 257, 498, 372],
                "image": "data\\images\\2316492.jpg"
            },
            {
                "VG_image_id": "2405318",
                "VG_object_id": "1108974",
                "bbox": [3, 279, 332, 495],
                "image": "data\\images\\2405318.jpg"
            }
        ],
        "questions_with_scores": [["what color is the player's shirt", 2]],
        "org_questions": [
            ["what color is the player's shirt", 2],
            ["what is the main color of the court", -1],
            ["how many people are there", -1],
            ["what sport is it", -1],
            ["what is the court ground made of", -1],
            ["How many photos are there in the image", -1],
            ["where was this photo taken", -1],
            ["what is on the court", -1],
            ["where are the white lines", -1],
            ["where is the man standing", -1],
            ["where is the court", -1]
        ],
        "context": [
            "a man playing tennis on a tennis court.",
            "a tennis player swinging a racket at a ball"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2322191",
                "VG_object_id": "3037383",
                "bbox": [1, 350, 373, 498],
                "image": "data\\images\\2322191.jpg"
            },
            {
                "VG_image_id": "2359850",
                "VG_object_id": "3763626",
                "bbox": [4, 255, 372, 497],
                "image": "data\\images\\2359850.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of food is on the plate", 1],
            ["How many people are there", 1],
            ["What is the background of photo", 1],
            ["how many plates are there on the table", 1],
            ["what is the plate sitting on", 1],
            ["how many plates are on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what color is the plate", -1],
            ["what kind of food is on the plate", 1],
            ["How many people are there", 1],
            ["what is the table made of", -1],
            ["What is the background of photo", 1],
            ["how many plates are there on the table", 1],
            ["what is the plate sitting on", 1],
            ["what is sitting on the table", -1],
            ["what is under the table", -1],
            ["what is on the plate", -1],
            ["how many plates are on the table", 1],
            ["what shape is the table", -1],
            ["where is the table", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a woman with her mouth open looking at her phone",
            "a table set for a meal"
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2395297",
                "VG_object_id": "456069",
                "bbox": [71, 107, 460, 363],
                "image": "data\\images\\2395297.jpg"
            },
            {
                "VG_image_id": "2339036",
                "VG_object_id": "2173838",
                "bbox": [232, 226, 373, 337],
                "image": "data\\images\\2339036.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the fork", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what is on the plate", -1],
            ["where is the fork", 1],
            ["How many people are there", 1],
            ["what shape is the plate", -1],
            ["what color is the table", -1],
            ["what is the table made of", -1],
            ["What is color of image", -1],
            ["what is the pizza sitting on", -1],
            ["what is next to the pizza", -1],
            ["what is under the pizza", -1],
            ["what is the plate made of", -1],
            ["what is the main color of the plate", -1]
        ],
        "context": [
            "a pizza on a plate",
            "a man taking a picture of a pizza with a camera."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2388800",
                "VG_object_id": "1266766",
                "bbox": [13, 191, 498, 329],
                "image": "data\\images\\2388800.jpg"
            },
            {
                "VG_image_id": "2359931",
                "VG_object_id": "2226403",
                "bbox": [278, 125, 360, 313],
                "image": "data\\images\\2359931.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the shirt", 2],
            ["What color is the tie", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["What color is the shirt", 2],
            ["How many people are there", -1],
            ["What color is the tie", 1],
            ["what gender is the person in the shirt", -1],
            ["What is man wearing on his head", -1],
            ["what is the man doing", -1],
            ["what is the person holding", 1],
            ["what is hanging on the shirt", -1],
            ["who is wearing a tie", -1],
            ["where is the tie", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is around the man's neck", -1]
        ],
        "context": [
            "a man wearing a tie and a tie holding up a small pink box.",
            "two young boys wearing ties and ties standing next to each other."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2368185",
                "VG_object_id": "3870502",
                "bbox": [2, 0, 497, 372],
                "image": "data\\images\\2368185.jpg"
            },
            {
                "VG_image_id": "2328987",
                "VG_object_id": "2756564",
                "bbox": [13, 335, 423, 456],
                "image": "data\\images\\2328987.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the land", 2],
            ["what is on the land", 1],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["where was this photo taken", 1],
            ["what is the color of the floor", 1],
            ["what is covering the ground", 1],
            ["where is the scene", 1],
            ["where is this scene", 1]
        ],
        "org_questions": [
            ["what is on the land", 1],
            ["how many people are there on the land", 2],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["how many giraffes are there on the ground", -1],
            ["where was this photo taken", 1],
            ["what is the color of the floor", 1],
            ["what is covering the ground", 1],
            ["where is the scene", 1],
            ["where is this scene", 1]
        ],
        "context": [
            "a toilet with a broken seat and a broken toilet in the background.",
            "a man and a dog on a sidewalk"
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2408653",
                "VG_object_id": "364747",
                "bbox": [208, 422, 374, 480],
                "image": "data\\images\\2408653.jpg"
            },
            {
                "VG_image_id": "2318606",
                "VG_object_id": "1009134",
                "bbox": [3, 147, 59, 181],
                "image": "data\\images\\2318606.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 2],
            ["what color is the tray", 1],
            ["where is the tray", 1],
            ["what shape is the tray", 1],
            ["how many people are in the photo", 1],
            ["where was this photo taken", 1],
            ["what is green", 1]
        ],
        "org_questions": [
            ["what color is the tray", 1],
            ["where is the tray", 1],
            ["what shape is the tray", 1],
            ["what is the tray made of", -1],
            ["how many people are in the photo", 1],
            ["when was the picture taken", -1],
            ["what is in the background", 2],
            ["where was this photo taken", 1],
            ["what is green", 1]
        ],
        "context": [
            "a fruit stand with lots of different fruits.",
            "a man standing in a store looking at a bird cage."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2399586",
                "VG_object_id": "415268",
                "bbox": [407, 83, 470, 248],
                "image": "data\\images\\2399586.jpg"
            },
            {
                "VG_image_id": "2372645",
                "VG_object_id": "1780101",
                "bbox": [29, 72, 97, 261],
                "image": "data\\images\\2372645.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man ", 1],
            ["how many people are there", 1],
            ["What is in the background of image", 1],
            ["What is man doing", 1],
            ["what is the persion riding", 1],
            ["where is the photo taken", 1],
            ["what gesture is the man", 1],
            ["what kind of pants is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["where is the man ", 1],
            ["how many people are there", 1],
            ["What is in the background of image", 1],
            ["what is the man wearing", -1],
            ["What is man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion riding", 1],
            ["what is in the background", -1],
            ["where is the photo taken", 1],
            ["what gesture is the man", 1],
            ["when was the picture taken", -1],
            ["what kind of pants is the man wearing", 1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a woman riding a bike down a street next to a bus.",
            "a red and white food truck parked on gravel."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2357661",
                "VG_object_id": "2218132",
                "bbox": [140, 89, 306, 468],
                "image": "data\\images\\2357661.jpg"
            },
            {
                "VG_image_id": "2392695",
                "VG_object_id": "1226956",
                "bbox": [247, 54, 312, 279],
                "image": "data\\images\\2392695.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["What is the background of image", 1],
            ["what is the man's posture", 1],
            ["what is on the man's face", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["What is the background of image", 1],
            ["what is the man's posture", 1],
            ["when was this photo taken", -1],
            ["what is on the man's face", 1],
            ["what is the man on", 1]
        ],
        "context": [
            "a man wearing a beard and a beard",
            "a group of people standing around a motorcycle."
        ]
    },
    {
        "object_category": "bird",
        "images": [
            {
                "VG_image_id": "2316495",
                "VG_object_id": "2796123",
                "bbox": [303, 112, 413, 207],
                "image": "data\\images\\2316495.jpg"
            },
            {
                "VG_image_id": "2344645",
                "VG_object_id": "2893911",
                "bbox": [194, 286, 386, 399],
                "image": "data\\images\\2344645.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bird", 2],
            ["what is the bird standing on", 1],
            ["what is in the water", 1]
        ],
        "org_questions": [
            ["what color is the bird", 2],
            ["where is the bird", -1],
            ["how many birds are there", -1],
            ["what is the bird doing", -1],
            ["what is the bird standing on", 1],
            ["what kind of animal is this", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is in the water", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a bird sitting on a boat in the water.",
            "a bird flying over a body of water."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2317725",
                "VG_object_id": "3102631",
                "bbox": [3, 15, 314, 115],
                "image": "data\\images\\2317725.jpg"
            },
            {
                "VG_image_id": "2392449",
                "VG_object_id": "1229969",
                "bbox": [3, 3, 500, 112],
                "image": "data\\images\\2392449.jpg"
            }
        ],
        "questions_with_scores": [["what is in the distance", 1]],
        "org_questions": [
            ["what is in the distance", 1],
            ["how many people are in the picture", -1],
            ["what color is the sky", -1],
            ["What time is it", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", -1],
            ["how is the weather", -1],
            ["when was this picture taken", -1],
            ["when is this", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a man jumping into the water with his surfboard.",
            "a man is standing on a surfboard in the ocean."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2342412",
                "VG_object_id": "2364416",
                "bbox": [333, 56, 494, 348],
                "image": "data\\images\\2342412.jpg"
            },
            {
                "VG_image_id": "2377194",
                "VG_object_id": "567292",
                "bbox": [6, 54, 79, 222],
                "image": "data\\images\\2377194.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 2],
            ["what is the man doing", 2],
            ["what are the people doing", 1],
            ["what is the man holding", 1],
            ["what is the man on the left doing", 1],
            ["what is the person wearing", 1]
        ],
        "org_questions": [
            ["what is the weather like", 2],
            ["what is on the road", -1],
            ["how many people are there", -1],
            ["Where is the man", -1],
            ["what are the people doing", 1],
            ["what is the man holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man on the right wearing", -1],
            ["what is the man on the left doing", 1],
            ["what is the person wearing", 1],
            ["what is the man doing", 2]
        ],
        "context": [
            "people walking in the rain",
            "a woman walking a dog on a skateboard."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2369481",
                "VG_object_id": "743468",
                "bbox": [364, 199, 462, 320],
                "image": "data\\images\\2369481.jpg"
            },
            {
                "VG_image_id": "2317617",
                "VG_object_id": "1019170",
                "bbox": [8, 0, 215, 96],
                "image": "data\\images\\2317617.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["what color is the table", 2]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["what is the chair made of", -1],
            ["what color is the table", 2],
            ["how many people are there", -1],
            ["where is the picture taken", -1],
            ["what type of chair is shown", -1],
            ["what is behind the chair", -1],
            ["what is on the chair", -1]
        ],
        "context": [
            "a vase with flowers on a table in a room.",
            "a table with a white table cloth and a plate of food on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2398677",
                "VG_object_id": "1177350",
                "bbox": [104, 145, 358, 441],
                "image": "data\\images\\2398677.jpg"
            },
            {
                "VG_image_id": "2386601",
                "VG_object_id": "1284490",
                "bbox": [218, 108, 333, 294],
                "image": "data\\images\\2386601.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the woman's dress", 1],
            ["What is the woman doing", 1],
            ["How many people are there", 1],
            ["where is the person", 1],
            ["what gesture is the woman", 1],
            ["what is the woman holding", 1],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["What color is the woman's dress", 1],
            ["What is the woman doing", 1],
            ["How many people are there", 1],
            ["where is the person", 1],
            ["what is the woman wearing", -1],
            ["what gesture is the woman", 1],
            ["what is the woman holding", 1],
            ["what color is the background", 1],
            ["when was the photo taken", -1],
            ["what is the gender of the person on the right", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "a woman sitting in a chair reading a book.",
            "a man and woman standing together and laughing."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2380363",
                "VG_object_id": "710838",
                "bbox": [120, 38, 443, 325],
                "image": "data\\images\\2380363.jpg"
            },
            {
                "VG_image_id": "2390818",
                "VG_object_id": "493860",
                "bbox": [9, 52, 493, 264],
                "image": "data\\images\\2390818.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there in the picture", 2],
            ["how many people are there in the picture", 1],
            ["where is the zebra", 1],
            ["what is in the distance", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["how many zebras are there in the picture", 2],
            ["how many people are there in the picture", 1],
            ["where is the zebra", 1],
            ["what is the main color of the grass", -1],
            ["What is zebra doing", -1],
            ["what is in the distance", 1],
            ["what main color is the background", -1],
            ["when was the picture taken", -1],
            ["what type of animals are shown", -1],
            ["what are the zebras standing on", -1],
            ["where was this picture taken", 1]
        ],
        "context": [
            "a zebra is standing in a cage at a zoo.",
            "three zebras are eating hay in a field."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2327481",
                "VG_object_id": "980245",
                "bbox": [131, 221, 434, 278],
                "image": "data\\images\\2327481.jpg"
            },
            {
                "VG_image_id": "2373965",
                "VG_object_id": "2216908",
                "bbox": [4, 0, 497, 321],
                "image": "data\\images\\2373965.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the animal doing", 2],
            ["what color is the land", 1],
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["where was the photo taken", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is the land made of", 1],
            ["what is on the land", 1],
            ["where is the land", -1],
            ["What is the weather like", -1],
            ["what is the animal doing", 2],
            ["how many people are there", -1],
            ["where was the photo taken", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a cat sleeping on a pair of shoes.",
            "a white bird standing on top of a dirt ground."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2407129",
                "VG_object_id": "1924061",
                "bbox": [169, 64, 409, 373],
                "image": "data\\images\\2407129.jpg"
            },
            {
                "VG_image_id": "2361836",
                "VG_object_id": "2000075",
                "bbox": [115, 67, 358, 461],
                "image": "data\\images\\2361836.jpg"
            }
        ],
        "questions_with_scores": [["what color is the man's shirt", 2]],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is in the background", -1],
            ["how many people are there in the picture", -1],
            ["what is the man doing", -1],
            ["where is the man facing to", -1],
            ["what is the man wearing", -1],
            ["what is the persion staying on", -1],
            ["when was the picture taken", -1],
            ["what sport is being played", -1],
            ["who is in the photo", -1],
            ["what is the man holding", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man holding a tennis racquet on a tennis court.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2364762",
                "VG_object_id": "2304548",
                "bbox": [29, 148, 302, 494],
                "image": "data\\images\\2364762.jpg"
            },
            {
                "VG_image_id": "2356918",
                "VG_object_id": "2591889",
                "bbox": [99, 83, 384, 330],
                "image": "data\\images\\2356918.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the umbrella", 2],
            ["what color is the woman's clothes", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", 1],
            ["what color is the umbrella", 2],
            ["how many people are there", 1],
            ["where is the woman", -1],
            ["What is woman doing", -1],
            ["what is the woman holding", -1],
            ["what gesture is the woman", -1],
            ["when was this photo taken", -1],
            ["who is holding the umbrella", -1],
            ["what is on the woman's head", -1],
            ["why is the woman holding an umbrella", -1]
        ],
        "context": [
            "a woman with a polka dot umbrella walks down a street.",
            "two girls walking in the street with umbrellas."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2355073",
                "VG_object_id": "2472202",
                "bbox": [1, 158, 497, 327],
                "image": "data\\images\\2355073.jpg"
            },
            {
                "VG_image_id": "2358635",
                "VG_object_id": "1815567",
                "bbox": [0, 0, 399, 497],
                "image": "data\\images\\2358635.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people playing in the picture", 2],
            ["how many people are there in the picture", 1],
            ["what gender is the person on the field", 1],
            ["what are the people doing on the field", 1],
            ["what is on the grass", 1]
        ],
        "org_questions": [
            ["what are the people playing in the picture", 2],
            ["how many people are there in the picture", 1],
            ["what gender is the person on the field", 1],
            ["what is the main color of the grass", -1],
            ["what is the ground covered with", -1],
            ["when is the picture taken", -1],
            ["what are the people doing on the field", 1],
            ["where was this picture taken", -1],
            ["how is the weather", -1],
            ["where is the grass", -1],
            ["what is on the grass", 1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a group of men playing frisbee in a field.",
            "a little girl is playing with a kite."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2330220",
                "VG_object_id": "3722251",
                "bbox": [244, 17, 492, 428],
                "image": "data\\images\\2330220.jpg"
            },
            {
                "VG_image_id": "2360264",
                "VG_object_id": "2039334",
                "bbox": [17, 17, 154, 371],
                "image": "data\\images\\2360264.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man wearing on his face", 2],
            ["What color is the man's shirt", 1],
            ["What color is the man's hair", 1],
            ["what sport is the man doing", 1],
            ["what is the man holding", 1],
            ["what kind of shirt is the man wearing", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 1],
            ["What color is the man's hair", 1],
            ["What is man wearing on his face", 2],
            ["how many people are there", -1],
            ["what sport is the man doing", 1],
            ["What is in the background of image", -1],
            ["what is the man holding", 1],
            ["where is the man", -1],
            ["what kind of shirt is the man wearing", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man and a woman posing for a picture.",
            "a man and a woman playing a video game."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2375505",
                "VG_object_id": "2135033",
                "bbox": [240, 77, 351, 242],
                "image": "data\\images\\2375505.jpg"
            },
            {
                "VG_image_id": "2339571",
                "VG_object_id": "2978786",
                "bbox": [171, 153, 227, 255],
                "image": "data\\images\\2339571.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is in the distance", 1],
            ["what color is the ground", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", -1],
            ["what is in the distance", 1],
            ["how many shirts are there", -1],
            ["where is the person", -1],
            ["who is wearing the shirt", -1],
            ["what color is the ground", 1],
            ["what is the player wearing", -1],
            ["who is in the photo", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a man holding a tennis racket on a tennis court.",
            "a woman jumping up to hit a tennis ball with a racket."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2369255",
                "VG_object_id": "2320684",
                "bbox": [219, 72, 473, 327],
                "image": "data\\images\\2369255.jpg"
            },
            {
                "VG_image_id": "2344138",
                "VG_object_id": "2884159",
                "bbox": [0, 158, 105, 329],
                "image": "data\\images\\2344138.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["What is woman holding", 1],
            ["What color is woman's hair", 1],
            ["what is on the woman's face", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["what is the woman wearing", -1],
            ["What is woman holding", 1],
            ["What color is woman's hair", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", 1],
            ["what is on the woman's head", -1],
            ["where is the woman looking", -1]
        ],
        "context": [
            "a woman sitting on a blue bench holding a cell phone.",
            "a man in a blue shirt and tie standing in front of a crowd."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2316623",
                "VG_object_id": "3053472",
                "bbox": [3, 144, 374, 498],
                "image": "data\\images\\2316623.jpg"
            },
            {
                "VG_image_id": "2352548",
                "VG_object_id": "2534524",
                "bbox": [4, 190, 498, 372],
                "image": "data\\images\\2352548.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 2],
            ["What is color of table", 2]
        ],
        "org_questions": [
            ["What color is the table", 2],
            ["How many laptops are there on the table", -1],
            ["What is on the table", -1],
            ["where is the table", -1],
            ["What is color of table", 2],
            ["how many plates are there", -1],
            ["what is the table made of", -1],
            ["what is the desk sitting on", -1],
            ["where was this photo taken", -1],
            ["what is next to the desk", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a glass desk.",
            "a desk with a laptop and a monitor on it."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2357661",
                "VG_object_id": "3552110",
                "bbox": [1, 297, 320, 498],
                "image": "data\\images\\2357661.jpg"
            },
            {
                "VG_image_id": "2349499",
                "VG_object_id": "3598900",
                "bbox": [2, 222, 497, 329],
                "image": "data\\images\\2349499.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there on the street", 2],
            ["how many luggages are there", 2],
            ["how many bags are there on the street", 1],
            ["what is on the ground", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["how many cars are there on the street", 2],
            ["what is in the distance", -1],
            ["how many bags are there on the street", 1],
            ["when is the picture taken", -1],
            ["what color is the sidewalk", -1],
            ["what is on the ground", 1],
            ["how many people are in the photo", 1],
            ["how is the weather", -1],
            ["what is the road made of", -1],
            ["where was the photo taken", -1],
            ["how many luggages are there", 2]
        ],
        "context": [
            "a man wearing a beard and a beard",
            "a woman pulling a suitcase"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2363155",
                "VG_object_id": "1756783",
                "bbox": [139, 60, 263, 288],
                "image": "data\\images\\2363155.jpg"
            },
            {
                "VG_image_id": "2375081",
                "VG_object_id": "723759",
                "bbox": [191, 191, 304, 374],
                "image": "data\\images\\2375081.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what is on the man head", 2],
            ["what is the man doing", 1],
            ["where is the man ", 1],
            ["what is the color of the man's shirt", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["where is the man ", 1],
            ["what is the color of the man's shirt", 1],
            ["how many people are there", -1],
            ["what is the man wearing around his neck", -1],
            ["what is the man holding", 2],
            ["what is on the man head", 2],
            ["where is the photo taken", 1],
            ["what is the gender of the person", -1],
            ["when was the photo taken", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a tennis player is walking across the court.",
            "a man in a field with a frisbee."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2398905",
                "VG_object_id": "1174409",
                "bbox": [269, 295, 331, 419],
                "image": "data\\images\\2398905.jpg"
            },
            {
                "VG_image_id": "2358271",
                "VG_object_id": "2032015",
                "bbox": [305, 185, 412, 228],
                "image": "data\\images\\2358271.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is on the table", -1],
            ["what color is the bag", -1],
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["where is the bag", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a woman sitting at a table with a cell phone.",
            "a kitchen with a sink, cabinets, and a counter top."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2411933",
                "VG_object_id": "206594",
                "bbox": [1, 129, 169, 478],
                "image": "data\\images\\2411933.jpg"
            },
            {
                "VG_image_id": "2365474",
                "VG_object_id": "633978",
                "bbox": [88, 76, 283, 353],
                "image": "data\\images\\2365474.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["what color is the floor the chair placed on", 2],
            ["what is the chair made of", 1],
            ["What is above the chair", 1],
            ["What is on the chair", 1],
            ["what is the floor made of", 1],
            ["what is next to the chair", 1],
            ["what is in the chair", 1]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["what is the chair made of", 1],
            ["what color is the floor the chair placed on", 2],
            ["how many people are there", -1],
            ["where is the chair", -1],
            ["What is above the chair", 1],
            ["What is on the chair", 1],
            ["what is the floor made of", 1],
            ["what is the chair sitting on", -1],
            ["what is next to the chair", 1],
            ["what is under the chair", -1],
            ["what is in the chair", 1]
        ],
        "context": [
            "a red office chair",
            "a chair with a laptop on it next to a bed."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2323209",
                "VG_object_id": "3133933",
                "bbox": [154, 184, 226, 227],
                "image": "data\\images\\2323209.jpg"
            },
            {
                "VG_image_id": "2330393",
                "VG_object_id": "3339995",
                "bbox": [317, 28, 451, 88],
                "image": "data\\images\\2330393.jpg"
            }
        ],
        "questions_with_scores": [["how many airplanes are in the sky", 1]],
        "org_questions": [
            ["how many airplanes are in the sky", 1],
            ["what is in the distance", -1],
            ["what color is the sky", -1],
            ["where is the plane", -1],
            ["what is the airplane doing", -1],
            ["what is behind the planes", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["what is flying", -1],
            ["what is flying in the sky", -1]
        ],
        "context": [
            "a group of jets flying through a cloudy sky.",
            "a plane flying low over a bridge with a bridge in the background."
        ]
    },
    {
        "object_category": "banana",
        "images": [
            {
                "VG_image_id": "2386962",
                "VG_object_id": "679941",
                "bbox": [106, 71, 286, 249],
                "image": "data\\images\\2386962.jpg"
            },
            {
                "VG_image_id": "2406359",
                "VG_object_id": "1102335",
                "bbox": [134, 14, 379, 171],
                "image": "data\\images\\2406359.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 2],
            ["what color is the food except the banana", 1],
            ["what shape is the container holding the banana", 1],
            ["what is beside the bananas", 1],
            ["how many bananas are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the plate", 2],
            ["what color is the food except the banana", 1],
            ["what shape is the container holding the banana", 1],
            ["how many people are there", -1],
            ["where are the bananas", -1],
            ["what is beside the bananas", 1],
            ["how many bananas are there in the picture", 1],
            ["what type of fruit is shown", -1],
            ["what is on the plate", -1],
            ["what is the yellow fruit", -1],
            ["what is yellow", -1],
            ["what is yellow in the picture", -1]
        ],
        "context": [
            "a sandwich on a plate with a banana and a carton of juice",
            "a bowl of fruit is sitting on a table."
        ]
    },
    {
        "object_category": "computer",
        "images": [
            {
                "VG_image_id": "2398608",
                "VG_object_id": "423735",
                "bbox": [176, 181, 348, 287],
                "image": "data\\images\\2398608.jpg"
            },
            {
                "VG_image_id": "2400127",
                "VG_object_id": "1162684",
                "bbox": [33, 129, 223, 294],
                "image": "data\\images\\2400127.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many computers are there", 1],
            ["what is the table made of", 1],
            ["how many computers are there in the photo", 1]
        ],
        "org_questions": [
            ["where is the computer", -1],
            ["what color is the computer", -1],
            ["how many computers are there", 1],
            ["what is the table made of", 1],
            ["what is on the desk", -1],
            ["how many computers are there in the photo", 1],
            ["what is the person doing", -1],
            ["what is the person holding", -1],
            ["what is in front of the laptop", -1],
            ["how many people are in the picture", -1]
        ],
        "context": [
            "a man sitting in a train with a laptop.",
            "a man sitting at a table with two laptops."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2415383",
                "VG_object_id": "3063814",
                "bbox": [51, 109, 152, 335],
                "image": "data\\images\\2415383.jpg"
            },
            {
                "VG_image_id": "2411837",
                "VG_object_id": "208901",
                "bbox": [57, 224, 195, 485],
                "image": "data\\images\\2411837.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is ground", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["What is girl doing", -1],
            ["What color is ground", 1],
            ["What color is girl's shirt", -1],
            ["how many people are there", -1],
            ["what is the persion wearing", 1],
            ["what is the persion holding", -1],
            ["when was this picture taken", -1],
            ["who is holding the kite", -1],
            ["where is the girl", -1],
            ["what is the girl standing on", -1]
        ],
        "context": [
            "a little girl on the beach holding a kite.",
            "a girl flying a kite in a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2343676",
                "VG_object_id": "921122",
                "bbox": [176, 44, 416, 213],
                "image": "data\\images\\2343676.jpg"
            },
            {
                "VG_image_id": "2387162",
                "VG_object_id": "514879",
                "bbox": [63, 79, 271, 474],
                "image": "data\\images\\2387162.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is the man;s shirt", 2],
            ["What sports is man doing", 1],
            ["where is the man", 1],
            ["what is in the background", 1],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what sport is being played", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is the man;s shirt", 2],
            ["What sports is man doing", 1],
            ["what is the man wearing around his neck", -1],
            ["what shape is the man", -1],
            ["where is the man", 1],
            ["what is in the background", 1],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["what is the man holding", 1],
            ["what sport is being played", 1],
            ["where was this photo taken", 1]
        ],
        "context": [
            "a baseball player throwing a ball during a game.",
            "a man holding a tennis racquet on a court."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2374513",
                "VG_object_id": "1952767",
                "bbox": [8, 40, 499, 320],
                "image": "data\\images\\2374513.jpg"
            },
            {
                "VG_image_id": "2352041",
                "VG_object_id": "3784779",
                "bbox": [14, 204, 480, 315],
                "image": "data\\images\\2352041.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animal is there", 1],
            ["How many animals are there", 1],
            ["who is in the photo", 1],
            ["what is on the grass", 1],
            ["what animal is on the field", 1]
        ],
        "org_questions": [
            ["what color is the grass", -1],
            ["what animal is there", 1],
            ["How many animals are there", 1],
            ["what is in the background", -1],
            ["where was this photo taken", -1],
            ["how is the weather", -1],
            ["who is in the photo", 1],
            ["where is the grass", -1],
            ["what is on the grass", 1],
            ["How many people are there", -1],
            ["what animal is on the field", 1],
            ["where is the picture taken", -1],
            ["what is the weather like", -1],
            ["what is the ground covered with", -1],
            ["how is the grass", -1]
        ],
        "context": [
            "a brown bear sitting in the grass with a fence in the background.",
            "two giraffes are walking in the grass near trees."
        ]
    },
    {
        "object_category": "computer",
        "images": [
            {
                "VG_image_id": "2317273",
                "VG_object_id": "1022214",
                "bbox": [166, 7, 329, 166],
                "image": "data\\images\\2317273.jpg"
            },
            {
                "VG_image_id": "19",
                "VG_object_id": "1060979",
                "bbox": [115, 224, 693, 486],
                "image": "data\\images\\19.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the desk", 2],
            ["what is the color under the computer", 1],
            ["what color is the computer's keyboard", 1],
            ["what color is the table", 1]
        ],
        "org_questions": [
            ["how many computers are there", -1],
            ["what is the color under the computer", 1],
            ["what color is the computer's keyboard", 1],
            ["where is the computer", -1],
            ["what color is the table", 1],
            ["what is the main color of the computer", -1],
            ["what type of computer is this", -1],
            ["what is next to the computer", -1],
            ["what is on the table", -1],
            ["what is in front of the monitor", -1],
            ["what is the color of the desk", 2]
        ],
        "context": [
            "a computer desk with a monitor and keyboard.",
            "a desk with a computer and a monitor on it."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2396861",
                "VG_object_id": "439864",
                "bbox": [195, 236, 266, 403],
                "image": "data\\images\\2396861.jpg"
            },
            {
                "VG_image_id": "2318590",
                "VG_object_id": "1009290",
                "bbox": [20, 54, 113, 342],
                "image": "data\\images\\2318590.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", -1],
            ["where is the photo taken", -1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is the boy wearing", -1],
            ["what is the boy holding", -1],
            ["where is the person", -1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is on the man's face", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a boy is standing in the middle of a sidewalk.",
            "a group of people standing around a herd of sheep."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2341127",
                "VG_object_id": "2151315",
                "bbox": [1, 149, 496, 374],
                "image": "data\\images\\2341127.jpg"
            },
            {
                "VG_image_id": "2362840",
                "VG_object_id": "2915555",
                "bbox": [23, 134, 191, 321],
                "image": "data\\images\\2362840.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the ground covered with", 1],
            ["where is the picture taken", 1],
            ["how many people are there", 1],
            ["What color is land", 1],
            ["what is on the land", 1],
            ["what is the person holding", 1],
            ["where is the ball", 1],
            ["what are the people playing", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["where is the picture taken", 1],
            ["how many people are there", 1],
            ["What color is land", 1],
            ["what is on the land", 1],
            ["what is the person holding", 1],
            ["where is the ball", 1],
            ["what are the people playing", 1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a woman and a child playing with a ball in a field.",
            "a tennis player jumping to hit a tennis ball."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2373668",
                "VG_object_id": "1718210",
                "bbox": [6, 117, 78, 251],
                "image": "data\\images\\2373668.jpg"
            },
            {
                "VG_image_id": "2362933",
                "VG_object_id": "1791268",
                "bbox": [183, 139, 276, 323],
                "image": "data\\images\\2362933.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 1],
            ["what is beside the person", 1],
            ["how many people are there", 1],
            ["how is the weather", 1],
            ["where is the person", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what is the person doing", -1],
            ["what is the person holding", 1],
            ["what is beside the person", 1],
            ["how many people are there", 1],
            ["how is the weather", 1],
            ["when is the picture taken", -1],
            ["where is the person", 1],
            ["who is in the picture", -1],
            ["what is the man wearing", -1],
            ["what is the man standing on", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a surf board pool with people on it",
            "a man walking with an elephant in the jungle."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2382346",
                "VG_object_id": "1329362",
                "bbox": [187, 376, 500, 500],
                "image": "data\\images\\2382346.jpg"
            },
            {
                "VG_image_id": "2339083",
                "VG_object_id": "3929424",
                "bbox": [63, 104, 234, 256],
                "image": "data\\images\\2339083.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what is on the plate", 1],
            ["how many plates are on the table", 1],
            ["what kind of food is on the plate", 1],
            ["what is next to the plate", 1],
            ["what is in the bowl", 1]
        ],
        "org_questions": [
            ["what is on the plate", 1],
            ["what color is the table", 2],
            ["how many plates are on the table", 1],
            ["what is the table made of", -1],
            ["what kind of food is on the plate", 1],
            ["what is the plate made of", -1],
            ["what is the main color of the plate", -1],
            ["what is the food on", -1],
            ["what is next to the plate", 1],
            ["what is in the bowl", 1],
            ["what is on the table", -1]
        ],
        "context": [
            "a young boy sitting at a table with a plate of food.",
            "a bowl of soup and a plate of food."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2379181",
                "VG_object_id": "554451",
                "bbox": [165, 120, 311, 179],
                "image": "data\\images\\2379181.jpg"
            },
            {
                "VG_image_id": "2354209",
                "VG_object_id": "1726850",
                "bbox": [14, 29, 306, 376],
                "image": "data\\images\\2354209.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the color of image", 2],
            ["How many trees are there in the image", 2],
            ["what is on the street", 1],
            ["what is in front of the building", 1],
            ["how many people are there", 1],
            ["what is color of the building", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["What is the color of image", 2],
            ["How many trees are there in the image", 2],
            ["what is the building made of", -1],
            ["what is on the street", 1],
            ["what is in front of the building", 1],
            ["how many people are there", 1],
            ["what is color of the building", 1],
            ["when was this picture taken", -1],
            ["who is in the picture", -1],
            ["where is the photo taken", 1],
            ["how is the photo", -1]
        ],
        "context": [
            "a pool with a bench and a palm tree in the background.",
            "a woman sitting on a bench in front of a window."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2381120",
                "VG_object_id": "707215",
                "bbox": [230, 108, 314, 248],
                "image": "data\\images\\2381120.jpg"
            },
            {
                "VG_image_id": "2319808",
                "VG_object_id": "3230104",
                "bbox": [255, 25, 416, 473],
                "image": "data\\images\\2319808.jpg"
            }
        ],
        "questions_with_scores": [
            ["how any players are there", 2],
            ["how many people are there in the picture", 2],
            ["what color is the player's shirt", 1],
            ["what is the man doing", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what color is the player's shirt", 1],
            ["how any players are there", 2],
            ["what is the player wearing", -1],
            ["what is the man doing", 1],
            ["What is the gender of player", -1],
            ["how many people are there in the picture", 2],
            ["what color is the floor", -1],
            ["when was the photo taken", -1],
            ["where is the man", -1],
            ["what sport is being played", 1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a group of men standing on top of a field.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2381209",
                "VG_object_id": "706388",
                "bbox": [322, 67, 485, 319],
                "image": "data\\images\\2381209.jpg"
            },
            {
                "VG_image_id": "2377999",
                "VG_object_id": "2007628",
                "bbox": [33, 123, 120, 494],
                "image": "data\\images\\2377999.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the man's hair", 1],
            ["what kind of animal is the man touching", 1],
            ["where is the man", 1],
            ["What is the man holding", 1],
            ["what is the man doing", 1],
            ["what is the land covered with", 1],
            ["where is the photo taken", 1],
            ["who is in the photo", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's hair", 1],
            ["what kind of animal is the man touching", 1],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["What is the man holding", 1],
            ["what is the man doing", 1],
            ["what is the land covered with", 1],
            ["where is the photo taken", 1],
            ["who is in the photo", 1],
            ["when was the picture taken", -1],
            ["what is on the man's head", -1],
            ["what is the persion wearing", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man and a boy playing with two dogs.",
            "a man standing next to an elephant in a river."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2363125",
                "VG_object_id": "2536003",
                "bbox": [16, 12, 304, 331],
                "image": "data\\images\\2363125.jpg"
            },
            {
                "VG_image_id": "2332724",
                "VG_object_id": "3710889",
                "bbox": [96, 188, 337, 392],
                "image": "data\\images\\2332724.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the horse's hair", 2],
            ["How many horses are there", 2],
            ["Where is the horse", 1],
            ["what are the horses doing", 1],
            ["what is in the background", 1],
            ["what is behind the horse", 1]
        ],
        "org_questions": [
            ["What color is the horse's hair", 2],
            ["Where is the horse", 1],
            ["How many horses are there", 2],
            ["what are the horses doing", 1],
            ["what is in the background", 1],
            ["what is on the horse", -1],
            ["what kind of animal is in the picture", -1],
            ["what animal is shown", -1],
            ["what is behind the horse", 1],
            ["what type of animal is shown", -1]
        ],
        "context": [
            "a man kissing a horse with a horse in the background.",
            "two horses grazing in a field next to a windmill."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2411396",
                "VG_object_id": "312526",
                "bbox": [4, 301, 485, 361],
                "image": "data\\images\\2411396.jpg"
            },
            {
                "VG_image_id": "2371766",
                "VG_object_id": "1703707",
                "bbox": [6, 192, 493, 339],
                "image": "data\\images\\2371766.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is the ground covered with", 1],
            ["what is the man doing", 1],
            ["where was this picture taken", 1],
            ["what is in the background", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["when is the picture taken", -1],
            ["what is the ground covered with", 1],
            ["what is the man doing", 1],
            ["how many elephants are there", -1],
            ["What is the weather like", -1],
            ["where was this picture taken", 1],
            ["what is in the background", 1],
            ["what is on the ground", 1],
            ["how is the weather", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a man on skis on a rail",
            "a soldier stands guard in front of a military truck."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2360226",
                "VG_object_id": "3534790",
                "bbox": [2, 283, 496, 330],
                "image": "data\\images\\2360226.jpg"
            },
            {
                "VG_image_id": "2369654",
                "VG_object_id": "1812379",
                "bbox": [0, 169, 500, 331],
                "image": "data\\images\\2369654.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the picture taken", 2],
            ["what is on the land", 2],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what is the picture taken", 2],
            ["how many people are there in the picture", 1],
            ["what is on the land", 2],
            ["What color is the ground", -1],
            ["what is the ground covered with", -1],
            ["how many tennis rackets are there on the ground", -1],
            ["where is the grass", -1],
            ["how is the weather", -1],
            ["where is this scene", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a man and woman loading luggage onto a train.",
            "a couple of small airplanes sitting on top of a tarmac."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2328547",
                "VG_object_id": "977069",
                "bbox": [136, 146, 283, 337],
                "image": "data\\images\\2328547.jpg"
            },
            {
                "VG_image_id": "2370447",
                "VG_object_id": "2341604",
                "bbox": [28, 107, 358, 331],
                "image": "data\\images\\2370447.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the animal standing on", 1],
            ["How many animals are there", 1],
            ["What is in the background", 1],
            ["what is on the ground", 1],
            ["what is the number of sheep", 1]
        ],
        "org_questions": [
            ["What is the animal standing on", 1],
            ["How many animals are there", 1],
            ["What is in the background", 1],
            ["what animal is it", -1],
            ["where is the animal", -1],
            ["what are the animals", -1],
            ["when was the picture taken", -1],
            ["what is on the ground", 1],
            ["what is the number of sheep", 1],
            ["what type of animal is in the picture", -1]
        ],
        "context": [
            "a goat standing on top of a large rock.",
            "a group of sheep eating food from a trough."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2398854",
                "VG_object_id": "421458",
                "bbox": [93, 67, 262, 327],
                "image": "data\\images\\2398854.jpg"
            },
            {
                "VG_image_id": "2404737",
                "VG_object_id": "338654",
                "bbox": [51, 13, 116, 216],
                "image": "data\\images\\2404737.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's top", 1],
            ["what is the woman doing", 1],
            ["what color is the woman's hair", 1],
            ["What is woman holding", 1],
            ["what kind of shoes is the woman wearing", 1]
        ],
        "org_questions": [
            ["what color is the woman's top", 1],
            ["what is the woman doing", 1],
            ["what color is the woman's hair", 1],
            ["how many women are there", -1],
            ["where is the woman", -1],
            ["What is woman holding", 1],
            ["when was the photo taken", -1],
            ["what kind of shoes is the woman wearing", 1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a man and a woman sitting on a bench.",
            "a young boy riding a skateboard down a street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2331086",
                "VG_object_id": "2943690",
                "bbox": [3, 361, 374, 491],
                "image": "data\\images\\2331086.jpg"
            },
            {
                "VG_image_id": "2344650",
                "VG_object_id": "2951622",
                "bbox": [1, 209, 495, 374],
                "image": "data\\images\\2344650.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there in the picture", 2],
            ["what color is the ground", 1],
            ["how many zebras are there on the ground", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["how many zebras are there on the ground", 1],
            ["what is on the ground", -1],
            ["What season is it", -1],
            ["what is in the distance", -1],
            ["how many people are there", -1],
            ["where are the shadows", -1],
            ["what is the weather like", -1],
            ["when was this picture taken", -1],
            ["how many zebras are there in the picture", 2]
        ],
        "context": [
            "a parking meter in a park near a tree.",
            "a group of zebras standing around in a fenced in area."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2375194",
                "VG_object_id": "583661",
                "bbox": [194, 377, 371, 495],
                "image": "data\\images\\2375194.jpg"
            },
            {
                "VG_image_id": "2326030",
                "VG_object_id": "3033719",
                "bbox": [282, 232, 372, 328],
                "image": "data\\images\\2326030.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what color is the rug", 1],
            ["what is on the rug", 1]
        ],
        "org_questions": [
            ["what color is the rug", 1],
            ["what is the ground covered with", -1],
            ["what room is the rug in", -1],
            ["where is the rug", -1],
            ["what is on the rug", 1],
            ["what is on the ground", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a bag of plastic bags is sitting on a table.",
            "a living room with a couch, coffee table, and a lamp."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2368884",
                "VG_object_id": "615384",
                "bbox": [262, 30, 375, 491],
                "image": "data\\images\\2368884.jpg"
            },
            {
                "VG_image_id": "2361929",
                "VG_object_id": "778470",
                "bbox": [158, 87, 338, 246],
                "image": "data\\images\\2361929.jpg"
            }
        ],
        "questions_with_scores": [["how many buses in the picture", 2]],
        "org_questions": [
            ["what is the color of the bus", -1],
            ["how many buses in the picture", 2],
            ["When is the picture taken", -1],
            ["where is the bus", -1],
            ["how many levels does the bus have", -1],
            ["what type of vehicle is shown", -1],
            ["what is on the side of the bus", -1],
            ["what is the bus doing", -1],
            ["who is in the photo", -1],
            ["what kind of bus is this", -1]
        ],
        "context": [
            "a bus stop with people getting on it.",
            "a blue and white bus parked in a parking lot."
        ]
    },
    {
        "object_category": "vegetable",
        "images": [
            {
                "VG_image_id": "2321446",
                "VG_object_id": "3254680",
                "bbox": [239, 4, 494, 300],
                "image": "data\\images\\2321446.jpg"
            },
            {
                "VG_image_id": "2405106",
                "VG_object_id": "334750",
                "bbox": [284, 124, 488, 288],
                "image": "data\\images\\2405106.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many kinds of vegetables are there", 1],
            ["what is the red vegetable", 1]
        ],
        "org_questions": [
            ["how many kinds of vegetables are there", 1],
            ["where is the vegetables placed on", -1],
            ["what kind of vegetable is it", -1],
            ["what is the vegetable place on", -1],
            ["What vegetable is it", -1],
            ["what is in the photo", -1],
            ["what is the red vegetable", 1],
            ["what is on the table", -1]
        ],
        "context": [
            "a pile of fruit and vegetables on a table.",
            "a plate of food with a bowl of dipping sauce."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2399964",
                "VG_object_id": "1164310",
                "bbox": [53, 176, 278, 311],
                "image": "data\\images\\2399964.jpg"
            },
            {
                "VG_image_id": "2332311",
                "VG_object_id": "3237310",
                "bbox": [93, 64, 415, 254],
                "image": "data\\images\\2332311.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the back ground", 1],
            ["where is the bench", 1],
            ["what is in the background", 1],
            ["What is in the background of image", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the bench", -1],
            ["what is the bench made of", -1],
            ["what is in the back ground", 1],
            ["how many people are sitting on the bench", -1],
            ["when is the picture taken", -1],
            ["where is the bench", 1],
            ["what is the bench sitting on", -1],
            ["how is the weather", -1],
            ["what is behind the bench", -1],
            ["what is on the bench", -1],
            ["what is in the background", 1],
            ["who is sitting on the bench", -1],
            ["what is placed on the bench", -1],
            ["What is in the background of image", 1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a bench on a sidewalk near a body of water.",
            "a bench sitting on the sidewalk in front of a car."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2413382",
                "VG_object_id": "174611",
                "bbox": [1, 122, 408, 450],
                "image": "data\\images\\2413382.jpg"
            },
            {
                "VG_image_id": "2328879",
                "VG_object_id": "975789",
                "bbox": [0, 146, 427, 312],
                "image": "data\\images\\2328879.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["how many plates are there on the table", 1],
            ["what shape is the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what kind of food is on the plate", -1],
            ["how many plates are there on the table", 1],
            ["where is the plate", -1],
            ["what is the table made of", -1],
            ["what is the plate on ", -1],
            ["what is on the plate", -1],
            ["what color is the dish", -1],
            ["what shape is the plate", 1],
            ["what is under the plate", -1],
            ["what is the food sitting on", -1],
            ["what kind of food is this", -1]
        ],
        "context": [
            "a hot dog on a bun with cheese and a cup of coffee.",
            "a bird is sitting on a plate with a sandwich."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2351931",
                "VG_object_id": "857163",
                "bbox": [2, 249, 431, 356],
                "image": "data\\images\\2351931.jpg"
            },
            {
                "VG_image_id": "2358339",
                "VG_object_id": "803842",
                "bbox": [95, 164, 293, 211],
                "image": "data\\images\\2358339.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["where was the picture taken", -1],
            ["what room is this", -1],
            ["what are the cabinets made of", -1],
            ["what is next to the counter", -1],
            ["where is the counter", -1]
        ],
        "context": [
            "a kitchen with a microwave, sink, and a microwave.",
            "a kitchen with a stainless steel refrigerator and wooden cabinets."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2354869",
                "VG_object_id": "3214549",
                "bbox": [59, 179, 139, 275],
                "image": "data\\images\\2354869.jpg"
            },
            {
                "VG_image_id": "2376063",
                "VG_object_id": "2304641",
                "bbox": [66, 24, 438, 332],
                "image": "data\\images\\2376063.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the truck", 2],
            ["where is the truck", 1],
            ["what is at the back of the truck", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the truck", 2],
            ["where is the truck", 1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is in the distance", -1],
            ["What is the truck driving on", -1],
            ["what is at the back of the truck", 1],
            ["what type of vehicle is this", -1],
            ["when was this photo taken", -1],
            ["where was this photo taken", -1],
            ["how many vehicles are there", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a large jetliner sitting on top of an airport tarmac.",
            "a large red truck driving down a road."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2364939",
                "VG_object_id": "1731753",
                "bbox": [2, 117, 497, 371],
                "image": "data\\images\\2364939.jpg"
            },
            {
                "VG_image_id": "2406949",
                "VG_object_id": "1099770",
                "bbox": [19, 148, 209, 299],
                "image": "data\\images\\2406949.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many buses are in the picture", 2],
            ["which two colors are on the bus", 2],
            ["what color is the bus besieds write", 2]
        ],
        "org_questions": [
            ["how many buses are in the picture", 2],
            ["which two colors are on the bus", 2],
            ["When is the picture taken", -1],
            ["what is the weather like", -1],
            ["what is in the background", -1],
            ["What color is the house", -1],
            ["how many people are there", -1],
            ["where was the photo taken", -1],
            ["what type of vehicle is this", -1],
            ["what is the bus doing", -1],
            ["what kind of bus is this", -1],
            ["what is on the side of the bus", -1],
            ["what color is the bus besieds write", 2]
        ],
        "context": [
            "a couple of buses that are parked in a lot",
            "a red and white bus parked in front of a stone building."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2365348",
                "VG_object_id": "634850",
                "bbox": [179, 62, 299, 323],
                "image": "data\\images\\2365348.jpg"
            },
            {
                "VG_image_id": "2328124",
                "VG_object_id": "2795869",
                "bbox": [104, 67, 243, 436],
                "image": "data\\images\\2328124.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["where is the photo taken", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["what is the man doing", -1],
            ["how many people are there", -1],
            ["what color are the man's pants", -1],
            ["what is the man wearing", -1],
            ["where is the photo taken", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is on the man's head", 2],
            ["what is the persion on the left wearing", 1]
        ],
        "context": [
            "a man standing in a field with a accordion.",
            "a man holding a skateboard on a beach."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2333456",
                "VG_object_id": "3099578",
                "bbox": [144, 160, 328, 422],
                "image": "data\\images\\2333456.jpg"
            },
            {
                "VG_image_id": "2324227",
                "VG_object_id": "3246419",
                "bbox": [362, 227, 459, 314],
                "image": "data\\images\\2324227.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the shirt", 1],
            ["what is the man in the shirt holding", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the man in the shirt holding", 1],
            ["how many people are there", -1],
            ["What time is it now", -1],
            ["where is the person", -1],
            ["What is the boy wearing on his head", -1],
            ["how many children are there in the picture", -1],
            ["what type of shirt is the man wearing", -1],
            ["who is in the picture", -1],
            ["what is the man doing", 2],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a young man standing with a skateboard in his hand.",
            "a man feeding a giraffe at a zoo."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2328879",
                "VG_object_id": "975789",
                "bbox": [0, 146, 427, 312],
                "image": "data\\images\\2328879.jpg"
            },
            {
                "VG_image_id": "2336965",
                "VG_object_id": "3665761",
                "bbox": [34, 238, 297, 368],
                "image": "data\\images\\2336965.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["How many people are there", 1],
            ["What food  in the plate", 1],
            ["what type of food is on the plate", 1]
        ],
        "org_questions": [
            ["what is on the plate", -1],
            ["what color is the table", 1],
            ["How many people are there", 1],
            ["where is the plate", -1],
            ["what pattern is on the plate", -1],
            ["what color is the dish", -1],
            ["What food  in the plate", 1],
            ["what is the shape of the plate", -1],
            ["what is the table made of", -1],
            ["what type of food is on the plate", 1],
            ["what is the food on", -1]
        ],
        "context": [
            "a bird is sitting on a plate with a sandwich.",
            "a young boy is eating pizza at a restaurant."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2340389",
                "VG_object_id": "3714648",
                "bbox": [43, 67, 283, 258],
                "image": "data\\images\\2340389.jpg"
            },
            {
                "VG_image_id": "2329974",
                "VG_object_id": "2878827",
                "bbox": [190, 101, 476, 271],
                "image": "data\\images\\2329974.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many airplanes are in the picture", 2],
            ["how many planes", 2],
            ["what color the head of the airplane", 2],
            ["what color is in the front of the airplane", 1]
        ],
        "org_questions": [
            ["how many airplanes are in the picture", 2],
            ["what color is in the front of the airplane", 1],
            ["what is the ground at the bottom of the picture made of", -1],
            ["what is the plane doing", -1],
            ["what is in the background", -1],
            ["what is in front of the plane", -1],
            ["how many propellers does the plane have", -1],
            ["when was the photo taken", -1],
            ["what is flying", -1],
            ["what is flying in the sky", -1],
            ["how many planes", 2],
            ["what color the head of the airplane", 2]
        ],
        "context": [
            "a colorful airplane flying in the sky.",
            "a group of planes flying in formation in the sky."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2404625",
                "VG_object_id": "340224",
                "bbox": [1, 0, 259, 138],
                "image": "data\\images\\2404625.jpg"
            },
            {
                "VG_image_id": "2372866",
                "VG_object_id": "1993945",
                "bbox": [130, 74, 232, 136],
                "image": "data\\images\\2372866.jpg"
            }
        ],
        "questions_with_scores": [["what color is the cabinet", 2]],
        "org_questions": [
            ["what color is the cabinet", 2],
            ["what is on the cabinet", -1],
            ["how many people are there", -1],
            ["where is the cabinet", -1],
            ["what color is the floor under the cabinet", -1],
            ["what is the color of the wall", -1],
            ["what is on the wall", -1],
            ["what room is this", -1],
            ["when was the picture taken", -1],
            ["what are the cabinets made of", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a kitchen with wooden cabinets and stainless steel appliances.",
            "a living room with a couch, table, and a chair."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2401340",
                "VG_object_id": "659718",
                "bbox": [155, 118, 333, 179],
                "image": "data\\images\\2401340.jpg"
            },
            {
                "VG_image_id": "2387400",
                "VG_object_id": "513549",
                "bbox": [73, 155, 333, 283],
                "image": "data\\images\\2387400.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the side of the bus", 1],
            ["where is the bus", 1],
            ["what is the weather like", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["what is on the side of the bus", 1],
            ["where is the bus", 1],
            ["what is the weather like", 1],
            ["what is the number of the bus", -1],
            ["how many buses are there", -1],
            ["what type of vehicle is shown", -1],
            ["what is the bus doing", -1],
            ["what is the vehicle", -1],
            ["what is in front of the bus", -1],
            ["what is on the road", -1]
        ],
        "context": [
            "a bus driving through a river filled with water.",
            "a yellow bus driving through a desert landscape."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2400463",
                "VG_object_id": "408542",
                "bbox": [129, 36, 309, 320],
                "image": "data\\images\\2400463.jpg"
            },
            {
                "VG_image_id": "2316712",
                "VG_object_id": "3513650",
                "bbox": [113, 176, 216, 339],
                "image": "data\\images\\2316712.jpg"
            }
        ],
        "questions_with_scores": [["what color is the player's helmet", 2]],
        "org_questions": [
            ["what is the player wearing", -1],
            ["what color is the player's helmet", 2],
            ["what is the player doing", -1],
            ["how many people are there", -1],
            ["What is the person holding", -1],
            ["how many players are there in the picture", -1],
            ["what is the man doing", -1],
            ["when was the photo taken", -1],
            ["what sport is being played", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man swinging a baseball bat at a ball.",
            "a pitcher throwing a baseball from the mound."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2317574",
                "VG_object_id": "3790947",
                "bbox": [239, 198, 334, 398],
                "image": "data\\images\\2317574.jpg"
            },
            {
                "VG_image_id": "2412851",
                "VG_object_id": "3423405",
                "bbox": [2, 202, 90, 365],
                "image": "data\\images\\2412851.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person's posture", 2],
            ["how many people are in the picture", 2],
            ["where is the person", 1],
            ["What is person doing", 1]
        ],
        "org_questions": [
            ["what is the person's posture", 2],
            ["how many people are in the picture", 2],
            ["where is the person", 1],
            ["what is on the person's head", -1],
            ["What is person doing", 1],
            ["when was the photo taken", -1],
            ["what is the gender of the person on the right", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a man standing outside of a building talking on a cell phone.",
            "a group of people waiting for their luggage."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2364151",
                "VG_object_id": "1854681",
                "bbox": [363, 115, 489, 257],
                "image": "data\\images\\2364151.jpg"
            },
            {
                "VG_image_id": "2411619",
                "VG_object_id": "309206",
                "bbox": [214, 129, 418, 375],
                "image": "data\\images\\2411619.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man holding", 2],
            ["What is person doing", 2],
            ["How many people are there", 1],
            ["What color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is on the side of the street", 1],
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["what are the boys doing", 1]
        ],
        "org_questions": [
            ["What is man holding", 2],
            ["How many people are there", 1],
            ["What color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is on the side of the street", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", -1],
            ["What is person doing", 2],
            ["when was the photo taken", -1],
            ["what is on the man's head", 1],
            ["what are the boys doing", 1]
        ],
        "context": [
            "two men skateboarding on a sidewalk.",
            "a man throwing a frisbee in a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2363604",
                "VG_object_id": "2406000",
                "bbox": [158, 139, 213, 204],
                "image": "data\\images\\2363604.jpg"
            },
            {
                "VG_image_id": "2368707",
                "VG_object_id": "2621549",
                "bbox": [155, 54, 385, 317],
                "image": "data\\images\\2368707.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["what are the people doing", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the man wearing", -1],
            ["what is the man playing", -1],
            ["what is on the man's head", -1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["what sport is the man doing", -1],
            ["what are the people doing", 1],
            ["what is the gender of the person in the photo", -1],
            ["who is in the photo", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a man playing basketball on a wooden court.",
            "a group of young men playing a game of frisbee."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "713012",
                "VG_object_id": "1577565",
                "bbox": [819, 311, 1259, 662],
                "image": "data\\images\\713012.jpg"
            },
            {
                "VG_image_id": "2406966",
                "VG_object_id": "3102184",
                "bbox": [144, 81, 499, 130],
                "image": "data\\images\\2406966.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the bench", 2],
            ["where was the photo taken", 2],
            ["how many benches are in the picture", 1],
            ["What is on the bench", 1],
            ["what are the people doing", 1],
            ["what is on the left side of the picture", 1]
        ],
        "org_questions": [
            ["what is the color of the bench", 2],
            ["how many benches are in the picture", 1],
            ["where was the photo taken", 2],
            ["what is behind the bench", -1],
            ["what is the bench made of", -1],
            ["When is photo taken", -1],
            ["What is on the bench", 1],
            ["what are the people doing", 1],
            ["what is in the back of the picture", -1],
            ["what is on the left side of the picture", 1],
            ["what is in the photo", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a woman is taking a picture of another woman.",
            "a woman riding a horse in a dirt arena."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2405433",
                "VG_object_id": "332144",
                "bbox": [184, 370, 398, 456],
                "image": "data\\images\\2405433.jpg"
            },
            {
                "VG_image_id": "2375367",
                "VG_object_id": "582666",
                "bbox": [2, 231, 499, 331],
                "image": "data\\images\\2375367.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the floor", 1],
            ["Where is the photo taken", 1],
            ["What is the floor made of", 1],
            ["what type of flooring is shown", 1],
            ["what is covering the floor", 1],
            ["what is the floor made of", 1],
            ["what is standing on the floor", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", 1],
            ["where is the shadow", 1],
            ["where is this scene", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is on the floor", 1],
            ["Where is the photo taken", 1],
            ["what pattern is the floor", -1],
            ["What is the floor made of", 1],
            ["what shape is the floor", -1],
            ["what type of flooring is shown", 1],
            ["where is the floor", -1],
            ["what is covering the floor", 1],
            ["what is the floor made of", 1],
            ["what is standing on the floor", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", 1],
            ["where is the shadow", 1],
            ["where is this scene", 1]
        ],
        "context": [
            "a little boy playing a video game",
            "a woman swinging a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2404184",
                "VG_object_id": "378207",
                "bbox": [5, 224, 237, 496],
                "image": "data\\images\\2404184.jpg"
            },
            {
                "VG_image_id": "2325724",
                "VG_object_id": "985297",
                "bbox": [85, 48, 218, 243],
                "image": "data\\images\\2325724.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shorts", 2],
            ["what in the man's doing", 1],
            ["what is the man's posture", 1],
            ["what is the man wearing", 1],
            ["what are the people doing", 1],
            ["how many people are there", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what in the man's doing", 1],
            ["what color is the man's shorts", 2],
            ["what is the man's posture", 1],
            ["where is the photo taken", -1],
            ["what is the man wearing on his head", -1],
            ["what is the persion holding", -1],
            ["what is the man wearing", 1],
            ["what are the people doing", 1],
            ["who is in the photo", -1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["what is the persion standing on", 1]
        ],
        "context": [
            "a group of people riding in a boat on a lake.",
            "a man riding a surfboard on top of a wave."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2386614",
                "VG_object_id": "1284335",
                "bbox": [136, 189, 238, 306],
                "image": "data\\images\\2386614.jpg"
            },
            {
                "VG_image_id": "2352835",
                "VG_object_id": "1957041",
                "bbox": [230, 85, 353, 174],
                "image": "data\\images\\2352835.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 2],
            ["what color is the shirt", 1],
            ["what is the person doing", 1],
            ["what is the person wearing", 1],
            ["what is the persion wearing on his head", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the person doing", 1],
            ["what is the person wearing", 1],
            ["how many people are there in the picture", -1],
            ["where is the person", 2],
            ["what is the persion wearing on his head", 1],
            ["who is in the picture", 1]
        ],
        "context": [
            "a baseball player is swinging a bat at a ball.",
            "a woman riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2316781",
                "VG_object_id": "3032673",
                "bbox": [3, 201, 302, 497],
                "image": "data\\images\\2316781.jpg"
            },
            {
                "VG_image_id": "2415745",
                "VG_object_id": "3463945",
                "bbox": [222, 178, 487, 278],
                "image": "data\\images\\2415745.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is in the man's hands", 1],
            ["what is on the man's shirt", 1]
        ],
        "org_questions": [
            ["what are the people doing", -1],
            ["what color is the shirt", 2],
            ["where is the person", -1],
            ["what is in the man's hands", 1],
            ["what is the person's age", -1],
            ["how many people are in the photo", -1],
            ["what kind of shirt is the man wearing", -1],
            ["who is in the picture", -1],
            ["what is the man wearing", -1],
            ["what is on the man's shirt", 1]
        ],
        "context": [
            "a man wearing a white shirt and sunglasses.",
            "a woman holding a bird on her hand."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2337578",
                "VG_object_id": "3205074",
                "bbox": [171, 8, 482, 461],
                "image": "data\\images\\2337578.jpg"
            },
            {
                "VG_image_id": "2363165",
                "VG_object_id": "1782385",
                "bbox": [103, 0, 498, 497],
                "image": "data\\images\\2363165.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["how many elephants are there in the picture", 1],
            ["what color is the background", -1],
            ["what are the elephants doing", -1],
            ["where is the elephant", -1],
            ["what is in the background", -1],
            ["what is in front of the elephants", -1],
            ["where is the nose of the elephant", -1],
            ["what type of animal is shown", -1],
            ["when was the photo taken", -1],
            ["what is on the elephant's head", -1],
            ["what is the elephant standing on", -1]
        ],
        "context": [
            "an elephant standing next to a tire on a road.",
            "a group of elephants standing next to each other."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2377932",
                "VG_object_id": "715744",
                "bbox": [266, 19, 430, 200],
                "image": "data\\images\\2377932.jpg"
            },
            {
                "VG_image_id": "2417312",
                "VG_object_id": "3348443",
                "bbox": [191, 20, 319, 149],
                "image": "data\\images\\2417312.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is on the head on the man", 2],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["what color is the background", 1],
            ["what is the man wearing on the head", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["How many people are there", -1],
            ["what color is the background", 1],
            ["how old is the man", -1],
            ["what is on the head on the man", 2],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is on the man's face", -1],
            ["what is the man wearing on the head", 1]
        ],
        "context": [
            "a man sitting on a bench with his dog.",
            "a man flying through the air while riding a skateboard."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2382873",
                "VG_object_id": "695588",
                "bbox": [197, 0, 417, 279],
                "image": "data\\images\\2382873.jpg"
            },
            {
                "VG_image_id": "2378212",
                "VG_object_id": "561311",
                "bbox": [219, 1, 500, 117],
                "image": "data\\images\\2378212.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["what is in front of the building", 1],
            ["how many people are there in front of the building", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["what is the building made of", -1],
            ["how many cars are there", 1],
            ["when is the picture taken", -1],
            ["how is the weather", -1],
            ["what is on the ground", -1],
            ["where was this photo taken", -1],
            ["where are the buildings", -1],
            ["what is in the background", -1],
            ["when was the photo taken", -1],
            ["what kind of vehicles are in front of the building", -1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["what is in front of the building", 1],
            ["how many people are there in front of the building", 1]
        ],
        "context": [
            "a street with a lot of cars and a traffic light.",
            "a woman walking her dog on a leash next to a river."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2380897",
                "VG_object_id": "708591",
                "bbox": [2, 205, 499, 374],
                "image": "data\\images\\2380897.jpg"
            },
            {
                "VG_image_id": "2330443",
                "VG_object_id": "3043337",
                "bbox": [8, 182, 394, 498],
                "image": "data\\images\\2330443.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is on the land", -1],
            ["what color is the land", -1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["when was the photo taken", -1],
            ["where was this photo taken", -1],
            ["how is the weather", -1],
            ["where is the shadow", -1]
        ],
        "context": [
            "a bus driving down a snowy road in front of a gas station.",
            "a fire hydrant on a sidewalk in a park."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2329102",
                "VG_object_id": "3199916",
                "bbox": [9, 2, 366, 212],
                "image": "data\\images\\2329102.jpg"
            },
            {
                "VG_image_id": "2371871",
                "VG_object_id": "2133452",
                "bbox": [135, 5, 344, 287],
                "image": "data\\images\\2371871.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the woman's shoes", 2],
            ["what is the woman on", 1],
            ["where is the girl", 1],
            ["what  is the girl wearing", 1],
            ["what is the girl doing", 1],
            ["what kind of shoes is the woman wearing", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color are the woman's shoes", 2],
            ["what is the woman on", 1],
            ["how many people are there in the picture", -1],
            ["where is the girl", 1],
            ["what  is the girl wearing", 1],
            ["what is the girl doing", 1],
            ["what is the gender of the person on the left", -1],
            ["when was the photo taken", -1],
            ["what kind of shoes is the woman wearing", 1],
            ["what is on the woman's feet", -1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a dog with a red shoe on its back.",
            "a woman sitting on a motorcycle with her hand up."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2341089",
                "VG_object_id": "3290265",
                "bbox": [96, 51, 208, 298],
                "image": "data\\images\\2341089.jpg"
            },
            {
                "VG_image_id": "2414143",
                "VG_object_id": "159136",
                "bbox": [229, 99, 400, 320],
                "image": "data\\images\\2414143.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the ground", 2],
            ["what is on the man's head", 2],
            ["What color is person's shirt", 1],
            ["WHat color is person's trouser", 1]
        ],
        "org_questions": [
            ["What color is person's shirt", 1],
            ["What color is the ground", 2],
            ["WHat color is person's trouser", 1],
            ["how many players are there in the photo", -1],
            ["What is the person holding", -1],
            ["what is in the background", -1],
            ["what is the player doing", -1],
            ["who is playing tennis", -1],
            ["what is on the man's head", 2],
            ["where is the man", -1],
            ["what sport is being played", -1]
        ],
        "context": [
            "a man jumping in the air with a tennis racket.",
            "a man playing tennis on a clay court."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2348223",
                "VG_object_id": "883917",
                "bbox": [237, 75, 310, 202],
                "image": "data\\images\\2348223.jpg"
            },
            {
                "VG_image_id": "2350481",
                "VG_object_id": "2043401",
                "bbox": [36, 78, 78, 166],
                "image": "data\\images\\2350481.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["where is the man", 2],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["where is the man", 2],
            ["what is the man wearing", -1],
            ["how many people are there in the photo", -1],
            ["what time is it", -1],
            ["what color is the background", 1],
            ["what is the gender of the person", -1],
            ["What is weather like", -1],
            ["who is in the picture", -1],
            ["when was this photo taken", -1],
            ["what color is the man's hair", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man is sitting on a motorcycle on the road.",
            "a refrigerator that is sitting on a brick sidewalk."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2391243",
                "VG_object_id": "490500",
                "bbox": [0, 3, 498, 374],
                "image": "data\\images\\2391243.jpg"
            },
            {
                "VG_image_id": "2366129",
                "VG_object_id": "1676584",
                "bbox": [3, 2, 498, 329],
                "image": "data\\images\\2366129.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["what color is the table", 1],
            ["what is on the counter", 1]
        ],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what color is the woman's shirt", 1],
            ["who is in the kitchen", -1],
            ["what color is the table", 1],
            ["what are the cabinets made of", -1],
            ["what room is this", -1],
            ["how many people are there", -1],
            ["what is on the counter", 1],
            ["where is this picture taken", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a woman standing in a kitchen making food.",
            "a woman is putting something in the oven."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2336740",
                "VG_object_id": "3504434",
                "bbox": [120, 41, 324, 336],
                "image": "data\\images\\2336740.jpg"
            },
            {
                "VG_image_id": "2353422",
                "VG_object_id": "846105",
                "bbox": [70, 171, 217, 454],
                "image": "data\\images\\2353422.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man wearing on his head", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["where is the man", -1],
            ["what color is the man's pant", -1],
            ["how many people are there", -1],
            ["what is the man wearing on his head", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["where is the photo taken", -1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a police officer riding a motorcycle down a street.",
            "a man standing on a street corner next to a street sign."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2400036",
                "VG_object_id": "1163690",
                "bbox": [55, 424, 333, 499],
                "image": "data\\images\\2400036.jpg"
            },
            {
                "VG_image_id": "2323579",
                "VG_object_id": "3239988",
                "bbox": [3, 220, 498, 324],
                "image": "data\\images\\2323579.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what color is the plate", 1],
            ["what is next to the table", 1],
            ["what is covering the table", 1],
            ["what is on top of the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what color is the plate", 1],
            ["what is on the plate", -1],
            ["what shape is the table", -1],
            ["how many plates are there", -1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["what is the food on", -1],
            ["what is next to the table", 1],
            ["what is covering the table", 1],
            ["what is on top of the table", 1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a man and a child are cutting a cake.",
            "a baby sitting in a high chair with a piece of cake on it."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2355077",
                "VG_object_id": "1863747",
                "bbox": [215, 40, 472, 331],
                "image": "data\\images\\2355077.jpg"
            },
            {
                "VG_image_id": "2392356",
                "VG_object_id": "480756",
                "bbox": [42, 48, 297, 458],
                "image": "data\\images\\2392356.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's shirt", 2],
            ["what is the child holding", 1],
            ["who has long hair", 1],
            ["What is child doing", 1],
            ["what is the child on", 1],
            ["who is in the photo", 1],
            ["what is the person doing", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", 2],
            ["what is the child holding", 1],
            ["how many people are there", -1],
            ["who has long hair", 1],
            ["What is child doing", 1],
            ["what is the child on", 1],
            ["when was this photo taken", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", 1],
            ["what is the person doing", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a boy looks out the window of a train as it passes by.",
            "a young girl eating a doughnut with a smile on her face."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2390334",
                "VG_object_id": "1251417",
                "bbox": [262, 172, 332, 372],
                "image": "data\\images\\2390334.jpg"
            },
            {
                "VG_image_id": "2356011",
                "VG_object_id": "824424",
                "bbox": [266, 315, 313, 462],
                "image": "data\\images\\2356011.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the woman wearing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what are the people doing", -1],
            ["What color is the woman's top", -1],
            ["When is photo taken", -1],
            ["What is woman holding", -1],
            ["what is the woman wearing", 1],
            ["What is woman doing", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is the woman wearing on the head", -1],
            ["when was the photo taken", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a group of people posing for a picture.",
            "a woman taking a picture of a traffic light."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2396300",
                "VG_object_id": "1199060",
                "bbox": [54, 85, 442, 261],
                "image": "data\\images\\2396300.jpg"
            },
            {
                "VG_image_id": "2348396",
                "VG_object_id": "2617207",
                "bbox": [125, 124, 440, 387],
                "image": "data\\images\\2348396.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bus", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the bus", 2],
            ["what is on the side of the bus", -1],
            ["how many people are there", 1],
            ["where is the bus", -1],
            ["how many decks does the bus have", -1],
            ["what color of the road does the bus park on", -1],
            ["what kind of vehicle is this", -1],
            ["when was the picture taken", -1],
            ["what is the bus doing", -1],
            ["what type of bus is this", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a bus driving down a street next to a sidewalk.",
            "a bus is stopped at a bus stop."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "1159351",
                "VG_object_id": "1592109",
                "bbox": [49, 109, 376, 933],
                "image": "data\\images\\1159351.jpg"
            },
            {
                "VG_image_id": "2403603",
                "VG_object_id": "1123215",
                "bbox": [1, 1, 148, 150],
                "image": "data\\images\\2403603.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is on the man's face", 2],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 1],
            ["where is the photo taken", -1],
            ["how many people are there", -1],
            ["what is the man wearing on his head", -1],
            ["What is man doing", -1],
            ["what gesture is the man", -1],
            ["what is in the background", -1],
            ["what is the person wearing", -1],
            ["what is on the man's face", 2],
            ["where is the man", -1]
        ],
        "context": [
            "a group of men standing next to each other holding wii controllers.",
            "a boy is standing in front of a birthday cake."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2354385",
                "VG_object_id": "838923",
                "bbox": [1, 1, 496, 328],
                "image": "data\\images\\2354385.jpg"
            },
            {
                "VG_image_id": "2369769",
                "VG_object_id": "2004640",
                "bbox": [0, 185, 492, 332],
                "image": "data\\images\\2369769.jpg"
            }
        ],
        "questions_with_scores": [["How many people are there", 1]],
        "org_questions": [
            ["How many people are there", 1],
            ["What is on the ground", -1],
            ["What season is it", -1],
            ["what is the land made of", -1],
            ["what is the picture taken", -1],
            ["how many tennis rackets are there on the ground", -1],
            ["where is the snow", -1],
            ["how is the weather", -1],
            ["when was the photo taken", -1],
            ["where was this picture taken", -1],
            ["what are the people doing", -1]
        ],
        "context": [
            "a man is skiing down a snowy hill.",
            "a couple of people skiing down a snow covered slope."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "1159944",
                "VG_object_id": "3705899",
                "bbox": [154, 336, 612, 923],
                "image": "data\\images\\1159944.jpg"
            },
            {
                "VG_image_id": "2347774",
                "VG_object_id": "3609254",
                "bbox": [203, 25, 326, 316],
                "image": "data\\images\\2347774.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["what color is the person's shirt", 1],
            ["where is the person", 1],
            ["How many people are there", 1],
            ["what color is the ground", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what color is the person's shirt", 1],
            ["where is the person", 1],
            ["How many people are there", 1],
            ["what is the person wearing", -1],
            ["what is the person holding", -1],
            ["what color is the ground", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a man sitting on a bench looking out over the water.",
            "a red motorcycle parked on a showroom floor."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2317482",
                "VG_object_id": "3410422",
                "bbox": [211, 125, 381, 373],
                "image": "data\\images\\2317482.jpg"
            },
            {
                "VG_image_id": "2382929",
                "VG_object_id": "1324969",
                "bbox": [7, 6, 301, 289],
                "image": "data\\images\\2382929.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the girl", 1],
            ["What color is the background", 1],
            ["who is playing tennis", 1]
        ],
        "org_questions": [
            ["What color is the girl", 1],
            ["What color is the background", 1],
            ["how many people are there", -1],
            ["What is girl doing", -1],
            ["where is the woman", -1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["who is playing tennis", 1],
            ["what sport is the woman playing", -1],
            ["what is the woman holding", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a young girl playing tennis on a tennis court.",
            "a woman in a blue tank top is holding a tennis racket."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2336337",
                "VG_object_id": "3271111",
                "bbox": [168, 178, 247, 446],
                "image": "data\\images\\2336337.jpg"
            },
            {
                "VG_image_id": "2414854",
                "VG_object_id": "150606",
                "bbox": [104, 117, 153, 431],
                "image": "data\\images\\2414854.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the necktie", 2],
            ["what is the man holding in hands", 2],
            ["what is the man's hairstyle", 1],
            ["What is man holding", 1],
            ["what pattern is on the man's tie", 1]
        ],
        "org_questions": [
            ["what color is the necktie", 2],
            ["what is the man holding in hands", 2],
            ["what is the man's hairstyle", 1],
            ["who is wearing the necktie", -1],
            ["where is the person", -1],
            ["what is the gender of the person", -1],
            ["what is the man doing", -1],
            ["What is man holding", 1],
            ["what pattern is on the man's tie", 1],
            ["what is the man wearing on his neck", -1],
            ["what is around the man's neck", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man wearing a striped shirt and tie.",
            "a man holding a tie in his hands."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2379129",
                "VG_object_id": "554800",
                "bbox": [2, 178, 451, 499],
                "image": "data\\images\\2379129.jpg"
            },
            {
                "VG_image_id": "2415869",
                "VG_object_id": "3081180",
                "bbox": [206, 126, 498, 373],
                "image": "data\\images\\2415869.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 2],
            ["What food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the plate", 2],
            ["how many plates are in the picture", -1],
            ["what shape is the plate", -1],
            ["what is the table made of", -1],
            ["What food is on the plate", 1],
            ["what is the plate on ", -1],
            ["what is beside the plate", -1],
            ["where was the photo taken", -1],
            ["where is the fork", -1],
            ["what is on the table", -1],
            ["what is the food on", -1],
            ["what is the color of the table", -1]
        ],
        "context": [
            "a plate of food with a sandwich and some fries.",
            "a table with plates of food on it"
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2404590",
                "VG_object_id": "376649",
                "bbox": [3, 0, 336, 439],
                "image": "data\\images\\2404590.jpg"
            },
            {
                "VG_image_id": "2355493",
                "VG_object_id": "2906867",
                "bbox": [8, 11, 394, 310],
                "image": "data\\images\\2355493.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["how many clock on the building", 1]
        ],
        "org_questions": [
            ["what color is the building", 2],
            ["how many clock on the building", 1],
            ["what shape is the building", -1],
            ["what is on the wall", -1],
            ["where is the building", -1],
            ["what is the building made of", -1],
            ["when was this photo taken", -1],
            ["where was this photo taken", -1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a fountain with a bird on it",
            "a truck is driving down the street in front of a large building."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2381172",
                "VG_object_id": "706671",
                "bbox": [156, 221, 209, 288],
                "image": "data\\images\\2381172.jpg"
            },
            {
                "VG_image_id": "2388541",
                "VG_object_id": "3829325",
                "bbox": [178, 149, 245, 201],
                "image": "data\\images\\2388541.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's trousers", 1],
            ["what sport is it", 1],
            ["where is the girl on", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color is the person's trousers", 1],
            ["what sport is it", 1],
            ["what is the weather like", -1],
            ["how many people are there", -1],
            ["where is the girl on", 1],
            ["what is the gender of the person", -1],
            ["how old is the girl", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a boy skateboarding at a skate park",
            "a skier is skiing down a hill."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2397175",
                "VG_object_id": "1192076",
                "bbox": [253, 50, 368, 220],
                "image": "data\\images\\2397175.jpg"
            },
            {
                "VG_image_id": "2319353",
                "VG_object_id": "3064641",
                "bbox": [160, 189, 276, 454],
                "image": "data\\images\\2319353.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the person's shirt", 2],
            ["what is the person doing", 2],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the person wearing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["what is the color of the person's shirt", 2],
            ["what is the person doing", 2],
            ["what is the person on", -1],
            ["how many people are there", 1],
            ["when is the picture taken", -1],
            ["where is the person", 1],
            ["what is the person wearing", 1],
            ["how many dogs are there in the picture", -1],
            ["who is in the photo", -1],
            ["what is the persion holding", -1],
            ["when was the photo taken", -1],
            ["what is the main color of the shirt", 1]
        ],
        "context": [
            "a person riding a skateboard on a road.",
            "a group of young people playing basketball on a court."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2378847",
                "VG_object_id": "1363859",
                "bbox": [126, 272, 183, 384],
                "image": "data\\images\\2378847.jpg"
            },
            {
                "VG_image_id": "2412851",
                "VG_object_id": "3395338",
                "bbox": [2, 201, 90, 373],
                "image": "data\\images\\2412851.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's jacket", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's jacket", 1],
            ["what is the woman doing", -1],
            ["where is the woman", -1],
            ["how many people are there", 1],
            ["what is the woman wearing", -1],
            ["what is the woman holding", 1],
            ["when was the photo taken", -1],
            ["what is the man sitting on", -1],
            ["what is on the bench", -1]
        ],
        "context": [
            "two people sitting on a bench under an umbrella.",
            "a group of people waiting for their luggage."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2391897",
                "VG_object_id": "1236258",
                "bbox": [104, 27, 186, 368],
                "image": "data\\images\\2391897.jpg"
            },
            {
                "VG_image_id": "2355906",
                "VG_object_id": "2774780",
                "bbox": [194, 131, 333, 320],
                "image": "data\\images\\2355906.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is main color of tower", 2],
            ["what color is the sky", 2],
            ["what color is the ground", 2],
            ["what is the color of the tower", 1],
            ["what is the ground covered with", 1],
            ["what time is on the clock", 1],
            ["what is the weather like", 1],
            ["how many clocks are there", 1]
        ],
        "org_questions": [
            ["what is the color of the tower", 1],
            ["where is the clock", -1],
            ["what is the ground covered with", 1],
            ["what time is on the clock", 1],
            ["what is the weather like", 1],
            ["when is this picture taken", -1],
            ["What is main color of tower", 2],
            ["what color is the sky", 2],
            ["how many clocks are there", 1],
            ["what time is it", -1],
            ["what is on the building", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a clock tower in a large city.",
            "a clock tower in the middle of a park."
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2388285",
                "VG_object_id": "509239",
                "bbox": [257, 120, 373, 229],
                "image": "data\\images\\2388285.jpg"
            },
            {
                "VG_image_id": "2416999",
                "VG_object_id": "1057380",
                "bbox": [1, 173, 255, 445],
                "image": "data\\images\\2416999.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the motorcycle", 1],
            ["what is in the background", 1],
            ["what is on the motorcycle", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["where is the motorcycle", 1],
            ["who is riding the motorcycle", 1]
        ],
        "org_questions": [
            ["what color is the motorcycle", 1],
            ["what is in the background", 1],
            ["what is on the motorcycle", 1],
            ["how many people are there", 1],
            ["where are the motor ", -1],
            ["where is the photo taken", 1],
            ["how many motorcycles are there on the ground", -1],
            ["where is the motorcycle", 1],
            ["what is the persion doing", -1],
            ["who is riding the motorcycle", 1],
            ["what is the persion riding on", -1]
        ],
        "context": [
            "a man on a motorcycle is flying through the air.",
            "a woman riding a motorcycle with a dog on the back."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2348205",
                "VG_object_id": "2532137",
                "bbox": [10, 92, 495, 367],
                "image": "data\\images\\2348205.jpg"
            },
            {
                "VG_image_id": "2345434",
                "VG_object_id": "3624587",
                "bbox": [0, 290, 319, 498],
                "image": "data\\images\\2345434.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bulls are there on the ground", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what animals are on the ground", -1],
            ["what color is the grass", -1],
            ["how many bulls are there on the ground", 1],
            ["what is in the distance", -1],
            ["what is the weather like", -1],
            ["what kind of animal is there", -1],
            ["where was this photo taken", -1],
            ["what is on the ground", -1],
            ["what animal is on the ground", -1],
            ["How many people are there", -1],
            ["where was this picture taken", -1],
            ["who is in the field", -1],
            ["where is the grass", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a herd of cattle grazing in a field.",
            "two cows standing in a field with a castle in the background."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2382424",
                "VG_object_id": "1328786",
                "bbox": [141, 281, 233, 374],
                "image": "data\\images\\2382424.jpg"
            },
            {
                "VG_image_id": "2376730",
                "VG_object_id": "570152",
                "bbox": [223, 337, 325, 375],
                "image": "data\\images\\2376730.jpg"
            }
        ],
        "questions_with_scores": [["what is the man in the shirt holding", 1]],
        "org_questions": [
            ["what color is the shirt upon the trouser", -1],
            ["what is the man in the shirt holding", 1],
            ["what color is the hair of the man in the shirt", -1],
            ["how many people are there", -1],
            ["who is wearing the trousers", -1],
            ["where is the man", -1],
            ["WHat is man doing", -1],
            ["when was this photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["what kind of shirt is the man wearing", -1]
        ],
        "context": [
            "two men standing in the woods",
            "a man holding a surfboard in front of a tree."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2317981",
                "VG_object_id": "2903503",
                "bbox": [7, 185, 499, 330],
                "image": "data\\images\\2317981.jpg"
            },
            {
                "VG_image_id": "2386993",
                "VG_object_id": "679755",
                "bbox": [2, 153, 496, 332],
                "image": "data\\images\\2386993.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are there on the road", 1],
            ["how many animals are there on the road", 1]
        ],
        "org_questions": [
            ["what is on the road", -1],
            ["how many dogs are there on the road", 1],
            ["how many animals are there on the road", 1],
            ["what time is it", -1],
            ["how is the weather", -1],
            ["when is the photo taken", -1],
            ["what is in the background", -1],
            ["where was this photo taken", -1],
            ["what is the road made of", -1],
            ["where are the white lines", -1]
        ],
        "context": [
            "a man riding a bike with a dog on the back of it.",
            "a police officer riding a motorcycle down a street."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2379731",
                "VG_object_id": "2935843",
                "bbox": [1, 41, 182, 122],
                "image": "data\\images\\2379731.jpg"
            },
            {
                "VG_image_id": "2412613",
                "VG_object_id": "190579",
                "bbox": [0, 199, 499, 281],
                "image": "data\\images\\2412613.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are on the street", 2],
            ["when is the picture taken", 2],
            ["what time is it", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["how many cars are on the street", 2],
            ["when is the picture taken", 2],
            ["what is the road made of", -1],
            ["what is in the background", -1],
            ["what time is it", 1],
            ["where was this picture taken", -1],
            ["what is on the ground", 1],
            ["what color is the ground", -1]
        ],
        "context": [
            "a suitcase on the sidewalk next to a sidewalk.",
            "a group of people standing outside of a restaurant."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2417311",
                "VG_object_id": "2882555",
                "bbox": [315, 166, 452, 306],
                "image": "data\\images\\2417311.jpg"
            },
            {
                "VG_image_id": "2385552",
                "VG_object_id": "1295529",
                "bbox": [239, 29, 397, 128],
                "image": "data\\images\\2385552.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person doing", 2],
            ["what is the person wearing on his head", 1],
            ["what shape is the shirt's collar", 1],
            ["what sport is it", 1],
            ["what kind of pants is the man wearing", 1],
            ["what type of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person doing", 2],
            ["what is the person wearing on his head", 1],
            ["what shape is the shirt's collar", 1],
            ["what gender is the person in the shirt", -1],
            ["Where is person", -1],
            ["what is the person in the shirt holding", -1],
            ["what sport is it", 1],
            ["how many people are in the photo", -1],
            ["when was this photo taken", -1],
            ["what kind of pants is the man wearing", 1],
            ["what type of shirt is the man wearing", 1]
        ],
        "context": [
            "a man throwing a frisbee in a park.",
            "a young man riding a skateboard on a sidewalk."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "150350",
                "VG_object_id": "1566889",
                "bbox": [6, 58, 991, 748],
                "image": "data\\images\\150350.jpg"
            },
            {
                "VG_image_id": "2392356",
                "VG_object_id": "480777",
                "bbox": [3, 392, 293, 499],
                "image": "data\\images\\2392356.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["How many people are there", 1],
            ["what is the table made of", 1],
            ["how many wine glasses are there on the table", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["How many people are there", 1],
            ["what is the table made of", 1],
            ["how many wine glasses are there on the table", 1],
            ["who is in the photo", 1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "two cakes sitting on a table with a knife and fork.",
            "a young girl eating a doughnut with a smile on her face."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2410717",
                "VG_object_id": "1618982",
                "bbox": [28, 271, 172, 331],
                "image": "data\\images\\2410717.jpg"
            },
            {
                "VG_image_id": "2417919",
                "VG_object_id": "3096549",
                "bbox": [2, 365, 221, 498],
                "image": "data\\images\\2417919.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is placed on the rug", 2],
            ["what color is the rug", 1],
            ["where is the photo taken", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["where is the rug", -1],
            ["what color is the rug", 1],
            ["what is beside the rug", -1],
            ["how many people are there on the rug", -1],
            ["what is the ground covered with", -1],
            ["where is the photo taken", 1],
            ["what color is the wall beside the rug", -1],
            ["what room is this", 1],
            ["what is placed on the rug", 2]
        ],
        "context": [
            "a bedroom with a bed, dresser, and a window.",
            "a bathroom with a toilet, sink, and a mirror."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2340608",
                "VG_object_id": "3659461",
                "bbox": [203, 202, 303, 285],
                "image": "data\\images\\2340608.jpg"
            },
            {
                "VG_image_id": "2345592",
                "VG_object_id": "2299355",
                "bbox": [196, 196, 482, 373],
                "image": "data\\images\\2345592.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many chairs are there", 1],
            ["what is on the table", 1],
            ["where is the table", 1],
            ["what is next to the table", 1],
            ["what is sitting on the table", 1]
        ],
        "org_questions": [
            ["how many chairs are there", 1],
            ["what is on the table", 1],
            ["what shape is the table", -1],
            ["where is the table", 1],
            ["what color is the table", -1],
            ["what color is the floor", -1],
            ["what is the table made out of", -1],
            ["what is next to the table", 1],
            ["what is under the table", -1],
            ["what is the table color", -1],
            ["what is sitting on the table", 1]
        ],
        "context": [
            "a table and chairs in a courtyard with a table and vases.",
            "a living room with a couch, coffee table, and a coffee table."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2373687",
                "VG_object_id": "2129193",
                "bbox": [268, 205, 376, 267],
                "image": "data\\images\\2373687.jpg"
            },
            {
                "VG_image_id": "2412142",
                "VG_object_id": "307024",
                "bbox": [420, 194, 481, 259],
                "image": "data\\images\\2412142.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the box", 1],
            ["where is the box", 1],
            ["what is on the side of the box", 1],
            ["what is the box placed on", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["what color is the box", 1],
            ["where is the box", 1],
            ["how many people are there", 2],
            ["what is on the side of the box", 1],
            ["what is the box placed on", 1],
            ["where was this photo taken", 1]
        ],
        "context": [
            "a man sitting in a chair in a kitchen.",
            "a living room with a couch, chair, and television."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2384612",
                "VG_object_id": "525578",
                "bbox": [384, 286, 449, 332],
                "image": "data\\images\\2384612.jpg"
            },
            {
                "VG_image_id": "2368430",
                "VG_object_id": "2539464",
                "bbox": [398, 208, 478, 255],
                "image": "data\\images\\2368430.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the truck", 1],
            ["what is on the side of the truck", 1],
            ["where is the truck", 1],
            ["where was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the truck", 1],
            ["what is on the side of the truck", 1],
            ["what color is the ground", -1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["where is the truck", 1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1],
            ["what kind of vehicle is in the picture", -1],
            ["when was the photo taken", -1],
            ["where was the picture taken", 1],
            ["what type of vehicle is shown", -1]
        ],
        "context": [
            "a large jetliner sitting on top of an airport tarmac.",
            "a group of people riding motorcycles down a street."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2343290",
                "VG_object_id": "925409",
                "bbox": [12, 0, 284, 217],
                "image": "data\\images\\2343290.jpg"
            },
            {
                "VG_image_id": "2358401",
                "VG_object_id": "1863964",
                "bbox": [0, 1, 485, 291],
                "image": "data\\images\\2358401.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many decks does the bus have", 2],
            ["what color is the bus in front of the building", 1],
            ["how many decks does the bus in front of the building have", 1],
            ["what color is the wall", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["what color is the bus in front of the building", 1],
            ["how many decks does the bus in front of the building have", 1],
            ["Where is the photo taken", -1],
            ["what is the building made of", -1],
            ["what is in the background", -1],
            ["what color is the wall", 1],
            ["what kind of vehicle is this", -1],
            ["when was the picture taken", -1],
            ["what is the bus doing", -1],
            ["what type of bus is this", -1],
            ["how many decks does the bus have", 2]
        ],
        "context": [
            "a red double decker bus parked on the side of the road.",
            "a bus with graffiti on the side of it."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2322100",
                "VG_object_id": "3479875",
                "bbox": [14, 11, 487, 296],
                "image": "data\\images\\2322100.jpg"
            },
            {
                "VG_image_id": "2317373",
                "VG_object_id": "1021501",
                "bbox": [194, 139, 330, 202],
                "image": "data\\images\\2317373.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are there", 2],
            ["what color is the ground under the dogs", 2]
        ],
        "org_questions": [
            ["how many dogs are there", 2],
            ["what color is the ground under the dogs", 2],
            ["what is on the dog's neck", -1],
            ["what is the dog doing ", -1],
            ["where is the dog", -1],
            ["what gesture is the dog", -1],
            ["when was this photo taken", -1],
            ["what is on the ground", -1],
            ["when was the picture taken", -1],
            ["what is the dog on", -1],
            ["what color are the animals", -1]
        ],
        "context": [
            "two dogs are running around on a dirt field.",
            "a dog chasing a herd of sheep in a field."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2411317",
                "VG_object_id": "313431",
                "bbox": [63, 290, 165, 385],
                "image": "data\\images\\2411317.jpg"
            },
            {
                "VG_image_id": "2326974",
                "VG_object_id": "3408989",
                "bbox": [262, 91, 482, 449],
                "image": "data\\images\\2326974.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many layers are there in cake", 2],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["How many layers are there in cake", 2],
            ["How many people are there", 1],
            ["What color is the top of cake", -1],
            ["What is cake on", -1],
            ["What is on the cake", -1],
            ["What color is the table", -1],
            ["how many candles are there on the cake", -1],
            ["what color is the cake", -1],
            ["what is the table made of", -1],
            ["where is the cake", -1]
        ],
        "context": [
            "a man holding a cake on a plate.",
            "a man and woman cutting a wedding cake."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2374768",
                "VG_object_id": "3295624",
                "bbox": [269, 207, 347, 314],
                "image": "data\\images\\2374768.jpg"
            },
            {
                "VG_image_id": "2320567",
                "VG_object_id": "2898482",
                "bbox": [156, 44, 310, 301],
                "image": "data\\images\\2320567.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["what is on the man's head", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what color is the person's shirt", -1],
            ["how many persons are in the picture", -1],
            ["what is on the man's head", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["what is the weather like", -1],
            ["what is the person holding", 1],
            ["who is playing", -1],
            ["what sport is being played", 1],
            ["what is the man standing on", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man playing tennis on a tennis court.",
            "a man running in a field with a frisbee."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2381842",
                "VG_object_id": "1333653",
                "bbox": [1, 0, 499, 375],
                "image": "data\\images\\2381842.jpg"
            },
            {
                "VG_image_id": "2403502",
                "VG_object_id": "657146",
                "bbox": [6, 5, 374, 498],
                "image": "data\\images\\2403502.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many sinks are in the bathroom", 2],
            ["what color is the wall", 1],
            ["what is the pattern of the wall", 1],
            ["What is the texture of wall", 1]
        ],
        "org_questions": [
            ["how many sinks are in the bathroom", 2],
            ["what color is the wall", 1],
            ["what is the pattern of the wall", 1],
            ["What is the texture of wall", 1],
            ["how may toilets are there", -1],
            ["how many people are there", -1],
            ["where was this photo taken", -1],
            ["who is in the photo", -1],
            ["what room is this", -1],
            ["what is the wall made of", -1],
            ["where is the mirror", -1]
        ],
        "context": [
            "a bathroom with black and white tiles and a sink.",
            "a bathroom with a sink and a mirror"
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2361628",
                "VG_object_id": "2601331",
                "bbox": [0, 47, 336, 332],
                "image": "data\\images\\2361628.jpg"
            },
            {
                "VG_image_id": "2328436",
                "VG_object_id": "3936141",
                "bbox": [271, 0, 499, 373],
                "image": "data\\images\\2328436.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's shirt", 2],
            ["what is the girl holding", 1],
            ["what color is the table", 1],
            ["what is the girl looking at", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", 2],
            ["what is the girl holding", 1],
            ["what color is the table", 1],
            ["how many people are there", -1],
            ["what is in front of the girl", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the girl wearing", -1],
            ["what is the girl looking at", 1]
        ],
        "context": [
            "a little girl writing on a piece of paper.",
            "a little girl is trying to eat a cake."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2391265",
                "VG_object_id": "490362",
                "bbox": [54, 130, 174, 322],
                "image": "data\\images\\2391265.jpg"
            },
            {
                "VG_image_id": "2386102",
                "VG_object_id": "519815",
                "bbox": [173, 81, 353, 326],
                "image": "data\\images\\2386102.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's shirt", 2],
            ["what color is the boy's helmet", 1],
            ["what color is the boy's trouser", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the boy's shirt", 2],
            ["what color is the boy's helmet", 1],
            ["what color is the boy's trouser", 1],
            ["who is wearing glasses", -1],
            ["how many people are there", 1],
            ["what is the persion holding", -1],
            ["what is on the batter's head", -1],
            ["what sport is being played", -1],
            ["what is behind the batter", -1],
            ["what is the batter wearing", -1]
        ],
        "context": [
            "a boy swinging a bat at a ball on a baseball field.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2365899",
                "VG_object_id": "1997596",
                "bbox": [1, 414, 112, 498],
                "image": "data\\images\\2365899.jpg"
            },
            {
                "VG_image_id": "150338",
                "VG_object_id": "1074300",
                "bbox": [58, 459, 350, 683],
                "image": "data\\images\\150338.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["what is beside the chair", 1],
            ["what is next to the chair", 1]
        ],
        "org_questions": [
            ["what is the chair made of", -1],
            ["what color is the chair", 2],
            ["what is beside the chair", 1],
            ["how many couches are there", -1],
            ["where is the photo taken", -1],
            ["where is the chair", -1],
            ["what is in the picture", -1],
            ["what is next to the chair", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a black cat laying on a wooden shelf.",
            "a bedroom with a large bed and a fireplace."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2415908",
                "VG_object_id": "3333318",
                "bbox": [7, 82, 498, 320],
                "image": "data\\images\\2415908.jpg"
            },
            {
                "VG_image_id": "2375941",
                "VG_object_id": "1853477",
                "bbox": [3, 18, 498, 319],
                "image": "data\\images\\2375941.jpg"
            }
        ],
        "questions_with_scores": [["what are the animals on the field ", 2]],
        "org_questions": [
            ["what is on the field", -1],
            ["what are the animals on the field ", 2],
            ["what is the field mainly made of", -1],
            ["how many women are there in the picture", -1],
            ["What color is the grass", -1],
            ["what is in the background", -1],
            ["when was this picture taken", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a group of horses walking across a lush green field.",
            "two sheep are sitting in a field of grass."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2365984",
                "VG_object_id": "755156",
                "bbox": [0, 23, 498, 280],
                "image": "data\\images\\2365984.jpg"
            },
            {
                "VG_image_id": "2347628",
                "VG_object_id": "3610116",
                "bbox": [6, 16, 494, 361],
                "image": "data\\images\\2347628.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what color is the food on the plate", 1],
            ["what kind of food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["how many plates are there on the table", -1],
            ["what color is the food on the plate", 1],
            ["what kind of food is on the plate", 1],
            ["what is the table made of", -1],
            ["how many people are there", -1],
            ["what is on the plate", -1],
            ["where was this picture taken", -1],
            ["what is the table sitting on", -1],
            ["what type of table is this", -1]
        ],
        "context": [
            "a box of pepperoni pizza on a table.",
            "two plates of food with rice and broccoli on them."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2330599",
                "VG_object_id": "972870",
                "bbox": [181, 178, 284, 465],
                "image": "data\\images\\2330599.jpg"
            },
            {
                "VG_image_id": "2360911",
                "VG_object_id": "1835652",
                "bbox": [31, 69, 256, 371],
                "image": "data\\images\\2360911.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bears are there in the photo", 1],
            ["where is the bear", 1],
            ["what is in the background", 1],
            ["what is behind the bear", 1]
        ],
        "org_questions": [
            ["what color is the bear", -1],
            ["how many bears are there in the photo", 1],
            ["where is the bear", 1],
            ["what is on the bear's neck", -1],
            ["what is the bear doing", -1],
            ["what is in the background", 1],
            ["what is the bear holding", -1],
            ["what is the bear wearing", -1],
            ["what is behind the bear", 1]
        ],
        "context": [
            "two teddy bears hanging from a car window.",
            "a teddy bear sitting on a couch in a room."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2416334",
                "VG_object_id": "3380000",
                "bbox": [211, 0, 498, 372],
                "image": "data\\images\\2416334.jpg"
            },
            {
                "VG_image_id": "2387465",
                "VG_object_id": "513099",
                "bbox": [150, 57, 462, 195],
                "image": "data\\images\\2387465.jpg"
            }
        ],
        "questions_with_scores": [["what is the cat holding", 1]],
        "org_questions": [
            ["what is the cat holding", 1],
            ["what color is the table the cat sitting on", -1],
            ["how many cats together", -1],
            ["where is the cat", -1],
            ["what is the cat sitting on", -1],
            ["what is the cat doing", -1],
            ["how many cats are in the picture", -1],
            ["what animal is in the picture", -1],
            ["what is the cat looking at", -1],
            ["what kind of animal is this", -1]
        ],
        "context": [
            "a cat laying on a couch with a remote control.",
            "a cat sitting on top of a desk next to a computer."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2370802",
                "VG_object_id": "1868380",
                "bbox": [133, 313, 245, 498],
                "image": "data\\images\\2370802.jpg"
            },
            {
                "VG_image_id": "2405039",
                "VG_object_id": "335513",
                "bbox": [106, 92, 233, 222],
                "image": "data\\images\\2405039.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man holding", 2],
            ["what is the man wearing", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["what is on the head on the man", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what is the man holding", 2],
            ["what is the man wearing", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["what is the ground covered with", -1],
            ["where is the man", 1],
            ["what is on the head on the man", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion standing on", 1],
            ["what is on the man's face", -1]
        ],
        "context": [
            "a protester holds a cross and a cross during a protest.",
            "a man riding a skateboard down a wet road."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2377790",
                "VG_object_id": "563766",
                "bbox": [95, 139, 255, 234],
                "image": "data\\images\\2377790.jpg"
            },
            {
                "VG_image_id": "2359978",
                "VG_object_id": "2010636",
                "bbox": [204, 351, 403, 429],
                "image": "data\\images\\2359978.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["What is on the bench", 1],
            ["what is the ground covered with", 1],
            ["how many people are shown", 1],
            ["who is sitting on the bench", 1]
        ],
        "org_questions": [
            ["what is near the bench", -1],
            ["what color is the ground", 2],
            ["Where is the bench", -1],
            ["what is the bench made of", -1],
            ["What is on the bench", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", -1],
            ["how many people are shown", 1],
            ["who is sitting on the bench", 1],
            ["how is the bench", -1]
        ],
        "context": [
            "a man sitting on a bench in the woods",
            "a bench in the snow"
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2374515",
                "VG_object_id": "726360",
                "bbox": [214, 146, 292, 211],
                "image": "data\\images\\2374515.jpg"
            },
            {
                "VG_image_id": "2325061",
                "VG_object_id": "3250408",
                "bbox": [123, 217, 226, 301],
                "image": "data\\images\\2325061.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the horse", 1],
            ["what color is the grass", 1]
        ],
        "org_questions": [
            ["what color is the horse", 1],
            ["what color is the grass", 1],
            ["how many horses are there", -1],
            ["what is the ground covered with", -1],
            ["what is the horse doing", -1],
            ["what animal is in the picture", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is the horse eating", -1],
            ["how many horses", -1]
        ],
        "context": [
            "a horse grazing in a field with mountains in the background.",
            "a horse in a field with mountains in the background."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2376226",
                "VG_object_id": "574815",
                "bbox": [47, 44, 483, 156],
                "image": "data\\images\\2376226.jpg"
            },
            {
                "VG_image_id": "2407443",
                "VG_object_id": "279399",
                "bbox": [12, 3, 500, 192],
                "image": "data\\images\\2407443.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["How many animal are there", 1],
            ["what is the color of the wall", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["How many animal are there", 1],
            ["Where is the photo taken", -1],
            ["what is the color of the wall", 1],
            ["What is the weather like", -1],
            ["what is the building made of", -1],
            ["when was the picture taken", -1],
            ["what is on the wall", -1]
        ],
        "context": [
            "a cow walking down a dirt road next to buildings.",
            "a man talking on a cell phone while standing in a crowd."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2360492",
                "VG_object_id": "3533907",
                "bbox": [78, 51, 335, 456],
                "image": "data\\images\\2360492.jpg"
            },
            {
                "VG_image_id": "2369143",
                "VG_object_id": "744076",
                "bbox": [107, 147, 169, 268],
                "image": "data\\images\\2369143.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what color is the man's pant", 1],
            ["what is in the background", 1],
            ["what kind of sport is the man playing", 1],
            ["how many people are shown", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what color is the man's pant", 1],
            ["where is the picture taken", -1],
            ["what is the man wearing on his head", -1],
            ["what is in the background", 1],
            ["what kind of sport is the man playing", 1],
            ["what is the man wearing", -1],
            ["how many people are shown", 1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a baseball player is about to throw a ball.",
            "a man playing tennis on a red clay court."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2411937",
                "VG_object_id": "206501",
                "bbox": [104, 104, 273, 195],
                "image": "data\\images\\2411937.jpg"
            },
            {
                "VG_image_id": "2402905",
                "VG_object_id": "1130172",
                "bbox": [104, 64, 312, 417],
                "image": "data\\images\\2402905.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the dog", 2],
            ["what is the dog doing ", 1],
            ["what kind of dog is this", 1]
        ],
        "org_questions": [
            ["What color is the dog", 2],
            ["What is the background of image", -1],
            ["how many people are there", -1],
            ["what is on the dog's neck", -1],
            ["what is the ground the dog standing on made of", -1],
            ["where is the dog", -1],
            ["what is the dog doing ", 1],
            ["what animal is in the picture", -1],
            ["when was this photo taken", -1],
            ["what kind of dog is this", 1]
        ],
        "context": [
            "a dog laying in the grass with a frisbee.",
            "a white dog standing in the grass next to a frisbee."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2317794",
                "VG_object_id": "2938492",
                "bbox": [221, 42, 358, 314],
                "image": "data\\images\\2317794.jpg"
            },
            {
                "VG_image_id": "2341927",
                "VG_object_id": "3065450",
                "bbox": [5, 1, 262, 373],
                "image": "data\\images\\2341927.jpg"
            }
        ],
        "questions_with_scores": [["what color is the wall", 1]],
        "org_questions": [
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["what color is the wall", 1],
            ["how many people are there", -1],
            ["what is in the distance", -1],
            ["what is on the woman's face", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1],
            ["what is the woman holding", -1]
        ],
        "context": [
            "a man and a woman looking at a cell phone in a store.",
            "a man and a woman standing next to each other."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2334767",
                "VG_object_id": "2197684",
                "bbox": [153, 75, 223, 163],
                "image": "data\\images\\2334767.jpg"
            },
            {
                "VG_image_id": "2360525",
                "VG_object_id": "2357813",
                "bbox": [71, 130, 139, 261],
                "image": "data\\images\\2360525.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the plant", 1],
            ["what color is the table", 1],
            ["what is on the wall", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["where is the plant", 1],
            ["what color is the table", 1],
            ["how many people are there", -1],
            ["what is on the wall", 1],
            ["what is the plant sitting on", -1],
            ["How many dogs are there in the picture", -1],
            ["when was this photo taken", -1],
            ["what is in the background", 1],
            ["where was the photo taken", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a kitchen with a table and a microwave.",
            "a living room with a fireplace and a large mirror."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2391326",
                "VG_object_id": "489908",
                "bbox": [358, 50, 426, 229],
                "image": "data\\images\\2391326.jpg"
            },
            {
                "VG_image_id": "2343676",
                "VG_object_id": "921122",
                "bbox": [176, 44, 416, 213],
                "image": "data\\images\\2343676.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what color is the man's pants", 2],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the man's pants", 2],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is the man wearing on head", -1],
            ["where is the man", -1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["what is the man standing on", -1],
            ["what is the man holding", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a young boy swinging a bat at a baseball.",
            "a baseball player throwing a ball during a game."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2384325",
                "VG_object_id": "1310791",
                "bbox": [174, 157, 499, 240],
                "image": "data\\images\\2384325.jpg"
            },
            {
                "VG_image_id": "2409479",
                "VG_object_id": "240930",
                "bbox": [0, 370, 356, 492],
                "image": "data\\images\\2409479.jpg"
            }
        ],
        "questions_with_scores": [
            ["where was the picture taken", 2],
            ["What color is the shelf", 1],
            ["What is on the shelf", 1],
            ["where is the photo taken", 1],
            ["what is the shelf made of", 1],
            ["What is in the shelf", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["What color is the shelf", 1],
            ["What is on the shelf", 1],
            ["where is the photo taken", 1],
            ["what is the shelf made of", 1],
            ["What is in the shelf", 1],
            ["what color of the wall", -1],
            ["how many people are there", -1],
            ["where was the picture taken", 2],
            ["what is on the wall", 1]
        ],
        "context": [
            "a bathroom with a toilet and a book on the shelf.",
            "a display case with vases and other vases on display."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2370013",
                "VG_object_id": "2169752",
                "bbox": [14, 29, 211, 194],
                "image": "data\\images\\2370013.jpg"
            },
            {
                "VG_image_id": "2370428",
                "VG_object_id": "1856693",
                "bbox": [384, 25, 456, 374],
                "image": "data\\images\\2370428.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in front of the woman", 1],
            ["where is the woman", 1],
            ["what is the persion on the right doing", 1]
        ],
        "org_questions": [
            ["what is in front of the woman", 1],
            ["How many people are there", -1],
            ["where is the woman", 1],
            ["what is on the woman's head", -1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion on the right doing", 1]
        ],
        "context": [
            "a couple of women sitting at a table.",
            "a large teddy bear sitting at a table"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2352294",
                "VG_object_id": "855039",
                "bbox": [22, 152, 484, 302],
                "image": "data\\images\\2352294.jpg"
            },
            {
                "VG_image_id": "2320510",
                "VG_object_id": "3090524",
                "bbox": [43, 85, 364, 370],
                "image": "data\\images\\2320510.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's hair", 2],
            ["how many men are in the picture", 2],
            ["what is the man wearing", 1],
            ["what is in the background", 1],
            ["where are the people", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what is the man wearing", 1],
            ["what color is the man's hair", 2],
            ["how many men are in the picture", 2],
            ["what is in the background", 1],
            ["What is man holding", -1],
            ["What is the man holding", -1],
            ["when was the picture taken", -1],
            ["where are the people", 1],
            ["who is in the photo", -1],
            ["what is the man doing", 1]
        ],
        "context": [
            "a man wearing a suit and tie",
            "a shirtless man holding a wii remote in his hand."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2378982",
                "VG_object_id": "1868114",
                "bbox": [268, 17, 463, 315],
                "image": "data\\images\\2378982.jpg"
            },
            {
                "VG_image_id": "2393388",
                "VG_object_id": "1220435",
                "bbox": [169, 19, 442, 416],
                "image": "data\\images\\2393388.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what sport is being played ", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["who is in the picture", 1]
        ],
        "org_questions": [
            ["what sport is being played ", 1],
            ["what color is the man's shirt", 1],
            ["how many people are there", 2],
            ["what is the man wearing on head", -1],
            ["where is the man", 1],
            ["what is the man holding", -1],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", 1],
            ["where was this photo taken", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man sitting on a tennis court holding a tennis racket.",
            "a man wearing a white shirt and black pants."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2350093",
                "VG_object_id": "870249",
                "bbox": [205, 189, 297, 249],
                "image": "data\\images\\2350093.jpg"
            },
            {
                "VG_image_id": "2391587",
                "VG_object_id": "1239765",
                "bbox": [284, 244, 374, 302],
                "image": "data\\images\\2391587.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["where is the bicycle", 1],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["What is the background of image", 1],
            ["Where are the bicycles placed", 1]
        ],
        "org_questions": [
            ["where is the bicycle", 1],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["how many people are there", -1],
            ["What color is the bicycle", -1],
            ["What is the background of image", 1],
            ["what is on the bike", -1],
            ["Where are the bicycles placed", 1],
            ["when was this photo taken", -1],
            ["what kind of vehicle is this", -1],
            ["what kind of vehicle is in the photo", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a bus stopped at a bus stop with a bicycle on the front.",
            "a bike leaning against a tree in a park."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2360226",
                "VG_object_id": "3762291",
                "bbox": [256, 198, 314, 317],
                "image": "data\\images\\2360226.jpg"
            },
            {
                "VG_image_id": "2368550",
                "VG_object_id": "746010",
                "bbox": [210, 114, 275, 203],
                "image": "data\\images\\2368550.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there in the picture", 1],
            ["what is in the background", 1],
            ["what is the person holding", 1],
            ["what is the man wearing", 1],
            ["what is the man standing on", 1],
            ["how many people are there", 1],
            ["What is man doing", 1],
            ["what is the gender of the person on the left", 1]
        ],
        "org_questions": [
            ["what color are the man's pants", -1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there in the picture", 1],
            ["what is in the background", 1],
            ["what is the person holding", 1],
            ["what is the man wearing", 1],
            ["who is in the photo", -1],
            ["what is the man standing on", 1],
            ["what is on the man's head", -1],
            ["how many people are there", 1],
            ["What is man doing", 1],
            ["what is the gender of the person on the left", 1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a man and woman loading luggage onto a train.",
            "a man taking a picture of a polar bear in an aquarium."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2324936",
                "VG_object_id": "2770374",
                "bbox": [4, 5, 495, 331],
                "image": "data\\images\\2324936.jpg"
            },
            {
                "VG_image_id": "2318846",
                "VG_object_id": "2916078",
                "bbox": [0, 272, 373, 499],
                "image": "data\\images\\2318846.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plate are there on the table", 2],
            ["how many plates are there", 2],
            ["what color is the table", 1],
            ["what color is the plate on the table", 1],
            ["how many people are there", 1],
            ["what color is the food", 1],
            ["what kind of food is on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what color is the plate on the table", 1],
            ["how many plate are there on the table", 2],
            ["What is on the table", -1],
            ["how many people are there", 1],
            ["how many plates are there", 2],
            ["what color is the food", 1],
            ["where is the food", -1],
            ["what is the plate sitting on", -1],
            ["what kind of food is on the table", 1],
            ["what is next to the plate", -1]
        ],
        "context": [
            "a plate of food with a glass of wine.",
            "a man sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2393115",
                "VG_object_id": "474716",
                "bbox": [235, 32, 323, 203],
                "image": "data\\images\\2393115.jpg"
            },
            {
                "VG_image_id": "2324244",
                "VG_object_id": "3158196",
                "bbox": [62, 137, 175, 259],
                "image": "data\\images\\2324244.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the batter's cap", 2],
            ["who is behind the batter", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what color is the batter's cap", 2],
            ["how many people are there", -1],
            ["what sport is the man playing", -1],
            ["what is the player holding", -1],
            ["what is the player doing", -1],
            ["what is the player wearing", -1],
            ["who is behind the batter", 1],
            ["where is the batter", -1],
            ["what is on the batter's head", -1],
            ["what game is being played", -1],
            ["who is holding the bat", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a baseball player throwing a ball on a field."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2366869",
                "VG_object_id": "2169895",
                "bbox": [2, 255, 442, 374],
                "image": "data\\images\\2366869.jpg"
            },
            {
                "VG_image_id": "2365460",
                "VG_object_id": "3884158",
                "bbox": [3, 269, 479, 499],
                "image": "data\\images\\2365460.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what are on the floor", -1],
            ["what is the floor made of", -1],
            ["what time is it", -1],
            ["how many bags are on the floor", -1],
            ["what is the ground covered with", -1],
            ["what is on the ground", -1],
            ["what room is this", -1],
            ["where was the photo taken", -1],
            ["what is covering the floor", -1],
            ["how many people are in the photo", 1]
        ],
        "context": [
            "a toilet with a toilet seat and a book on the lid.",
            "a bathroom with a bathtub and a toilet."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2387026",
                "VG_object_id": "1280114",
                "bbox": [200, 255, 334, 465],
                "image": "data\\images\\2387026.jpg"
            },
            {
                "VG_image_id": "2380960",
                "VG_object_id": "1340788",
                "bbox": [3, 213, 499, 352],
                "image": "data\\images\\2380960.jpg"
            }
        ],
        "questions_with_scores": [
            ["When is photo taken", 2],
            ["What color is person's shirt", 1]
        ],
        "org_questions": [
            ["When is photo taken", 2],
            ["What color is person's shirt", 1],
            ["What is the ground made of", -1],
            ["what is on the land", -1],
            ["How many people are there", -1],
            ["where was this picture taken", -1],
            ["what is the man doing", -1],
            ["where is the snow", -1],
            ["what is covering the ground", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a snowboarder is going down a steep slope at night.",
            "a man skiing down a snowy hill on a sunny day."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2317879",
                "VG_object_id": "3467774",
                "bbox": [1, 4, 331, 389],
                "image": "data\\images\\2317879.jpg"
            },
            {
                "VG_image_id": "2372772",
                "VG_object_id": "2128875",
                "bbox": [17, 41, 317, 474],
                "image": "data\\images\\2372772.jpg"
            }
        ],
        "questions_with_scores": [["what is the child wearing on head", 1]],
        "org_questions": [
            ["what color is the child's shirt", -1],
            ["what is the child wearing on head", 1],
            ["how many people are there in the picture", -1],
            ["what is the boy doing", -1],
            ["where is the person", -1],
            ["what is in the background", -1],
            ["who is wearing glasses", -1],
            ["what is the boy holding", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["who is in the photo", -1],
            ["who is skateboarding", -1]
        ],
        "context": [
            "a man doing a trick on a skateboard.",
            "a young man riding a skateboard on a ramp."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2368937",
                "VG_object_id": "615048",
                "bbox": [155, 168, 229, 295],
                "image": "data\\images\\2368937.jpg"
            },
            {
                "VG_image_id": "2320562",
                "VG_object_id": "3368623",
                "bbox": [313, 5, 452, 192],
                "image": "data\\images\\2320562.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's pants", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what color is the man's pants", 1],
            ["How many people are there", 1],
            ["what is the man wearing on his head", -1],
            ["Where is man", -1],
            ["what kind of sport is the man playing", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person holding", -1],
            ["what is on the man's head", -1],
            ["where is the man", -1]
        ],
        "context": [
            "a man riding a snowboard down a rail.",
            "a man skiing down a hill"
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2402917",
                "VG_object_id": "1130022",
                "bbox": [104, 197, 437, 317],
                "image": "data\\images\\2402917.jpg"
            },
            {
                "VG_image_id": "2325835",
                "VG_object_id": "3096820",
                "bbox": [8, 231, 497, 373],
                "image": "data\\images\\2325835.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the counter", 2],
            ["what color is the shelf above the counter", 1],
            ["What is on the counter", 1],
            ["what is on top of the counter", 1]
        ],
        "org_questions": [
            ["what color is the counter", 2],
            ["what color is the shelf above the counter", 1],
            ["What is on the counter", 1],
            ["how many wines are there on the counter", -1],
            ["how many sinks are in the picture", -1],
            ["where was this picture taken", -1],
            ["what room is this", -1],
            ["what is on top of the counter", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a kitchen with a sink, microwave, and a refrigerator.",
            "a microwave on a counter"
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2403291",
                "VG_object_id": "1126237",
                "bbox": [1, 224, 497, 328],
                "image": "data\\images\\2403291.jpg"
            },
            {
                "VG_image_id": "2364026",
                "VG_object_id": "1697055",
                "bbox": [0, 1, 497, 396],
                "image": "data\\images\\2364026.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is person's hat", 2]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is person's hat", 2],
            ["what are the people doing on the field", -1],
            ["how many horses are there on the field", -1],
            ["what is the main color of the grass", -1],
            ["what animal is in the field", -1],
            ["where is this scene", -1],
            ["what is covering the ground", -1],
            ["what is the field made of", -1],
            ["where is the grass", -1],
            ["what is the pitcher standing on", -1]
        ],
        "context": [
            "a baseball player swinging a bat on a field.",
            "a baseball pitcher is throwing a baseball on a field."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2394773",
                "VG_object_id": "461220",
                "bbox": [150, 93, 346, 267],
                "image": "data\\images\\2394773.jpg"
            },
            {
                "VG_image_id": "2351369",
                "VG_object_id": "2581199",
                "bbox": [103, 41, 419, 298],
                "image": "data\\images\\2351369.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the keyboard of the laptop", 2],
            ["what color is the screen of the laptop", 1],
            ["what color is in the screen", 1]
        ],
        "org_questions": [
            ["what color is the screen of the laptop", 1],
            ["what color is the desk", -1],
            ["how many people are there", -1],
            ["what is the laptop showing", -1],
            ["how many laptops are in the picture", -1],
            ["what color is in the screen", 1],
            ["How many laptops are there", -1],
            ["what is the table made of", -1],
            ["where is the laptop", -1],
            ["what kind of computer is on the table", -1],
            ["what is the laptop sitting on", -1],
            ["what is sitting on the table", -1],
            ["what color is the keyboard of the laptop", 2]
        ],
        "context": [
            "a cat is laying on a table with a laptop.",
            "a laptop computer sitting on top of a wooden table."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2372186",
                "VG_object_id": "1837143",
                "bbox": [130, 39, 237, 305],
                "image": "data\\images\\2372186.jpg"
            },
            {
                "VG_image_id": "2385503",
                "VG_object_id": "1296099",
                "bbox": [95, 29, 149, 177],
                "image": "data\\images\\2385503.jpg"
            }
        ],
        "questions_with_scores": [["what color is the curtain", 1]],
        "org_questions": [
            ["what color is the curtain", 1],
            ["what color is the wall behind the curtain", -1],
            ["what room is the curtain in", -1],
            ["where was the photo taken", -1],
            ["what is in the background", -1],
            ["when is this picture taken", -1],
            ["what is the pattern on the curtain", -1],
            ["what is hanging from the window", -1],
            ["where are the curtains", -1],
            ["what is covering the window", -1]
        ],
        "context": [
            "a room with a suitcase and clothes on the floor.",
            "a living room with a couch, coffee table, and television."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2351134",
                "VG_object_id": "2307893",
                "bbox": [0, 0, 497, 137],
                "image": "data\\images\\2351134.jpg"
            },
            {
                "VG_image_id": "2411317",
                "VG_object_id": "313440",
                "bbox": [0, 0, 137, 196],
                "image": "data\\images\\2411317.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 2],
            ["what gender is the person", 2],
            ["what color is the table", 1],
            ["where is the curtain", 1],
            ["what room is the curtain in", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 2],
            ["what color is the table", 1],
            ["what gender is the person", 2],
            ["how many beds are in the picture", -1],
            ["when is this picture taken", -1],
            ["where is the curtain", 1],
            ["what room is the curtain in", 1],
            ["what is behind the curtain", -1],
            ["what is hanging on the wall", -1],
            ["where was this picture taken", -1],
            ["what is covering the wall", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a little girl with her hand on her face",
            "a man holding a cake on a plate."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2327986",
                "VG_object_id": "3135992",
                "bbox": [11, 57, 471, 319],
                "image": "data\\images\\2327986.jpg"
            },
            {
                "VG_image_id": "2324863",
                "VG_object_id": "987138",
                "bbox": [3, 159, 354, 411],
                "image": "data\\images\\2324863.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the cake", 2],
            ["What color is the plate", 1],
            ["What is next to the cake", 1]
        ],
        "org_questions": [
            ["What color is the cake", 2],
            ["What color is the plate", 1],
            ["What is next to the cake", 1],
            ["how many people are there", -1],
            ["what shape is the plate", -1],
            ["what is on the tray", -1],
            ["how many candles are there on the cake", -1],
            ["how many cakes are there", -1],
            ["where is the plate", -1],
            ["what is the cake made of", -1],
            ["what kind of cake is this", -1]
        ],
        "context": [
            "a cake with white frosting and a white frosting.",
            "a piece of chocolate cake with ice cream and a scoop of ice cream."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2347562",
                "VG_object_id": "889601",
                "bbox": [2, 1, 500, 330],
                "image": "data\\images\\2347562.jpg"
            },
            {
                "VG_image_id": "2320502",
                "VG_object_id": "3534954",
                "bbox": [1, 304, 330, 498],
                "image": "data\\images\\2320502.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is on the bench", 1],
            ["what are the people doing on the beach", 1],
            ["what is standing on the beach", 1],
            ["what is the person standing on", 1],
            ["what is the person doing", 1]
        ],
        "org_questions": [
            ["who is on the bench", 1],
            ["what are the people doing on the beach", 1],
            ["how many people are there", -1],
            ["what is in the distance", -1],
            ["what is standing on the beach", 1],
            ["where was this photo taken", -1],
            ["what is the person standing on", 1],
            ["where is the water", -1],
            ["what is the person doing", 1],
            ["where are the people", -1]
        ],
        "context": [
            "a woman poses for a photo in front of the ocean.",
            "a man sitting on a surfboard on the beach."
        ]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2322911",
                "VG_object_id": "3172413",
                "bbox": [432, 50, 487, 125],
                "image": "data\\images\\2322911.jpg"
            },
            {
                "VG_image_id": "285984",
                "VG_object_id": "1571008",
                "bbox": [274, 9, 658, 274],
                "image": "data\\images\\285984.jpg"
            }
        ],
        "questions_with_scores": [["What is status of light", 1]],
        "org_questions": [
            ["What is status of light", 1],
            ["Where is the light", -1],
            ["how many people are there", -1],
            ["what shape is the light", -1],
            ["what is on the wall", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a kitchen with a sink, stove, and cabinets.",
            "a kitchen with a large window and a large window."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2405250",
                "VG_object_id": "333645",
                "bbox": [102, 147, 159, 370],
                "image": "data\\images\\2405250.jpg"
            },
            {
                "VG_image_id": "2366310",
                "VG_object_id": "1737999",
                "bbox": [322, 73, 385, 243],
                "image": "data\\images\\2366310.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the people doing", 2],
            ["what is the ground covered with", 1],
            ["what is the color of the girl's shirt", 1],
            ["what is the persion wearing", 1],
            ["what gesture is the person", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["how many women are there", -1],
            ["what is the people doing", 2],
            ["what is the ground covered with", 1],
            ["what is the color of the girl's shirt", 1],
            ["where is the person", -1],
            ["what is the persion wearing", 1],
            ["what gesture is the person", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion holding", 1],
            ["what is on the woman's head", -1],
            ["what kind of shirt is the woman wearing", -1]
        ],
        "context": [
            "a group of people standing next to a bus.",
            "two women riding horses on a road."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2381850",
                "VG_object_id": "701408",
                "bbox": [0, 216, 499, 273],
                "image": "data\\images\\2381850.jpg"
            },
            {
                "VG_image_id": "2369911",
                "VG_object_id": "2465871",
                "bbox": [47, 122, 479, 317],
                "image": "data\\images\\2369911.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color of the field", 2],
            ["what color is the ground", 1],
            ["how many players standing on the ground in the box", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["how many players standing on the ground in the box", 1],
            ["what is in the background", 1],
            ["when is this picture taken", -1],
            ["what is the man doing", -1],
            ["what is the ground covered with", -1],
            ["what is the weather like", -1],
            ["What is the background of image", -1],
            ["where was the photo taken", -1],
            ["what is on the ground", -1],
            ["where is the dirt", -1],
            ["what is the main color of the field", 2]
        ],
        "context": [
            "a baseball game is being played on a field.",
            "a baseball player swinging a bat at a ball"
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2353416",
                "VG_object_id": "1960411",
                "bbox": [186, 18, 486, 252],
                "image": "data\\images\\2353416.jpg"
            },
            {
                "VG_image_id": "2347346",
                "VG_object_id": "2865212",
                "bbox": [8, 4, 427, 370],
                "image": "data\\images\\2347346.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the cat on", 1],
            ["where are the cats", 1],
            ["what is in front of the cat", 1],
            ["what is behind the cat", 1],
            ["where is the cat looking", 1],
            ["what is the cat lying on", 1],
            ["where is the cat", 1]
        ],
        "org_questions": [
            ["what color is the cat", -1],
            ["what is the cat on", 1],
            ["how many cats are in the picture", -1],
            ["where are the cats", 1],
            ["what is the cat doing", -1],
            ["what is in front of the cat", 1],
            ["what is on the cat's face", -1],
            ["what animal is in the picture", -1],
            ["who is in the photo", -1],
            ["what is behind the cat", 1],
            ["where is the cat looking", 1],
            ["what is the cat lying on", 1],
            ["where is the cat", 1],
            ["what is on the cat's head", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a cat sitting in a suitcase with clothes in it.",
            "a cat laying on top of a bookshelf"
        ]
    },
    {
        "object_category": "keyboard",
        "images": [
            {
                "VG_image_id": "2369254",
                "VG_object_id": "2026208",
                "bbox": [103, 242, 446, 364],
                "image": "data\\images\\2369254.jpg"
            },
            {
                "VG_image_id": "2412064",
                "VG_object_id": "203567",
                "bbox": [251, 312, 496, 374],
                "image": "data\\images\\2412064.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the keyboard", 2],
            ["what color is the mouse", 2],
            ["what color is the desk", 1],
            ["What is on the right of keyboard", 1]
        ],
        "org_questions": [
            ["what color is the keyboard", 2],
            ["what color is the mouse", 2],
            ["what color is the desk", 1],
            ["how many laptops are in the picture", -1],
            ["What is on the right of keyboard", 1],
            ["where is the computer", -1],
            ["What is the type of the computer", -1],
            ["what is on the keyboard", -1],
            ["what is the keyboard sitting on", -1],
            ["what is the desk made of", -1],
            ["what is next to the keyboard", -1]
        ],
        "context": [
            "a computer monitor and keyboard on a desk.",
            "a computer monitor sitting on top of a desk."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2409011",
                "VG_object_id": "1092339",
                "bbox": [215, 57, 384, 187],
                "image": "data\\images\\2409011.jpg"
            },
            {
                "VG_image_id": "2399870",
                "VG_object_id": "413162",
                "bbox": [1, 90, 447, 247],
                "image": "data\\images\\2399870.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the truck", 2],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the truck", 2],
            ["what is the floor made of", -1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["Where is the truck", -1],
            ["what is at the back of the truck", -1],
            ["What color is the sky", -1],
            ["what is the ground covered with", 1],
            ["what is the truck doing", -1],
            ["what kind of truck is this", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man riding a bike down a street.",
            "a red dump truck driving down a road."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2362874",
                "VG_object_id": "770392",
                "bbox": [147, 214, 259, 348],
                "image": "data\\images\\2362874.jpg"
            },
            {
                "VG_image_id": "2406976",
                "VG_object_id": "287359",
                "bbox": [327, 260, 374, 346],
                "image": "data\\images\\2406976.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["what is the person doing", 2],
            ["what color is the background", 2]
        ],
        "org_questions": [
            ["where is the photo taken", 2],
            ["what is the person doing", 2],
            ["How many people are there", -1],
            ["what color is the background", 2],
            ["when is this photo taken", -1],
            ["who is wearing a blue shirt", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a girl sitting on a bed with a blow dryer",
            "two giraffes standing next to a wooden fence."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2407944",
                "VG_object_id": "270928",
                "bbox": [2, 297, 334, 495],
                "image": "data\\images\\2407944.jpg"
            },
            {
                "VG_image_id": "2409780",
                "VG_object_id": "233711",
                "bbox": [3, 235, 494, 329],
                "image": "data\\images\\2409780.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is person's shirt", 2],
            ["What color is person's trouser", 2]
        ],
        "org_questions": [
            ["What color is person's shirt", 2],
            ["What color is person;s trouse", -1],
            ["how many people are there", -1],
            ["Where is the ground", -1],
            ["what is the man doing", -1],
            ["What is the ground made of", -1],
            ["what is in the distance", -1],
            ["how is the weather", -1],
            ["what is white", -1],
            ["where is this scene", -1],
            ["what is covering the snow", -1],
            ["what is the weather like", -1],
            ["What color is person's trouser", 2]
        ],
        "context": [
            "a man skiing down a snowy hill.",
            "a person skiing down a snowy hill with a blue sky in the background."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2410855",
                "VG_object_id": "361943",
                "bbox": [41, 6, 344, 402],
                "image": "data\\images\\2410855.jpg"
            },
            {
                "VG_image_id": "2413799",
                "VG_object_id": "165987",
                "bbox": [104, 71, 244, 313],
                "image": "data\\images\\2413799.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bear", 2],
            ["what color is the bear", 2],
            ["what is in the distance", 1],
            ["what is behind the bear", 1]
        ],
        "org_questions": [
            ["what is the bear doing", -1],
            ["where is the bear", 2],
            ["what color is the bear", 2],
            ["How many bears are there", -1],
            ["what is the weather like", -1],
            ["what is in the distance", 1],
            ["what is behind the bear", 1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["what is the bear eating", -1]
        ],
        "context": [
            "a bear is looking down at a small pond.",
            "a bear climbing a tree in a forest."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2379372",
                "VG_object_id": "553294",
                "bbox": [293, 236, 374, 370],
                "image": "data\\images\\2379372.jpg"
            },
            {
                "VG_image_id": "2334004",
                "VG_object_id": "3211760",
                "bbox": [178, 36, 291, 361],
                "image": "data\\images\\2334004.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["where is the person", 1],
            ["what is the persion wearing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person doing", -1],
            ["where is the person", 1],
            ["How many people are there", -1],
            ["what is the persion wearing", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a person sitting on the sidewalk next to a fence with bags.",
            "a woman riding on the back of an elephant."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2399095",
                "VG_object_id": "419245",
                "bbox": [62, 183, 335, 303],
                "image": "data\\images\\2399095.jpg"
            },
            {
                "VG_image_id": "2358622",
                "VG_object_id": "800742",
                "bbox": [176, 95, 354, 242],
                "image": "data\\images\\2358622.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cow", 1],
            ["how many cows are there", 1],
            ["what is the cow doing", 1]
        ],
        "org_questions": [
            ["what color is the cow", 1],
            ["how many cows are there", 1],
            ["what is the cow doing", 1],
            ["where are the cows", -1],
            ["what is in the distance", -1],
            ["what color is the grass", -1],
            ["what color is the ground the cow standing on", -1],
            ["where is the cow", -1],
            ["when was the picture taken", -1],
            ["what kind of animal is in the picture", -1],
            ["what are the cows standing on", -1],
            ["what animal is shown", -1]
        ],
        "context": [
            "a couple of cows grazing on a lush green field.",
            "a man on a tractor pulling a cow through a field."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2383640",
                "VG_object_id": "1318900",
                "bbox": [2, 168, 499, 375],
                "image": "data\\images\\2383640.jpg"
            },
            {
                "VG_image_id": "2318844",
                "VG_object_id": "1006708",
                "bbox": [3, 228, 499, 499],
                "image": "data\\images\\2318844.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what is in the background", -1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is standing on the land", -1],
            ["what is on the ground", -1],
            ["where was this photo taken", -1],
            ["how is the weather", -1],
            ["where is the grass", -1],
            ["what is the weather like", -1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a group of zebras eating from a trough in a field.",
            "a person riding a horse in a gravel area."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2343903",
                "VG_object_id": "3635282",
                "bbox": [152, 298, 229, 458],
                "image": "data\\images\\2343903.jpg"
            },
            {
                "VG_image_id": "2327426",
                "VG_object_id": "3320211",
                "bbox": [276, 6, 461, 183],
                "image": "data\\images\\2327426.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 1],
            ["what is the person doing", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["what is in the distance", 1],
            ["How many people are there", 1],
            ["who is in the photo", 1],
            ["where are the people", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", -1],
            ["how many people are there in the picture", 1],
            ["what is the person doing", 1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["what is in the distance", 1],
            ["How many people are there", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["where are the people", 1],
            ["what is the person wearing on the head", -1],
            ["how many people are there", 1],
            ["what is the weather like", -1],
            ["what is the person holding", 1],
            ["what is on the person's head", -1]
        ],
        "context": [
            "a man and woman flying a kite on a beach.",
            "a boy on a skateboard doing a trick."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2348569",
                "VG_object_id": "2107791",
                "bbox": [31, 114, 183, 291],
                "image": "data\\images\\2348569.jpg"
            },
            {
                "VG_image_id": "2341434",
                "VG_object_id": "2673057",
                "bbox": [163, 132, 220, 332],
                "image": "data\\images\\2341434.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's clothes", 1],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["what is the weather like", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", 1],
            ["what is the man holding", 1],
            ["how many people are there", 1],
            ["where is the person", -1],
            ["what is in the background", 1],
            ["what is the persion doing", -1],
            ["what kind of trousers is the man wearing", -1],
            ["what is the person standing on", -1],
            ["when was this photo taken", -1],
            ["what is the weather like", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a person holding an umbrella on a sidewalk.",
            "a group of people standing around an elephant."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2355457",
                "VG_object_id": "3568497",
                "bbox": [5, 159, 483, 371],
                "image": "data\\images\\2355457.jpg"
            },
            {
                "VG_image_id": "2390445",
                "VG_object_id": "497443",
                "bbox": [16, 213, 490, 331],
                "image": "data\\images\\2390445.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the floor", 2],
            ["where is the picture taken", 2],
            ["What is on the floor", 1],
            ["What is the pattern of floor", 1],
            ["what is the floor made of", 1],
            ["what color is the wall", 1],
            ["what type of flooring is shown", 1],
            ["what material is the floor made of", 1],
            ["what is the main color of the floor", 1]
        ],
        "org_questions": [
            ["What color is the floor", 2],
            ["What is on the floor", 1],
            ["What is the pattern of floor", 1],
            ["How many people are there", -1],
            ["where is the picture taken", 2],
            ["what is the floor made of", 1],
            ["what color is the wall", 1],
            ["what type of flooring is shown", 1],
            ["what material is the floor made of", 1],
            ["what is the main color of the floor", 1]
        ],
        "context": [
            "a red seat on a train.",
            "a motorcycle parked in front of a garage door."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2357560",
                "VG_object_id": "2110474",
                "bbox": [176, 273, 233, 333],
                "image": "data\\images\\2357560.jpg"
            },
            {
                "VG_image_id": "2343004",
                "VG_object_id": "928459",
                "bbox": [154, 140, 233, 213],
                "image": "data\\images\\2343004.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's trousers", 2],
            ["what color is the the man's jacket", 1],
            ["How many people are there", 1],
            ["what is the person in the trousers holding", 1],
            ["what is on the man's feet", 1]
        ],
        "org_questions": [
            ["what color are the man's trousers", 2],
            ["what color is the the man's jacket", 1],
            ["what is the man doing", -1],
            ["How many people are there", 1],
            ["what is the gender of the person", -1],
            ["where is the man", -1],
            ["what is the person in the trousers holding", 1],
            ["when was the photo taken", -1],
            ["what is on the man's feet", 1],
            ["what is the person wearing", -1],
            ["what color is the sky", -1]
        ],
        "context": [
            "three men standing in the snow with a snowboard.",
            "a skier in the air doing a trick in the air."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2315885",
                "VG_object_id": "3577960",
                "bbox": [121, 104, 265, 197],
                "image": "data\\images\\2315885.jpg"
            },
            {
                "VG_image_id": "2371778",
                "VG_object_id": "595942",
                "bbox": [230, 136, 316, 217],
                "image": "data\\images\\2371778.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["what is he wearing", 1],
            ["who is in the photo", 1],
            ["what is the person doing ", 1],
            ["how many faces are there in the picture", 1],
            ["who is wearing a black shirt", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his head", -1],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["when was the photo taken", -1],
            ["what is he wearing", 1],
            ["who is in the photo", 1],
            ["what is the person doing ", 1],
            ["what color is the person shirt", -1],
            ["how many faces are there in the picture", 1],
            ["who is wearing a black shirt", 1]
        ],
        "context": [
            "a man is water skiing on the beach.",
            "a black and white photo of a man and woman standing on top of a building."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2332336",
                "VG_object_id": "3278441",
                "bbox": [152, 169, 270, 200],
                "image": "data\\images\\2332336.jpg"
            },
            {
                "VG_image_id": "2366105",
                "VG_object_id": "1712756",
                "bbox": [102, 139, 414, 244],
                "image": "data\\images\\2366105.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the board", 1],
            ["what is on the person's head", 1],
            ["what is the man doing", 1],
            ["what is the man standing on", 1],
            ["what is on the surfboard", 1]
        ],
        "org_questions": [
            ["what color is the board", 1],
            ["where is the board", -1],
            ["what is on the person's head", 1],
            ["How many people are there", -1],
            ["what is the man doing", 1],
            ["what is the board made of", -1],
            ["how many trees are there in the picture", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is the man standing on", 1],
            ["what is on the surfboard", 1]
        ],
        "context": [
            "a man riding a wave on top of a surfboard.",
            "a man carrying a surfboard on a beach."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2360093",
                "VG_object_id": "2423393",
                "bbox": [23, 40, 291, 449],
                "image": "data\\images\\2360093.jpg"
            },
            {
                "VG_image_id": "2333456",
                "VG_object_id": "3419481",
                "bbox": [143, 72, 354, 497],
                "image": "data\\images\\2333456.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the boy holding", 1],
            ["what color is the boy's hair", 1],
            ["what is the child standing on", 1],
            ["what is in the background", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what is the boy holding", 1],
            ["what color is the boy's hair", 1],
            ["What is child doing", -1],
            ["what is the child standing on", 1],
            ["what gesture is the child", -1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["what is on the boy's face", -1],
            ["what is in the background", 1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a young boy with a blue jacket eating a donut.",
            "a young man standing with a skateboard in his hand."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2400937",
                "VG_object_id": "404057",
                "bbox": [111, 183, 374, 499],
                "image": "data\\images\\2400937.jpg"
            },
            {
                "VG_image_id": "2337505",
                "VG_object_id": "2338092",
                "bbox": [6, 2, 498, 364],
                "image": "data\\images\\2337505.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the floor", 1],
            ["how many people are there", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is on the floor", 1],
            ["how many people are there", 1],
            ["where is the floor", -1],
            ["what is the floor made of", -1],
            ["where is the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the floor of", -1],
            ["what is the main color of the carpet", -1]
        ],
        "context": [
            "a little girl playing a game on the television.",
            "a cat is laying on a computer mouse."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2385384",
                "VG_object_id": "1297560",
                "bbox": [1, 96, 57, 216],
                "image": "data\\images\\2385384.jpg"
            },
            {
                "VG_image_id": "2392719",
                "VG_object_id": "1226571",
                "bbox": [38, 222, 169, 316],
                "image": "data\\images\\2392719.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 1],
            ["what is below the towel", 1],
            ["what color is the towel hanging on the wall", 1]
        ],
        "org_questions": [
            ["what color is the wall", 1],
            ["what is below the towel", 1],
            ["what room is the towel in", -1],
            ["where is the picture taken", -1],
            ["Where is the toilet", -1],
            ["what is the towel put on ", -1],
            ["how many towels are there", -1],
            ["what is hanging on the wall", -1],
            ["what is on the towel", -1],
            ["what is hanging from the towel", -1],
            ["what color is the towel hanging on the wall", 1]
        ],
        "context": [
            "a bathroom with a sink and a mirror",
            "a bathroom with a toilet and a shower"
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2342424",
                "VG_object_id": "2272061",
                "bbox": [78, 37, 153, 234],
                "image": "data\\images\\2342424.jpg"
            },
            {
                "VG_image_id": "2367252",
                "VG_object_id": "623703",
                "bbox": [2, 1, 126, 213],
                "image": "data\\images\\2367252.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 2],
            ["how many beds are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 2],
            ["how many beds are in the picture", 1],
            ["when is this picture taken", -1],
            ["what is behind the curtain", -1],
            ["what room is the curtain in", -1],
            ["where is the curtain", -1],
            ["where is the photo taken", -1],
            ["what is hanging on the window", -1],
            ["what is covering the window", -1],
            ["what is on the wall", -1]
        ],
        "context": [
            "a bedroom with two beds and a window.",
            "a chair with a book on it next to a window."
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2373889",
                "VG_object_id": "3850462",
                "bbox": [0, 120, 494, 370],
                "image": "data\\images\\2373889.jpg"
            },
            {
                "VG_image_id": "2409704",
                "VG_object_id": "235705",
                "bbox": [143, 195, 422, 499],
                "image": "data\\images\\2409704.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the screen of the laptop", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", -1],
            ["how many screens are in the picture", -1],
            ["what is the table made of", -1],
            ["what is on the screen of the laptop", 1],
            ["where is the picture taken", -1],
            ["where is the phone", -1]
        ],
        "context": [
            "a cell phone sitting on top of a speaker.",
            "a woman sitting at a desk with a laptop."
        ]
    },
    {
        "object_category": "banana",
        "images": [
            {
                "VG_image_id": "2356649",
                "VG_object_id": "3560726",
                "bbox": [357, 337, 497, 498],
                "image": "data\\images\\2356649.jpg"
            },
            {
                "VG_image_id": "2376482",
                "VG_object_id": "2152467",
                "bbox": [61, 130, 420, 498],
                "image": "data\\images\\2376482.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the banana", 2],
            ["how many bananas are there in the picture", 1],
            ["what color is the background of the banana", 1],
            ["where are the bananas", 1],
            ["what is beside the bananas", 1],
            ["how many people are there in the picture", 1],
            ["where is the banana placed", 1],
            ["where is the banana", 1]
        ],
        "org_questions": [
            ["what color is the banana", 2],
            ["how many bananas are there in the picture", 1],
            ["what color is the background of the banana", 1],
            ["where are the bananas", 1],
            ["what is beside the bananas", 1],
            ["how many people are there in the picture", 1],
            ["where is the banana placed", 1],
            ["where is the banana", 1],
            ["what kind of fruit is in the picture", -1],
            ["where was the photo taken", -1],
            ["what is on the banana", -1],
            ["what is in the photo", -1]
        ],
        "context": [
            "two men holding bananas in their hands.",
            "a bunch of bananas hanging from a tree."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2387257",
                "VG_object_id": "514427",
                "bbox": [8, 2, 100, 99],
                "image": "data\\images\\2387257.jpg"
            },
            {
                "VG_image_id": "2347575",
                "VG_object_id": "889521",
                "bbox": [112, 25, 399, 127],
                "image": "data\\images\\2347575.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shelf", 1],
            ["what is on the shelf", 1],
            ["what is under the shelf", 1],
            ["What is in the shelf", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["what color is the shelf", 1],
            ["what is on the shelf", 1],
            ["what is under the shelf", 1],
            ["where is the shelf", -1],
            ["what is the shelf made of ", -1],
            ["What is in the shelf", 1],
            ["how many people are in the photo", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", 1]
        ],
        "context": [
            "a kitchen with a sink, dishwasher and a window.",
            "a desk with two laptops and a lamp."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2371417",
                "VG_object_id": "597771",
                "bbox": [269, 6, 439, 272],
                "image": "data\\images\\2371417.jpg"
            },
            {
                "VG_image_id": "2338731",
                "VG_object_id": "2882533",
                "bbox": [365, 146, 469, 297],
                "image": "data\\images\\2338731.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is it", 1],
            ["where is the person", 1],
            ["who is in the photo", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what sport is it", 1],
            ["what is the person wearing on the head", -1],
            ["what is the person holding", -1],
            ["how many people are there", -1],
            ["What color is person's shirt", -1],
            ["where is the person", 1],
            ["what is on the person's head", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["where was the photo taken", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a person riding a skateboard down a street.",
            "a man holding a tennis racket in his right hand."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2376377",
                "VG_object_id": "2104315",
                "bbox": [2, 189, 358, 304],
                "image": "data\\images\\2376377.jpg"
            },
            {
                "VG_image_id": "2383519",
                "VG_object_id": "1319688",
                "bbox": [2, 128, 498, 356],
                "image": "data\\images\\2383519.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 1],
            ["what color is the ground", 1]
        ],
        "org_questions": [
            ["what color is the train", 1],
            ["what color is the ground", 1],
            ["what is behind the train", -1],
            ["how many trains are there", -1],
            ["where is the photo taken", -1],
            ["what is in the distance", -1],
            ["where is the train", -1],
            ["what is the train doing", -1],
            ["when was the photo taken", -1],
            ["what type of train is shown", -1]
        ],
        "context": [
            "a train is passing by a train station.",
            "a train sitting on the tracks in a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2345154",
                "VG_object_id": "2390017",
                "bbox": [185, 193, 312, 331],
                "image": "data\\images\\2345154.jpg"
            },
            {
                "VG_image_id": "2397022",
                "VG_object_id": "1193459",
                "bbox": [30, 155, 115, 306],
                "image": "data\\images\\2397022.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the person holding", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["what is the person holding", 1],
            ["when is the picture taken", -1],
            ["what is the ground covered with", -1],
            ["what is the persion doing", 1],
            ["what color is the person's jacket", -1],
            ["how many people are in the picture", -1],
            ["what is the persion wearing", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a woman is talking on a cell phone.",
            "a man holding a stop sign on a city street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2368973",
                "VG_object_id": "1863336",
                "bbox": [9, 159, 499, 328],
                "image": "data\\images\\2368973.jpg"
            },
            {
                "VG_image_id": "2360226",
                "VG_object_id": "3534790",
                "bbox": [2, 283, 496, 330],
                "image": "data\\images\\2360226.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 1],
            ["how many people on the ground", 1],
            ["what is on the land", 1],
            ["where was the photo taken", 1],
            ["when was the photo taken", 1],
            ["how many people are there on the ground", 1],
            ["what is the person doing on the ground", 1],
            ["what are people doing", 1],
            ["how is the weather", 1],
            ["what is in the background", 1],
            ["where is this scene", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what is the weather like", 1],
            ["how many people on the ground", 1],
            ["what is the land made of", -1],
            ["what is on the land", 1],
            ["where was the photo taken", 1],
            ["when was the photo taken", 1],
            ["how many people are there on the ground", 1],
            ["what is the person doing on the ground", 1],
            ["What is the ground made of", -1],
            ["what are people doing", 1],
            ["how is the weather", 1],
            ["what is in the background", 1],
            ["where is this scene", 1]
        ],
        "context": [
            "a woman walking down a street holding an umbrella.",
            "a man and woman loading luggage onto a train."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2395263",
                "VG_object_id": "456431",
                "bbox": [3, 42, 499, 375],
                "image": "data\\images\\2395263.jpg"
            },
            {
                "VG_image_id": "2375447",
                "VG_object_id": "3151778",
                "bbox": [1, 192, 498, 372],
                "image": "data\\images\\2375447.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what is on the ground", 1],
            ["How many people are there", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is on the ground", 1],
            ["How many people are there", 1],
            ["where is the land", -1],
            ["what is the ground covered with", 1],
            ["when was this picture taken", -1],
            ["how is the weather", -1],
            ["where was this picture taken", -1],
            ["what is in the background", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a stop sign on a yellow fence with leaves.",
            "a person sitting on a bench with a blanket on it."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2319530",
                "VG_object_id": "2764857",
                "bbox": [130, 152, 255, 245],
                "image": "data\\images\\2319530.jpg"
            },
            {
                "VG_image_id": "2337266",
                "VG_object_id": "2619153",
                "bbox": [117, 166, 172, 241],
                "image": "data\\images\\2337266.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["what color is the shirt", 2],
            ["what is the man holding", 1],
            ["what color shirt is the man wearing", 1],
            ["who is wearing a white shirt", 1],
            ["what color is the shirt of the man on the right", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what are the people doing", 2],
            ["what color is the shirt", 2],
            ["who is wearing the shirt", -1],
            ["what is the man wearing", -1],
            ["what is the man holding", 1],
            ["what color is the ground", -1],
            ["when was the picture taken", -1],
            ["what type of shirt is the man wearing", -1],
            ["what color shirt is the man wearing", 1],
            ["who is wearing a white shirt", 1],
            ["what color is the shirt of the man on the right", 1]
        ],
        "context": [
            "a man is playing tennis on a grass court.",
            "a soccer player is kicking a soccer ball."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2357624",
                "VG_object_id": "2115856",
                "bbox": [268, 108, 411, 281],
                "image": "data\\images\\2357624.jpg"
            },
            {
                "VG_image_id": "2320536",
                "VG_object_id": "3373419",
                "bbox": [235, 103, 311, 275],
                "image": "data\\images\\2320536.jpg"
            }
        ],
        "questions_with_scores": [["what is the man wearing", 1]],
        "org_questions": [
            ["where is the photo taken", -1],
            ["how many people are there", -1],
            ["What is the boy wearing on his head", -1],
            ["what is the boy doing", -1],
            ["what color is the boy's pants", -1],
            ["what is in the background", -1],
            ["what is the boy holding", -1],
            ["who is wearing a helmet", -1],
            ["when was the picture taken", -1],
            ["what sport is being played", -1],
            ["what is the man wearing", 1],
            ["where is the batter", -1]
        ],
        "context": [
            "a young boy swinging a bat at a baseball.",
            "a man is swinging a bat at a ball."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2408098",
                "VG_object_id": "268277",
                "bbox": [128, 182, 246, 374],
                "image": "data\\images\\2408098.jpg"
            },
            {
                "VG_image_id": "2371249",
                "VG_object_id": "598842",
                "bbox": [288, 9, 436, 202],
                "image": "data\\images\\2371249.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is on the woman's head", 2],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["where is the woman", 1],
            ["what are the people doing", 1],
            ["what is the woman looking at", 1],
            ["where is the woman looking", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["where is the woman", 1],
            ["how many people are there", 2],
            ["WHat color is woman's hair", -1],
            ["what are the people doing", 1],
            ["what is the woman wearing on her face", -1],
            ["what is on the woman's head", 2],
            ["what is the woman looking at", 1],
            ["where is the woman looking", 1],
            ["what is on the woman's face", -1]
        ],
        "context": [
            "a restaurant with a menu on the wall.",
            "a woman with a dog wearing a hat."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2405713",
                "VG_object_id": "3404063",
                "bbox": [253, 170, 486, 318],
                "image": "data\\images\\2405713.jpg"
            },
            {
                "VG_image_id": "2344028",
                "VG_object_id": "2993035",
                "bbox": [63, 237, 288, 313],
                "image": "data\\images\\2344028.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bench", 2],
            ["what is in the background", 2],
            ["where is the bench", 1],
            ["what is in front of the bench", 1],
            ["what is behind the bench", 1]
        ],
        "org_questions": [
            ["what color is the bench", 2],
            ["what is in the background", 2],
            ["how many people are sitting on the bench", -1],
            ["what shape is the bench", -1],
            ["what is the bench made of", -1],
            ["where is the bench", 1],
            ["what is made of wood", -1],
            ["what is under the bench", -1],
            ["what is in front of the bench", 1],
            ["what is behind the bench", 1]
        ],
        "context": [
            "a church with a bench and a cross on the wall.",
            "a small restaurant with a table and two benches."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2380147",
                "VG_object_id": "547459",
                "bbox": [3, 1, 499, 498],
                "image": "data\\images\\2380147.jpg"
            },
            {
                "VG_image_id": "2395953",
                "VG_object_id": "1201801",
                "bbox": [25, 214, 493, 371],
                "image": "data\\images\\2395953.jpg"
            }
        ],
        "questions_with_scores": [
            ["how  many people are there in the picture", 2]
        ],
        "org_questions": [
            ["what is the pizza made of", -1],
            ["how  many people are there in the picture", 2],
            ["what is beside the pizza", -1],
            ["what color is the table", -1],
            ["where is the food", -1],
            ["what is on the plate", -1],
            ["how many keyboards are there on the table", -1],
            ["what is the food on", -1],
            ["where was the photo taken", -1],
            ["what is the pizza sitting on", -1]
        ],
        "context": [
            "a pizza on a plate with a knife and a knife.",
            "a man sitting at a table with a pizza."
        ]
    },
    {
        "object_category": "ocean",
        "images": [
            {
                "VG_image_id": "2385168",
                "VG_object_id": "1300334",
                "bbox": [14, 13, 472, 235],
                "image": "data\\images\\2385168.jpg"
            },
            {
                "VG_image_id": "2370596",
                "VG_object_id": "3857821",
                "bbox": [5, 61, 465, 282],
                "image": "data\\images\\2370596.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the ocean", 1],
            ["what are the people doing", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["how many people are in the ocean", 1],
            ["what are the people doing", 1],
            ["where is the beach", -1],
            ["what is the man doing", 1],
            ["when was the picture taken", -1],
            ["what is in the background", -1],
            ["what is the weather like", -1],
            ["who is in the photo", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "two surfers walking on the beach with their boards.",
            "a man kiteboarding on a large body of water."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2347455",
                "VG_object_id": "890694",
                "bbox": [141, 106, 364, 315],
                "image": "data\\images\\2347455.jpg"
            },
            {
                "VG_image_id": "2379948",
                "VG_object_id": "1352418",
                "bbox": [208, 33, 382, 199],
                "image": "data\\images\\2379948.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the ground", 1],
            ["what is behind the bear", 1],
            ["what is in the distance", 1]
        ],
        "org_questions": [
            ["What color is the ground", 1],
            ["How many bears are there", -1],
            ["what is behind the bear", 1],
            ["what is the weather like", -1],
            ["what is the bear holding", -1],
            ["what is in the distance", 1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["what is on the bear's head", -1],
            ["what is the bear doing", -1],
            ["what is the bear standing on", -1]
        ],
        "context": [
            "a black bear is in the middle of a field.",
            "two black bears sitting on the ground next to a tree."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2374917",
                "VG_object_id": "1889501",
                "bbox": [189, 75, 316, 341],
                "image": "data\\images\\2374917.jpg"
            },
            {
                "VG_image_id": "2329675",
                "VG_object_id": "2895800",
                "bbox": [210, 11, 337, 332],
                "image": "data\\images\\2329675.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the gender of player", 2],
            ["What color is the player's shirt", 1],
            ["What color is the ground", 1],
            ["what color is the shirt", 1],
            ["what is the persion wearing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["What color is the player's shirt", 1],
            ["What is the gender of player", 2],
            ["What color is the ground", 1],
            ["how many people are there", -1],
            ["what color is the shirt", 1],
            ["what color is the wall behind the player", -1],
            ["how many people are there in the picture", -1],
            ["what sport is being played", -1],
            ["what is the persion wearing", 1],
            ["what is the persion doing", -1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a woman playing tennis on a tennis court.",
            "a man playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2373829",
                "VG_object_id": "3724201",
                "bbox": [1, 0, 374, 498],
                "image": "data\\images\\2373829.jpg"
            },
            {
                "VG_image_id": "2351673",
                "VG_object_id": "1671865",
                "bbox": [2, 238, 122, 330],
                "image": "data\\images\\2351673.jpg"
            }
        ],
        "questions_with_scores": [["what color is the table", 1]],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", -1],
            ["how many people are there", -1],
            ["how many wine glasses are there on the table", -1],
            ["what is the food on", -1]
        ],
        "context": [
            "a sandwich is sitting on a plate next to a bottle of beer.",
            "a sandwich and corn on a plate with a fork."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2328823",
                "VG_object_id": "2991298",
                "bbox": [248, 62, 487, 310],
                "image": "data\\images\\2328823.jpg"
            },
            {
                "VG_image_id": "2385624",
                "VG_object_id": "1294506",
                "bbox": [215, 99, 485, 333],
                "image": "data\\images\\2385624.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the dog", 2],
            ["How many dogs are there", 1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["what is in front of the dog", 1]
        ],
        "org_questions": [
            ["What color is the dog", 2],
            ["How many dogs are there", 1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["What is the dog on", -1],
            ["what gesture is the dog", -1],
            ["what is on the dog's head", -1],
            ["who is in the picture", -1],
            ["what is the dog wearing", -1],
            ["where is the dog looking", -1],
            ["what is in front of the dog", 1]
        ],
        "context": [
            "a dog is resting in the seat of a car.",
            "a dog standing on a table next to a laptop."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2320361",
                "VG_object_id": "993872",
                "bbox": [66, 150, 476, 373],
                "image": "data\\images\\2320361.jpg"
            },
            {
                "VG_image_id": "2315857",
                "VG_object_id": "1055421",
                "bbox": [1, 357, 242, 499],
                "image": "data\\images\\2315857.jpg"
            }
        ],
        "questions_with_scores": [["what color is the floor", 1]],
        "org_questions": [
            ["what color is the floor", 1],
            ["How many people are there", -1],
            ["what is on the ground", -1],
            ["what is the floor made of", -1],
            ["how many dogs are there on the floor", -1],
            ["what is the color of the wall", -1],
            ["what room is this", -1],
            ["what shape is the floor", -1],
            ["where is the tile", -1],
            ["where is the picture taken", -1],
            ["what is covering the floor", -1]
        ],
        "context": [
            "a urinal in a bathroom stall with a toilet paper dispenser.",
            "a bathroom with a walk in shower and a sink."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2358225",
                "VG_object_id": "2557067",
                "bbox": [0, 386, 372, 499],
                "image": "data\\images\\2358225.jpg"
            },
            {
                "VG_image_id": "2336128",
                "VG_object_id": "3665164",
                "bbox": [309, 305, 499, 372],
                "image": "data\\images\\2336128.jpg"
            }
        ],
        "questions_with_scores": [
            ["what room is the floor in", 2],
            ["where is the picture taken", 1],
            ["how many tables are on the floor", 1],
            ["what is standing on the floor", 1],
            ["what is the floor made of", 1],
            ["where is the floor", 1],
            ["what kind of flooring is this", 1],
            ["what room is this", 1],
            ["what is on the floor", 1],
            ["what material is the floor made of", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["where is the picture taken", 1],
            ["how many tables are on the floor", 1],
            ["what is standing on the floor", 1],
            ["what is the floor made of", 1],
            ["what room is the floor in", 2],
            ["where is the floor", 1],
            ["what is covering the floor", -1],
            ["what kind of flooring is this", 1],
            ["what room is this", 1],
            ["what is on the floor", 1],
            ["what material is the floor made of", 1]
        ],
        "context": [
            "a kitchen with a refrigerator, microwave and a microwave.",
            "a bedroom with a desk and a chair."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2414233",
                "VG_object_id": "157266",
                "bbox": [10, 104, 483, 323],
                "image": "data\\images\\2414233.jpg"
            },
            {
                "VG_image_id": "2364320",
                "VG_object_id": "2219172",
                "bbox": [4, 2, 497, 373],
                "image": "data\\images\\2364320.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are there on the table", 1],
            ["how many glasses are there on the table", 1],
            ["how many people are there", 1],
            ["what is the plate made of", 1],
            ["what is the pizza sitting on", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["how many plates are there on the table", 1],
            ["how many glasses are there on the table", 1],
            ["what is on the plate", -1],
            ["how many people are there", 1],
            ["what is the plate made of", 1],
            ["what color is the plate", -1],
            ["what color is the plate on the table", -1],
            ["what is the table made of", -1],
            ["what is on the table", -1],
            ["where was this photo taken", -1],
            ["what is the pizza sitting on", 1],
            ["where is the pizza", -1],
            ["what is under the pizza", -1]
        ],
        "context": [
            "a person giving a thumbs up while sitting at a table with a pizza.",
            "a pizza on a plate"
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2396354",
                "VG_object_id": "1198625",
                "bbox": [0, 1, 499, 332],
                "image": "data\\images\\2396354.jpg"
            },
            {
                "VG_image_id": "2317482",
                "VG_object_id": "3154743",
                "bbox": [1, 173, 498, 373],
                "image": "data\\images\\2317482.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what color is the shirt", 2],
            ["what gender are the players on the court", 1]
        ],
        "org_questions": [
            ["what color is the ground", 2],
            ["what color is the shirt", 2],
            ["How many person are there", -1],
            ["what gender are the players on the court", 1],
            ["what is the persion wearing", -1],
            ["how many players are there on the court", -1],
            ["when was the photo taken", -1],
            ["what sport is being played", -1],
            ["what is on the court", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a man holding a tennis racquet on a tennis court.",
            "a young girl playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2327377",
                "VG_object_id": "3441176",
                "bbox": [151, 41, 278, 330],
                "image": "data\\images\\2327377.jpg"
            },
            {
                "VG_image_id": "2379567",
                "VG_object_id": "551971",
                "bbox": [219, 120, 304, 353],
                "image": "data\\images\\2379567.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 1],
            ["what color are the man's pants", 1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["what color is the ground", 1],
            ["what kind of pants is the man wearing", 1],
            ["what is the man riding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 1],
            ["what color are the man's pants", 1],
            ["How many people are there", -1],
            ["what is the man wearing on head", -1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["what color is the ground", 1],
            ["who is wearing a hat", -1],
            ["when was the photo taken", -1],
            ["what kind of pants is the man wearing", 1],
            ["what is the man riding", 1]
        ],
        "context": [
            "a man and woman riding a motorcycle.",
            "a man walking a large horse on a field."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2342724",
                "VG_object_id": "931271",
                "bbox": [2, 215, 171, 368],
                "image": "data\\images\\2342724.jpg"
            },
            {
                "VG_image_id": "2390445",
                "VG_object_id": "497425",
                "bbox": [19, 224, 490, 331],
                "image": "data\\images\\2390445.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what is on the land", 1],
            ["what is the ground covered with", 1],
            ["what is on the ground", 1],
            ["where was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is on the land", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["what is the land made of", -1],
            ["what is on the ground", 1],
            ["where was the picture taken", 1]
        ],
        "context": [
            "a large blue airplane is parked on the tarmac.",
            "a motorcycle parked in front of a garage door."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2381120",
                "VG_object_id": "707203",
                "bbox": [114, 171, 436, 308],
                "image": "data\\images\\2381120.jpg"
            },
            {
                "VG_image_id": "2361710",
                "VG_object_id": "2350794",
                "bbox": [48, 176, 190, 276],
                "image": "data\\images\\2361710.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what are the people doing", 2],
            ["what is the ground covered with", 1],
            ["what is on the land", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["how many people are there", 2],
            ["what is on the land", 1],
            ["how many motorcycles are there on the ground", -1],
            ["what is the color of the land", -1],
            ["when was the picture taken", -1],
            ["where was this picture taken", -1],
            ["where was the photo taken", -1],
            ["what are the people doing", 2]
        ],
        "context": [
            "a group of men standing on top of a field.",
            "a man swinging a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2349013",
                "VG_object_id": "2673939",
                "bbox": [219, 28, 391, 298],
                "image": "data\\images\\2349013.jpg"
            },
            {
                "VG_image_id": "2377707",
                "VG_object_id": "3675628",
                "bbox": [107, 95, 260, 371],
                "image": "data\\images\\2377707.jpg"
            }
        ],
        "questions_with_scores": [["how many people are there", 1]],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", -1],
            ["what is the man holding", -1],
            ["what is the ground under the man made of", -1],
            ["how many people are there", 1],
            ["what is in the background", -1],
            ["what is the man wearing", -1],
            ["what sport is the man doing", -1],
            ["what is on the player's head", -1],
            ["where is the picture taken", -1],
            ["what game is being played", -1],
            ["where is the man", -1],
            ["what is the persion wearing on his face", -1],
            ["what gesture is the man", -1],
            ["who is holding the ball", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2408693",
                "VG_object_id": "3808796",
                "bbox": [197, 126, 351, 480],
                "image": "data\\images\\2408693.jpg"
            },
            {
                "VG_image_id": "2332294",
                "VG_object_id": "3067086",
                "bbox": [1, 43, 235, 254],
                "image": "data\\images\\2332294.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 1],
            ["what sport is it", 1],
            ["what is the person wearing on the head", 1],
            ["what is the man doing", 1],
            ["what is on the person's head", 1],
            ["what color are the person's trousers", 1],
            ["what is the man in the white shirt doing", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what sport is it", 1],
            ["what is the person wearing on the head", 1],
            ["How many people are there", -1],
            ["where is the person", -1],
            ["what is the man doing", 1],
            ["what is on the person's head", 1],
            ["what color are the person's trousers", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man in the white shirt doing", 1]
        ],
        "context": [
            "a group of children playing tennis on a tennis court.",
            "a man doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2344534",
                "VG_object_id": "915051",
                "bbox": [28, 39, 227, 496],
                "image": "data\\images\\2344534.jpg"
            },
            {
                "VG_image_id": "2368845",
                "VG_object_id": "615686",
                "bbox": [41, 9, 195, 243],
                "image": "data\\images\\2368845.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what color is the woman's pant", 2],
            ["what is the ground covered with", 1],
            ["what color is the ground", 1],
            ["what is the woman doing", 1],
            ["what is on the woman's head", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 2],
            ["what is the ground covered with", 1],
            ["what color is the ground", 1],
            ["How many people are there", -1],
            ["Where is the woman", -1],
            ["what sport is the woman playing", -1],
            ["what is in the background", -1],
            ["what is the woman doing", 1],
            ["when was the photo taken", -1],
            ["who is playing tennis", -1],
            ["what is on the woman's head", 1],
            ["what is the woman wearing", -1],
            ["what color is the woman's pant", 2]
        ],
        "context": [
            "a woman holding two tennis balls on a tennis court.",
            "two pictures of a woman playing tennis."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2378497",
                "VG_object_id": "2173874",
                "bbox": [80, 5, 315, 484],
                "image": "data\\images\\2378497.jpg"
            },
            {
                "VG_image_id": "2350101",
                "VG_object_id": "2138657",
                "bbox": [134, 73, 404, 299],
                "image": "data\\images\\2350101.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman's posture", 1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding", 1],
            ["what is behind the woman", 1],
            ["where was this photo taken", 1],
            ["what is the woman sitting on", 1],
            ["what is the lady doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", -1],
            ["what kind of clothes is the woman wearing", -1],
            ["what is the woman's posture", 1],
            ["how many women are there", -1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding", 1],
            ["who is in the photo", -1],
            ["what is behind the woman", 1],
            ["where was this photo taken", 1],
            ["what is the woman wearing", -1],
            ["what is the woman sitting on", 1],
            ["how many people are there", -1],
            ["what is the lady doing", 1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a woman wearing a blue dress and heels",
            "a woman sitting on a bed looking out a window."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2375312",
                "VG_object_id": "1883374",
                "bbox": [175, 92, 307, 207],
                "image": "data\\images\\2375312.jpg"
            },
            {
                "VG_image_id": "2375041",
                "VG_object_id": "584459",
                "bbox": [223, 196, 309, 267],
                "image": "data\\images\\2375041.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the screen", 2],
            ["what color is the table the screen placed on", 2],
            ["what is in front of the screen", 1]
        ],
        "org_questions": [
            ["where is the screen", -1],
            ["what color is the screen", 2],
            ["What creature is near the screen", -1],
            ["what is in front of the screen", 1],
            ["what color is the table the screen placed on", 2],
            ["what color is the table", -1],
            ["how many televisions are there", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a computer monitor and keyboard on a desk.",
            "a desk with a television and a desk"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2409561",
                "VG_object_id": "239151",
                "bbox": [20, 164, 98, 227],
                "image": "data\\images\\2409561.jpg"
            },
            {
                "VG_image_id": "2347268",
                "VG_object_id": "892127",
                "bbox": [357, 201, 477, 288],
                "image": "data\\images\\2347268.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 1],
            ["what number is on the batter's shirt", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what gender is the person in the shirt", -1],
            ["how many people are there", -1],
            ["what is the man wearing on his head", -1],
            ["what is the man doing", -1],
            ["who is wearing the shirt", -1],
            ["when was the picture taken", -1],
            ["where was the photo taken", -1],
            ["what number is on the batter's shirt", 1],
            ["what color is the grass", -1],
            ["how many players are there", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2371430",
                "VG_object_id": "2474097",
                "bbox": [215, 109, 366, 311],
                "image": "data\\images\\2371430.jpg"
            },
            {
                "VG_image_id": "2333403",
                "VG_object_id": "3259190",
                "bbox": [208, 182, 382, 341],
                "image": "data\\images\\2333403.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the person's coat", 1],
            ["what is the person wearing on the head", 1],
            ["where is the photo taken", 1],
            ["what is the person doing", 1],
            ["what is the person holding", 1],
            ["what is the person wearing", 1],
            ["where is the person", 1]
        ],
        "org_questions": [
            ["what color is the person's coat", 1],
            ["what is the person wearing on the head", 1],
            ["how many people are there", 2],
            ["where is the photo taken", 1],
            ["what is the person doing", 1],
            ["what is the person holding", 1],
            ["what is the person wearing", 1],
            ["where is the person", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man holding a stop sign on a road.",
            "a man riding a horse drawn carriage down a street."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2389343",
                "VG_object_id": "505292",
                "bbox": [162, 234, 202, 341],
                "image": "data\\images\\2389343.jpg"
            },
            {
                "VG_image_id": "2323207",
                "VG_object_id": "3432287",
                "bbox": [208, 32, 286, 209],
                "image": "data\\images\\2323207.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's pant", 1],
            ["what is on the person's head", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's pant", 1],
            ["how many people are there", -1],
            ["what is on the person's head", 1],
            ["what is in the background", -1],
            ["what is the man holding", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a group of children flying kites in a park.",
            "a man riding a skateboard on top of a cement surface."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2377792",
                "VG_object_id": "2420598",
                "bbox": [190, 34, 323, 272],
                "image": "data\\images\\2377792.jpg"
            },
            {
                "VG_image_id": "2360010",
                "VG_object_id": "3763129",
                "bbox": [158, 146, 319, 381],
                "image": "data\\images\\2360010.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man holding", -1],
            ["how many people are there", 1],
            ["where is the child", -1],
            ["What is child doing", -1],
            ["what gesture is the child", -1],
            ["when was the photo taken", -1],
            ["what is on the man's head", -1],
            ["what sport is this", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man riding a skateboard on top of a ramp.",
            "a skateboarder is doing a trick at a skate park."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2364457",
                "VG_object_id": "2398933",
                "bbox": [4, 12, 303, 380],
                "image": "data\\images\\2364457.jpg"
            },
            {
                "VG_image_id": "2343942",
                "VG_object_id": "2835247",
                "bbox": [55, 109, 165, 270],
                "image": "data\\images\\2343942.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's hair", 1],
            ["what color is the girl's shirt", 1],
            ["what is on the table", 1],
            ["how many people are there", 1],
            ["How old is this person", 1],
            ["what is the woman looking at", 1]
        ],
        "org_questions": [
            ["what color is the girl's hair", 1],
            ["what color is the girl's shirt", 1],
            ["what is on the table", 1],
            ["how many people are there", 1],
            ["Where is the girl", -1],
            ["How old is this person", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the woman wearing", -1],
            ["what is the woman looking at", 1]
        ],
        "context": [
            "a little girl sitting at a table with a pizza.",
            "a woman and a man sitting in a chair with a laptop."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2384277",
                "VG_object_id": "1311466",
                "bbox": [110, 11, 209, 324],
                "image": "data\\images\\2384277.jpg"
            },
            {
                "VG_image_id": "2383905",
                "VG_object_id": "1315572",
                "bbox": [253, 40, 394, 254],
                "image": "data\\images\\2383905.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["what color is the woman's shirt", 1],
            ["where is the girl", 1],
            ["What is woman doing", 1],
            ["What is the background of image", 1],
            ["what is the persion doing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["how many people are there in the photo", 2],
            ["what is the woman wearing", -1],
            ["where is the girl", 1],
            ["What is woman doing", 1],
            ["What is the background of image", 1],
            ["what is the woman holding", -1],
            ["when was the photo taken", -1],
            ["what is the persion doing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "context": [
            "a vase with a yellow flower and purple flowers.",
            "a group of people sitting on a bench."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2344471",
                "VG_object_id": "2758796",
                "bbox": [171, 128, 266, 188],
                "image": "data\\images\\2344471.jpg"
            },
            {
                "VG_image_id": "2379440",
                "VG_object_id": "3152460",
                "bbox": [133, 76, 205, 140],
                "image": "data\\images\\2379440.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the blanket", 1],
            ["what color is the horse", 1],
            ["what color are the man's trousers", 1],
            ["what is the ground covered with", 1],
            ["What is the person wearing", 1]
        ],
        "org_questions": [
            ["what color is the blanket", 1],
            ["what color is the horse", 1],
            ["what color are the man's trousers", 1],
            ["how many pillows are there", -1],
            ["Where is the person", -1],
            ["what is the ground covered with", 1],
            ["What is the person wearing", 1],
            ["what is on the blanket", -1],
            ["when was the photo taken", -1],
            ["what is around the horse's neck", -1],
            ["what is on the horse's neck", -1]
        ],
        "context": [
            "a woman riding a horse through a lush green forest.",
            "a man riding a horse down a street."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2397332",
                "VG_object_id": "1190745",
                "bbox": [5, 0, 499, 173],
                "image": "data\\images\\2397332.jpg"
            },
            {
                "VG_image_id": "2370649",
                "VG_object_id": "3857547",
                "bbox": [13, 5, 374, 118],
                "image": "data\\images\\2370649.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["how many trees are there in the distance", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is on the ground", 1],
            ["what color is the ground", -1],
            ["What time is it", -1],
            ["how many trees are there in the distance", 1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", 1],
            ["how is the weather", -1],
            ["when was this picture taken", -1],
            ["what is in the sky", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a man and two children standing on a beach with a kite.",
            "a group of giraffes standing around a tree."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2348137",
                "VG_object_id": "1975582",
                "bbox": [11, 53, 413, 498],
                "image": "data\\images\\2348137.jpg"
            },
            {
                "VG_image_id": "2347943",
                "VG_object_id": "886085",
                "bbox": [2, 2, 291, 450],
                "image": "data\\images\\2347943.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man wearing on his face", 2],
            ["what color is the man's shirt", 1],
            ["where is the photo taken", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", 2],
            ["what is the man wearing on his face", 2],
            ["where is the photo taken", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["what is the man wearing", 1]
        ],
        "context": ["a man wearing a suit and tie", "a man eating a banana"]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2360264",
                "VG_object_id": "3762168",
                "bbox": [16, 149, 276, 344],
                "image": "data\\images\\2360264.jpg"
            },
            {
                "VG_image_id": "2346486",
                "VG_object_id": "898958",
                "bbox": [142, 240, 428, 374],
                "image": "data\\images\\2346486.jpg"
            }
        ],
        "questions_with_scores": [["what is the sofa made of", 1]],
        "org_questions": [
            ["what is the sofa made of", 1],
            ["what color is the sofa", -1],
            ["how many sofa are in the picture", -1],
            ["what is the person doing", -1],
            ["where is the sofa placed", -1],
            ["what is sitting on the sofa", -1],
            ["what game are the people playing", -1],
            ["what are the people holding", -1],
            ["what are the men playing", -1],
            ["where are the people", -1],
            ["what are they doing", -1]
        ],
        "context": [
            "a man and a woman playing a video game.",
            "a group of people standing around a tv."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2346276",
                "VG_object_id": "3618214",
                "bbox": [259, 114, 329, 187],
                "image": "data\\images\\2346276.jpg"
            },
            {
                "VG_image_id": "2349281",
                "VG_object_id": "2303221",
                "bbox": [343, 75, 418, 149],
                "image": "data\\images\\2349281.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many horses are there in the picture", 2],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["what is the man riding on", 1],
            ["what is the man riding", 1]
        ],
        "org_questions": [
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["how many men are there", -1],
            ["what is the man wearing on head", -1],
            ["how is the weather", -1],
            ["what is the man wearing", -1],
            ["what color is the man's shirt", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man riding on", 1],
            ["what is the man riding", 1],
            ["how many horses are there in the picture", 2]
        ],
        "context": [
            "a man on a surfboard riding a wave.",
            "two horses pulling a red wagon down a street."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2393599",
                "VG_object_id": "1218536",
                "bbox": [81, 221, 267, 450],
                "image": "data\\images\\2393599.jpg"
            },
            {
                "VG_image_id": "2403953",
                "VG_object_id": "379370",
                "bbox": [220, 47, 347, 183],
                "image": "data\\images\\2403953.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the guy doing", 2],
            ["how many people are there", 2],
            ["what color is the guy's shirt", 2],
            ["what kind of sport is the guy doing", 2],
            ["what are on the guy's feet", 1],
            ["what color is the man's shirt", 1],
            ["where is the guy", 1],
            ["what is the person holding", 1],
            ["What is guy doing", 1],
            ["who is on the skateboard", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what is the guy doing", 2],
            ["what are on the guy's feet", 1],
            ["what color is the man's shirt", 1],
            ["where is the guy", 1],
            ["what is the person holding", 1],
            ["what is the man wearing", -1],
            ["What is guy doing", 1],
            ["how many people are there", 2],
            ["when was the picture taken", -1],
            ["who is on the skateboard", 1],
            ["what is on the ground", 1],
            ["where was the photo taken", -1],
            ["what color is the guy's shirt", 2],
            ["what kind of sport is the guy doing", 2]
        ],
        "context": [
            "a man walking on a beach carrying a surfboard.",
            "a person on a skateboard doing a trick in a skate park."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2363990",
                "VG_object_id": "760392",
                "bbox": [126, 241, 446, 333],
                "image": "data\\images\\2363990.jpg"
            },
            {
                "VG_image_id": "2321268",
                "VG_object_id": "2852763",
                "bbox": [0, 401, 234, 498],
                "image": "data\\images\\2321268.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the floor", 2]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is on the floor", -1],
            ["how many people are there on the floor", 2],
            ["where is the photo taken", -1],
            ["what is the floor under the rug made of", -1],
            ["where is the rug", -1],
            ["how is the floor made", -1],
            ["what type of flooring is shown", -1],
            ["what is in the floor", -1],
            ["what is covering the floor", -1]
        ],
        "context": [
            "a bathroom with a sink, a rug and a rug.",
            "a man and a woman standing in a room."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2410020",
                "VG_object_id": "228012",
                "bbox": [280, 58, 499, 383],
                "image": "data\\images\\2410020.jpg"
            },
            {
                "VG_image_id": "2359760",
                "VG_object_id": "2255984",
                "bbox": [9, 32, 475, 397],
                "image": "data\\images\\2359760.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["where is the photo taken", 1],
            ["where is the person", 1],
            ["What is man doing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["how many people are in the picture", 2],
            ["what is the man holding", 1],
            ["where is the photo taken", 1],
            ["what color is the man's shirt", -1],
            ["where is the person", 1],
            ["What is man doing", 1],
            ["when was the picture taken", -1],
            ["what is on the man's head", -1],
            ["who is in the picture", -1],
            ["what is behind the man", 1]
        ],
        "context": [
            "a man and a woman on a cell phone.",
            "a man wearing a white shirt and a tie"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "498012",
                "VG_object_id": "1039251",
                "bbox": [516, 1, 985, 228],
                "image": "data\\images\\498012.jpg"
            },
            {
                "VG_image_id": "2337274",
                "VG_object_id": "958358",
                "bbox": [203, 190, 456, 311],
                "image": "data\\images\\2337274.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["how many people are there", 1],
            ["how long is the sleeves of the shirt", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the table made of", -1],
            ["how many people are there", 1],
            ["what gender is the person in the shirt", -1],
            ["What is person doing", -1],
            ["how long is the sleeves of the shirt", 1],
            ["What is the gender of person", -1],
            ["what is the man sitting on", -1],
            ["what is on the man's shirt", -1]
        ],
        "context": [
            "a picnic table with a person sitting at it",
            "a man and a child sitting at a table with food."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2345295",
                "VG_object_id": "2150452",
                "bbox": [115, 58, 270, 330],
                "image": "data\\images\\2345295.jpg"
            },
            {
                "VG_image_id": "2358601",
                "VG_object_id": "3543873",
                "bbox": [102, 129, 295, 342],
                "image": "data\\images\\2358601.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the person doing", 1],
            ["what is the man holding", 1],
            ["what is the ground covered with", 1],
            ["what is the man on the left doing", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what sport is the person doing", 1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["what color is the person's shirt", -1],
            ["where is the person", -1],
            ["what is the ground covered with", 1],
            ["what is in front of the person", -1],
            ["when was the photo taken", -1],
            ["what is the man wearing on his head", -1],
            ["who is in the photo", -1],
            ["what is the man on the left doing", 1],
            ["what is the persion standing on", 1]
        ],
        "context": [
            "a man holding a bat on a baseball field.",
            "a man is doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2318248",
                "VG_object_id": "1012672",
                "bbox": [168, 153, 500, 331],
                "image": "data\\images\\2318248.jpg"
            },
            {
                "VG_image_id": "2362219",
                "VG_object_id": "776159",
                "bbox": [11, 184, 254, 488],
                "image": "data\\images\\2362219.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["how many plates are there on the table", 1],
            ["what is the food on", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["how many plates are there on the table", 1],
            ["Where is the photo taken", -1],
            ["what is on the counter", -1],
            ["what is the counter made of", -1],
            ["what is the man doing", -1],
            ["what is the food on", 1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a man and woman preparing food in a kitchen.",
            "a man in a white apron is making a pizza."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2385282",
                "VG_object_id": "1298865",
                "bbox": [29, 18, 471, 394],
                "image": "data\\images\\2385282.jpg"
            },
            {
                "VG_image_id": "2342970",
                "VG_object_id": "928804",
                "bbox": [109, 0, 410, 309],
                "image": "data\\images\\2342970.jpg"
            }
        ],
        "questions_with_scores": [["What color is the bus", 2]],
        "org_questions": [
            ["What color is the bus", 2],
            ["How many people are there in the image", -1],
            ["How many cars are there", -1],
            ["what time is it", -1],
            ["what is on the side of the bus", -1],
            ["where is the bus", -1],
            ["When is the picture taken", -1],
            ["what type of bus is this", -1],
            ["what is the bus doing", -1],
            ["where was this picture taken", -1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a double decker bus parked next to a table.",
            "a blue and white bus driving down a street."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2350677",
                "VG_object_id": "3592617",
                "bbox": [9, 214, 499, 321],
                "image": "data\\images\\2350677.jpg"
            },
            {
                "VG_image_id": "2351544",
                "VG_object_id": "2373555",
                "bbox": [3, 312, 498, 374],
                "image": "data\\images\\2351544.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["what shape is the table", -1],
            ["where is the photo taken", -1],
            ["what is the table made of", -1],
            ["how many glasses are there on the table", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a woman standing in front of a table with a cake on it.",
            "two men sitting at a table eating pizza."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2373985",
                "VG_object_id": "2564988",
                "bbox": [52, 312, 144, 373],
                "image": "data\\images\\2373985.jpg"
            },
            {
                "VG_image_id": "2396719",
                "VG_object_id": "441074",
                "bbox": [164, 353, 220, 498],
                "image": "data\\images\\2396719.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the gender of the person", 2],
            ["where is the person", 2],
            ["what color are the person's pants", 1],
            ["HOw many people are there", 1],
            ["What is the background of image", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", 1],
            ["what color is the shirt", 1],
            ["what is the main color of the pants", 1]
        ],
        "org_questions": [
            ["what color are the person's pants", 1],
            ["what is the gender of the person", 2],
            ["where is the person", 2],
            ["HOw many people are there", 1],
            ["What is the background of image", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", 1],
            ["what color is the shirt", 1],
            ["what is the main color of the pants", 1]
        ],
        "context": [
            "two young girls playing a video game in a living room.",
            "a man standing next to a street sign."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2367424",
                "VG_object_id": "751669",
                "bbox": [78, 132, 179, 374],
                "image": "data\\images\\2367424.jpg"
            },
            {
                "VG_image_id": "1593169",
                "VG_object_id": "1616112",
                "bbox": [353, 177, 809, 765],
                "image": "data\\images\\1593169.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["what color is the wall", 1],
            ["what is in the distance", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what color is the wall", 1],
            ["how many people are there in the picture", -1],
            ["what is on the woman's head", -1],
            ["where is the woman", -1],
            ["what is the woman doing", -1],
            ["what is in the distance", 1],
            ["when was the photo taken", -1],
            ["what kind of shirt is the woman wearing", -1],
            ["what is the persion holding", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a group of people standing around a table with glasses.",
            "person, person, and actor at the bar."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2327715",
                "VG_object_id": "979613",
                "bbox": [4, 175, 499, 326],
                "image": "data\\images\\2327715.jpg"
            },
            {
                "VG_image_id": "2354091",
                "VG_object_id": "2032004",
                "bbox": [1, 109, 497, 331],
                "image": "data\\images\\2354091.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is on the ground", -1],
            ["what are the people doing", 1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the weather like", -1],
            ["Whatis the background of image", -1],
            ["What is land made of", -1],
            ["where is this picture taken", -1],
            ["where is the grass", -1],
            ["what is in the background", -1],
            ["what is green in color", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a dog catching a frisbee in its mouth."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2373786",
                "VG_object_id": "1999636",
                "bbox": [222, 118, 319, 249],
                "image": "data\\images\\2373786.jpg"
            },
            {
                "VG_image_id": "2381209",
                "VG_object_id": "706390",
                "bbox": [95, 0, 208, 249],
                "image": "data\\images\\2381209.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the boy", 2],
            ["what color is the shirt of the boy", 1],
            ["what is the boy doing", 1],
            ["how many people are there", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the shirt of the boy", 1],
            ["what is the boy doing", 1],
            ["where is the boy", 2],
            ["how many people are there", 1],
            ["What is the boy wearing on his head", -1],
            ["what is in the background", 1],
            ["what is the boy holding", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "two men playing frisbee in a gym",
            "a man and a boy playing with two dogs."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2412939",
                "VG_object_id": "183664",
                "bbox": [361, 22, 419, 98],
                "image": "data\\images\\2412939.jpg"
            },
            {
                "VG_image_id": "2399028",
                "VG_object_id": "419792",
                "bbox": [19, 25, 168, 190],
                "image": "data\\images\\2399028.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the shape of the board", 2],
            ["what color is the board", 1],
            ["what is the ground covered with", 1],
            ["what is the sign on", 1],
            ["what does the sign say", 1]
        ],
        "org_questions": [
            ["what color is the board", 1],
            ["what is the shape of the board", 2],
            ["where is the photo taken", -1],
            ["what direction is the board heading to", -1],
            ["what is the ground covered with", 1],
            ["how many signs are there", -1],
            ["when was the photo taken", -1],
            ["what is the sign on", 1],
            ["what does the sign say", 1]
        ],
        "context": [
            "a yellow forklift parked in front of a building.",
            "a stop sign is in front of some trees."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2322709",
                "VG_object_id": "3457176",
                "bbox": [147, 116, 215, 333],
                "image": "data\\images\\2322709.jpg"
            },
            {
                "VG_image_id": "2347514",
                "VG_object_id": "2263477",
                "bbox": [167, 36, 269, 177],
                "image": "data\\images\\2347514.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is on the man's face", 2],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["Where is the person", 1],
            ["What is person doing", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is on the person's head", -1],
            ["Where is the person", 1],
            ["what is the man holding", -1],
            ["What is person doing", 1],
            ["when was the photo taken", -1],
            ["what is the man wearing", 1],
            ["what is on the man's face", 2],
            ["what is the man wearing on the head", -1]
        ],
        "context": [
            "three people standing on a road in the woods.",
            "two men riding on the back of an elephant."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2376867",
                "VG_object_id": "569142",
                "bbox": [317, 0, 499, 253],
                "image": "data\\images\\2376867.jpg"
            },
            {
                "VG_image_id": "2369938",
                "VG_object_id": "607844",
                "bbox": [56, 21, 188, 216],
                "image": "data\\images\\2369938.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["what is the man holding", 2],
            ["WHat color is the field", 1],
            ["what is the man wearing on his head", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["WHat color is the field", 1],
            ["what is the man wearing on his head", 1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what is the man holding", 2],
            ["what type of shirt is the man wearing", -1],
            ["what is on the man's head", 1],
            ["what is the persion on the right wearing", -1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a man is shearing a sheep with a knife."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2356374",
                "VG_object_id": "1700083",
                "bbox": [2, 0, 496, 328],
                "image": "data\\images\\2356374.jpg"
            },
            {
                "VG_image_id": "2406072",
                "VG_object_id": "1104088",
                "bbox": [27, 80, 388, 267],
                "image": "data\\images\\2406072.jpg"
            }
        ],
        "questions_with_scores": [["what colors are the buildings", 1]],
        "org_questions": [
            ["how many buildings are there", -1],
            ["what colors are the buildings", 1],
            ["what is the weather like", -1],
            ["where is the photo taken", -1],
            ["what is the building made of", -1],
            ["what time is it", -1],
            ["when is this photo taken ", -1],
            ["what is on the building", -1],
            ["what is in the background of the photo", -1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a street with a bus and a car on it",
            "a blue building with a green traffic light."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2379440",
                "VG_object_id": "3664035",
                "bbox": [133, 9, 194, 143],
                "image": "data\\images\\2379440.jpg"
            },
            {
                "VG_image_id": "2380257",
                "VG_object_id": "1348818",
                "bbox": [18, 8, 214, 298],
                "image": "data\\images\\2380257.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man's hat made of", 2],
            ["what is the man doing", 1],
            ["how many people are in the picture", 1],
            ["What is man doing", 1],
            ["what is the man riding on", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man's hat made of", 2],
            ["how many people are in the picture", 1],
            ["Where is the photo taken", -1],
            ["where is the man", -1],
            ["What is man doing", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is the man riding on", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man riding a horse down a street.",
            "a man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2356825",
                "VG_object_id": "2951766",
                "bbox": [131, 123, 191, 218],
                "image": "data\\images\\2356825.jpg"
            },
            {
                "VG_image_id": "2364433",
                "VG_object_id": "1690963",
                "bbox": [291, 103, 386, 246],
                "image": "data\\images\\2364433.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's clothes", 2],
            ["what is the woman wearing", 1],
            ["what is the gesture of the woman", 1],
            ["what is the person doing", 1],
            ["What is woman holding", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["where is the woman", -1],
            ["what is the woman wearing", 1],
            ["what is the gesture of the woman", 1],
            ["what is the person doing", 1],
            ["what color is the woman's clothes", 2],
            ["what is the weather like", -1],
            ["What is woman holding", 1],
            ["how many people are there", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a group of people riding on the back of an elephant.",
            "a man is kneeling down while holding a kite."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2388801",
                "VG_object_id": "670640",
                "bbox": [207, 96, 339, 171],
                "image": "data\\images\\2388801.jpg"
            },
            {
                "VG_image_id": "2352323",
                "VG_object_id": "1965280",
                "bbox": [385, 124, 479, 221],
                "image": "data\\images\\2352323.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the cow", 1],
            ["what is the cow standing on", 1],
            ["how many cows are in the picture", 1],
            ["where is the cow", 1]
        ],
        "org_questions": [
            ["what color is the cow", 1],
            ["what is the cow standing on", 1],
            ["how many cows are in the picture", 1],
            ["what kind of animal is it", -1],
            ["where is the cow", 1],
            ["what is the cow doing ", -1],
            ["what color is the ground the cow standing on", -1],
            ["what color is the cow's head", -1],
            ["when was the photo taken", -1],
            ["what is on the cow's head", -1]
        ],
        "context": [
            "a couple of cows standing in a lake.",
            "a cow standing in the middle of a field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2360226",
                "VG_object_id": "1960507",
                "bbox": [198, 208, 269, 320],
                "image": "data\\images\\2360226.jpg"
            },
            {
                "VG_image_id": "2398056",
                "VG_object_id": "1183375",
                "bbox": [397, 127, 462, 227],
                "image": "data\\images\\2398056.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 2],
            ["where is the woman", 1],
            ["What is woman holding", 1],
            ["what gesture is the woman", 1],
            [" what is the woman wearing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what is the woman doing", 2],
            ["where is the woman", 1],
            ["What is the woman wearing on her head", -1],
            ["What is woman holding", 1],
            ["what gesture is the woman", 1],
            [" what is the woman wearing", 1],
            ["how many people are there", -1],
            ["when was this taken", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man and woman loading luggage onto a train.",
            "a baseball player swinging a bat at a ball"
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2373113",
                "VG_object_id": "1828043",
                "bbox": [191, 3, 476, 96],
                "image": "data\\images\\2373113.jpg"
            },
            {
                "VG_image_id": "2345030",
                "VG_object_id": "2052414",
                "bbox": [126, 104, 354, 176],
                "image": "data\\images\\2345030.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 1],
            ["what color is the train", 1]
        ],
        "org_questions": [
            ["what color is the building", 1],
            ["what color is the train", 1],
            ["how many people are there", -1],
            ["what is in front of the building", -1],
            ["how is the weather", -1],
            ["what is the building made of", -1],
            ["when was this picture taken", -1],
            ["what is above the train", -1],
            ["what is on the side of the train", -1]
        ],
        "context": [
            "a red train is pulling into a station.",
            "a train station with a train on the tracks."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2341392",
                "VG_object_id": "2660169",
                "bbox": [209, 160, 493, 373],
                "image": "data\\images\\2341392.jpg"
            },
            {
                "VG_image_id": "2388691",
                "VG_object_id": "671498",
                "bbox": [6, 351, 332, 500],
                "image": "data\\images\\2388691.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["what is the floor made of", 1],
            ["how many tables are on the floor", 1],
            ["what is on the ground", 1],
            ["how is the floor made", 1],
            ["what type of floor is this", 1],
            ["what is in the room", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is the floor made of", 1],
            ["how many tables are on the floor", 1],
            ["where is the picture taken", -1],
            ["what is on the ground", 1],
            ["how is the floor made", 1],
            ["what type of floor is this", 1],
            ["what is in the room", 1]
        ],
        "context": [
            "a living room with a couch, coffee table and a couch.",
            "a bedroom with a bed, window, and a window."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2371414",
                "VG_object_id": "597806",
                "bbox": [130, 34, 359, 332],
                "image": "data\\images\\2371414.jpg"
            },
            {
                "VG_image_id": "2346486",
                "VG_object_id": "898943",
                "bbox": [17, 60, 163, 375],
                "image": "data\\images\\2346486.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["how many people are there in the picture", 1],
            ["What is man doing", 1],
            ["where was the picture taken", 1],
            ["who is in the photo", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the wall", -1],
            ["how many people are there in the picture", 1],
            ["what is the man wearing on head", -1],
            ["What is man doing", 1],
            ["what is the man wearing", -1],
            ["where was the picture taken", 1],
            ["who is in the photo", 1],
            ["what is the man standing on", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man is opening a cabinet in a kitchen.",
            "a group of people standing around a tv."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2348080",
                "VG_object_id": "1906780",
                "bbox": [29, 296, 362, 485],
                "image": "data\\images\\2348080.jpg"
            },
            {
                "VG_image_id": "2380799",
                "VG_object_id": "1342141",
                "bbox": [1, 2, 499, 71],
                "image": "data\\images\\2380799.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["what color is the table", 1],
            ["how many plates are there", 1],
            ["what is in the background", 1],
            ["what is on top of the table", 1]
        ],
        "org_questions": [
            ["what is on the table", 1],
            ["what color is the table", 1],
            ["how many plates are there", 1],
            ["what is the food on", -1],
            ["where was this photo taken", -1],
            ["what is in the background", 1],
            ["what is on top of the table", 1]
        ],
        "context": [
            "a kitchen with a large window and a small table with a hot dog on it.",
            "a slice of pepperoni pizza on a paper plate."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2408411",
                "VG_object_id": "262359",
                "bbox": [193, 114, 487, 324],
                "image": "data\\images\\2408411.jpg"
            },
            {
                "VG_image_id": "2323352",
                "VG_object_id": "3254134",
                "bbox": [0, 228, 498, 387],
                "image": "data\\images\\2323352.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the line on the street", 2],
            ["What is the weather like", 2],
            ["how many cars are there on the street", 1]
        ],
        "org_questions": [
            ["What color is the line on the street", 2],
            ["What is the weather like", 2],
            ["how many cars are there on the street", 1],
            ["When is photo taken", -1],
            ["what is on the side of the street", -1],
            ["what time is it", -1],
            ["where are the people", -1],
            ["what is the road made of", -1],
            ["what is in the background", -1],
            ["where was this picture taken", -1],
            ["what is on the road", -1]
        ],
        "context": [
            "a man in a striped hoodie is crossing the street.",
            "a group of women with umbrellas walking down a road."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2412339",
                "VG_object_id": "306772",
                "bbox": [135, 60, 239, 176],
                "image": "data\\images\\2412339.jpg"
            },
            {
                "VG_image_id": "2398719",
                "VG_object_id": "1176722",
                "bbox": [162, 129, 217, 221],
                "image": "data\\images\\2398719.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the person holding", 2],
            ["what color is the background", 1],
            ["how many shirts are in the picture", 1],
            ["who is wearing the shirt", 1],
            ["what are the people wearing", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the person holding", 2],
            ["what color is the background", 1],
            ["how many shirts are in the picture", 1],
            ["who is wearing the shirt", 1],
            ["what is on the person's head", -1],
            ["when was the photo taken", -1],
            ["what are the people wearing", 1],
            ["how many people are in the photo", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a group of men playing hockey on a court.",
            "two young girls holding umbrellas in the rain."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2377790",
                "VG_object_id": "563766",
                "bbox": [95, 139, 255, 234],
                "image": "data\\images\\2377790.jpg"
            },
            {
                "VG_image_id": "2323804",
                "VG_object_id": "3479870",
                "bbox": [34, 201, 124, 265],
                "image": "data\\images\\2323804.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is near the bench", 1],
            ["Where is the bench", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["how many people are shown", 1],
            ["who is sitting on the bench", 1],
            ["how many benches are there", 1],
            ["where is the bench", 1],
            ["what is on the back of the bench", 1],
            ["what is under the bench", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what is near the bench", 1],
            ["Where is the bench", 1],
            ["what is the bench made of", -1],
            ["What is on the bench", -1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["how many people are shown", 1],
            ["who is sitting on the bench", 1],
            ["how is the bench", -1],
            ["how many benches are there", 1],
            ["what shape is the bench", -1],
            ["where is the bench", 1],
            ["what is on the back of the bench", 1],
            ["what is under the bench", 1]
        ],
        "context": [
            "a man sitting on a bench in the woods",
            "a dog standing on a dock next to a man and woman."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2399639",
                "VG_object_id": "1167861",
                "bbox": [56, 42, 479, 330],
                "image": "data\\images\\2399639.jpg"
            },
            {
                "VG_image_id": "2338430",
                "VG_object_id": "2845632",
                "bbox": [93, 22, 263, 229],
                "image": "data\\images\\2338430.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many elephants are there", 2],
            ["what is in front of the elephants", 1]
        ],
        "org_questions": [
            ["How many elephants are there", 2],
            ["What is in the background", -1],
            ["where is the nose of the elephant", -1],
            ["what are the elephants doing", -1],
            ["what color is the elephant", -1],
            ["what is in front of the elephants", 1],
            ["where is the elephant", -1],
            ["what animal is in the picture", -1],
            ["when was this picture taken", -1],
            ["where was this picture taken", -1],
            ["what is on the elephant", -1],
            ["what kind of animal is this", -1]
        ],
        "context": [
            "a woman standing next to a group of elephants.",
            "a large elephant standing next to a building."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2343004",
                "VG_object_id": "928455",
                "bbox": [148, 51, 268, 238],
                "image": "data\\images\\2343004.jpg"
            },
            {
                "VG_image_id": "2373239",
                "VG_object_id": "1715810",
                "bbox": [199, 125, 284, 285],
                "image": "data\\images\\2373239.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 1],
            ["what is in the background", 1],
            ["what is the man wearing", 1],
            ["what is on the person's face", 1],
            ["what is on the man's head", 1],
            ["what is on the man's hands", 1],
            ["what is the man wearing on head", 1]
        ],
        "org_questions": [
            ["what color is the background", -1],
            ["how many people are there", -1],
            ["what is the man holding", 1],
            ["how is the weather", -1],
            ["what is in the background", 1],
            ["what is the man doing", -1],
            ["what is the man wearing", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the person's face", 1],
            ["what is on the man's head", 1],
            ["what is on the man's hands", 1],
            ["where is the man", -1],
            ["what is the man wearing on head", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a skier in the air doing a trick in the air.",
            "a man is holding onto a rope while riding a board."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2384736",
                "VG_object_id": "1305700",
                "bbox": [336, 161, 470, 372],
                "image": "data\\images\\2384736.jpg"
            },
            {
                "VG_image_id": "2408098",
                "VG_object_id": "268277",
                "bbox": [128, 182, 246, 374],
                "image": "data\\images\\2408098.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is woman's shirt", 1],
            ["What color is woman's hair", 1],
            ["Where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["What color is woman's shirt", 1],
            ["What color is woman's hair", 1],
            ["Where is the woman", 1],
            ["How many people are there", -1],
            ["What is woman holding", -1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["what is the woman holding", -1],
            ["what is the gender of the person in the photo", -1],
            ["what are the people wearing", -1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a family sitting on a couch playing a video game.",
            "a restaurant with a menu on the wall."
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2402655",
                "VG_object_id": "1133133",
                "bbox": [0, 6, 499, 326],
                "image": "data\\images\\2402655.jpg"
            },
            {
                "VG_image_id": "2348892",
                "VG_object_id": "878965",
                "bbox": [4, 126, 350, 318],
                "image": "data\\images\\2348892.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the boat", 2],
            ["what color is the boat", 1],
            ["what is the boat doing", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the boat", 1],
            ["what is on the boat", 2],
            ["what is the boat doing", 1],
            ["how many people are there", 1],
            ["where is the boat", -1],
            ["what time is it", -1],
            ["what is the boat on", -1],
            ["what is in the distance", -1],
            ["when was this picture taken", -1],
            ["what kind of boat is this", -1],
            ["where was the photo taken", -1],
            ["what is the boat in", -1]
        ],
        "context": [
            "a boat with a large water pipe on the front.",
            "a man standing in a boat filled with bags of food."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2381620",
                "VG_object_id": "703155",
                "bbox": [21, 64, 143, 206],
                "image": "data\\images\\2381620.jpg"
            },
            {
                "VG_image_id": "2408446",
                "VG_object_id": "261635",
                "bbox": [203, 123, 407, 299],
                "image": "data\\images\\2408446.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what is the pattern of the man's shirt", 1],
            ["what is in front of the man", 1],
            ["what sport is the man doing", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the pattern of the man's shirt", 1],
            ["what is in front of the man", 1],
            ["How many people are there", -1],
            ["what gender is the person in the shirt", -1],
            ["what sport is the man doing", 1],
            ["what is the man doing", 1],
            ["who is in the photo", -1],
            ["where is the man", -1],
            ["what is the man wearing", -1],
            ["what kind of shirt is the man wearing", -1]
        ],
        "context": [
            "a man standing in a living room playing a video game.",
            "a man sitting at a table with a cake with candles on it."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2412132",
                "VG_object_id": "202200",
                "bbox": [71, 174, 183, 407],
                "image": "data\\images\\2412132.jpg"
            },
            {
                "VG_image_id": "2375046",
                "VG_object_id": "723927",
                "bbox": [51, 262, 148, 387],
                "image": "data\\images\\2375046.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["What is man holding", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["who is wearing the trousers", -1],
            ["where is the trousers", -1],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["when is the picture taken", -1],
            ["What is man holding", 1],
            ["what is the persion wearing", -1],
            ["what is on the ground", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "two women walking down the street in the sun",
            "a man is standing near a pile of bananas."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2333306",
                "VG_object_id": "2872367",
                "bbox": [356, 47, 491, 294],
                "image": "data\\images\\2333306.jpg"
            },
            {
                "VG_image_id": "2336740",
                "VG_object_id": "3504434",
                "bbox": [120, 41, 324, 336],
                "image": "data\\images\\2336740.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is man's shirt", 1],
            ["what is the man holding", 1],
            ["what sport is the man doing", 1],
            ["what is the man riding on", 1],
            ["what is on the man's head", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is man's shirt", 1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the man wearing around his neck", -1],
            ["what sport is the man doing", 1],
            ["when was the picture taken", -1],
            ["what is the man riding on", 1],
            ["what is on the man's head", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man doing a trick on a skateboard.",
            "a police officer riding a motorcycle down a street."
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2341652",
                "VG_object_id": "940876",
                "bbox": [47, 249, 316, 381],
                "image": "data\\images\\2341652.jpg"
            },
            {
                "VG_image_id": "2326119",
                "VG_object_id": "2746883",
                "bbox": [69, 6, 426, 329],
                "image": "data\\images\\2326119.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the motorcycle", 2],
            ["what color is the motorcycle", 1],
            ["what is in the background", 1],
            ["what is the ground covered with", 1],
            ["where is the motorcycle", 1],
            ["what is next to the motorcycle", 1]
        ],
        "org_questions": [
            ["what color is the motorcycle", 1],
            ["how many people are there on the motorcycle", 2],
            ["what is in the background", 1],
            ["where are the motor ", -1],
            ["what is the ground covered with", 1],
            ["how many motorcycles are there on the ground", -1],
            ["where is the motorcycle", 1],
            ["what is the weather like", -1],
            ["what is parked on the ground", -1],
            ["what is next to the motorcycle", 1]
        ],
        "context": [
            "a man on a motorcycle with a dog on the back.",
            "a motorcycle parked in front of a house."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2327992",
                "VG_object_id": "3503538",
                "bbox": [2, 378, 218, 498],
                "image": "data\\images\\2327992.jpg"
            },
            {
                "VG_image_id": "2403373",
                "VG_object_id": "382055",
                "bbox": [9, 378, 233, 499],
                "image": "data\\images\\2403373.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 2],
            ["what color is the wall", 2],
            ["what is the floor color", 1]
        ],
        "org_questions": [
            ["what color is the floor", 2],
            ["what color is the wall", 2],
            ["how many tables are on the floor", -1],
            ["where is the picture taken", -1],
            ["what is the floor made of", -1],
            ["what is standing on the floor", -1],
            ["what room is this", -1],
            ["what is covering the floor", -1],
            ["where are the tiles", -1],
            ["what is the floor color", 1],
            ["what is on the floor", -1]
        ],
        "context": [
            "a bathroom with a blue door and toilet paper on the floor.",
            "a bathroom with a bathtub, toilet and bathtub."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2411525",
                "VG_object_id": "358854",
                "bbox": [238, 251, 291, 350],
                "image": "data\\images\\2411525.jpg"
            },
            {
                "VG_image_id": "2372468",
                "VG_object_id": "2079352",
                "bbox": [219, 292, 477, 420],
                "image": "data\\images\\2372468.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is around the man's neck", 2],
            ["what shape is the collar of the shirt", 1],
            ["what color is the coat upon the shirt", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what type of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what shape is the collar of the shirt", 1],
            ["what color is the coat upon the shirt", 1],
            ["what is the man in the shirt wearing", -1],
            ["how many people are there", -1],
            ["who is wearing the shirt", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is around the man's neck", 2],
            ["what is on the man's face", -1],
            ["what type of shirt is the man wearing", 1],
            ["what is on the man's shirt", -1]
        ],
        "context": [
            "a man sitting at a table with a laptop and a bag.",
            "a man in a suit and tie holding a cell phone."
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2384390",
                "VG_object_id": "1309982",
                "bbox": [57, 119, 434, 299],
                "image": "data\\images\\2384390.jpg"
            },
            {
                "VG_image_id": "2393753",
                "VG_object_id": "1217218",
                "bbox": [223, 169, 453, 254],
                "image": "data\\images\\2393753.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the water", 1],
            ["what kind of boat is it", 1],
            ["how many people are there on the boat", 1],
            ["what is the boat doing", 1],
            ["what is behind the boats", 1]
        ],
        "org_questions": [
            ["what color is the water", 1],
            ["what kind of boat is it", 1],
            ["how many people are there on the boat", 1],
            ["what time is it", -1],
            ["where is the boat", -1],
            ["what is the boat doing", 1],
            ["what is in the background", -1],
            ["what is the boat on", -1],
            ["what is on the water", -1],
            ["where was this photo taken", -1],
            ["what color are the trees", -1],
            ["what is behind the boats", 1]
        ],
        "context": [
            "a boat with a flag on the front and a blue canopy on the side.",
            "a couple of people on a boat in the water."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2371051",
                "VG_object_id": "2504840",
                "bbox": [372, 161, 443, 263],
                "image": "data\\images\\2371051.jpg"
            },
            {
                "VG_image_id": "2371753",
                "VG_object_id": "2105449",
                "bbox": [228, 114, 352, 224],
                "image": "data\\images\\2371753.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is person's shirt", 2],
            ["What is person doing", 2],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["what is the person wearing", 1],
            ["what is the persion standing on", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["What color is person's shirt", 2],
            ["Where is the person", -1],
            ["What is person doing", 2],
            ["how many people are there", 1],
            ["what is the weather like", -1],
            ["what is the person holding", 1],
            ["what is the person wearing", 1],
            ["where is the picture taken", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion standing on", 1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a boat carrying produce to the shore.",
            "a man riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2389191",
                "VG_object_id": "1263223",
                "bbox": [101, 156, 187, 272],
                "image": "data\\images\\2389191.jpg"
            },
            {
                "VG_image_id": "2337414",
                "VG_object_id": "957809",
                "bbox": [279, 124, 359, 169],
                "image": "data\\images\\2337414.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person doing", 2],
            ["what gender is the person", 1],
            ["where is the photo taken", 1],
            ["who is wearing the shirt", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what gender is the person", 1],
            ["what is the person doing", 2],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["who is wearing the shirt", 1],
            ["what is the persion holding", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a woman playing a video game in a living room.",
            "a man laying in bed with two dogs."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2382131",
                "VG_object_id": "699088",
                "bbox": [143, 104, 229, 190],
                "image": "data\\images\\2382131.jpg"
            },
            {
                "VG_image_id": "2368796",
                "VG_object_id": "744986",
                "bbox": [273, 116, 363, 196],
                "image": "data\\images\\2368796.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["What sports is man doing", 2],
            ["What is man holding", 1],
            ["how many people are there", 1],
            ["what sport is it", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", 2],
            ["What is man holding", 1],
            ["What sports is man doing", 2],
            ["how many people are there", 1],
            ["what is the weather like", -1],
            ["What is on the man's head", -1],
            ["what sport is it", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man holding a baseball bat on a field.",
            "a man swinging a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2343148",
                "VG_object_id": "2097621",
                "bbox": [89, 94, 469, 318],
                "image": "data\\images\\2343148.jpg"
            },
            {
                "VG_image_id": "2397057",
                "VG_object_id": "1193071",
                "bbox": [136, 126, 413, 276],
                "image": "data\\images\\2397057.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the truck", 2],
            ["what is on the truck", 2],
            ["what color is the sky", 2],
            ["what direction is the truck facing to", 1],
            ["how many people are there", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what color is the truck", 2],
            ["what is on the truck", 2],
            ["what direction is the truck facing to", 1],
            ["how many people are there", 1],
            ["when is the picture taken", -1],
            ["how is the weather", 1],
            ["where is the picture taken", -1],
            ["what is the truck doing", -1],
            ["what kind of vehicle is this", -1],
            ["what is the truck parked on", -1],
            ["what is behind the truck", -1],
            ["what type of truck is shown", -1],
            ["what color is the sky", 2]
        ],
        "context": [
            "a large white truck driving down a street.",
            "a green truck driving down a street next to a tree."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2403032",
                "VG_object_id": "1128634",
                "bbox": [3, 164, 124, 259],
                "image": "data\\images\\2403032.jpg"
            },
            {
                "VG_image_id": "2324830",
                "VG_object_id": "3035074",
                "bbox": [345, 130, 494, 300],
                "image": "data\\images\\2324830.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many decks does the bus have", 2],
            ["what color is the bus", 1],
            ["How many people are there", 1],
            ["what time is it", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the bus", 1],
            ["what is in front of the bus", -1],
            ["How many people are there", 1],
            ["what time is it", 1],
            ["Where is the bus", -1],
            ["how many decks does the bus have", 2],
            ["what kind of vehicle is this", -1],
            ["what is the bus doing", -1],
            ["when was the photo taken", 1],
            ["what kind of bus is this", -1]
        ],
        "context": [
            "a double decker bus is stuck in traffic in a busy street.",
            "people walking on the sidewalk"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2362489",
                "VG_object_id": "2605426",
                "bbox": [251, 135, 356, 214],
                "image": "data\\images\\2362489.jpg"
            },
            {
                "VG_image_id": "2410534",
                "VG_object_id": "215183",
                "bbox": [290, 88, 383, 187],
                "image": "data\\images\\2410534.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on his head", 2],
            ["What color is the shirt", 1],
            ["How many people are there", 1],
            ["What sports is man doing", 1],
            ["what is the persion holding", 1],
            ["what is the posture of the person in the shirt", 1],
            ["what kind of pants is the man wearing", 1]
        ],
        "org_questions": [
            ["What color is the shirt", 1],
            ["How many people are there", 1],
            ["What sports is man doing", 1],
            ["what gender is the person in the shirt", -1],
            ["what is the land made of ", -1],
            ["what is the persion holding", 1],
            ["what is the posture of the person in the shirt", 1],
            ["when was this photo taken", -1],
            ["what kind of pants is the man wearing", 1],
            ["what is the man wearing on his head", 2]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a man in a plaid shirt and khaki shorts playing frisbee."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2375027",
                "VG_object_id": "3695569",
                "bbox": [226, 88, 498, 371],
                "image": "data\\images\\2375027.jpg"
            },
            {
                "VG_image_id": "2349610",
                "VG_object_id": "2636731",
                "bbox": [146, 98, 219, 207],
                "image": "data\\images\\2349610.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the woman's face", 2],
            ["what is the color of the woman's shirt", 1],
            ["what is the person holding", 1],
            ["where is the woman", 1],
            ["what is the woman looking at", 1]
        ],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what is the color of the woman's shirt", 1],
            ["how many people are there", -1],
            ["what is the woman wearing on the head", -1],
            ["what is the person holding", 1],
            ["when was this photo taken", -1],
            ["where is the woman", 1],
            ["who is in the photo", -1],
            ["what is on the woman's face", 2],
            ["what is the woman looking at", 1]
        ],
        "context": [
            "a woman is feeding a giraffe.",
            "a market with a variety of fruits and vegetables."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2382936",
                "VG_object_id": "1324899",
                "bbox": [167, 16, 432, 165],
                "image": "data\\images\\2382936.jpg"
            },
            {
                "VG_image_id": "2409385",
                "VG_object_id": "242868",
                "bbox": [318, 212, 378, 330],
                "image": "data\\images\\2409385.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bag", 2],
            ["what is the bag made of", 2],
            ["what is on the ground", 1],
            ["how many people are there", 1],
            ["how is the weather", 1],
            ["what is the bag on", 1],
            ["when was the picture taken", 1],
            ["where is the black bag", 1]
        ],
        "org_questions": [
            ["where is the bag", 2],
            ["what is on the ground", 1],
            ["what is the bag made of", 2],
            ["how many people are there", 1],
            ["how is the weather", 1],
            ["what color is the bag", -1],
            ["what is the bag on", 1],
            ["when was the picture taken", 1],
            ["where is the picture taken", -1],
            ["what color is the bag on the right", -1],
            ["where is the black bag", 1]
        ],
        "context": [
            "a pug dog sitting next to a trash can.",
            "a woman standing on a street corner at night."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2327599",
                "VG_object_id": "2934429",
                "bbox": [223, 90, 330, 465],
                "image": "data\\images\\2327599.jpg"
            },
            {
                "VG_image_id": "2350090",
                "VG_object_id": "2120927",
                "bbox": [70, 41, 271, 498],
                "image": "data\\images\\2350090.jpg"
            }
        ],
        "questions_with_scores": [["what color is the building", 1]],
        "org_questions": [
            ["where is the clock", -1],
            ["what color is the building", 1],
            ["how many clocks are on the tower", -1],
            ["what is on the top of the tower", -1],
            ["what is the tower made of", -1],
            ["what is the weather like", -1],
            ["where is the picture taken", -1],
            ["what time is it", -1],
            ["what is the shape of the clock", -1],
            ["what is behind the clock", -1],
            ["when was the photo taken", -1],
            ["what is in front of the clock", -1]
        ],
        "context": [
            "a clock tower on a sidewalk in a city.",
            "a large brick building with a clock on the front."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2381354",
                "VG_object_id": "1337261",
                "bbox": [226, 117, 499, 198],
                "image": "data\\images\\2381354.jpg"
            },
            {
                "VG_image_id": "713930",
                "VG_object_id": "2449465",
                "bbox": [32, 127, 595, 358],
                "image": "data\\images\\713930.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the front of the picture", 1],
            ["what color is the building", 1],
            ["what is in the background", 1],
            ["How many people are there", 1],
            ["how many floors does the building have", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is in the front of the picture", 1],
            ["what color is the building", 1],
            ["what is in the background", 1],
            ["How many people are there", 1],
            ["Where is the building", -1],
            ["what is the building made of", -1],
            ["how many floors does the building have", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["where was the photo taken", 1],
            ["what color are the trees", -1]
        ],
        "context": [
            "a man walking in the snow with skis.",
            "a group of people on motorcycles in a parking lot."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316585",
                "VG_object_id": "1052866",
                "bbox": [193, 17, 473, 370],
                "image": "data\\images\\2316585.jpg"
            },
            {
                "VG_image_id": "2347049",
                "VG_object_id": "1871669",
                "bbox": [253, 69, 366, 201],
                "image": "data\\images\\2347049.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["how is the weather", -1],
            ["what is the man on", 1],
            ["when was the photo taken", -1],
            ["what is on the man's head", 2],
            ["who is in the photo", -1],
            ["what kind of shirt is the man wearing", -1]
        ],
        "context": [
            "a man walking past a store filled with luggage.",
            "a man riding a skateboard on top of a wall."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2318262",
                "VG_object_id": "1012494",
                "bbox": [0, 51, 500, 373],
                "image": "data\\images\\2318262.jpg"
            },
            {
                "VG_image_id": "2355966",
                "VG_object_id": "2056878",
                "bbox": [0, 1, 400, 331],
                "image": "data\\images\\2355966.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many forks are there in the picture", 2],
            ["what  color is the table", 1],
            ["how many plates are on the table", 1]
        ],
        "org_questions": [
            ["what  color is the table", 1],
            ["how many plates are on the table", 1],
            ["how many forks are there in the picture", 2],
            ["what is on the plate", -1],
            ["what is the table made of", -1],
            ["how many people are there", -1],
            ["what is on the table", -1],
            ["where was the photo taken", -1],
            ["where is the plate", -1]
        ],
        "context": [
            "two plates with desserts on them on a table.",
            "a person sitting at a table with a plate of chocolate ice cream."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2389556",
                "VG_object_id": "1259548",
                "bbox": [84, 200, 251, 477],
                "image": "data\\images\\2389556.jpg"
            },
            {
                "VG_image_id": "2412512",
                "VG_object_id": "192801",
                "bbox": [256, 37, 395, 320],
                "image": "data\\images\\2412512.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is the man wearing", 2],
            ["How long is man's trouser", 1],
            ["What sports is man doing", 1],
            ["where is the photo taken", 1],
            ["what is in the man's hand", 1],
            ["what is the man playing", 1]
        ],
        "org_questions": [
            ["How long is man's trouser", 1],
            ["What sports is man doing", 1],
            ["where is the photo taken", 1],
            ["What color is the ground", -1],
            ["what is the man wearing on his face", -1],
            ["how many people are there in the picture", 2],
            ["what is the man wearing", 2],
            ["what is the man standing on", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is in the man's hand", 1],
            ["what is the man playing", 1]
        ],
        "context": [
            "a man swinging a baseball bat in a park.",
            "a man playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2375587",
                "VG_object_id": "1906142",
                "bbox": [162, 285, 275, 479],
                "image": "data\\images\\2375587.jpg"
            },
            {
                "VG_image_id": "2363534",
                "VG_object_id": "764147",
                "bbox": [0, 0, 269, 374],
                "image": "data\\images\\2363534.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's shirt", 2],
            ["what is the gender of the child", 2],
            ["what are the child playing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", 2],
            ["what is the gender of the child", 2],
            ["what color are the clothes of the man beside the child", -1],
            ["Where is child", -1],
            ["what are the child playing", 1],
            ["what color is the background", -1],
            ["how many people are there", -1],
            ["who is in the photo", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man and a boy playing a video game.",
            "a boy and a girl sitting on a couch looking at a cell phone."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2379349",
                "VG_object_id": "553489",
                "bbox": [6, 69, 330, 211],
                "image": "data\\images\\2379349.jpg"
            },
            {
                "VG_image_id": "2357503",
                "VG_object_id": "811619",
                "bbox": [54, 3, 283, 191],
                "image": "data\\images\\2357503.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the umbrella", 1],
            ["what is under the umbrella", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 1],
            ["what is under the umbrella", 1],
            ["where is the umbrella", -1],
            ["How many umbrellas are there", -1],
            ["what time is it", -1],
            ["what is the pattern on the umbrella", -1],
            ["what is the weather like", -1],
            ["what is behind the umbrella", -1],
            ["what is the umbrella made of", -1],
            ["when was the picture taken", -1],
            ["what is on the umbrella", -1],
            ["how is the weather", -1]
        ],
        "context": [
            "a table with a umbrella and chairs outside",
            "a man in a colorful outfit holding an umbrella."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2367208",
                "VG_object_id": "3157026",
                "bbox": [302, 240, 403, 332],
                "image": "data\\images\\2367208.jpg"
            },
            {
                "VG_image_id": "2356467",
                "VG_object_id": "820264",
                "bbox": [230, 156, 374, 252],
                "image": "data\\images\\2356467.jpg"
            }
        ],
        "questions_with_scores": [["What color is man's clothes", 1]],
        "org_questions": [
            ["What color is man's trouser", -1],
            ["What color is man's clothes", 1],
            ["how many people are there", -1],
            ["what is the person standing on", -1],
            ["where is the man", -1],
            ["what are the people doing", -1],
            ["what is on the batter's head", -1],
            ["what sport is being played", -1],
            ["what is the man wearing", -1],
            ["what color is the grass", -1],
            ["what is the batter doing", -1]
        ],
        "context": [
            "a baseball player is swinging at a pitch.",
            "a baseball player holding a bat"
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "150364",
                "VG_object_id": "1074466",
                "bbox": [742, 231, 1018, 513],
                "image": "data\\images\\150364.jpg"
            },
            {
                "VG_image_id": "2413401",
                "VG_object_id": "174275",
                "bbox": [169, 101, 330, 272],
                "image": "data\\images\\2413401.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bus", 2],
            ["what is the number on the bus", 1]
        ],
        "org_questions": [
            ["what color is the bus", 2],
            ["how many bus are there", -1],
            ["when is the picture taken", -1],
            ["what is the ground covered with", -1],
            ["what is in the distance", -1],
            ["how many people are there", -1],
            ["who is driving the bus", -1],
            ["what is the bus doing", -1],
            ["what type of vehicle is this", -1],
            ["what is on the front of the bus", -1],
            ["what is the number on the bus", 1]
        ],
        "context": [
            "a tow truck parked next to a double decker bus.",
            "a bus driving down a street next to a mcdonalds."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2405725",
                "VG_object_id": "372336",
                "bbox": [264, 218, 374, 371],
                "image": "data\\images\\2405725.jpg"
            },
            {
                "VG_image_id": "2369245",
                "VG_object_id": "612845",
                "bbox": [271, 62, 387, 157],
                "image": "data\\images\\2369245.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 2],
            ["what is on the man's head", 2],
            ["What color is the ground", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 2],
            ["What color is the ground", 1],
            ["what sport is the man doing", -1],
            ["where is the photo taken", -1],
            ["what is the gender of the person", -1],
            ["what is the man doing", -1],
            ["where is the person", -1],
            ["who is in the picture", -1],
            ["what is on the man's head", 2],
            ["when was the photo taken", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man holding a tennis racquet on a tennis court.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2351629",
                "VG_object_id": "859369",
                "bbox": [79, 189, 140, 262],
                "image": "data\\images\\2351629.jpg"
            },
            {
                "VG_image_id": "2376160",
                "VG_object_id": "720598",
                "bbox": [192, 302, 243, 372],
                "image": "data\\images\\2376160.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bag", 1],
            ["what is the man doing", 1],
            ["where is the picture taken", 1],
            ["what is in front of the bag", 1],
            ["what is the man on the left wearing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the bag", -1],
            ["where is the bag", 1],
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["where is the picture taken", 1],
            ["what is in front of the bag", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the man on the left wearing", 1],
            ["what is the man holding", 1],
            ["where was the photo taken", -1],
            ["how many people are in the picture", -1],
            ["what is the bag made of", -1]
        ],
        "context": [
            "a man and a woman riding a motorcycle down a street.",
            "a man is walking with an elephant on a street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2340456",
                "VG_object_id": "2744977",
                "bbox": [124, 54, 256, 292],
                "image": "data\\images\\2340456.jpg"
            },
            {
                "VG_image_id": "2354990",
                "VG_object_id": "2540258",
                "bbox": [32, 143, 176, 488],
                "image": "data\\images\\2354990.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["who is smiling", 1],
            ["what is on the man's face", 1],
            ["what is the persion holding", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", -1],
            ["what is the gesture of the man", -1],
            ["what is the man wearing", -1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what color is the hair", -1],
            ["who is smiling", 1],
            ["what is on the man's face", 1],
            ["what is the persion holding", 1],
            ["what is the man holding", 1],
            ["HOw many people are there", -1],
            ["What is the man wearing on his head", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man and woman standing next to a table with a cake.",
            "a young man carrying a basket of bananas"
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2359128",
                "VG_object_id": "2395273",
                "bbox": [121, 2, 498, 100],
                "image": "data\\images\\2359128.jpg"
            },
            {
                "VG_image_id": "2372349",
                "VG_object_id": "736766",
                "bbox": [1, 2, 496, 165],
                "image": "data\\images\\2372349.jpg"
            }
        ],
        "questions_with_scores": [
            ["where are the bikes", 2],
            ["how many people are there", 1],
            ["what is in front of the building", 1]
        ],
        "org_questions": [
            ["where are the bikes", 2],
            ["how many people are there", 1],
            ["what shape is the roof of the building", -1],
            ["what time is it", -1],
            ["how is the weather", -1],
            ["what is in the background", -1],
            ["what is the building made of", -1],
            ["when was this picture taken", -1],
            ["where was this picture taken", -1],
            ["what is on the building", -1],
            ["what is in front of the building", 1]
        ],
        "context": [
            "a car with bicycles on top of it",
            "a group of people riding bicycles on a street."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2319632",
                "VG_object_id": "3285897",
                "bbox": [232, 137, 369, 221],
                "image": "data\\images\\2319632.jpg"
            },
            {
                "VG_image_id": "2399876",
                "VG_object_id": "1165202",
                "bbox": [207, 49, 336, 112],
                "image": "data\\images\\2399876.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the background", 2],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["what is the man wearing", 1],
            ["what is on the man's face", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["what color is the background", 2],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the man wearing", 1],
            ["who is in the photo", -1],
            ["what is on the man's face", 1],
            ["when was the photo taken", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a person on a snowboard on a snowy mountain.",
            "a man is doing a trick on a dirt bike."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2369225",
                "VG_object_id": "743919",
                "bbox": [257, 38, 465, 366],
                "image": "data\\images\\2369225.jpg"
            },
            {
                "VG_image_id": "2366281",
                "VG_object_id": "1908763",
                "bbox": [323, 124, 499, 340],
                "image": "data\\images\\2366281.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on his neck", 2],
            ["how many people are there", 2],
            ["what is the man's posture", 1],
            ["Where is the man", 1],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", -1],
            ["what is the man wearing on his neck", 2],
            ["what is the man's posture", 1],
            ["how many people are there", 2],
            ["Where is the man", 1],
            ["what is the man doing", 1],
            ["what is the gender of the person in the photo", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man wearing a bow tie and a bow tie.",
            "a group of people sitting around a living room playing a video game."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2345977",
                "VG_object_id": "2453515",
                "bbox": [0, 94, 324, 427],
                "image": "data\\images\\2345977.jpg"
            },
            {
                "VG_image_id": "2375834",
                "VG_object_id": "2626105",
                "bbox": [66, 410, 112, 497],
                "image": "data\\images\\2375834.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the weather like", 1],
            ["where is the woman", 1],
            ["how many people are there in the picture", 1],
            ["What is the woman wearing on her head", 1],
            ["what is behind the woman", 1],
            ["where is the person", 1],
            ["what is the woman wearing", 1],
            ["what is on the woman's face", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", -1],
            ["what is the weather like", 1],
            ["where is the woman", 1],
            ["how many people are there in the picture", 1],
            ["What is the woman wearing on her head", 1],
            ["when is the picture taken", -1],
            ["What is the woman doing", -1],
            ["who is in the photo", -1],
            ["what is behind the woman", 1],
            ["how many women are there", -1],
            ["where is the person", 1],
            ["what is the woman doing", -1],
            ["what is the woman wearing", 1],
            ["when was this photo taken", -1],
            ["what is on the woman's face", 1]
        ],
        "context": [
            "a woman wearing a headdress.",
            "a clock on a brick tower in a city."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2364457",
                "VG_object_id": "3738690",
                "bbox": [10, 270, 363, 487],
                "image": "data\\images\\2364457.jpg"
            },
            {
                "VG_image_id": "2358795",
                "VG_object_id": "2166062",
                "bbox": [1, 10, 498, 279],
                "image": "data\\images\\2358795.jpg"
            }
        ],
        "questions_with_scores": [["how many plates are in the picture", 2]],
        "org_questions": [
            ["how many plates are in the picture", 2],
            ["what is in the background", -1],
            ["what color is the table", -1],
            ["what is the table made of", -1],
            ["what food is on the plate", -1],
            ["what color is the plate on the table", -1],
            ["what shape is the table", -1],
            ["where was this photo taken", -1],
            ["what is the pizza sitting on", -1],
            ["where is the pizza sitting", -1],
            ["what type of table is this", -1]
        ],
        "context": [
            "a little girl sitting at a table with a pizza.",
            "a salad and a salad are on a plate."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2346093",
                "VG_object_id": "902602",
                "bbox": [121, 78, 234, 290],
                "image": "data\\images\\2346093.jpg"
            },
            {
                "VG_image_id": "2369185",
                "VG_object_id": "3864738",
                "bbox": [60, 84, 250, 327],
                "image": "data\\images\\2369185.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the boy;s shirt", 2],
            ["what color is the child's hat", 2],
            ["What color is the boy's shirt", 2],
            ["What is the boy doing ", 1],
            ["What is the boy holding", 1],
            ["what color is the ground", 1],
            ["what color are the boy's clothes", 1],
            ["how many people are there", 1],
            ["what kind of pants is the boy wearing", 1]
        ],
        "org_questions": [
            ["What color is the boy;s shirt", 2],
            ["What is the boy doing ", 1],
            ["What is the boy holding", 1],
            ["where is the person", -1],
            ["what is on the child's head", -1],
            ["what color is the ground", 1],
            ["what color are the boy's clothes", 1],
            ["what color is the child's hat", 2],
            ["how many people are there", 1],
            ["who is wearing a helmet", -1],
            ["when was the picture taken", -1],
            ["what kind of pants is the boy wearing", 1],
            ["What color is the boy's shirt", 2]
        ],
        "context": [
            "a boy is doing a trick on a skateboard.",
            "a young boy swinging a bat at a baseball."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2396464",
                "VG_object_id": "443996",
                "bbox": [93, 20, 256, 499],
                "image": "data\\images\\2396464.jpg"
            },
            {
                "VG_image_id": "2322097",
                "VG_object_id": "2839037",
                "bbox": [1, 86, 237, 499],
                "image": "data\\images\\2322097.jpg"
            }
        ],
        "questions_with_scores": [
            ["how long is the man's hair", 2],
            ["What is man doing", 2],
            ["what color is the man's shirt", 1],
            ["what is the man wearing on his face", 1],
            ["what color is the background", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man wearing on his face", 1],
            ["how long is the man's hair", 2],
            ["where is the man", -1],
            ["how many people are there", -1],
            ["What is man doing", 2],
            ["what color is the background", 1],
            ["who is in the photo", -1],
            ["what is in the man's hand", -1],
            ["what is the persion wearing", 1],
            ["what is the person holding", -1]
        ],
        "context": [
            "a man standing in a living room holding a wii controller.",
            "a man smiling while holding a wii remote."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2417514",
                "VG_object_id": "2968004",
                "bbox": [373, 113, 468, 264],
                "image": "data\\images\\2417514.jpg"
            },
            {
                "VG_image_id": "2354209",
                "VG_object_id": "1883235",
                "bbox": [128, 274, 197, 397],
                "image": "data\\images\\2354209.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the woman", 2],
            ["how many people are there", 2],
            ["what color is the woman's shirt", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["where is the woman", 2],
            ["how many people are there", 2],
            ["What is woman holding", -1],
            ["what is the woman doing", -1],
            ["what is the woman's posture", -1],
            ["when was the photo taken", -1],
            ["what is the person wearing", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a man eating a hot dog at a stadium.",
            "a woman sitting on a bench in front of a window."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2347615",
                "VG_object_id": "1703506",
                "bbox": [52, 90, 250, 325],
                "image": "data\\images\\2347615.jpg"
            },
            {
                "VG_image_id": "2361658",
                "VG_object_id": "2620233",
                "bbox": [9, 9, 421, 331],
                "image": "data\\images\\2361658.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the player's cap", 1],
            ["what color is the player's shirt", 1],
            ["what color are the player's gloves", 1]
        ],
        "org_questions": [
            ["what color is the player's cap", 1],
            ["what color is the player's shirt", 1],
            ["what color are the player's gloves", 1],
            ["how many people are there", -1],
            ["What is the person holding", -1],
            ["what is in the background", -1],
            ["what is the player doing", -1],
            ["What color is the ground", -1],
            ["when was the photo taken", -1],
            ["where are the players", -1],
            ["who is holding the bat", -1],
            ["what sport is being played", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2373037",
                "VG_object_id": "3729767",
                "bbox": [182, 64, 420, 326],
                "image": "data\\images\\2373037.jpg"
            },
            {
                "VG_image_id": "2315847",
                "VG_object_id": "3063521",
                "bbox": [0, 150, 61, 312],
                "image": "data\\images\\2315847.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the man wearing on head", 1],
            ["WHat is man doing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", -1],
            ["How many people are there", -1],
            ["what time is it", -1],
            ["what is the man wearing on head", 1],
            ["where is the photo taken", -1],
            ["WHat is man doing", 1],
            ["who is in the photo", -1],
            ["what is the persion standing on", -1],
            ["when was this picture taken", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man doing a trick on a skateboard at a skate park.",
            "a group of people standing on a sidewalk next to a parking lot."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2361753",
                "VG_object_id": "779706",
                "bbox": [207, 343, 280, 495],
                "image": "data\\images\\2361753.jpg"
            },
            {
                "VG_image_id": "2349239",
                "VG_object_id": "1726275",
                "bbox": [287, 418, 332, 497],
                "image": "data\\images\\2349239.jpg"
            }
        ],
        "questions_with_scores": [["How many people are there", 1]],
        "org_questions": [
            ["Where is the woman", -1],
            ["How many people are there", 1],
            ["what is the woman in the middle holding", -1],
            ["what is the woman wearing", -1],
            ["what is the woman doing", -1],
            ["What is woman holding", -1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a woman standing in front of a church with a clock on the front.",
            "a man on a skateboard jumping over a fence."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2352310",
                "VG_object_id": "1823687",
                "bbox": [295, 143, 440, 331],
                "image": "data\\images\\2352310.jpg"
            },
            {
                "VG_image_id": "2372420",
                "VG_object_id": "3439254",
                "bbox": [318, 27, 493, 250],
                "image": "data\\images\\2372420.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["What is man holding", 1],
            ["where is the photo taken", 1],
            ["where is the person", 1],
            ["what is the persion wearing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is man's shirt", -1],
            ["What is man holding", 1],
            ["where is the photo taken", 1],
            ["who is wearing the shirt", -1],
            ["where is the person", 1],
            ["what is the persion wearing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "context": [
            "a woman sitting on a bench looking at her cell phone.",
            "a man holding a bat while sitting on the floor."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2398133",
                "VG_object_id": "1182497",
                "bbox": [170, 63, 303, 143],
                "image": "data\\images\\2398133.jpg"
            },
            {
                "VG_image_id": "2344016",
                "VG_object_id": "918167",
                "bbox": [160, 163, 219, 209],
                "image": "data\\images\\2344016.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 1],
            ["what is the man doing", 1],
            ["How many people are there", 1],
            ["when was this photo taken", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["What is person doing", 1],
            ["what is in the man's hands", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what is the person holding", 1],
            ["what is the man doing", 1],
            ["How many people are there", 1],
            ["What is the man wearing on his head", -1],
            ["how is the weather", -1],
            ["What is the gender of person", -1],
            ["when was this photo taken", 1],
            ["what is the man wearing", -1],
            ["when was the picture taken", -1],
            ["how many people are there", 1],
            ["what gender is the person in the shirt", -1],
            ["where is the man", 1],
            ["What is person doing", 1],
            ["what is in the man's hands", 1]
        ],
        "context": [
            "a boy doing a trick on a skateboard at a skate park.",
            "a man standing on a sidewalk next to a wall."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2391547",
                "VG_object_id": "1240067",
                "bbox": [2, 186, 500, 329],
                "image": "data\\images\\2391547.jpg"
            },
            {
                "VG_image_id": "2380523",
                "VG_object_id": "545056",
                "bbox": [0, 335, 70, 499],
                "image": "data\\images\\2380523.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is the floor", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["What color is the floor", 1],
            ["How many people are there", 2],
            ["where is the photo taken", 1],
            ["what is on the ground", -1],
            ["what pattern is the floor", -1],
            ["what is the floor made of", -1],
            ["what is the material of the floor", -1],
            ["what is covering the floor", -1]
        ],
        "context": [
            "a pile of luggage sitting on top of a floor.",
            "a woman is pushing a large piece of luggage."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2361240",
                "VG_object_id": "2195575",
                "bbox": [1, 0, 425, 500],
                "image": "data\\images\\2361240.jpg"
            },
            {
                "VG_image_id": "2397708",
                "VG_object_id": "431433",
                "bbox": [276, 68, 452, 245],
                "image": "data\\images\\2397708.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 1],
            ["Where is the man", 1],
            ["how many people are there", 1],
            ["what is the man holding", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 1],
            ["What is the man doing", -1],
            ["Where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his head", -1],
            ["when is the picture taken", -1],
            ["what kind of clothes is the man wearing", -1],
            ["what is the man holding", 1],
            ["who is in the picture", -1],
            ["what is behind the man", 1],
            ["what is on the man's face", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man holding a cow with a face on it.",
            "a man and woman sitting on a bench."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2386100",
                "VG_object_id": "1289457",
                "bbox": [403, 130, 483, 188],
                "image": "data\\images\\2386100.jpg"
            },
            {
                "VG_image_id": "2385995",
                "VG_object_id": "684524",
                "bbox": [439, 79, 500, 192],
                "image": "data\\images\\2385995.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 1],
            ["what is the color of the building", 1],
            ["where is the building", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 1],
            ["how many buildings are there", -1],
            ["what is the color of the building", 1],
            ["what is in the distance", -1],
            ["what is the weather like", -1],
            ["what is the building made of", -1],
            ["when was the picture taken", -1],
            ["what is on the ground", -1],
            ["where is the building", 1],
            ["what color is the sky", -1],
            ["what color are the trees", -1]
        ],
        "context": [
            "a fire truck parked in a parking lot.",
            "a giraffe sitting in a dirt field next to a tree."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2321974",
                "VG_object_id": "2911436",
                "bbox": [0, 107, 498, 374],
                "image": "data\\images\\2321974.jpg"
            },
            {
                "VG_image_id": "2391055",
                "VG_object_id": "1244300",
                "bbox": [4, 344, 360, 499],
                "image": "data\\images\\2391055.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the beach", -1],
            ["what is on the beach", -1],
            ["what is in the distance", -1],
            ["how many rocks are there in the picture", -1],
            ["how many people are there in the picture", 1],
            ["where was this taken", -1],
            ["where is the photo taken", -1],
            ["where is this picture taken", -1]
        ],
        "context": [
            "a man and a child holding a kite on a beach.",
            "a cloudy sky above the ocean"
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2339803",
                "VG_object_id": "2734822",
                "bbox": [346, 130, 409, 193],
                "image": "data\\images\\2339803.jpg"
            },
            {
                "VG_image_id": "2407622",
                "VG_object_id": "276482",
                "bbox": [313, 274, 396, 382],
                "image": "data\\images\\2407622.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the dog sitting on", 1],
            ["what color are the dog's ears", 1],
            ["where is the dog", 1],
            ["What color is the background", 1]
        ],
        "org_questions": [
            ["what is the dog sitting on", 1],
            ["what color are the dog's ears", 1],
            ["what is the dog doing ", -1],
            ["where is the dog", 1],
            ["What color is the background", 1],
            ["what animal is in the picture", -1],
            ["how many dogs are there", -1],
            ["when was the picture taken", -1],
            ["what is in the dog's mouth", -1]
        ],
        "context": [
            "a dog looking out a car window with its head out the window.",
            "a man is standing next to a dog on a leash."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "498341",
                "VG_object_id": "3595615",
                "bbox": [253, 41, 572, 958],
                "image": "data\\images\\498341.jpg"
            },
            {
                "VG_image_id": "2359741",
                "VG_object_id": "3257495",
                "bbox": [60, 73, 313, 470],
                "image": "data\\images\\2359741.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's coat", 2],
            ["what is the person doing", 1],
            ["What is the gender of the person", 1],
            ["What is person doing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the person's coat", 2],
            ["what is the person doing", 1],
            ["what is on the land", -1],
            ["how many people are there", -1],
            ["where is the person", -1],
            ["What is the gender of the person", 1],
            ["what is the persion wearing", -1],
            ["What is person doing", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", 1],
            ["what is the persion holding", -1]
        ],
        "context": [
            "a woman riding a snowboard down a snow covered slope.",
            "a man and a dog in the snow."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2373985",
                "VG_object_id": "3723228",
                "bbox": [337, 130, 498, 373],
                "image": "data\\images\\2373985.jpg"
            },
            {
                "VG_image_id": "2390218",
                "VG_object_id": "1252814",
                "bbox": [106, 203, 386, 301],
                "image": "data\\images\\2390218.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the cabinet", 1],
            ["where is the cabinet", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is on the cabinet", 1],
            ["where is the cabinet", 1],
            ["what is hanging on the cabinet", -1],
            ["what color is the wall behind the cabinet", -1],
            ["what color is the wall", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "two young girls playing a video game in a living room.",
            "a bathroom with a sink, mirror, and a large mirror."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2373659",
                "VG_object_id": "731299",
                "bbox": [4, 85, 487, 327],
                "image": "data\\images\\2373659.jpg"
            },
            {
                "VG_image_id": "2404934",
                "VG_object_id": "375258",
                "bbox": [1, 217, 475, 331],
                "image": "data\\images\\2404934.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bus", 1],
            ["how many people are there on the street", 1]
        ],
        "org_questions": [
            ["what color is the bus", 1],
            ["what color is the floor", -1],
            ["when is the picture taken", -1],
            ["how many buses are there on the street", -1],
            ["what is in the background", -1],
            ["how many people are there on the street", 1],
            ["what is the road made of", -1],
            ["how is the weather", -1],
            ["what is on the ground", -1],
            ["where was this photo taken", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a man is unloading a van from a van.",
            "a red double decker bus parked on the side of the road."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2337751",
                "VG_object_id": "3316772",
                "bbox": [49, 193, 272, 305],
                "image": "data\\images\\2337751.jpg"
            },
            {
                "VG_image_id": "2347108",
                "VG_object_id": "1673288",
                "bbox": [5, 6, 213, 172],
                "image": "data\\images\\2347108.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the umbrella", 1],
            ["How many people are there", 1],
            ["what pattern is on the umbrella", 1],
            ["what is the umbrella on", 1],
            ["What is the pattern of umbrella", 1],
            ["where was this photo taken", 1],
            ["Where is the umbrella", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", -1],
            ["where is the umbrella", 1],
            ["what color is the ground", -1],
            ["How many people are there", 1],
            ["what pattern is on the umbrella", 1],
            ["what is the umbrella on", 1],
            ["What is the pattern of umbrella", 1],
            ["How many umbrellas are there", -1],
            ["when was this photo taken", -1],
            ["what is in the background", -1],
            ["what is the weather like", -1],
            ["where was this photo taken", 1],
            ["When is photo taken", -1],
            ["Where is the umbrella", 1],
            ["how is the weather", -1]
        ],
        "context": [
            "a black umbrella sitting on the side of a road.",
            "a man standing on a bridge holding an umbrella."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2324248",
                "VG_object_id": "3235327",
                "bbox": [261, 119, 399, 200],
                "image": "data\\images\\2324248.jpg"
            },
            {
                "VG_image_id": "2396821",
                "VG_object_id": "440275",
                "bbox": [114, 189, 171, 337],
                "image": "data\\images\\2396821.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the person's clothes", 2],
            ["What is the background of image", 2],
            ["where is the person", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What color is the person's clothes", 2],
            ["What is the background of image", 2],
            ["what is on the person's head", -1],
            ["where is the person", 1],
            ["What is the man doing", -1],
            ["what is the person holding", 1],
            ["what is the gender of the person", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a man skiing down a snowy hill on a sunny day.",
            "a man riding a horse in the snow."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2341801",
                "VG_object_id": "2290956",
                "bbox": [45, 53, 280, 363],
                "image": "data\\images\\2341801.jpg"
            },
            {
                "VG_image_id": "2364745",
                "VG_object_id": "1914459",
                "bbox": [43, 135, 233, 360],
                "image": "data\\images\\2364745.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bear", 2],
            ["what is the bear wearing", 1],
            ["how many bears are there in the picture", 1],
            ["where are the teddy bears", 1],
            ["what is behind the bear", 1],
            ["what is on the bear's head", 1],
            ["where was the photo taken", 1],
            ["where is the bear", 1]
        ],
        "org_questions": [
            ["what color is the bear", 2],
            ["what is the bear wearing", 1],
            ["how many bears are there in the picture", 1],
            ["where are the teddy bears", 1],
            ["what is behind the bear", 1],
            ["what is on the bear's head", 1],
            ["where was the photo taken", 1],
            ["where is the bear", 1]
        ],
        "context": [
            "a bunch of stuffed animals sitting on top of a table.",
            "a teddy bear sitting on a rail of a train."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2318509",
                "VG_object_id": "3028110",
                "bbox": [0, 278, 319, 496],
                "image": "data\\images\\2318509.jpg"
            },
            {
                "VG_image_id": "2380215",
                "VG_object_id": "1349326",
                "bbox": [0, 45, 131, 249],
                "image": "data\\images\\2380215.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the photo", 2],
            ["what color is the blanket", 1],
            ["what is in the background", 1],
            ["where is the blanket", 1],
            ["who is in the picture", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the blanket", 1],
            ["what is in the background", 1],
            ["where is the blanket", 1],
            ["how many people are in the photo", 2],
            ["who is in the picture", 1],
            ["how is the photo", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a man laying on a beach under an umbrella.",
            "a teddy bear laying in a bed with a blanket."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2350677",
                "VG_object_id": "2694442",
                "bbox": [183, 203, 381, 260],
                "image": "data\\images\\2350677.jpg"
            },
            {
                "VG_image_id": "2356840",
                "VG_object_id": "1834044",
                "bbox": [182, 229, 379, 331],
                "image": "data\\images\\2356840.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["what is blue", 1]
        ],
        "org_questions": [
            ["what is on the table", -1],
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["What is cake on", -1],
            ["how many plates are there", -1],
            ["where is the cake", -1],
            ["what pattern is on the cake", -1],
            ["what is next to the cake", -1],
            ["what is blue", 1]
        ],
        "context": [
            "a woman standing in front of a table with a cake on it.",
            "two girls sitting at a table with a cake."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2380728",
                "VG_object_id": "543938",
                "bbox": [315, 99, 423, 179],
                "image": "data\\images\\2380728.jpg"
            },
            {
                "VG_image_id": "2340588",
                "VG_object_id": "2528372",
                "bbox": [260, 146, 314, 273],
                "image": "data\\images\\2340588.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["where is the man", 2],
            ["What color is man's shirt", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what is the man wearing", -1],
            ["what is the man doing", 2],
            ["where is the man", 2],
            ["how many bikes are there", -1],
            ["What color is man's shirt", 1],
            ["What is on the road", -1],
            ["where is the photo taken", 1],
            ["how many people are there", -1],
            ["who is wearing a black shirt", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man riding a skateboard up the side of a ramp.",
            "a man standing on a platform next to a train."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2360026",
                "VG_object_id": "1861871",
                "bbox": [161, 8, 401, 223],
                "image": "data\\images\\2360026.jpg"
            },
            {
                "VG_image_id": "2339702",
                "VG_object_id": "2300212",
                "bbox": [194, 62, 454, 277],
                "image": "data\\images\\2339702.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what are the people wearing", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what are the people wearing", 1],
            ["what are the people holding", -1],
            ["where is the photo taken", -1],
            ["how is the weather", -1],
            ["who is wearing glasses", -1],
            ["what is the man doing", -1],
            ["when was the picture taken", -1],
            ["where is the surfboard", -1],
            ["what is the man riding", -1],
            ["where is the man surfing", -1]
        ],
        "context": [
            "a man riding a surfboard on top of a wave.",
            "a man riding a wave on top of a surfboard."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2409692",
                "VG_object_id": "236027",
                "bbox": [371, 81, 496, 138],
                "image": "data\\images\\2409692.jpg"
            },
            {
                "VG_image_id": "2346424",
                "VG_object_id": "899617",
                "bbox": [98, 291, 197, 333],
                "image": "data\\images\\2346424.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the pillow", 2],
            ["where is the pillow placed on", 2],
            ["Where is the pillow", 1],
            ["what color is the pillow in the front", 1],
            ["where are the pillows", 1]
        ],
        "org_questions": [
            ["what color is the pillow", 2],
            ["How many pillows are there", -1],
            ["where is the pillow placed on", 2],
            ["Where is the pillow", 1],
            ["what color is the pillow in the front", 1],
            ["where are the pillows", 1]
        ],
        "context": [
            "a woman sitting on a bed in a room.",
            "a living room with a couch, tv and a tv."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2340608",
                "VG_object_id": "2745703",
                "bbox": [146, 177, 345, 289],
                "image": "data\\images\\2340608.jpg"
            },
            {
                "VG_image_id": "2413282",
                "VG_object_id": "298243",
                "bbox": [351, 247, 480, 370],
                "image": "data\\images\\2413282.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["what color is the wall", 1],
            ["where is the photo taken", 1],
            ["What color is the floor", 1]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["what color is the wall", 1],
            ["where is the photo taken", 1],
            ["How many chairs are there", -1],
            ["what is the legs of the chair made of", -1],
            ["What color is the floor", 1],
            ["where are the chairs", -1],
            ["what is in the chair", -1],
            ["what are the chairs made of", -1],
            ["how many chairs", -1]
        ],
        "context": [
            "a table and chairs in a courtyard with a table and vases.",
            "a kitchen with a red refrigerator and a white stove."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2316825",
                "VG_object_id": "3150053",
                "bbox": [43, 50, 268, 286],
                "image": "data\\images\\2316825.jpg"
            },
            {
                "VG_image_id": "2417452",
                "VG_object_id": "3356170",
                "bbox": [148, 34, 393, 238],
                "image": "data\\images\\2417452.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the ground", 2],
            ["what are the people doing", 1],
            ["what is on the person's head", 1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1],
            ["what sport is being played", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["what color is the ground", 2],
            ["what are the people doing", 1],
            ["what is on the person's head", 1],
            ["Where are people", -1],
            ["where is the person", -1],
            ["what color are the person's trousers", -1],
            ["what is the persion doing", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", 1],
            ["what sport is being played", 1],
            ["what is the persion standing on", 1]
        ],
        "context": [
            "a woman kicking a soccer ball on a field.",
            "a man riding a skateboard up the side of a ramp."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2408619",
                "VG_object_id": "258040",
                "bbox": [116, 108, 215, 399],
                "image": "data\\images\\2408619.jpg"
            },
            {
                "VG_image_id": "2357938",
                "VG_object_id": "807524",
                "bbox": [221, 82, 390, 301],
                "image": "data\\images\\2357938.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the man doing", 2],
            ["What color is the man's shirt", 2],
            ["Where is the man", 1],
            ["how many people are there", 1],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["What is the man doing", 2],
            ["What color is the man's shirt", 2],
            ["Where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", -1],
            ["how long is the man's hair", -1],
            ["what color is the background", 1],
            ["what gender is the person", -1],
            ["what is the man looking at", -1],
            ["what is the man wearing", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man holding a white frisbee in a room.",
            "a man and two children sitting on a couch with a dog."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2319931",
                "VG_object_id": "997482",
                "bbox": [84, 407, 252, 499],
                "image": "data\\images\\2319931.jpg"
            },
            {
                "VG_image_id": "2361069",
                "VG_object_id": "784321",
                "bbox": [231, 397, 328, 460],
                "image": "data\\images\\2361069.jpg"
            }
        ],
        "questions_with_scores": [["what main color is the box", 1]],
        "org_questions": [
            ["what main color is the box", 1],
            ["what is under the box", -1],
            ["how many people are there", -1],
            ["where is the photo taken", -1]
        ],
        "context": [
            "a kitchen with a bowl and a rolling pin",
            "a bunch of tools that are on a shelf"
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2344727",
                "VG_object_id": "913645",
                "bbox": [146, 184, 355, 301],
                "image": "data\\images\\2344727.jpg"
            },
            {
                "VG_image_id": "2341778",
                "VG_object_id": "2452567",
                "bbox": [38, 37, 499, 344],
                "image": "data\\images\\2341778.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cakes are there on the table", 2],
            ["how many people are there", 2],
            ["what color is the table", 1],
            ["what color is the plate", 1],
            ["What is on the cake", 1]
        ],
        "org_questions": [
            ["how many cakes are there on the table", 2],
            ["what color is the table", 1],
            ["what color is the plate", 1],
            ["what is the shape of the cake", -1],
            ["What is cake on", -1],
            ["What is on the cake", 1],
            ["how many people are there", 2],
            ["what color is the table the cake is placed on", -1],
            ["where are the cakes", -1],
            ["what kind of cake is this", -1]
        ],
        "context": [
            "a man and two children sitting at a table with a cake.",
            "a large cake with a slice cut out of it."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2417227",
                "VG_object_id": "3478541",
                "bbox": [2, 24, 447, 429],
                "image": "data\\images\\2417227.jpg"
            },
            {
                "VG_image_id": "2390135",
                "VG_object_id": "1253509",
                "bbox": [1, 1, 474, 290],
                "image": "data\\images\\2390135.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the dog", 2],
            ["what is on the dog's neck", 1],
            ["What is the dog doing", 1]
        ],
        "org_questions": [
            ["What color is the dog", 2],
            ["What is dog looking at", -1],
            ["how many people are there", -1],
            ["where is the dog", -1],
            ["what is on the dog's neck", 1],
            ["What is the dog doing", 1],
            ["what is the dog wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the dog sitting on", -1],
            ["what is in the photo", -1]
        ],
        "context": [
            "a small white dog laying on a couch looking out the window.",
            "a brown dog laying on a bed with a red blanket."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2366927",
                "VG_object_id": "2299197",
                "bbox": [226, 97, 290, 309],
                "image": "data\\images\\2366927.jpg"
            },
            {
                "VG_image_id": "2354255",
                "VG_object_id": "840066",
                "bbox": [202, 15, 314, 294],
                "image": "data\\images\\2354255.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many person's are there in the photo", 2],
            ["how many people are there in the photo", 2],
            ["what color is the woman's shirt", 1],
            ["what is the woman holding", 1],
            ["What is lady doing", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["how many person's are there in the photo", 2],
            ["what color is the ground the woman standing on", -1],
            ["what is the woman holding", 1],
            ["where is the woman", -1],
            ["What is lady doing", 1],
            ["What is the weather like", -1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what kind of pants is the woman wearing", -1],
            ["how many people are there in the photo", 2]
        ],
        "context": [
            "a woman standing next to a horse in a field.",
            "a woman and a child playing frisbee in a park."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2397286",
                "VG_object_id": "3821323",
                "bbox": [0, 261, 496, 331],
                "image": "data\\images\\2397286.jpg"
            },
            {
                "VG_image_id": "2412274",
                "VG_object_id": "198546",
                "bbox": [21, 243, 185, 321],
                "image": "data\\images\\2412274.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the land", 1],
            ["what is the ground covered with", 1],
            ["what is on the ground", 1],
            ["what is in the background", 1],
            ["what is the condition of the ground", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["what is the ground covered with", 1],
            ["what is on the ground", 1],
            ["What is the weather like", -1],
            ["what is in the background", 1],
            ["what is the condition of the ground", 1],
            ["how is the weather", -1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a giraffe standing next to a large rock formation.",
            "a wooden bench sitting in a field of grass."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2330317",
                "VG_object_id": "3682207",
                "bbox": [335, 254, 442, 416],
                "image": "data\\images\\2330317.jpg"
            },
            {
                "VG_image_id": "2408887",
                "VG_object_id": "253103",
                "bbox": [36, 36, 463, 291],
                "image": "data\\images\\2408887.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the motorcycle", 2],
            ["how many people are there in the picture", 2],
            ["when is this photo taken", 1],
            ["what is the ground covered with", 1],
            ["What is background of image", 1],
            ["what is in the distance", 1],
            ["where was the photo taken", 1],
            ["what is on the back of the bike", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the motorcycle", 2],
            ["when is this photo taken", 1],
            ["what is the ground covered with", 1],
            ["How many bicycles are there", -1],
            ["What is on the bicycle ", -1],
            ["What is background of image", 1],
            ["what is in the distance", 1],
            ["what type of vehicle is shown", -1],
            ["where was the photo taken", 1],
            ["what is on the back of the bike", 1],
            ["how many people are in the photo", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a person riding a dirt bike on a hill.",
            "a blue motorcycle parked on the street at night."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2349676",
                "VG_object_id": "3544061",
                "bbox": [0, 261, 499, 373],
                "image": "data\\images\\2349676.jpg"
            },
            {
                "VG_image_id": "2334555",
                "VG_object_id": "2693646",
                "bbox": [0, 124, 419, 498],
                "image": "data\\images\\2334555.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is on the land", 2],
            ["How many people are there", 2],
            ["What is the background of image", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["What is on the land", 2],
            ["How many people are there", 2],
            ["What is the background of image", 1],
            ["what is the ground covered with", -1],
            ["what color is the land", -1],
            ["when was this picture taken", -1],
            ["where was this picture taken", -1],
            ["what is in the background", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a woman flying a kite in a park.",
            "person in action during the race"
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2407754",
                "VG_object_id": "274111",
                "bbox": [19, 1, 104, 121],
                "image": "data\\images\\2407754.jpg"
            },
            {
                "VG_image_id": "2326422",
                "VG_object_id": "3014646",
                "bbox": [144, 326, 192, 471],
                "image": "data\\images\\2326422.jpg"
            }
        ],
        "questions_with_scores": [["what color is the bottle", 2]],
        "org_questions": [
            ["what color is the bottle", 2],
            ["where is the bottle", -1],
            ["how many bottles are there", -1],
            ["what is the bottle made of", -1],
            ["what is behind the bottle", -1],
            ["Where is the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the table", -1],
            ["what is in the photo", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a cat is laying in a bathroom sink.",
            "a table with a plate of food and wine glasses."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2364680",
                "VG_object_id": "758706",
                "bbox": [237, 188, 352, 430],
                "image": "data\\images\\2364680.jpg"
            },
            {
                "VG_image_id": "2370598",
                "VG_object_id": "2679047",
                "bbox": [19, 168, 302, 343],
                "image": "data\\images\\2370598.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the chair made of", 1],
            ["what color is the chair", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the chair made of", 1],
            ["what color is the chair", 1],
            ["where is the chair", -1],
            ["How many people are there", -1],
            ["what is the floor made of", -1],
            ["how is the weather", -1],
            ["what is on the ground", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a woman in shorts and a green shirt is standing in a field.",
            "a wooden swing hanging from a tree in a park."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2378619",
                "VG_object_id": "558664",
                "bbox": [92, 144, 197, 174],
                "image": "data\\images\\2378619.jpg"
            },
            {
                "VG_image_id": "2373392",
                "VG_object_id": "2059472",
                "bbox": [168, 229, 278, 400],
                "image": "data\\images\\2373392.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is in front of the pillow", 1],
            ["what is beside the pillow", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the pillow", -1],
            ["what is in front of the pillow", 1],
            ["what color is the bed", -1],
            ["how many pillows are there", -1],
            ["where is the photo taken", -1],
            ["where is the pillow placed on", -1],
            ["where is the pillow", -1],
            ["what is beside the pillow", 1],
            ["what is white", -1],
            ["what is on the bed", -1],
            ["how many people are in the photo", 1],
            ["where are the pillows", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a man laying on a bed with two cats.",
            "a woman and a child eating food in a bed."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "498008",
                "VG_object_id": "1701048",
                "bbox": [81, 311, 271, 467],
                "image": "data\\images\\498008.jpg"
            },
            {
                "VG_image_id": "2316469",
                "VG_object_id": "3493911",
                "bbox": [116, 92, 356, 262],
                "image": "data\\images\\2316469.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color of the dog", 1],
            ["how many dogs are there", 1],
            ["where is the dog", 1],
            ["what is the dog doing ", 1],
            ["what is in the distance", 1],
            ["what gesture is the dog", 1],
            ["what is the dog holding", 1],
            ["what is the dog on", 1]
        ],
        "org_questions": [
            ["what is the main color of the dog", 1],
            ["how many dogs are there", 1],
            ["where is the dog", 1],
            ["what is the dog doing ", 1],
            ["what is in the distance", 1],
            ["what is the dog wearing", -1],
            ["what gesture is the dog", 1],
            ["what kind of animal is in the picture", -1],
            ["what is the dog holding", 1],
            ["what is on the dog's head", -1],
            ["what is the dog on", 1]
        ],
        "context": [
            "a woman walking two dogs on a sidewalk.",
            "a dog is standing on a bike"
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2354948",
                "VG_object_id": "834206",
                "bbox": [145, 126, 248, 291],
                "image": "data\\images\\2354948.jpg"
            },
            {
                "VG_image_id": "2354388",
                "VG_object_id": "2142198",
                "bbox": [180, 133, 355, 239],
                "image": "data\\images\\2354388.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the man", 2],
            ["what is the background", 2],
            ["what color is the trouser", 1],
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["what is color of the man's pants", 1]
        ],
        "org_questions": [
            ["what color is the trouser", 1],
            ["what is the man doing", 1],
            ["where is the man", 2],
            ["how many people are there", 1],
            ["what is the background", 2],
            ["what is color of the man's pants", 1],
            ["what kind of shoes is the man wearing", -1],
            ["what type of pants is the person wearing", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man standing in a living room holding a nintendo wii controller.",
            "a man doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2363818",
                "VG_object_id": "3743449",
                "bbox": [34, 49, 271, 229],
                "image": "data\\images\\2363818.jpg"
            },
            {
                "VG_image_id": "2361383",
                "VG_object_id": "2569114",
                "bbox": [230, 110, 398, 280],
                "image": "data\\images\\2361383.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 1],
            ["what is on the dog's neck", 1],
            ["what is the dog doing", 1],
            ["where is the dog looking", 1]
        ],
        "org_questions": [
            ["what color is the dog", 1],
            ["what is the ground the dog standing on made of", -1],
            ["what is on the dog's neck", 1],
            ["how many people are there", -1],
            ["what is the dog doing", 1],
            ["what animal is in the picture", -1],
            ["where was the photo taken", -1],
            ["where is the dog looking", 1]
        ],
        "context": [
            "a dog laying on the ground with its tongue hanging out.",
            "a dog standing in the grass near a lake."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2396638",
                "VG_object_id": "442047",
                "bbox": [170, 91, 347, 391],
                "image": "data\\images\\2396638.jpg"
            },
            {
                "VG_image_id": "2345912",
                "VG_object_id": "2690324",
                "bbox": [20, 4, 178, 359],
                "image": "data\\images\\2345912.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on head", 2],
            ["what color is the land under the man", 1],
            ["how many people are there in the picture", 1],
            ["what is the man doing", 1],
            ["What is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man wearing on head", 2],
            ["what color is the land under the man", 1],
            ["how many people are there in the picture", 1],
            ["what is the man doing", 1],
            ["where is the man", -1],
            ["What is the man holding", 1],
            ["what is the man wearing", -1],
            ["What sports is man doing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what game is being played", -1]
        ],
        "context": [
            "a man is playing cricket on a field.",
            "a little girl is playing baseball with her parents."
        ]
    },
    {
        "object_category": "cake",
        "images": [
            {
                "VG_image_id": "2399892",
                "VG_object_id": "412996",
                "bbox": [225, 262, 333, 327],
                "image": "data\\images\\2399892.jpg"
            },
            {
                "VG_image_id": "2331742",
                "VG_object_id": "3134868",
                "bbox": [177, 108, 417, 294],
                "image": "data\\images\\2331742.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the cake", 1],
            ["what shape is the cake", 1],
            ["what is the person beside the cake doing", 1],
            ["what color is the plate", 1]
        ],
        "org_questions": [
            ["what color is the cake", 1],
            ["where is the cake", -1],
            ["what shape is the cake", 1],
            ["how many people are there", 2],
            ["what is the person beside the cake doing", 1],
            ["What is on the cake", -1],
            ["how many candles are there on the cake", -1],
            ["How many cakes are there", -1],
            ["what is the cake made of", -1],
            ["what color is the plate", 1]
        ],
        "context": [
            "a priest cutting a cake at a table.",
            "a man holding a cake with a knife."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2370035",
                "VG_object_id": "1793178",
                "bbox": [5, 7, 499, 372],
                "image": "data\\images\\2370035.jpg"
            },
            {
                "VG_image_id": "2362219",
                "VG_object_id": "776164",
                "bbox": [11, 10, 423, 491],
                "image": "data\\images\\2362219.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the photo", 1],
            ["what is the color of the photo", 1]
        ],
        "org_questions": [
            ["how many people are in the photo", 1],
            ["what is the man doing", -1],
            ["what is the color of the photo", 1],
            ["what is on the wall", -1],
            ["where is this scene", -1],
            ["when was the photo taken", -1],
            ["what room is this", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "two men in white hats are preparing pizzas.",
            "a man in a white apron is making a pizza."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2352894",
                "VG_object_id": "1863412",
                "bbox": [136, 114, 235, 222],
                "image": "data\\images\\2352894.jpg"
            },
            {
                "VG_image_id": "2319402",
                "VG_object_id": "2919816",
                "bbox": [366, 162, 446, 239],
                "image": "data\\images\\2319402.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is the man wearing on his head", 1],
            ["what is the man wearing", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 2],
            ["where is the man", 1],
            ["How many people are there", -1],
            ["what is the man wearing on his head", 1],
            ["what is the persion sitting on", -1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["where is the picture taken", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a man flying through the air while riding a snowboard.",
            "a man is sitting on the sidewalk next to a yellow building."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2361729",
                "VG_object_id": "779963",
                "bbox": [309, 212, 464, 299],
                "image": "data\\images\\2361729.jpg"
            },
            {
                "VG_image_id": "2357709",
                "VG_object_id": "809472",
                "bbox": [154, 238, 302, 370],
                "image": "data\\images\\2357709.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the box", 1],
            ["what is the box made of", 1],
            ["how many people are there", 1],
            ["what is the box placed on", 1],
            ["what is beside the box", 1],
            ["where is the picture taken", 1],
            ["who is in the picture", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the box", 1],
            ["what is the box made of", 1],
            ["when is this photo taken", -1],
            ["how many people are there", 1],
            ["what is the box placed on", 1],
            ["what is beside the box", 1],
            ["where is the picture taken", 1],
            ["who is in the picture", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a red suitcase with snow on it",
            "a woman standing next to a pile of luggage."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2351758",
                "VG_object_id": "2345313",
                "bbox": [289, 32, 497, 272],
                "image": "data\\images\\2351758.jpg"
            },
            {
                "VG_image_id": "2343360",
                "VG_object_id": "924588",
                "bbox": [79, 159, 274, 221],
                "image": "data\\images\\2343360.jpg"
            }
        ],
        "questions_with_scores": [
            ["WHat color is the building", 2],
            ["What color is the land", 2],
            ["Where is the building", 1],
            ["what is in front of the building", 1],
            ["where was this photo taken", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["WHat color is the building", 2],
            ["What color is the land", 2],
            ["Where is the building", 1],
            ["how many buses are there", -1],
            ["what animals are there", -1],
            ["what is the building made of", -1],
            ["what is in front of the building", 1],
            ["what color is the sky", -1],
            ["when was the photo taken", -1],
            ["where was this photo taken", 1],
            ["what is on the ground", 1],
            ["what is on the side of the building", -1]
        ],
        "context": [
            "a large jetliner sitting on top of an airport tarmac.",
            "the house is located in the town."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2356918",
                "VG_object_id": "2686677",
                "bbox": [115, 112, 213, 167],
                "image": "data\\images\\2356918.jpg"
            },
            {
                "VG_image_id": "2361885",
                "VG_object_id": "1727890",
                "bbox": [18, 372, 305, 499],
                "image": "data\\images\\2361885.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what pattern is on the umbrella", 1],
            ["How many umbrellas are there", 1],
            ["who is holding the umbrella", 1],
            ["what is holding the umbrella", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", -1],
            ["where is the umbrella", -1],
            ["how many people are there", 1],
            ["what pattern is on the umbrella", 1],
            ["How many umbrellas are there", 1],
            ["what is the umbrella made of", -1],
            ["who is holding the umbrella", 1],
            ["how is the weather", -1],
            ["what is in the background", -1],
            ["what is holding the umbrella", 1],
            ["where is the picture taken", -1],
            ["what is the person doing", -1],
            ["what is in the distance", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "two girls walking in the street with umbrellas.",
            "a crowd of people standing around a large building."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2393859",
                "VG_object_id": "1216107",
                "bbox": [294, 101, 458, 282],
                "image": "data\\images\\2393859.jpg"
            },
            {
                "VG_image_id": "2397922",
                "VG_object_id": "1185003",
                "bbox": [1, 216, 320, 498],
                "image": "data\\images\\2397922.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many beds are there", 2],
            ["What is on the bed ", 1],
            ["how many people are there on the bed", 1],
            ["what is the bed made of", 1]
        ],
        "org_questions": [
            ["How many beds are there", 2],
            ["What is on the bed ", 1],
            ["What is next to the bed", -1],
            ["how many people are there on the bed", 1],
            ["how many pillows are there on the bed", -1],
            ["what color is the wall", -1],
            ["where was the photo taken", -1],
            ["what room is this", -1],
            ["what is the bed made of", 1]
        ],
        "context": [
            "a room with several beds and a jacket hanging on the wall.",
            "a man laying in bed with a window in the background."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2352728",
                "VG_object_id": "1959721",
                "bbox": [64, 288, 149, 420],
                "image": "data\\images\\2352728.jpg"
            },
            {
                "VG_image_id": "2333704",
                "VG_object_id": "3263578",
                "bbox": [413, 175, 490, 317],
                "image": "data\\images\\2333704.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many chairs are there in the picture", 2],
            ["what color is the chair", 1],
            ["what is the legs of the chair made of", 1],
            ["where is the photo taken", 1],
            ["where is the chair", 1],
            ["what type of chair is shown", 1],
            ["what is behind the chair", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["what is the legs of the chair made of", 1],
            ["how many chairs are there in the picture", 2],
            ["where is the photo taken", 1],
            ["where is the chair", 1],
            ["what color is the wall", -1],
            ["what type of chair is shown", 1],
            ["what is behind the chair", 1],
            ["what is the table made of", -1],
            ["what is next to the chair", -1]
        ],
        "context": [
            "a man standing in a room next to a table.",
            "a bedroom with a bed, a desk, and a lamp."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2358402",
                "VG_object_id": "803210",
                "bbox": [92, 227, 282, 476],
                "image": "data\\images\\2358402.jpg"
            },
            {
                "VG_image_id": "2370815",
                "VG_object_id": "2408692",
                "bbox": [379, 113, 491, 265],
                "image": "data\\images\\2370815.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl wearing", 1],
            ["where is the girl", 1],
            ["what is the ground covered with", 1],
            ["What color is girl's shirt", 1],
            ["what is in front of the girl", 1],
            ["how many people are there", 1],
            ["what is the woman sitting on", 1]
        ],
        "org_questions": [
            ["what is the girl doing", -1],
            ["what is the girl wearing", 1],
            ["where is the girl", 1],
            ["how old is the girl", -1],
            ["what is the ground covered with", 1],
            ["What color is girl's shirt", 1],
            ["what is in front of the girl", 1],
            ["what is on the girl's head", -1],
            ["who is in the photo", -1],
            ["how many people are there", 1],
            ["what is the woman holding", -1],
            ["what is the woman sitting on", 1]
        ],
        "context": [
            "a group of people sitting around a table eating food.",
            "a group of people sitting around a table."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2344471",
                "VG_object_id": "2634259",
                "bbox": [38, 12, 425, 305],
                "image": "data\\images\\2344471.jpg"
            },
            {
                "VG_image_id": "2417605",
                "VG_object_id": "3792605",
                "bbox": [91, 46, 294, 434],
                "image": "data\\images\\2417605.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the horse", 2],
            ["how many horses are there in the picture", 2],
            ["who is riding the horse", 1]
        ],
        "org_questions": [
            ["what is on the horse", 2],
            ["what is the horse doing", -1],
            ["how many horses are there in the picture", 2],
            ["where are the horses", -1],
            ["what is the ground the horse standing on made of", -1],
            ["What is horse doing", -1],
            ["what color are the horses", -1],
            ["when was the photo taken", -1],
            ["who is riding the horse", 1],
            ["what is in the background", -1],
            ["what is the horse on", -1]
        ],
        "context": [
            "a woman riding a horse through a lush green forest.",
            "two horses standing on a grass field with people walking around."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2328337",
                "VG_object_id": "3525788",
                "bbox": [7, 259, 121, 497],
                "image": "data\\images\\2328337.jpg"
            },
            {
                "VG_image_id": "2393429",
                "VG_object_id": "472490",
                "bbox": [344, 158, 397, 324],
                "image": "data\\images\\2393429.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["what is the woman holding", 1],
            ["How many people are there", 1],
            ["what color is the background", 1],
            ["what is on the woman's head", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what is the woman holding", 1],
            ["what is in the background", -1],
            ["How many people are there", 1],
            ["where is the woman", -1],
            ["what are the people doing", -1],
            ["what is the woman wearing", -1],
            ["what color is the background", 1],
            ["when was this picture taken", -1],
            ["what is on the woman's head", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a person walking with an umbrella in the rain.",
            "a group of people walking down a street."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2369234",
                "VG_object_id": "1989588",
                "bbox": [56, 45, 271, 328],
                "image": "data\\images\\2369234.jpg"
            },
            {
                "VG_image_id": "2341976",
                "VG_object_id": "2354724",
                "bbox": [307, 8, 422, 293],
                "image": "data\\images\\2341976.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's shirt", 2],
            ["what is the girl doing", 2],
            ["where is the girl", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", 2],
            ["what is the girl doing", 2],
            ["where is the girl", 1],
            ["how many people are there", -1],
            ["How old is the girl", -1],
            ["what is the woman wearing", -1],
            ["what is the food in front of the girl", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a young girl petting an elephant at a zoo.",
            "a woman riding a horse on the beach."
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2317561",
                "VG_object_id": "1019753",
                "bbox": [10, 148, 427, 247],
                "image": "data\\images\\2317561.jpg"
            },
            {
                "VG_image_id": "2395692",
                "VG_object_id": "451833",
                "bbox": [167, 81, 273, 176],
                "image": "data\\images\\2395692.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the desk", 2],
            ["how many screens are in the picture", 2],
            ["how many screens are there on the desk", 1],
            ["where was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the desk", -1],
            ["what is on the desk", 2],
            ["how many screens are in the picture", 2],
            ["what is the table made of", -1],
            ["how many screens are there on the desk", 1],
            ["where was the picture taken", 1],
            ["who is in the picture", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a desk with three computer monitors and a keyboard.",
            "a bed with a deer head and antlers on it."
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2344459",
                "VG_object_id": "3190597",
                "bbox": [79, 86, 363, 271],
                "image": "data\\images\\2344459.jpg"
            },
            {
                "VG_image_id": "2316693",
                "VG_object_id": "2862648",
                "bbox": [0, 7, 430, 373],
                "image": "data\\images\\2316693.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is vehicle", 2],
            ["What is the background of image", 1],
            ["how many vehicles are there", 1],
            ["how is the weather", 1],
            ["when is the picture taken", 1],
            ["what kind of vehicle is shown", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["What color is vehicle", 2],
            ["What is the background of image", 1],
            ["how many vehicles are there", 1],
            ["how is the weather", 1],
            ["when is the picture taken", 1],
            ["what is on the car", -1],
            ["what color is the floor", -1],
            ["what kind of vehicle is shown", 1],
            ["where was the photo taken", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a red and white bus parked in a parking lot.",
            "a fire truck is parked in a garage."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2374030",
                "VG_object_id": "587456",
                "bbox": [69, 50, 167, 200],
                "image": "data\\images\\2374030.jpg"
            },
            {
                "VG_image_id": "2393884",
                "VG_object_id": "469106",
                "bbox": [30, 366, 76, 444],
                "image": "data\\images\\2393884.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the woman's face", 2],
            ["what color is the woman's shirt", 1],
            ["how many people are there", 1],
            ["what is the woman holding", 1],
            ["what is the persion sitting on", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["where is the woman", -1],
            ["what is the woman wearing", -1],
            ["how many people are there", 1],
            ["what is the person doing", -1],
            ["what gesture is the woman", -1],
            ["what is the woman holding", 1],
            ["where is the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1],
            ["what is on the woman's face", 2],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a man and a woman sitting at a table with computers.",
            "a clock on the wall of a building."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2405649",
                "VG_object_id": "2574671",
                "bbox": [161, 2, 418, 295],
                "image": "data\\images\\2405649.jpg"
            },
            {
                "VG_image_id": "2405463",
                "VG_object_id": "1107568",
                "bbox": [98, 50, 291, 311],
                "image": "data\\images\\2405463.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["how many people are in the picture", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what color is the man's shirt", 1],
            ["how many people are in the picture", 1],
            ["where is the guy", -1],
            ["what is the man standing on", -1],
            ["what is the person holding", -1],
            ["What is guy doing", -1],
            ["when was the photo taken", -1],
            ["who is on the skateboard", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a man riding a skateboard on a stage."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2341584",
                "VG_object_id": "941466",
                "bbox": [7, 147, 497, 277],
                "image": "data\\images\\2341584.jpg"
            },
            {
                "VG_image_id": "2406358",
                "VG_object_id": "652482",
                "bbox": [0, 66, 499, 280],
                "image": "data\\images\\2406358.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the ground", 1],
            ["what is in the background", 1],
            ["how many giraffes are there on the ground", 1],
            ["What is the object on the ground", 1],
            ["where is this scene", 1],
            ["what type of animals are shown", 1]
        ],
        "org_questions": [
            ["what is on the ground", 1],
            ["what is in the background", 1],
            ["what is the main color of the grass", -1],
            ["How many people are there", -1],
            ["what is the land made of", -1],
            ["how many giraffes are there on the ground", 1],
            ["What is the object on the ground", 1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["where is this scene", 1],
            ["what type of animals are shown", 1]
        ],
        "context": [
            "a group of giraffes standing around a wooden pole.",
            "a herd of cattle standing on a dirt path."
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2392105",
                "VG_object_id": "1233928",
                "bbox": [163, 42, 462, 294],
                "image": "data\\images\\2392105.jpg"
            },
            {
                "VG_image_id": "2328044",
                "VG_object_id": "2921126",
                "bbox": [88, 334, 240, 496],
                "image": "data\\images\\2328044.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the vehicle", 1],
            ["what is behind the vehicle", 1]
        ],
        "org_questions": [
            ["what color is the vehicle", 1],
            ["what is behind the vehicle", 1],
            ["how many vehicle are there in the photo", -1],
            ["what time is it", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1],
            ["what is on the side of the car", -1],
            ["where was the photo taken", -1],
            ["what type of vehicle is shown", -1],
            ["when was the photo taken", -1],
            ["where is the truck", -1]
        ],
        "context": [
            "a white truck parked in a parking lot.",
            "a man on a motor bike in front of a blue building."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2394474",
                "VG_object_id": "3326780",
                "bbox": [134, 39, 187, 113],
                "image": "data\\images\\2394474.jpg"
            },
            {
                "VG_image_id": "2355973",
                "VG_object_id": "1712534",
                "bbox": [313, 85, 442, 331],
                "image": "data\\images\\2355973.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is woman's shirt", 2],
            ["What color is woman's frisbee", 2],
            ["HOw many people are there", 1]
        ],
        "org_questions": [
            ["HOw many people are there", 1],
            ["What color is woman's shirt", 2],
            ["What color is woman's frisbee", 2],
            ["what is the woman wearing on her face", -1],
            ["where is the woman", -1],
            ["What is person doing", -1],
            ["what is the woman holding", -1],
            ["what is the woman wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a woman throwing a frisbee in a wooded area.",
            "two women playing frisbee in a field"
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2354718",
                "VG_object_id": "836212",
                "bbox": [1, 307, 108, 374],
                "image": "data\\images\\2354718.jpg"
            },
            {
                "VG_image_id": "2377858",
                "VG_object_id": "2796457",
                "bbox": [1, 0, 499, 370],
                "image": "data\\images\\2377858.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the land", 1],
            ["what is the land made of", 1],
            ["how many people are there", 1],
            ["where is this scene", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what is on the land", 1],
            ["what is the land made of", 1],
            ["how many people are there", 1],
            ["What season is it", -1],
            ["how is the weather", -1],
            ["where is this scene", 1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a dead sheep being cut by a man in the street",
            "a man kneeling down next to a toothbrush."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2373263",
                "VG_object_id": "2870252",
                "bbox": [172, 109, 274, 353],
                "image": "data\\images\\2373263.jpg"
            },
            {
                "VG_image_id": "2376723",
                "VG_object_id": "719373",
                "bbox": [140, 169, 271, 482],
                "image": "data\\images\\2376723.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is in the man's hand", 1],
            ["what color is the wall", 1],
            ["what sport is the man doing", 1],
            ["what is the person holding", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what are the people doing", -1],
            ["what is in the man's hand", 1],
            ["how many people are there", 2],
            ["what color is the wall", 1],
            ["what is the person wearing", -1],
            ["what sport is the man doing", 1],
            ["what is the person holding", 1],
            ["when was this photo taken", -1],
            ["where is the man", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a group of men walking on a field.",
            "a man holding a bat in his hand"
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2371156",
                "VG_object_id": "599386",
                "bbox": [203, 30, 300, 280],
                "image": "data\\images\\2371156.jpg"
            },
            {
                "VG_image_id": "2342703",
                "VG_object_id": "931450",
                "bbox": [172, 12, 317, 212],
                "image": "data\\images\\2342703.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["how many pizzas are there", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["how many pizzas are there", 1],
            ["what is the woman doing", 1],
            ["what is the girl wearing on the head", -1],
            ["where is the photo taken", -1],
            ["how many people are there", 2],
            ["when was this photo taken", -1],
            ["what are the people eating", -1],
            ["what is the girl wearing", -1],
            ["what is the woman holding", 1]
        ],
        "context": [
            "a group of girls holding a pizza on a pan.",
            "a group of people sitting at a table with pizza."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2359668",
                "VG_object_id": "1752813",
                "bbox": [27, 146, 324, 392],
                "image": "data\\images\\2359668.jpg"
            },
            {
                "VG_image_id": "2395011",
                "VG_object_id": "458622",
                "bbox": [197, 162, 387, 237],
                "image": "data\\images\\2395011.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table under the plate", 1]
        ],
        "org_questions": [
            ["what is on the plate", -1],
            ["what is on the table", -1],
            ["what color is the table", -1],
            ["How many pots are there", -1],
            ["where is the food placed on", -1],
            ["what is the table made of", -1],
            ["What is the background of image", -1],
            ["what color is the table under the plate", 1],
            ["what shape is the plate", -1],
            ["what kind of food is this", -1],
            ["what is the plate on", -1],
            ["where is the plate", -1]
        ],
        "context": [
            "a plate of donuts and a glass of beer.",
            "a plate of food on a table with a pitcher of coffee."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2351782",
                "VG_object_id": "2595609",
                "bbox": [147, 104, 331, 358],
                "image": "data\\images\\2351782.jpg"
            },
            {
                "VG_image_id": "2360780",
                "VG_object_id": "785954",
                "bbox": [292, 444, 357, 499],
                "image": "data\\images\\2360780.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the box", 1],
            ["what is in the box", 1],
            ["what is the box placed on", 1],
            ["what is above the box", 1],
            ["What is on the box", 1]
        ],
        "org_questions": [
            ["where is the box", 1],
            ["what is in the box", 1],
            ["what is the box placed on", 1],
            ["how many people are there", -1],
            ["what color is the box", -1],
            ["what is above the box", 1],
            ["What is on the box", 1]
        ],
        "context": [
            "a wooden bench with a plant in it",
            "a vase of flowers sitting on top of a table."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2375921",
                "VG_object_id": "578056",
                "bbox": [2, 224, 497, 374],
                "image": "data\\images\\2375921.jpg"
            },
            {
                "VG_image_id": "2369813",
                "VG_object_id": "2460643",
                "bbox": [151, 302, 413, 373],
                "image": "data\\images\\2369813.jpg"
            }
        ],
        "questions_with_scores": [["what room is this", 1]],
        "org_questions": [
            ["What is on the floor", -1],
            ["Where is the door", -1],
            ["How many people are there", -1],
            ["WHat color is the floor", -1],
            ["what is the pattern of the floor", -1],
            ["what is the floor made of", -1],
            ["what room is this", 1],
            ["how is the floor made", -1],
            ["what is the flooring", -1],
            ["what kind of flooring is this", -1],
            ["what is covering the floor", -1]
        ],
        "context": [
            "a kitchen with a table and chairs and a refrigerator.",
            "a bedroom with a bed, desk, and lamp."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2406273",
                "VG_object_id": "1102860",
                "bbox": [371, 205, 499, 275],
                "image": "data\\images\\2406273.jpg"
            },
            {
                "VG_image_id": "2377316",
                "VG_object_id": "566421",
                "bbox": [333, 179, 496, 332],
                "image": "data\\images\\2377316.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is beside the box", 1],
            ["what is on the box", 1],
            ["how large is the box", 1],
            ["where is the photo taken", 1],
            ["what is the box made of", 1],
            ["what is in the background", 1],
            ["where is the box", 1],
            ["what is on top of the box", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the box", -1],
            ["what is beside the box", 1],
            ["what is on the box", 1],
            ["How many people are there", -1],
            ["how large is the box", 1],
            ["where is the photo taken", 1],
            ["what is the box made of", 1],
            ["what is in the background", 1],
            ["where is the box", 1],
            ["what shape is the box", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on top of the box", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a black fan sitting next to a black fan.",
            "a clock on a wall next to a box."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2374185",
                "VG_object_id": "2119896",
                "bbox": [123, 148, 200, 199],
                "image": "data\\images\\2374185.jpg"
            },
            {
                "VG_image_id": "2400585",
                "VG_object_id": "407691",
                "bbox": [107, 307, 151, 430],
                "image": "data\\images\\2400585.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man holding", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's coat", -1],
            ["what is the man doing", 2],
            ["what is the man holding", 1],
            ["How many people are there", -1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["when was this picture taken", -1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a snowboarder is in the air doing a trick.",
            "a group of people walking down a street next to a tall clock tower."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2404739",
                "VG_object_id": "3814737",
                "bbox": [23, 3, 276, 498],
                "image": "data\\images\\2404739.jpg"
            },
            {
                "VG_image_id": "2334700",
                "VG_object_id": "2213091",
                "bbox": [145, 106, 257, 285],
                "image": "data\\images\\2334700.jpg"
            }
        ],
        "questions_with_scores": [["what color is the batter's shirt", 2]],
        "org_questions": [
            ["how many people are there", -1],
            ["what color is the batter's shirt", 2],
            ["what sport is the person playing", -1],
            ["what is the man wearing on the head", -1],
            ["what is in the background", -1],
            ["What color is the ground", -1],
            ["what sport is the player doing", -1],
            ["when was the photo taken", -1],
            ["who is holding the bat", -1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a baseball player is getting ready to hit a ball.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2360526",
                "VG_object_id": "2521792",
                "bbox": [114, 245, 264, 378],
                "image": "data\\images\\2360526.jpg"
            },
            {
                "VG_image_id": "2397659",
                "VG_object_id": "1187865",
                "bbox": [142, 339, 240, 428],
                "image": "data\\images\\2397659.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the background of image", 1],
            ["what time does the clock say", 1]
        ],
        "org_questions": [
            ["How many clocks are there", -1],
            ["What is the background of image", 1],
            ["What color is the clock", -1],
            ["what shape is the clock", -1],
            ["when is this picture taken", -1],
            ["where is the clock", -1],
            ["what is the clock made of", -1],
            ["what time is it", -1],
            ["what type of numbers are on the clock", -1],
            ["what time does the clock say", 1],
            ["what is above the clock", -1],
            ["what is on the clock", -1]
        ],
        "context": [
            "a large clock on the side of a building.",
            "a clock tower with a gold and black clock."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2407407",
                "VG_object_id": "280098",
                "bbox": [224, 171, 314, 232],
                "image": "data\\images\\2407407.jpg"
            },
            {
                "VG_image_id": "2413320",
                "VG_object_id": "298081",
                "bbox": [362, 312, 445, 365],
                "image": "data\\images\\2413320.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bags are there", 2],
            ["where is the bag", 1],
            ["how many people are there", 1],
            ["what is the persion doing", 1],
            ["how is the weather", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["where is the bag", 1],
            ["how many bags are there", 2],
            ["how many people are there", 1],
            ["what is the persion doing", 1],
            ["what is the bag made of", -1],
            ["how is the weather", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a woman sitting on a bench using her cell phone.",
            "a man standing next to a luggage cart."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2371241",
                "VG_object_id": "2030078",
                "bbox": [391, 10, 497, 121],
                "image": "data\\images\\2371241.jpg"
            },
            {
                "VG_image_id": "2408606",
                "VG_object_id": "258379",
                "bbox": [353, 279, 499, 401],
                "image": "data\\images\\2408606.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bowl", 1],
            ["what is in the bowl", 1],
            ["what is the bowl made of", 1],
            ["what kind of vegetables is in the bowl", 1],
            ["what is beside the bowl", 1],
            ["what is the food in", 1],
            ["how many plates are there", 1]
        ],
        "org_questions": [
            ["what color is the bowl", 1],
            ["what is in the bowl", 1],
            ["what the bowl is on", -1],
            ["how many people are there", -1],
            ["where is the bowl", -1],
            ["what is the bowl made of", 1],
            ["what kind of vegetables is in the bowl", 1],
            ["what is beside the bowl", 1],
            ["what is the food in", 1],
            ["how many plates are there", 1]
        ],
        "context": [
            "a table topped with mushrooms and broccoli.",
            "a table with plates of food and a glass of wine."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2410999",
                "VG_object_id": "3814271",
                "bbox": [30, 72, 459, 323],
                "image": "data\\images\\2410999.jpg"
            },
            {
                "VG_image_id": "2360563",
                "VG_object_id": "3533767",
                "bbox": [47, 14, 455, 264],
                "image": "data\\images\\2360563.jpg"
            }
        ],
        "questions_with_scores": [["what color is the plane", 1]],
        "org_questions": [
            ["what color is the sky", -1],
            ["what color is the plane", 1],
            ["how many people are there", -1],
            ["where is the plane", -1],
            ["what is the plane doing", -1],
            ["how many airplanes are in the picture", -1],
            ["when was the photo taken", -1],
            ["what is flying in the sky", -1],
            ["what is the weather like", -1],
            ["how is the sky", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "a plane flying through a barbed wire fence.",
            "a blue and white airplane flying in the sky."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2323067",
                "VG_object_id": "3070522",
                "bbox": [64, 203, 171, 320],
                "image": "data\\images\\2323067.jpg"
            },
            {
                "VG_image_id": "2347307",
                "VG_object_id": "3612041",
                "bbox": [173, 118, 304, 271],
                "image": "data\\images\\2347307.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", 2],
            ["what is the person doing", 1],
            ["What is person doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["who is wearing the shirt", 2],
            ["what is the person doing", 1],
            ["how many people are there", -1],
            ["where is the person", -1],
            ["how old is the person", -1],
            ["What is person doing", 1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "two women sitting in chairs in a room.",
            "a bride and groom standing next to a wedding cake."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2365273",
                "VG_object_id": "635434",
                "bbox": [3, 194, 448, 331],
                "image": "data\\images\\2365273.jpg"
            },
            {
                "VG_image_id": "2317952",
                "VG_object_id": "1015833",
                "bbox": [240, 266, 373, 349],
                "image": "data\\images\\2317952.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the counter", 1],
            ["what color is the cabinet under the counter", 1],
            ["how many bottles are there on the counter", 1],
            ["what is on the counter", 1],
            ["what on the counter", 1]
        ],
        "org_questions": [
            ["what color is the counter", 1],
            ["what color is the cabinet under the counter", 1],
            ["how many bottles are there on the counter", 1],
            ["where is the counter", -1],
            ["what is on the counter", 1],
            ["how many people are in the picture", -1],
            ["what on the counter", 1],
            ["what room is this", -1],
            ["where is the picture taken", -1],
            ["what is in the kitchen", -1]
        ],
        "context": [
            "a kitchen sink with a bowl of flowers",
            "a kitchen with a stove, microwave and dishwasher."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "150357",
                "VG_object_id": "1038298",
                "bbox": [1, 178, 1021, 572],
                "image": "data\\images\\150357.jpg"
            },
            {
                "VG_image_id": "2366266",
                "VG_object_id": "2512192",
                "bbox": [17, 164, 492, 372],
                "image": "data\\images\\2366266.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many giraffes are there", 2],
            ["what kind of animals are in the picture", 1],
            ["what are the animals doing", 1]
        ],
        "org_questions": [
            ["how many giraffes are there", 2],
            ["what is on the ground", -1],
            ["when is this picture taken", -1],
            ["where is the picture taken", -1],
            ["what is the land made of", -1],
            ["What color is the court", -1],
            ["what kind of animals are in the picture", 1],
            ["where are the trees", -1],
            ["what are the giraffes standing on", -1],
            ["what are the animals doing", 1]
        ],
        "context": [
            "two giraffes standing in a field with other animals.",
            "a group of zebras and giraffes in a field."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2349982",
                "VG_object_id": "2318130",
                "bbox": [11, 125, 428, 201],
                "image": "data\\images\\2349982.jpg"
            },
            {
                "VG_image_id": "2396261",
                "VG_object_id": "665087",
                "bbox": [167, 126, 331, 338],
                "image": "data\\images\\2396261.jpg"
            }
        ],
        "questions_with_scores": [["how many cars are in the picture", 1]],
        "org_questions": [
            ["how many cars are in the picture", 1],
            ["where is the car", -1],
            ["what is on the ground", -1],
            ["what shape is the car", -1],
            ["when is the photo taken", -1],
            ["what color is the car", -1],
            ["what is the weather like", -1],
            ["what is in the background", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a horse rolling on its back in a dirt field.",
            "a man is standing next to an elephant."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2366617",
                "VG_object_id": "2155318",
                "bbox": [188, 98, 329, 368],
                "image": "data\\images\\2366617.jpg"
            },
            {
                "VG_image_id": "2316781",
                "VG_object_id": "3327662",
                "bbox": [0, 87, 303, 498],
                "image": "data\\images\\2316781.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing on his face", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the man wearing on his face", 1],
            ["what color is the wall", -1],
            ["how many people are there", 1],
            ["what is the person holding", -1],
            ["what is the guy doing", -1],
            ["what is the gender of the person", -1],
            ["who is in the photo", -1],
            ["where is the man", -1],
            ["what is behind the man", -1]
        ],
        "context": [
            "two men standing in a living room with a dog.",
            "a man wearing a white shirt and sunglasses."
        ]
    },
    {
        "object_category": "bird",
        "images": [
            {
                "VG_image_id": "2340452",
                "VG_object_id": "2158205",
                "bbox": [3, 97, 439, 308],
                "image": "data\\images\\2340452.jpg"
            },
            {
                "VG_image_id": "2375094",
                "VG_object_id": "2867550",
                "bbox": [175, 60, 382, 253],
                "image": "data\\images\\2375094.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many birds are there", 2],
            ["Where is the bird", 1],
            ["What is the background of image", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["How many birds are there", 2],
            ["Where is the bird", 1],
            ["what is the birds standing on", -1],
            ["What is the background of image", 1],
            ["What color is bird", -1],
            ["what color is the bird's feet", -1],
            ["when was this photo taken", -1],
            ["where was this picture taken", 1],
            ["what is on the ground", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a flock of birds standing on a concrete surface.",
            "a small bird walking on a beach near the water."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2405046",
                "VG_object_id": "655171",
                "bbox": [104, 147, 285, 236],
                "image": "data\\images\\2405046.jpg"
            },
            {
                "VG_image_id": "2369937",
                "VG_object_id": "607859",
                "bbox": [62, 121, 234, 251],
                "image": "data\\images\\2369937.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 1],
            ["who is wearing the shirt", 1],
            ["what is the person doing", 1],
            ["How many people are there", 1],
            ["where is the person", 1],
            ["What is person doing", 1],
            ["what are the people doing", 1],
            ["what pattern is on the shirt", 1],
            ["what is on the shirt", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["who is wearing the shirt", 1],
            ["what is the person doing", 1],
            ["How many people are there", 1],
            ["where is the person", 1],
            ["What is person doing", 1],
            ["what are the people doing", 1],
            ["what pattern is on the shirt", 1],
            ["what is the shirt made of", -1],
            ["what is on the shirt", 1]
        ],
        "context": [
            "a man is taking a picture of himself using his laptop.",
            "two young girls sitting on the steps eating food."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2365913",
                "VG_object_id": "2588150",
                "bbox": [8, 79, 319, 290],
                "image": "data\\images\\2365913.jpg"
            },
            {
                "VG_image_id": "2321404",
                "VG_object_id": "3374538",
                "bbox": [178, 139, 374, 271],
                "image": "data\\images\\2321404.jpg"
            }
        ],
        "questions_with_scores": [["what is on the side of the train", 1]],
        "org_questions": [
            ["what color is the train", -1],
            ["how many trains are there", -1],
            ["what is on the side of the train", 1],
            ["where is the train", -1],
            ["what is in the distance", -1],
            ["how many carriages does the train have", -1],
            ["when was the picture taken", -1],
            ["what is the train doing", -1],
            ["what kind of train is this", -1],
            ["what is the train on", -1],
            ["where was this picture taken", -1],
            ["how many people are there in the picture", -1],
            ["what color is the head of the train", -1],
            ["what is on the ground", -1],
            ["what is behind the train", -1]
        ],
        "context": [
            "a train is on the tracks in the country.",
            "a train coming out of a tunnel"
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2412557",
                "VG_object_id": "191790",
                "bbox": [333, 159, 417, 204],
                "image": "data\\images\\2412557.jpg"
            },
            {
                "VG_image_id": "2353788",
                "VG_object_id": "1678565",
                "bbox": [128, 274, 182, 346],
                "image": "data\\images\\2353788.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["what color is the background", 2],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["What is color of image", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what are the people doing", 2],
            ["what color is the background", 2],
            ["what is the gender of the person", 1],
            ["where is the person", 1],
            ["What is color of image", 1],
            ["when was the photo taken", -1],
            ["what are the people wearing", -1]
        ],
        "context": [
            "a man and a woman sitting on a bench with a child.",
            "a girl walking in a line of buses."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2414364",
                "VG_object_id": "321314",
                "bbox": [233, 13, 358, 285],
                "image": "data\\images\\2414364.jpg"
            },
            {
                "VG_image_id": "2352774",
                "VG_object_id": "2003687",
                "bbox": [19, 8, 442, 331],
                "image": "data\\images\\2352774.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the shirt", 1],
            ["What sports is man doing", 1],
            ["where is the man", 1],
            ["what is the man holding", 1],
            ["what is on the man's head", 1],
            ["where was the photo taken", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["What color is the shirt", 1],
            ["What sports is man doing", 1],
            ["how many people are there", -1],
            ["when is this picture taken", -1],
            ["where is the man", 1],
            ["what is the man wearing", -1],
            ["What is the weather like", -1],
            ["who is in the photo", -1],
            ["what is the man holding", 1],
            ["what is on the man's head", 1],
            ["where was the photo taken", 1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a boy riding a skateboard down a street.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "bird",
        "images": [
            {
                "VG_image_id": "2367753",
                "VG_object_id": "2928182",
                "bbox": [234, 134, 319, 210],
                "image": "data\\images\\2367753.jpg"
            },
            {
                "VG_image_id": "2391807",
                "VG_object_id": "1237165",
                "bbox": [100, 368, 226, 431],
                "image": "data\\images\\2391807.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many birds are there", 1],
            ["what color is the water", 1],
            ["when is this picture taken", 1],
            ["what is the ground the bird standing on made of", 1],
            ["what is the bird standing on", 1],
            ["what is the color of the background", 1]
        ],
        "org_questions": [
            ["how many birds are there", 1],
            ["what color is the water", 1],
            ["when is this picture taken", 1],
            ["what is the ground the bird standing on made of", 1],
            ["what are the birds doing", -1],
            ["what is the bird standing on", 1],
            ["what is the color of the background", 1],
            ["what animal is in the photo", -1],
            ["where was the photo taken", -1],
            ["where are the birds", -1]
        ],
        "context": [
            "a bird standing on a beach next to the ocean.",
            "a bird sitting on a rock in front of the sun"
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2346556",
                "VG_object_id": "898343",
                "bbox": [87, 146, 206, 193],
                "image": "data\\images\\2346556.jpg"
            },
            {
                "VG_image_id": "2404251",
                "VG_object_id": "344903",
                "bbox": [298, 64, 400, 133],
                "image": "data\\images\\2404251.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the motorcycle", 1],
            ["what is the ground under the motorcycle made of", 1],
            ["what is on the back of the motorcycle", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the motorcycle", 1],
            ["what is behind the motorcycle", -1],
            ["what is the ground under the motorcycle made of", 1],
            ["How many people are there", -1],
            ["what time is it", -1],
            ["how many motorcycles are there", -1],
            ["when was this photo taken", -1],
            ["what is on the seat", -1],
            ["what is the seat made of", -1],
            ["what is on the back of the motorcycle", 1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a motorcycle parked next to a car in a parking lot.",
            "a white scooter parked in front of a red wall."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2340388",
                "VG_object_id": "2311925",
                "bbox": [108, 111, 366, 315],
                "image": "data\\images\\2340388.jpg"
            },
            {
                "VG_image_id": "2381403",
                "VG_object_id": "541773",
                "bbox": [198, 11, 455, 322],
                "image": "data\\images\\2381403.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the dog", 1],
            ["what is the dog doing", 1],
            ["what is in the background", 1],
            ["what is the ground the dog standing on made of", 1],
            ["what is the dog wearing", 1]
        ],
        "org_questions": [
            ["where is the dog", 1],
            ["what is the dog doing", 1],
            ["what is in the background", 1],
            ["how many dogs are there in the picture", -1],
            ["what is the ground the dog standing on made of", 1],
            ["what is the dog wearing", 1],
            ["what gesture is the dog", -1],
            ["what type of animal is shown", -1],
            ["when was the photo taken", -1],
            ["what is around the dog's neck", -1],
            ["what is on the dog's feet", -1]
        ],
        "context": [
            "a dog laying in the sand on a beach.",
            "a dog on a surfboard in the water."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2323352",
                "VG_object_id": "3428368",
                "bbox": [26, 148, 167, 385],
                "image": "data\\images\\2323352.jpg"
            },
            {
                "VG_image_id": "2333885",
                "VG_object_id": "3699049",
                "bbox": [0, 208, 129, 497],
                "image": "data\\images\\2333885.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["what is the person wearing", 1],
            ["what is on the girl's head", 1],
            ["what is the woman doing", 1]
        ],
        "org_questions": [
            ["what are the people doing", 2],
            ["what is the weather like", -1],
            ["How many people are there", -1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["what is the person wearing", 1],
            ["when was this photo taken", -1],
            ["what is on the woman's face", -1],
            ["what is on the girl's head", 1],
            ["when was the picture taken", -1],
            ["what is the woman doing", 1]
        ],
        "context": [
            "a group of women with umbrellas walking down a road.",
            "a girl is looking at a giraffe in a zoo."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2327874",
                "VG_object_id": "3311130",
                "bbox": [1, 252, 331, 374],
                "image": "data\\images\\2327874.jpg"
            },
            {
                "VG_image_id": "2354275",
                "VG_object_id": "839809",
                "bbox": [0, 267, 499, 331],
                "image": "data\\images\\2354275.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what color is the land", 1],
            ["what is on the ground", 1],
            ["what is the land made of", 1],
            ["how is the ground made", 1],
            ["where is the picture taken", 1],
            ["what is in the ground", 1]
        ],
        "org_questions": [
            ["what color is the land", 1],
            ["how many people are in the picture", 2],
            ["what is on the ground", 1],
            ["how is the weather", -1],
            ["what is the land made of", 1],
            ["how many motorcycles are there on the ground", -1],
            ["how is the ground made", 1],
            ["where is the picture taken", 1],
            ["what is in the ground", 1]
        ],
        "context": [
            "a blue car with a surfboard on top",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2405091",
                "VG_object_id": "1110590",
                "bbox": [159, 140, 498, 332],
                "image": "data\\images\\2405091.jpg"
            },
            {
                "VG_image_id": "2373421",
                "VG_object_id": "732508",
                "bbox": [4, 38, 495, 365],
                "image": "data\\images\\2373421.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is the ground", 1],
            ["What color is the train", 1],
            ["where is the photo taken", 1],
            ["what is on the land", 1],
            ["what is the ground covered with", 1],
            ["what is this picture taken", 1],
            ["what is beside the train", 1]
        ],
        "org_questions": [
            ["What color is the ground", 1],
            ["What color is the train", 1],
            ["How many people are there", 2],
            ["where is the photo taken", 1],
            ["what is on the land", 1],
            ["what is the ground covered with", 1],
            ["what is this picture taken", 1],
            ["what is beside the train", 1],
            ["when was the picture taken", -1],
            ["where are the trains", -1],
            ["how many trains are in the picture", -1]
        ],
        "context": [
            "a train is stopped at a train station.",
            "a model train set with a model train on the tracks."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2359931",
                "VG_object_id": "2143970",
                "bbox": [286, 138, 366, 289],
                "image": "data\\images\\2359931.jpg"
            },
            {
                "VG_image_id": "2409662",
                "VG_object_id": "236816",
                "bbox": [157, 128, 193, 197],
                "image": "data\\images\\2409662.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many neckties are there in the photo", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what color is the necktie", -1],
            ["what color is the shirt behind the necktie", -1],
            ["how many neckties are there in the photo", 1],
            ["what shape is the necktie", -1],
            ["what is the man doing", -1],
            ["who is wearing the necktie", -1],
            ["where is the man", 1],
            ["what type of necktie is it", -1],
            ["what is around the man's neck", -1],
            ["what is the man wearing", -1],
            ["what is on the man's neck", -1],
            ["what color is the man wearing", -1]
        ],
        "context": [
            "two young boys wearing ties and ties standing next to each other.",
            "a man wearing a mask and a mask holding a cell phone."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2379641",
                "VG_object_id": "1355705",
                "bbox": [53, 24, 105, 63],
                "image": "data\\images\\2379641.jpg"
            },
            {
                "VG_image_id": "2344150",
                "VG_object_id": "917523",
                "bbox": [233, 154, 374, 196],
                "image": "data\\images\\2344150.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bag", 1],
            ["where is the bag", 1],
            ["what is the persion doing", 1],
            ["what is the bag on", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the bag", 1],
            ["where is the bag", 1],
            ["what color is the ground", -1],
            ["how many people are there in the picture", -1],
            ["what is the persion doing", 1],
            ["what is the bag on", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a woman sitting on the floor with a child playing with a toy.",
            "a man standing in a bathroom next to a sink."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2320673",
                "VG_object_id": "3353351",
                "bbox": [134, 27, 374, 181],
                "image": "data\\images\\2320673.jpg"
            },
            {
                "VG_image_id": "2367488",
                "VG_object_id": "2340295",
                "bbox": [0, 0, 497, 368],
                "image": "data\\images\\2367488.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the car", 1],
            ["how many birds are in the background", 1]
        ],
        "org_questions": [
            ["how many dogs are in the picture", -1],
            ["what is on the car", 1],
            ["how many birds are in the background", 1],
            ["what time is it", -1],
            ["how is the weather", -1],
            ["when is the photo taken", -1]
        ],
        "context": [
            "a small bird sitting on a yellow bench.",
            "a dog sitting in the back of a car."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2405179",
                "VG_object_id": "334370",
                "bbox": [38, 77, 354, 273],
                "image": "data\\images\\2405179.jpg"
            },
            {
                "VG_image_id": "2349417",
                "VG_object_id": "2393413",
                "bbox": [109, 170, 275, 239],
                "image": "data\\images\\2349417.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is beside the cow", 1],
            ["where is the cow standing", 1],
            ["what is in the background", 1],
            ["how many people are there around the cows", 1],
            ["what is in front the cow", 1],
            ["what is on the cow's head", 1],
            ["what is the cow doing", 1],
            ["where is the cow", 1]
        ],
        "org_questions": [
            ["what color is the cow", -1],
            ["what is beside the cow", 1],
            ["where is the cow standing", 1],
            ["how many cows are there", -1],
            ["what color is the cow's head", -1],
            ["what is in the background", 1],
            ["how many people are there around the cows", 1],
            ["what is in front the cow", 1],
            ["what type of animal is shown", -1],
            ["when was the photo taken", -1],
            ["what is on the cow's head", 1],
            ["what is the cow doing", 1],
            ["where is the cow", 1],
            ["what animal is in the photo", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man milking a cow on a field.",
            "a cow is crossing a river in front of a boat."
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2354186",
                "VG_object_id": "2113267",
                "bbox": [131, 10, 498, 496],
                "image": "data\\images\\2354186.jpg"
            },
            {
                "VG_image_id": "2392274",
                "VG_object_id": "481574",
                "bbox": [6, 15, 352, 331],
                "image": "data\\images\\2392274.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's hair", 2],
            ["what color is the woman's shirt", 2],
            ["What is the woman doing", 1],
            ["what is in the woman's hand", 1]
        ],
        "org_questions": [
            ["what color is the woman's hair", 2],
            ["what color is the woman's shirt", 2],
            ["how many people are there", -1],
            ["What season is it", -1],
            ["what is on the lady's head", -1],
            ["where is the woman", -1],
            ["What is the woman doing", 1],
            ["who is in the photo", -1],
            ["what is behind the woman", -1],
            ["what is in the woman's hand", 1],
            ["what is the woman wearing", -1],
            ["what is the woman looking at", -1]
        ],
        "context": [
            "a woman sitting at a table with a wine bottle and a plate of food.",
            "a woman talking on a cell phone in a restaurant."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2362126",
                "VG_object_id": "776973",
                "bbox": [157, 156, 212, 230],
                "image": "data\\images\\2362126.jpg"
            },
            {
                "VG_image_id": "2391797",
                "VG_object_id": "486122",
                "bbox": [267, 21, 328, 212],
                "image": "data\\images\\2391797.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the child doing", 2],
            ["what is the boy doing", 1],
            ["where is the boy", 1],
            ["what is the persion riding", 1]
        ],
        "org_questions": [
            ["what color is the boy's shirt", -1],
            ["what is the boy doing", 1],
            ["where is the boy", 1],
            ["how many children are there ", -1],
            ["what is the child wearing on his head", -1],
            ["what sport is the child doing", 2],
            ["who is wearing a white shirt", -1],
            ["when was this photo taken", -1],
            ["what is the persion riding", 1],
            ["what is the persion on the left wearing", -1]
        ],
        "context": [
            "a group of people standing around a skateboard park.",
            "two people riding horses on a street."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2411038",
                "VG_object_id": "317406",
                "bbox": [78, 5, 140, 256],
                "image": "data\\images\\2411038.jpg"
            },
            {
                "VG_image_id": "2417345",
                "VG_object_id": "3075452",
                "bbox": [171, 10, 498, 158],
                "image": "data\\images\\2417345.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 2],
            ["what room is it", 1],
            ["What is in front of the curtain", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 2],
            ["where is the curtain", -1],
            ["what room is it", 1],
            ["what is the pattern on the curtain", -1],
            ["What is in front of the curtain", 1],
            ["where was the photo taken", 1],
            ["what is covering the window", -1],
            ["what is hanging on the wall", -1],
            ["what is in the background", -1],
            ["what is on the window", -1],
            ["what is hanging from the window", -1]
        ],
        "context": [
            "a living room with a couch, chair, and a painting.",
            "a woman laying on the floor with a laptop."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2372331",
                "VG_object_id": "592484",
                "bbox": [2, 160, 328, 431],
                "image": "data\\images\\2372331.jpg"
            },
            {
                "VG_image_id": "2393896",
                "VG_object_id": "3824386",
                "bbox": [139, 314, 395, 485],
                "image": "data\\images\\2393896.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["What animal are there", 1],
            ["what shape is the floor", 1]
        ],
        "org_questions": [
            ["what color is the ground", 2],
            ["what is on the ground", -1],
            ["when is this photo taken", -1],
            ["What animal are there", 1],
            ["what is the land made of", -1],
            ["what shape is the floor", 1],
            ["where is the carpet", -1],
            ["what type of floor is this", -1],
            ["where was this photo taken", -1],
            ["what material is the floor made of", -1]
        ],
        "context": [
            "a cat playing with a toy on the floor",
            "a man is eating a piece of food."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2390105",
                "VG_object_id": "1253870",
                "bbox": [53, 52, 374, 332],
                "image": "data\\images\\2390105.jpg"
            },
            {
                "VG_image_id": "2336366",
                "VG_object_id": "2093736",
                "bbox": [307, 35, 485, 172],
                "image": "data\\images\\2336366.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cows are there", 2],
            ["what color is the cow's head", 1],
            ["What color is cow", 1],
            ["what is in the background", 1],
            ["what is behind the cows", 1]
        ],
        "org_questions": [
            ["what color is the cow's head", 1],
            ["how many cows are there", 2],
            ["what is the ground covered with", -1],
            ["what is the cow doing ", -1],
            ["where is the cow", -1],
            ["What color is cow", 1],
            ["What color is the ground", -1],
            ["what animal is in the picture", -1],
            ["when was this photo taken", -1],
            ["what is in the background", 1],
            ["what type of animal is shown", -1],
            ["what is behind the cows", 1]
        ],
        "context": [
            "a brown and white cow standing on top of a lush green field.",
            "a herd of cattle standing next to a river."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2355459",
                "VG_object_id": "829328",
                "bbox": [221, 131, 329, 238],
                "image": "data\\images\\2355459.jpg"
            },
            {
                "VG_image_id": "2392710",
                "VG_object_id": "1226730",
                "bbox": [246, 228, 324, 350],
                "image": "data\\images\\2392710.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the trouser", 2],
            ["what color is the man's pant", 2],
            ["How many people are there", 1],
            ["what is on the man's hands", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is the trouser", 2],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["what color is the man's pant", 2],
            ["when was the photo taken", -1],
            ["what is on the man's hands", 1],
            ["what kind of pants is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is the boy wearing", -1]
        ],
        "context": [
            "a man on a snowboard jumping in the air.",
            "three men on skis standing in the snow."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2417644",
                "VG_object_id": "3259766",
                "bbox": [88, 218, 460, 318],
                "image": "data\\images\\2417644.jpg"
            },
            {
                "VG_image_id": "2326860",
                "VG_object_id": "981520",
                "bbox": [1, 358, 500, 476],
                "image": "data\\images\\2326860.jpg"
            }
        ],
        "questions_with_scores": [["what is on the table", 2]],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 2],
            ["where is the photo taken", -1],
            ["what is the table made of", -1],
            ["where is the table", -1],
            ["what is the table sitting on", -1],
            ["how is the table made", -1],
            ["what is made of wood", -1]
        ],
        "context": [
            "a vase of flowers sitting on a table.",
            "a dog figurine with a frisbee in its mouth."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2398227",
                "VG_object_id": "1181455",
                "bbox": [5, 231, 332, 499],
                "image": "data\\images\\2398227.jpg"
            },
            {
                "VG_image_id": "2317234",
                "VG_object_id": "3130461",
                "bbox": [28, 297, 327, 498],
                "image": "data\\images\\2317234.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is on the plate", 1],
            ["what color is the table", 1],
            ["what is the plate made of", 1],
            ["what is the food on", 1]
        ],
        "org_questions": [
            ["what is on the plate", 1],
            ["what color is the table", 1],
            ["where is the plate", -1],
            ["how many people are there", 2],
            ["what shape is the plate", -1],
            ["what is the plate on ", -1],
            ["what is in the background", -1],
            ["what is the plate made of", 1],
            ["what is the main color of the plate", -1],
            ["what is the food on", 1]
        ],
        "context": [
            "a bowl of carrots and a spoon with a spoon in it.",
            "a little girl sitting in front of a chocolate cake."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2370742",
                "VG_object_id": "2509394",
                "bbox": [1, 61, 498, 371],
                "image": "data\\images\\2370742.jpg"
            },
            {
                "VG_image_id": "2355674",
                "VG_object_id": "827496",
                "bbox": [1, 107, 290, 374],
                "image": "data\\images\\2355674.jpg"
            }
        ],
        "questions_with_scores": [["how many lamps are there", 2]],
        "org_questions": [
            ["what color is the wall", -1],
            ["how many lamps are there", 2],
            ["what is on the bed besides pillows", -1],
            ["what is on the bed", -1],
            ["how many people are there on the bed", -1],
            ["what color is the sheet on  bed", -1],
            ["where was this picture taken", -1],
            ["what room is this", -1],
            ["where are the lamps", -1],
            ["what is behind the bed", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a bed with a white blanket and pillows on it.",
            "a bedroom with a bed, chair, and a chair."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2406818",
                "VG_object_id": "289773",
                "bbox": [54, 131, 486, 294],
                "image": "data\\images\\2406818.jpg"
            },
            {
                "VG_image_id": "1159898",
                "VG_object_id": "1598752",
                "bbox": [7, 510, 935, 958],
                "image": "data\\images\\1159898.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many tables are there in the picture", 2],
            ["what is on the table", 1],
            ["how many people are there", 1],
            ["what kind of food is in the picture", 1],
            ["what shape is the table", 1],
            ["what is the table color", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what is on the table", 1],
            ["how many people are there", 1],
            ["what kind of food is in the picture", 1],
            ["how many tables are there in the picture", 2],
            ["where was this picture taken", -1],
            ["what shape is the table", 1],
            ["what is the table color", 1]
        ],
        "context": [
            "two large cakes on a table",
            "a man sitting at a table with a cup of coffee."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2402485",
                "VG_object_id": "389241",
                "bbox": [226, 115, 347, 264],
                "image": "data\\images\\2402485.jpg"
            },
            {
                "VG_image_id": "2344655",
                "VG_object_id": "914131",
                "bbox": [118, 150, 229, 263],
                "image": "data\\images\\2344655.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the woman's shirt", 2],
            ["what is the woman holding", 2],
            ["what is the woman doing", 1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the color of the woman's shirt", 2],
            ["what is the woman holding", 2],
            ["what is the woman doing", 1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["how many people are there", 1]
        ],
        "context": [
            "a woman playing a video game in a living room.",
            "a woman and a man sitting on a bed."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2378181",
                "VG_object_id": "561564",
                "bbox": [28, 94, 384, 397],
                "image": "data\\images\\2378181.jpg"
            },
            {
                "VG_image_id": "2379563",
                "VG_object_id": "1356565",
                "bbox": [157, 45, 331, 223],
                "image": "data\\images\\2379563.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what is behind the man", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man doing", -1],
            ["how many people are there", -1],
            ["what is the man wearing", -1],
            ["what kind of clothes is the man wearing", -1],
            ["who is in the photo", -1],
            ["what is behind the man", 1],
            ["what is on the man's face", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man wearing a hat and tie with a tie around his neck.",
            "a man sitting at a table with a cake on it."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2355966",
                "VG_object_id": "2523100",
                "bbox": [48, 172, 363, 309],
                "image": "data\\images\\2355966.jpg"
            },
            {
                "VG_image_id": "2336874",
                "VG_object_id": "3223657",
                "bbox": [105, 171, 340, 317],
                "image": "data\\images\\2336874.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the food in the plate", 1],
            ["what kind of food is on the plate", 1],
            ["when is the photo taken", 1],
            ["what is in the plate", 1]
        ],
        "org_questions": [
            ["what color is the food in the plate", 1],
            ["what kind of food is on the plate", 1],
            ["when is the photo taken", 1],
            ["what shape of plate is the food put on", -1],
            ["how many plates are there", -1],
            ["what is the food placing on", -1],
            ["where is the plate", -1],
            ["what shape is the plate", -1],
            ["what is in the plate", 1]
        ],
        "context": [
            "a person sitting at a table with a plate of chocolate ice cream.",
            "a large pot pie on a plate"
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2363277",
                "VG_object_id": "2698106",
                "bbox": [38, 189, 494, 242],
                "image": "data\\images\\2363277.jpg"
            },
            {
                "VG_image_id": "2417605",
                "VG_object_id": "3792609",
                "bbox": [2, 134, 362, 496],
                "image": "data\\images\\2417605.jpg"
            }
        ],
        "questions_with_scores": [["How many people are there", 1]],
        "org_questions": [
            ["what kind of animal is there on the land", -1],
            ["what is in the distance", -1],
            ["How many people are there", 1],
            ["What season is it", -1],
            ["what is the ground covered with", -1],
            ["what is the picture taken", -1],
            ["what color is the grass", -1],
            ["where are the trees", -1],
            ["when was the picture taken", -1],
            ["what is the weather like", -1],
            ["where is this scene", -1]
        ],
        "context": [
            "a group of zebras walking across a field.",
            "two horses standing on a grass field with people walking around."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2373738",
                "VG_object_id": "1777032",
                "bbox": [139, 73, 422, 490],
                "image": "data\\images\\2373738.jpg"
            },
            {
                "VG_image_id": "2373332",
                "VG_object_id": "1915533",
                "bbox": [302, 105, 433, 328],
                "image": "data\\images\\2373332.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where are the men", 1],
            ["what gesture is the man", 1],
            ["what is the man standing on", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is on the man's head", -1],
            ["what color is the man's shirt", 1],
            ["how many people are there", -1],
            ["where are the men", 1],
            ["what is the man holding", 2],
            ["what gesture is the man", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what sport is being played", -1],
            ["what is the man standing on", 1]
        ],
        "context": [
            "a baseball player is getting ready to bat.",
            "a baseball player is throwing a ball on a field."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2366129",
                "VG_object_id": "3880779",
                "bbox": [211, 186, 498, 331],
                "image": "data\\images\\2366129.jpg"
            },
            {
                "VG_image_id": "2364803",
                "VG_object_id": "2020789",
                "bbox": [25, 249, 279, 326],
                "image": "data\\images\\2364803.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["what room is the floor in", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["what is the floor made of", -1],
            ["how many people are there on the floor", -1],
            ["where is the picture taken", -1],
            ["what room is the floor in", 1],
            ["what is on the wall", 1],
            ["how many chairs are on the floor", -1],
            ["what is the pattern on the floor", -1],
            ["how is the floor made", -1],
            ["what type of flooring is shown", -1]
        ],
        "context": [
            "a woman is putting something in the oven.",
            "a living room with a painting on the wall"
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2332116",
                "VG_object_id": "3296438",
                "bbox": [385, 2, 493, 232],
                "image": "data\\images\\2332116.jpg"
            },
            {
                "VG_image_id": "2377429",
                "VG_object_id": "565647",
                "bbox": [208, 0, 303, 239],
                "image": "data\\images\\2377429.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 1],
            ["what color is the thing in front of the curtain", 1],
            ["what is behind the curtain", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 1],
            ["what color is the thing in front of the curtain", 1],
            ["how many people are there", -1],
            ["what room is the curtain in", -1],
            ["when is this picture taken", -1],
            ["what is behind the curtain", 1],
            ["where is the curtain", -1]
        ],
        "context": [
            "a bed with a computer and a monitor in the background.",
            "a living room with a couch and a mirror"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2364556",
                "VG_object_id": "3030174",
                "bbox": [24, 402, 77, 493],
                "image": "data\\images\\2364556.jpg"
            },
            {
                "VG_image_id": "2406055",
                "VG_object_id": "326764",
                "bbox": [95, 314, 133, 389],
                "image": "data\\images\\2406055.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["what is the man wearing", 1],
            ["what is the ground covered with", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["how many umbrellas are there in the picture", -1],
            ["what is the man wearing", 1],
            ["what is in the background", -1],
            ["what is the ground covered with", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["where was this photo taken", 1],
            ["how many people are pictured", -1]
        ],
        "context": [
            "a red frisbee is in the air near a tree.",
            "a man walking on the beach with a surfboard."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2407242",
                "VG_object_id": "1098322",
                "bbox": [3, 219, 228, 333],
                "image": "data\\images\\2407242.jpg"
            },
            {
                "VG_image_id": "2348974",
                "VG_object_id": "1862097",
                "bbox": [3, 176, 498, 311],
                "image": "data\\images\\2348974.jpg"
            }
        ],
        "questions_with_scores": [["how many people are there", 2]],
        "org_questions": [
            ["what color is the land", -1],
            ["what is on the land", -1],
            ["where is the land", -1],
            ["how many people are there", 2],
            ["What is the ground made of", -1],
            ["what is in the distance", -1],
            ["when was the picture taken", -1],
            ["how is the weather", -1],
            ["what is green", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a group of giraffes standing in a zoo enclosure.",
            "a herd of elephants standing on top of a dirt field."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2404939",
                "VG_object_id": "1111653",
                "bbox": [139, 48, 373, 323],
                "image": "data\\images\\2404939.jpg"
            },
            {
                "VG_image_id": "2396205",
                "VG_object_id": "446927",
                "bbox": [93, 169, 310, 499],
                "image": "data\\images\\2396205.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the horse", 2],
            ["what color is the person's shirt", 2],
            ["what is behind the horses", 1],
            ["what is on the horse's head", 1]
        ],
        "org_questions": [
            ["what color is the horse", 2],
            ["what color is the person's shirt", 2],
            ["how many people are there", -1],
            ["what is behind the horses", 1],
            ["what are the horses doing", -1],
            ["how many horses are there on the ground", -1],
            ["what animal is in the picture", -1],
            ["what is on the horse's head", 1],
            ["who is in the picture", -1],
            ["where is the horse", -1],
            ["what is the animal", -1]
        ],
        "context": [
            "a person riding a horse in an arena.",
            "a man sitting on a horse in a dirt field."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2376402",
                "VG_object_id": "2278525",
                "bbox": [10, 160, 498, 373],
                "image": "data\\images\\2376402.jpg"
            },
            {
                "VG_image_id": "2369690",
                "VG_object_id": "743100",
                "bbox": [0, 322, 497, 498],
                "image": "data\\images\\2369690.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the counter", 2],
            ["how many people are there in the picture", 1],
            ["what is next to the table", 1]
        ],
        "org_questions": [
            ["what color is the counter", 2],
            ["what is on the counter", -1],
            ["where is the picture taken", -1],
            ["how many people are there in the picture", 1],
            ["what is on the wall", -1],
            ["what color is the wall", -1],
            ["what is the table sitting on", -1],
            ["what is next to the table", 1],
            ["where is the food", -1],
            ["what is in the table", -1]
        ],
        "context": [
            "a person is preparing sandwiches on a counter.",
            "a wooden table with a bowl of fruit and a bottle of juice."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2376319",
                "VG_object_id": "3686049",
                "bbox": [21, 72, 328, 494],
                "image": "data\\images\\2376319.jpg"
            },
            {
                "VG_image_id": "2355334",
                "VG_object_id": "830559",
                "bbox": [243, 54, 471, 265],
                "image": "data\\images\\2355334.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["What is man doing", 1],
            ["What color is man's shirt", 1],
            ["WHat color is man's hair", 1],
            ["what shape is the man's collar", 1],
            ["where is the picture taken", 1],
            ["what is the man holding", 1],
            ["where is the man", 1],
            ["when was the photo taken", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["What is man doing", 1],
            ["What color is man's shirt", 1],
            ["WHat color is man's hair", 1],
            ["what shape is the man's collar", 1],
            ["how many people are there", -1],
            ["where is the picture taken", 1],
            ["what is the man holding", 1],
            ["where is the man", 1],
            ["when was the photo taken", 1],
            ["who is in the photo", -1],
            ["what is on the man's head", 2],
            ["what is behind the man", 1]
        ],
        "context": [
            "a man cooking food on a grill on a city street.",
            "a man sitting at a table with a lot of cans and a table full of items."
        ]
    },
    {
        "object_category": "board",
        "images": [
            {
                "VG_image_id": "2416509",
                "VG_object_id": "3240004",
                "bbox": [119, 128, 284, 265],
                "image": "data\\images\\2416509.jpg"
            },
            {
                "VG_image_id": "2343492",
                "VG_object_id": "3231368",
                "bbox": [85, 191, 324, 349],
                "image": "data\\images\\2343492.jpg"
            }
        ],
        "questions_with_scores": [["what color is the surfboard", 2]],
        "org_questions": [
            ["what is the man doing", -1],
            ["what color is the surfboard", 2],
            ["How many people are there", -1],
            ["what is in the background", -1],
            ["what is on the board", -1],
            ["how many trees are there in the picture", -1],
            ["what is the persion standing on", -1],
            ["what is the man wearing", -1],
            ["what is white", -1],
            ["what is in the water", -1]
        ],
        "context": [
            "a man laying on a surfboard in the water.",
            "a man in a wet suit surfing on a wave."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2394111",
                "VG_object_id": "667142",
                "bbox": [214, 204, 314, 334],
                "image": "data\\images\\2394111.jpg"
            },
            {
                "VG_image_id": "2416059",
                "VG_object_id": "3545000",
                "bbox": [239, 159, 333, 280],
                "image": "data\\images\\2416059.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the trousers", 2],
            ["what is the person  holding", 2],
            ["what gender is the person", 2],
            ["how many people are there in the photo", 1]
        ],
        "org_questions": [
            ["what color is the trousers", 2],
            ["what is the person in the trousers holding", -1],
            ["what is the person in the trousers doing", -1],
            ["how many people are there in the photo", 1],
            ["where is the man", -1],
            ["what is on the man's head", -1],
            ["what is the ground behind covered with", -1],
            ["when was the photo taken", -1],
            ["what kind of pants is the man wearing", -1],
            ["what is the person wearing", -1],
            ["what is the man standing on", -1],
            ["what is the person  holding", 2],
            ["what gender is the person", 2]
        ],
        "context": [
            "a person holding a snowboard on a snowy surface",
            "a man riding skis down a snow covered slope."
        ]
    },
    {
        "object_category": "clock",
        "images": [
            {
                "VG_image_id": "2411999",
                "VG_object_id": "205127",
                "bbox": [135, 140, 321, 285],
                "image": "data\\images\\2411999.jpg"
            },
            {
                "VG_image_id": "2319401",
                "VG_object_id": "2723545",
                "bbox": [174, 97, 359, 259],
                "image": "data\\images\\2319401.jpg"
            }
        ],
        "questions_with_scores": [
            ["what time is it", 2],
            ["what color is the edge of the clock", 2],
            ["How many clocks are there", 1],
            ["where is the clock", 1],
            ["what is on the clock", 1],
            ["what color are the clock", 1]
        ],
        "org_questions": [
            ["what time is it", 2],
            ["what color is the edge of the clock", 2],
            ["what is the shape of the clock", -1],
            ["How many clocks are there", 1],
            ["where is the clock", 1],
            ["what is the clock made of", -1],
            ["what is on the clock", 1],
            ["when is this picture taken", -1],
            ["what time of day is it", -1],
            ["what shape is the clock", -1],
            ["what color are the clock", 1]
        ],
        "context": [
            "a green clock with a white face",
            "a clock hanging from the ceiling of a building."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2344363",
                "VG_object_id": "2264193",
                "bbox": [125, 172, 344, 323],
                "image": "data\\images\\2344363.jpg"
            },
            {
                "VG_image_id": "2334685",
                "VG_object_id": "2367181",
                "bbox": [91, 72, 289, 271],
                "image": "data\\images\\2334685.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color of the motorcycle", 1],
            ["where is the motorcycle", 1],
            ["how many people are in the picture", 1],
            ["what is in the background", 1],
            ["What is background of image", 1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the main color of the motorcycle", 1],
            ["where is the motorcycle", 1],
            ["how many people are in the picture", 1],
            ["what is in the background", 1],
            ["What is background of image", 1],
            ["how many bicycles are there", -1],
            ["when was the picture taken", -1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a couple sitting on a bench in front of a pink building.",
            "a person on a motorcycle on a race track."
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2322858",
                "VG_object_id": "2810631",
                "bbox": [77, 192, 498, 332],
                "image": "data\\images\\2322858.jpg"
            },
            {
                "VG_image_id": "2362825",
                "VG_object_id": "2277400",
                "bbox": [229, 125, 498, 292],
                "image": "data\\images\\2362825.jpg"
            }
        ],
        "questions_with_scores": [
            ["where was the photo taken", 1],
            ["how many boats are in the photo", 1]
        ],
        "org_questions": [
            ["what is in the water", -1],
            ["how many people are there", -1],
            ["what color is the boat", -1],
            ["what is the boat made of", -1],
            ["what is the boat doing", -1],
            ["what is on the boat", -1],
            ["what is the boat in", -1],
            ["where was the photo taken", 1],
            ["how many boats are in the photo", 1],
            ["where are the boats", -1],
            ["what color is the water", -1]
        ],
        "context": [
            "a group of elephants walking through a river.",
            "a woman sitting on a bench next to a body of water."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2357756",
                "VG_object_id": "2029000",
                "bbox": [184, 53, 410, 343],
                "image": "data\\images\\2357756.jpg"
            },
            {
                "VG_image_id": "2375046",
                "VG_object_id": "723920",
                "bbox": [38, 23, 246, 447],
                "image": "data\\images\\2375046.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man doing", 2],
            ["where is the man", 2],
            ["what is the man doing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", -1],
            ["What is man doing", 2],
            ["how many people are there", -1],
            ["where is the man", 2],
            ["what is the man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", -1],
            ["what is behind the man", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man in a white hat is making pizza.",
            "a man is standing near a pile of bananas."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2400813",
                "VG_object_id": "1154760",
                "bbox": [185, 369, 238, 485],
                "image": "data\\images\\2400813.jpg"
            },
            {
                "VG_image_id": "2389343",
                "VG_object_id": "505308",
                "bbox": [13, 242, 257, 379],
                "image": "data\\images\\2389343.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there", 2],
            ["what is beside the female child", 1],
            ["what color is the grass", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is beside the female child", 1],
            ["what is the child holding", -1],
            ["what color is the grass", 1],
            ["how many people are there", 1],
            ["What is child doing", -1],
            ["what are the children wearing", -1],
            ["when was the photo taken", -1],
            ["where are the people", -1],
            ["what are the people standing on", -1],
            ["how many children are there", 2]
        ],
        "context": [
            "a woman and child are flying kites on a hill.",
            "a group of children flying kites in a park."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2333155",
                "VG_object_id": "3489505",
                "bbox": [3, 119, 499, 372],
                "image": "data\\images\\2333155.jpg"
            },
            {
                "VG_image_id": "2359619",
                "VG_object_id": "2058421",
                "bbox": [7, 95, 489, 364],
                "image": "data\\images\\2359619.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animals are on the field", 2],
            ["what is the main color of the field", 1],
            ["what kind of animal is on the field", 1]
        ],
        "org_questions": [
            ["what is the main color of the field", 1],
            ["what animals are on the field", 2],
            ["how many people are there in the picture", -1],
            ["what is on the ground", -1],
            ["what kind of animal is on the field", 1],
            ["where was this photo taken", -1],
            ["when was the picture taken", -1],
            ["what is the weather like", -1],
            ["where are the animals", -1],
            ["what color is the grass", -1]
        ],
        "context": [
            "a cow and calf standing on a beach.",
            "a group of zebras are standing in a field."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2341317",
                "VG_object_id": "2625226",
                "bbox": [185, 112, 353, 318],
                "image": "data\\images\\2341317.jpg"
            },
            {
                "VG_image_id": "2370717",
                "VG_object_id": "2065507",
                "bbox": [69, 9, 281, 272],
                "image": "data\\images\\2370717.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the background", 2],
            ["when was the photo taken", 2],
            ["what color is the guy's shirt", 1],
            ["what is the guy doing", 1],
            ["what is the guy on", 1],
            ["What is guy doing", 1],
            ["where are the people", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the guy's shirt", 1],
            ["what is the guy doing", 1],
            ["what is the guy on", 1],
            ["how many people are there", -1],
            ["what is the man wearing on his face", -1],
            ["What is guy doing", 1],
            ["what color is the background", 2],
            ["when was the photo taken", 2],
            ["who is in the picture", -1],
            ["where are the people", 1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man sitting on a bench reading a book.",
            "a man riding a skateboard down a sidewalk."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2414567",
                "VG_object_id": "155985",
                "bbox": [392, 216, 451, 373],
                "image": "data\\images\\2414567.jpg"
            },
            {
                "VG_image_id": "2397022",
                "VG_object_id": "1193460",
                "bbox": [22, 19, 140, 315],
                "image": "data\\images\\2397022.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 1],
            ["What is man doing", 1],
            ["what kind of pants is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what color is the ground", -1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["What is man doing", 1],
            ["where is the man", -1],
            ["how many bikes are there", -1],
            ["what kind of pants is the man wearing", 1],
            ["when was the picture taken", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a group of elephants walking down a dirt road.",
            "a man holding a stop sign on a city street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2383926",
                "VG_object_id": "1315387",
                "bbox": [14, 182, 355, 499],
                "image": "data\\images\\2383926.jpg"
            },
            {
                "VG_image_id": "2339335",
                "VG_object_id": "2824305",
                "bbox": [2, 46, 137, 296],
                "image": "data\\images\\2339335.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many persons are there", 2],
            ["what color is the man's shirt", 2],
            ["what is the man wearing", 1],
            ["how many people are there in the picture", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["how many persons are there", 2],
            ["what color is the man's shirt", 2],
            ["what is the man doing", -1],
            ["what is the man wearing", 1],
            ["how many people are there in the picture", 1],
            ["when was this picture taken", -1],
            ["where are the people", 1],
            ["who is in the picture", -1],
            ["what is on the man's head", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man wearing a red jacket",
            "a group of police officers riding a motorcycle with a teddy bear on the side."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2346546",
                "VG_object_id": "3616552",
                "bbox": [73, 59, 331, 368],
                "image": "data\\images\\2346546.jpg"
            },
            {
                "VG_image_id": "2392333",
                "VG_object_id": "481041",
                "bbox": [82, 151, 146, 331],
                "image": "data\\images\\2392333.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the shorts of the man", 1],
            ["What is the man doing", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["what is the boy standing on", 1]
        ],
        "org_questions": [
            ["What color is the shorts of the man", 1],
            ["What is the man doing", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is on the man head", -1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is the boy standing on", 1],
            ["what is the gender of the person", -1]
        ],
        "context": [
            "a man on a surfboard riding a wave.",
            "a couple of people standing next to an elephant."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2403095",
                "VG_object_id": "1128047",
                "bbox": [42, 95, 281, 280],
                "image": "data\\images\\2403095.jpg"
            },
            {
                "VG_image_id": "2317042",
                "VG_object_id": "3717762",
                "bbox": [132, 58, 264, 314],
                "image": "data\\images\\2317042.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the player wearing on his head", 2],
            ["what color is the players's shirt", 2],
            ["what is the man wearing on the head", 1]
        ],
        "org_questions": [
            ["what is the player wearing on his head", 2],
            ["what color is the players's shirt", 2],
            ["what is in the background", -1],
            ["What is the person holding", -1],
            ["which two color is the field", -1],
            ["What color is the ground", -1],
            ["what is the man wearing on the head", 1],
            ["when was the picture taken", -1],
            ["where is the man", -1],
            ["what kind of shoes is the man wearing", -1],
            ["what sport is this", -1]
        ],
        "context": [
            "a man running to hit a tennis ball with a racket.",
            "a man hitting a tennis ball with a racquet."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2376989",
                "VG_object_id": "2702218",
                "bbox": [0, 97, 106, 236],
                "image": "data\\images\\2376989.jpg"
            },
            {
                "VG_image_id": "2392643",
                "VG_object_id": "1227535",
                "bbox": [1, 18, 499, 374],
                "image": "data\\images\\2392643.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 1],
            ["what is in front of the car", 1],
            ["what is behind the car", 1],
            ["what is on the side of the car", 1],
            ["what is in the distance", 1],
            ["where was this photo taken", 1],
            ["what type of vehicle is shown", 1]
        ],
        "org_questions": [
            ["what color is the car", 1],
            ["what is in front of the car", 1],
            ["what is behind the car", 1],
            ["how many cars are there", -1],
            ["what time is it", -1],
            ["which part of the car can we see in the picture", -1],
            ["what is on the side of the car", 1],
            ["what is in the distance", 1],
            ["where was this photo taken", 1],
            ["what type of vehicle is shown", 1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a man in a hat and glasses riding a bike.",
            "an old truck is sitting in a field of flowers."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2368135",
                "VG_object_id": "2179537",
                "bbox": [0, 16, 498, 459],
                "image": "data\\images\\2368135.jpg"
            },
            {
                "VG_image_id": "2364320",
                "VG_object_id": "2219172",
                "bbox": [4, 2, 497, 373],
                "image": "data\\images\\2364320.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what color is the plate on the table", 2],
            ["how many plates are there on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what color is the plate on the table", 2],
            ["how many plates are there on the table", 1],
            ["what shape is the table", -1],
            ["what is the table made of", -1],
            ["what is on the table", -1],
            ["how many people are there", -1],
            ["where was this photo taken", -1],
            ["what is the pizza sitting on", -1],
            ["where is the pizza", -1],
            ["what is under the pizza", -1]
        ],
        "context": [
            "a man cutting a pizza with a pizza cutter.",
            "a pizza on a plate"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2318452",
                "VG_object_id": "2886422",
                "bbox": [0, 19, 499, 297],
                "image": "data\\images\\2318452.jpg"
            },
            {
                "VG_image_id": "2349557",
                "VG_object_id": "3028317",
                "bbox": [2, 2, 499, 374],
                "image": "data\\images\\2349557.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the plate made of", 2],
            ["where was this photo taken", 2],
            ["what is the food on", 1],
            ["what is next to the plate", 1]
        ],
        "org_questions": [
            ["where is the food", -1],
            ["how many people are there", -1],
            ["how many tables are there", -1],
            ["where is the table", -1],
            ["what is the plate made of", 2],
            ["what is the food on", 1],
            ["where was this photo taken", 2],
            ["what is next to the plate", 1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a plate with a sandwich and a bottle of wine.",
            "a grill with hot dogs and a tinfoil."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2366927",
                "VG_object_id": "2006682",
                "bbox": [223, 88, 303, 310],
                "image": "data\\images\\2366927.jpg"
            },
            {
                "VG_image_id": "2409211",
                "VG_object_id": "246559",
                "bbox": [105, 3, 285, 285],
                "image": "data\\images\\2409211.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["where is the photo taken", 1],
            ["what is the girl wearing on the head", 1],
            ["what is the ground covered with", 1],
            ["what is the woman on", 1],
            ["where is the woman", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what is the girl holding", -1],
            ["where is the photo taken", 1],
            ["What is the color of the girl's hair", -1],
            ["what is the girl wearing on the head", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["what is the woman on", 1],
            ["what is the girl wearing", -1],
            ["where is the woman", 1]
        ],
        "context": [
            "a woman standing next to a horse in a field.",
            "a little girl riding on the back of a brown horse."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2368879",
                "VG_object_id": "2345359",
                "bbox": [201, 29, 430, 266],
                "image": "data\\images\\2368879.jpg"
            },
            {
                "VG_image_id": "2325685",
                "VG_object_id": "985492",
                "bbox": [102, 172, 255, 499],
                "image": "data\\images\\2325685.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's shirt", 1],
            ["what is the girl wearing on the head", 1],
            ["how many people are there", 1],
            ["what is the child doing", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", 1],
            ["what is the girl wearing on the head", 1],
            ["where is the girl", -1],
            ["how many people are there", 1],
            ["how old is the girl", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the child doing", 1]
        ],
        "context": [
            "two young children sit on top of an elephant statue.",
            "a young girl is playing with a kite on the beach."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2383566",
                "VG_object_id": "531670",
                "bbox": [40, 179, 262, 340],
                "image": "data\\images\\2383566.jpg"
            },
            {
                "VG_image_id": "2404699",
                "VG_object_id": "339234",
                "bbox": [15, 141, 131, 218],
                "image": "data\\images\\2404699.jpg"
            }
        ],
        "questions_with_scores": [["what color is the screen", 1]],
        "org_questions": [
            ["how many laptops are there", -1],
            ["how many keyboards are there", -1],
            ["what color is the screen", 1],
            ["what is next to the computer", -1],
            ["how many people are there in the picture", -1],
            ["what color is the table under the screen", -1],
            ["what is on the screen", -1],
            ["what is the color of the laptop", -1],
            ["what color are the computers", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a wooden desk.",
            "a desk with three computers and a monitor on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2355582",
                "VG_object_id": "2705218",
                "bbox": [293, 105, 431, 395],
                "image": "data\\images\\2355582.jpg"
            },
            {
                "VG_image_id": "2355919",
                "VG_object_id": "2045396",
                "bbox": [111, 33, 488, 331],
                "image": "data\\images\\2355919.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the woman's umbrella", 2],
            ["What color is woman's shirt", 2],
            ["what gesture is the woman", 1],
            ["what is on the woman's head", 1],
            ["why is the woman holding a umbrella", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What color is the woman's umbrella", 2],
            ["What color is woman's shirt", 2],
            ["when is this picture taken", -1],
            ["What is woman doing", -1],
            ["where is the woman", -1],
            ["what is the woman holding", -1],
            ["what gesture is the woman", 1],
            ["who is holding the umbrella", -1],
            ["what is on the woman's head", 1],
            ["why is the woman holding a umbrella", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a group of people walking down a street holding umbrellas.",
            "a woman wearing sunglasses holding a yellow umbrella."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2349013",
                "VG_object_id": "2552837",
                "bbox": [80, 178, 222, 330],
                "image": "data\\images\\2349013.jpg"
            },
            {
                "VG_image_id": "2375505",
                "VG_object_id": "2030169",
                "bbox": [225, 28, 355, 331],
                "image": "data\\images\\2375505.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What color is person's shirt", 1],
            ["WHat sports is person doing", 1],
            ["What is person holding", 1],
            ["what is on the player's head", 1],
            ["what game is being played", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What color is person's shirt", 1],
            ["WHat sports is person doing", 1],
            ["what is in the background", -1],
            ["where is the person", -1],
            ["what is the man wearing", -1],
            ["What is person holding", 1],
            ["what is on the player's head", 1],
            ["what game is being played", 1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a man holding a tennis racket on a tennis court."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2338946",
                "VG_object_id": "952761",
                "bbox": [4, 173, 500, 365],
                "image": "data\\images\\2338946.jpg"
            },
            {
                "VG_image_id": "2318262",
                "VG_object_id": "1012494",
                "bbox": [0, 51, 500, 373],
                "image": "data\\images\\2318262.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["what is the table made of", 1],
            ["how many people are there in the picture", 1],
            ["where is the table", 1],
            ["how many plates are there on the table", 1],
            ["what kind of food is on the table", 1],
            ["what is the food on", 1],
            ["where was this picture taken", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what is on the plate", 1],
            ["where is the picture taken", 1],
            ["what is sitting on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 1],
            ["what is the table made of", 1],
            ["how many people are there in the picture", 1],
            ["what is the shape of the table", -1],
            ["where is the table", 1],
            ["how many plates are there on the table", 1],
            ["what kind of food is on the table", 1],
            ["what is the food on", 1],
            ["where was this picture taken", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what is on the plate", 1],
            ["where is the picture taken", 1],
            ["what is sitting on the table", 1]
        ],
        "context": [
            "a woman standing in a kitchen preparing food.",
            "two plates with desserts on them on a table."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2383607",
                "VG_object_id": "531383",
                "bbox": [4, 248, 371, 499],
                "image": "data\\images\\2383607.jpg"
            },
            {
                "VG_image_id": "2378181",
                "VG_object_id": "561568",
                "bbox": [39, 197, 384, 396],
                "image": "data\\images\\2378181.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's tie", 2],
            ["What color is man's shirt", 1],
            ["What is the background of image", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", 1],
            ["What color is man's tie", 2],
            ["What is the background of image", 1],
            ["what gender is the person in the shirt", -1],
            ["where is the picture taken", -1],
            ["What is man doing", -1],
            ["What is the gender of person", -1],
            ["where is the man", -1],
            ["how many people are there", -1],
            ["what is on the man's face", 1],
            ["what type of shirt is the man wearing", -1],
            ["what is around the man's neck", -1]
        ],
        "context": [
            "a man wearing a tie and glasses smiling.",
            "a man wearing a hat and tie with a tie around his neck."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2414365",
                "VG_object_id": "296004",
                "bbox": [245, 83, 398, 281],
                "image": "data\\images\\2414365.jpg"
            },
            {
                "VG_image_id": "285850",
                "VG_object_id": "1569946",
                "bbox": [189, 230, 897, 613],
                "image": "data\\images\\285850.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["what is the woman wearing", 1],
            ["How many people are there", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["where is the woman", -1],
            ["what is the woman wearing", 1],
            ["How many people are there", 1],
            ["What is woman holding", -1],
            ["What color is woman's hair", -1],
            ["what is the woman holding", 1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is the woman looking at", -1]
        ],
        "context": [
            "a man and woman sitting at a table with laptops.",
            "a woman laying on a bed with a laptop."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2395232",
                "VG_object_id": "1206877",
                "bbox": [69, 95, 181, 214],
                "image": "data\\images\\2395232.jpg"
            },
            {
                "VG_image_id": "2367610",
                "VG_object_id": "2089623",
                "bbox": [205, 130, 301, 292],
                "image": "data\\images\\2367610.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the boy wearing", 2],
            ["what is the man on", 2],
            ["what is the man doing", 1],
            ["what is the race of the man", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the man wearing on the head", -1],
            ["what is the race of the man", 1],
            ["who is in the photo", -1],
            ["when was the picture taken", -1],
            ["what is the man holding", 1],
            ["what is the boy wearing", 2],
            ["what is the man on", 2]
        ],
        "context": [
            "two men in a boat with a bull in it.",
            "a man riding a wave on a surfboard."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2411794",
                "VG_object_id": "210135",
                "bbox": [29, 126, 247, 451],
                "image": "data\\images\\2411794.jpg"
            },
            {
                "VG_image_id": "2362196",
                "VG_object_id": "776412",
                "bbox": [27, 24, 330, 322],
                "image": "data\\images\\2362196.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 2],
            ["what is the dog sitting on", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the dog", 2],
            ["what is the dog sitting on", 1],
            ["what color is the ground", -1],
            ["how many people are there", -1],
            ["where is the dog", -1],
            ["what is the dog wearing", -1],
            ["what is in the background", -1],
            ["what is the ground covered with", 1],
            ["what kind of animal is this", -1],
            ["when was this photo taken", -1],
            ["what is the dog doing", -1],
            ["what is the dog holding", -1]
        ],
        "context": [
            "a white dog sitting on a bench with a leash.",
            "a dog laying in the grass with a frisbee."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2394002",
                "VG_object_id": "467834",
                "bbox": [222, 160, 434, 330],
                "image": "data\\images\\2394002.jpg"
            },
            {
                "VG_image_id": "2348932",
                "VG_object_id": "2683321",
                "bbox": [169, 57, 426, 331],
                "image": "data\\images\\2348932.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["how many girls are there", 1],
            ["how many people are there", 1],
            ["how many children are there", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", -1],
            ["what is the girl doing", 1],
            ["how many girls are there", 1],
            ["what is the gender of the person", -1],
            ["how many people are there", 1],
            ["WHat color is girls shirt", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["where is the girl", -1],
            ["what is on the girl's head", -1],
            ["how many children are there", 1],
            ["who has a long hair", -1],
            ["what is the child wearing", -1],
            ["how many people are in the photo", 1]
        ],
        "context": [
            "a group of young children eating cake at a table.",
            "a little girl holding a teddy bear in her hand."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2398133",
                "VG_object_id": "1182497",
                "bbox": [170, 63, 303, 143],
                "image": "data\\images\\2398133.jpg"
            },
            {
                "VG_image_id": "2362842",
                "VG_object_id": "2074126",
                "bbox": [154, 219, 225, 304],
                "image": "data\\images\\2362842.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["What is the man wearing on his head", 2],
            ["what is the person holding", 1],
            ["what is the man doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person holding", 1],
            ["what is the man doing", 1],
            ["How many people are there", -1],
            ["What is the man wearing on his head", 2],
            ["how is the weather", -1],
            ["What is the gender of person", -1],
            ["when was this photo taken", -1],
            ["what is the man wearing", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a boy doing a trick on a skateboard at a skate park.",
            "a group of children standing on a baseball field."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2357681",
                "VG_object_id": "1710307",
                "bbox": [38, 46, 470, 331],
                "image": "data\\images\\2357681.jpg"
            },
            {
                "VG_image_id": "2350114",
                "VG_object_id": "2100051",
                "bbox": [57, 195, 359, 306],
                "image": "data\\images\\2350114.jpg"
            }
        ],
        "questions_with_scores": [["what color is the bus", 2]],
        "org_questions": [
            ["what color is the bus", 2],
            ["what color is the ground", -1],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["when is the picture taken", -1],
            ["where is the truck", -1],
            ["what is the ground covered with", -1],
            ["how many buses are in the picture", -1],
            ["what is the bus doing", -1],
            ["who is in the photo", -1],
            ["what kind of bus is this", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a double decker bus driving down a street.",
            "a blue bus is parked at a bus stop."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2370613",
                "VG_object_id": "2249886",
                "bbox": [2, 149, 497, 331],
                "image": "data\\images\\2370613.jpg"
            },
            {
                "VG_image_id": "2372645",
                "VG_object_id": "3732467",
                "bbox": [1, 151, 499, 331],
                "image": "data\\images\\2372645.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is on the land", 1],
            ["What is the background of image", 1],
            ["how many people are there on the land", 1],
            ["where is the picture taken", 1],
            ["what is the weather like", 1],
            ["what is the ground covered with", 1],
            ["what is in the background", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["What color is the land", -1],
            ["What is on the land", 1],
            ["What is the background of image", 1],
            ["how many people are there on the land", 1],
            ["where is the picture taken", 1],
            ["what time is it", -1],
            ["what is the weather like", 1],
            ["what is the ground covered with", 1],
            ["where is the grass", -1],
            ["what is in the background", 1],
            ["how is the weather", 1]
        ],
        "context": [
            "a large passenger jet on a runway with a flag on it.",
            "a red and white food truck parked on gravel."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2333848",
                "VG_object_id": "2765708",
                "bbox": [270, 65, 389, 300],
                "image": "data\\images\\2333848.jpg"
            },
            {
                "VG_image_id": "2365348",
                "VG_object_id": "634850",
                "bbox": [179, 62, 299, 323],
                "image": "data\\images\\2365348.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the ground the man standing on made of", 2],
            ["where is the man", 2],
            ["what is the man holding", 1],
            ["what is the persion on the right doing", 1],
            ["what is on the man's head", 1],
            ["what is the man standing on", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is the man holding", 1],
            ["what is the man wearing", -1],
            ["what is the ground the man standing on made of", 2],
            ["how many people are there", -1],
            ["when is this photo taken", -1],
            ["where is the man", 2],
            ["what color are the man's pants", -1],
            ["what is the weather like", -1],
            ["who is in the photo", -1],
            ["what is the persion on the right doing", 1],
            ["what is on the man's head", 1],
            ["what is the man standing on", 1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "a man sitting on a bench with a skateboard.",
            "a man standing in a field with a accordion."
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2411933",
                "VG_object_id": "206599",
                "bbox": [79, 163, 332, 264],
                "image": "data\\images\\2411933.jpg"
            },
            {
                "VG_image_id": "2408541",
                "VG_object_id": "259698",
                "bbox": [30, 140, 460, 343],
                "image": "data\\images\\2408541.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["how many laptops are there", 1],
            ["how many screens are there on the desk", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", -1],
            ["what color is the ground", 1],
            ["how many laptops are there", 1],
            ["what is the table made of", -1],
            ["how many screens are there on the desk", 1],
            ["where was this picture taken", -1],
            ["where are the chairs", -1],
            ["what is next to the desk", -1],
            ["what is sitting on the desk", -1]
        ],
        "context": [
            "a red office chair",
            "a desk with a pizza box and a tv on it"
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2375372",
                "VG_object_id": "2328940",
                "bbox": [112, 7, 417, 297],
                "image": "data\\images\\2375372.jpg"
            },
            {
                "VG_image_id": "2369807",
                "VG_object_id": "2121918",
                "bbox": [165, 88, 358, 315],
                "image": "data\\images\\2369807.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the bear", 2],
            ["Where is the bear", 2],
            ["What color is the label of the bear", 1]
        ],
        "org_questions": [
            ["What color is the bear", 2],
            ["Where is the bear", 2],
            ["how many bears are there", -1],
            ["what is on the bear's neck", -1],
            ["what is in the distance", -1],
            ["What color is the label of the bear", 1],
            ["when was the picture taken", -1],
            ["what type of animal is shown", -1],
            ["what is the bear eating", -1],
            ["what is behind the bear", -1]
        ],
        "context": [
            "a bear is standing next to a tree.",
            "a bear walking through a lush green field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2394818",
                "VG_object_id": "460723",
                "bbox": [310, 124, 370, 163],
                "image": "data\\images\\2394818.jpg"
            },
            {
                "VG_image_id": "2404670",
                "VG_object_id": "3814863",
                "bbox": [82, 101, 183, 174],
                "image": "data\\images\\2404670.jpg"
            }
        ],
        "questions_with_scores": [
            ["what number is on the man's shirt", 2],
            ["what are the people doing", 1],
            ["what are the people playing", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what are the people doing", 1],
            ["what are the people playing", 1],
            ["what gender is the person in the shirt", -1],
            ["what color is the shirt", -1],
            ["where is the person", -1],
            ["what is the man standing on", -1],
            ["what is the man wearing", -1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["what number is on the man's shirt", 2],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a group of people playing frisbee in a field."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2344699",
                "VG_object_id": "2074001",
                "bbox": [40, 1, 408, 203],
                "image": "data\\images\\2344699.jpg"
            },
            {
                "VG_image_id": "2342314",
                "VG_object_id": "935149",
                "bbox": [367, 84, 499, 181],
                "image": "data\\images\\2342314.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the truck", 2],
            ["what is the color of the building", 1],
            ["how many people are there", 1],
            ["what is at the middle", 1]
        ],
        "org_questions": [
            ["what is the color of the building", 1],
            ["what is the color of the truck", 2],
            ["how many people are there", 1],
            ["What is in the center of image", -1],
            ["where is the building", -1],
            ["what is the weather like", -1],
            ["what is in the background", -1],
            ["what is at the middle", 1],
            ["when was the picture taken", -1],
            ["what is the building made of", -1],
            ["what is on the back of the truck", -1]
        ],
        "context": [
            "a man riding a bike down a street next to a truck.",
            "a red food truck with people standing outside"
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2359106",
                "VG_object_id": "3540135",
                "bbox": [271, 115, 388, 203],
                "image": "data\\images\\2359106.jpg"
            },
            {
                "VG_image_id": "2334223",
                "VG_object_id": "3446220",
                "bbox": [332, 113, 393, 273],
                "image": "data\\images\\2334223.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the person's head", 2],
            ["what is on the man's feet", 2],
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["how many people are there in the picture", 1],
            ["WHat color is person's shirt", 1],
            ["what is the ground covered with", 1],
            ["what is the person wearing", 1],
            ["what is the man holding", 1],
            ["what is the man on", 1]
        ],
        "org_questions": [
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["how many people are there in the picture", 1],
            ["what is on the person's head", 2],
            ["WHat color is person's shirt", 1],
            ["what is the ground covered with", 1],
            ["what is the person wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is the man holding", 1],
            ["what is the man on", 1],
            ["what is on the man's feet", 2]
        ],
        "context": [
            "a group of skateboarders riding down a road.",
            "a group of people walking on a beach next to the ocean."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2378152",
                "VG_object_id": "1801323",
                "bbox": [85, 22, 429, 367],
                "image": "data\\images\\2378152.jpg"
            },
            {
                "VG_image_id": "2321839",
                "VG_object_id": "3141209",
                "bbox": [3, 34, 263, 374],
                "image": "data\\images\\2321839.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what is the woman wearing on her face", 2],
            ["what color is the woman's clothes", 1],
            ["How many people are there", 1],
            ["what is the woman doing", 1],
            ["what is behind the woman", 1],
            ["what is on the woman's face", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", 1],
            ["what is the woman holding", 2],
            ["what is the woman wearing on her face", 2],
            ["How many people are there", 1],
            ["Where is the girl", -1],
            ["what is the woman doing", 1],
            ["what is the woman wearing", -1],
            ["who is in the photo", -1],
            ["what is in the background", -1],
            ["what is behind the woman", 1],
            ["what is on the woman's face", 1]
        ],
        "context": [
            "a woman holding a cell phone in her hand.",
            "two women sitting at a table eating food."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2403954",
                "VG_object_id": "1119972",
                "bbox": [99, 405, 272, 463],
                "image": "data\\images\\2403954.jpg"
            },
            {
                "VG_image_id": "2404609",
                "VG_object_id": "376620",
                "bbox": [133, 117, 442, 259],
                "image": "data\\images\\2404609.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["what color is the food on the plate", 1],
            ["what shape of plate is the food put on", 1],
            ["what type of food is on the plate", 1],
            ["what food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["what color is the food on the plate", 1],
            ["how many plates are there", -1],
            ["where is the food", -1],
            ["what is on the plate", -1],
            ["what shape of plate is the food put on", 1],
            ["what is the food on", -1],
            ["what type of food is on the plate", 1],
            ["what food is on the plate", 1]
        ],
        "context": [
            "a man sitting at a table with a plate of food.",
            "a person is cutting a pizza with a knife and fork."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2382865",
                "VG_object_id": "536730",
                "bbox": [10, 7, 464, 266],
                "image": "data\\images\\2382865.jpg"
            },
            {
                "VG_image_id": "2337274",
                "VG_object_id": "958355",
                "bbox": [222, 286, 428, 371],
                "image": "data\\images\\2337274.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the table", 1],
            ["how many plates are in the picture", 1],
            ["what kind of food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["how many plates are in the picture", 1],
            ["what shape is the plate", -1],
            ["where is the plate", -1],
            ["what is on the plate", -1],
            ["how many people are there", 2],
            ["what is the table made of", -1],
            ["what is the plate on", -1],
            ["what kind of food is on the plate", 1],
            ["what is next to the plate", -1],
            ["what is the plate made of", -1]
        ],
        "context": [
            "a plate with a sandwich and salad on it.",
            "a man and a child sitting at a table with food."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2350680",
                "VG_object_id": "1941924",
                "bbox": [101, 277, 168, 354],
                "image": "data\\images\\2350680.jpg"
            },
            {
                "VG_image_id": "2337232",
                "VG_object_id": "2938693",
                "bbox": [412, 222, 498, 334],
                "image": "data\\images\\2337232.jpg"
            }
        ],
        "questions_with_scores": [["what is on the table", 1]],
        "org_questions": [
            ["what shape is the table", -1],
            ["where is the table", -1],
            ["how many keyboards are there on the table", -1],
            ["What is the table made of", -1],
            ["how many people are there in the picture", -1],
            ["what is in the room", -1],
            ["what is made of wood", -1],
            ["what is brown", -1],
            ["what is next to the table", -1],
            ["what is on the table", 1]
        ],
        "context": [
            "a bathroom with a bathtub, a toilet and a sink.",
            "a bedroom with a bed, lamps, and a lamp."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "713188",
                "VG_object_id": "1580127",
                "bbox": [64, 327, 281, 580],
                "image": "data\\images\\713188.jpg"
            },
            {
                "VG_image_id": "2327926",
                "VG_object_id": "2818352",
                "bbox": [401, 58, 469, 148],
                "image": "data\\images\\2327926.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 2],
            ["what color is the shirt", 1],
            ["what gender is the person wearing the shirt", 1],
            ["where is the picture taken", 1],
            ["what is the persion doing", 1],
            ["where is the person", 1],
            ["what is the gender of the person", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what gender is the person wearing the shirt", 1],
            ["where is the picture taken", 1],
            ["how many people are there", -1],
            ["what is the persion doing", 1],
            ["what is the person holding", 2],
            ["where is the person", 1],
            ["what is the gender of the person", 1],
            ["when was the photo taken", -1],
            ["what are the people wearing", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a woman and her dog on a surfboard.",
            "a couple of motorcycles parked next to each other."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2338001",
                "VG_object_id": "2176661",
                "bbox": [316, 217, 421, 330],
                "image": "data\\images\\2338001.jpg"
            },
            {
                "VG_image_id": "2330759",
                "VG_object_id": "3112106",
                "bbox": [183, 202, 375, 287],
                "image": "data\\images\\2330759.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What is on the table ", 1],
            ["What is next to the table", 1],
            ["what color is the table", 1]
        ],
        "org_questions": [
            ["What is on the table ", 1],
            ["What color is the floor", -1],
            ["What is next to the table", 1],
            ["How many people are there", 2],
            ["what is the table made of", -1],
            ["how many plates are there on the table", -1],
            ["what color is the background", -1],
            ["what color is the table", 1],
            ["where is the picture taken", -1],
            ["what kind of flooring is this", -1],
            ["what is the floor made of", -1]
        ],
        "context": [
            "a hotel room with a bed, mirror, and a desk.",
            "a group of people sitting on a couch."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2377308",
                "VG_object_id": "1670553",
                "bbox": [417, 119, 498, 374],
                "image": "data\\images\\2377308.jpg"
            },
            {
                "VG_image_id": "2325815",
                "VG_object_id": "2925131",
                "bbox": [6, 24, 98, 151],
                "image": "data\\images\\2325815.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["where is the photo taken", 2],
            ["What is man doing", 1],
            ["what color is the background", 1],
            ["what is behind the person", 1],
            ["who is wearing glasses", 1],
            ["what is on the man's face", 1],
            ["what is the persion wearing", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["how many people are there", 2],
            ["where is the photo taken", 2],
            ["What is man doing", 1],
            ["what color is the background", 1],
            ["what is behind the person", 1],
            ["what is the man wearing on the head", -1],
            ["who is wearing glasses", 1],
            ["what is on the man's face", 1],
            ["what is the persion wearing", 1],
            ["where are the people", 1]
        ],
        "context": [
            "a bus with a tv on the top of it",
            "a group of women sitting at a table."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2342120",
                "VG_object_id": "2055065",
                "bbox": [276, 82, 435, 304],
                "image": "data\\images\\2342120.jpg"
            },
            {
                "VG_image_id": "2363126",
                "VG_object_id": "2028388",
                "bbox": [96, 133, 229, 318],
                "image": "data\\images\\2363126.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color child's shirt", 2],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["What color child's shirt", 2],
            ["What color is child's shoe", -1],
            ["how many people are there", -1],
            ["what gender is the child", -1],
            ["what are the children doing", -1],
            ["what is child doing", -1],
            ["what color is the background", 1],
            ["when was the photo taken", -1],
            ["where is the skateboard", -1],
            ["who is skateboarding", -1],
            ["what is the man riding on", -1],
            ["what is the boy riding", -1]
        ],
        "context": [
            "a young man riding a skateboard down a ramp.",
            "a boy is skateboarding on a sidewalk."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "498264",
                "VG_object_id": "3698914",
                "bbox": [214, 0, 1020, 545],
                "image": "data\\images\\498264.jpg"
            },
            {
                "VG_image_id": "2336021",
                "VG_object_id": "3504773",
                "bbox": [0, 35, 498, 273],
                "image": "data\\images\\2336021.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many floors does the building have", 1],
            ["what shape is the building", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["what time is it", -1],
            ["how many floors does the building have", 1],
            ["What is on the ground", -1],
            ["what shape is the building", 1],
            ["Where is the building", -1],
            ["what is the building made of", -1],
            ["what kind of vehicle is this", -1],
            ["what is the weather like", -1],
            ["when was this photo taken", -1],
            ["where was this photo taken", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a police van is parked on the side of the road",
            "a group of trucks parked in front of a building."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2370906",
                "VG_object_id": "3856033",
                "bbox": [22, 219, 371, 305],
                "image": "data\\images\\2370906.jpg"
            },
            {
                "VG_image_id": "2375447",
                "VG_object_id": "1895991",
                "bbox": [30, 59, 499, 291],
                "image": "data\\images\\2375447.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are sitting on the bench", 2],
            ["what color is the bench", 2],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["Where is the bench", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["how many people are sitting on the bench", 2],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["what is the persion wearing on her head", -1],
            ["what color is the bench", 2],
            ["Where is the bench", 1],
            ["what is the weather like", -1],
            ["what is on the bench", -1],
            ["what is the persion doing", -1],
            ["what is the persion sitting on", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a couple of people sitting on top of a wooden bench.",
            "a person sitting on a bench with a blanket on it."
        ]
    },
    {
        "object_category": "desk",
        "images": [
            {
                "VG_image_id": "2353887",
                "VG_object_id": "1778639",
                "bbox": [14, 168, 374, 454],
                "image": "data\\images\\2353887.jpg"
            },
            {
                "VG_image_id": "2334596",
                "VG_object_id": "2954822",
                "bbox": [0, 201, 465, 373],
                "image": "data\\images\\2334596.jpg"
            }
        ],
        "questions_with_scores": [["what color is the ground", 1]],
        "org_questions": [
            ["what color is the desktop", -1],
            ["what color is the ground", 1],
            ["how many computers are in the picture", -1],
            ["what is on the desk", -1],
            ["what is the table made of", -1],
            ["what is the main color of the table", -1],
            ["what color is the table", -1],
            ["where is this scene", -1],
            ["what room is this", -1],
            ["where is the computer", -1],
            ["what is next to the desk", -1],
            ["what is under the desk", -1]
        ],
        "context": [
            "a computer desk with a computer monitor and keyboard.",
            "a desk with a computer and a lamp on it."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2393062",
                "VG_object_id": "475111",
                "bbox": [9, 396, 347, 498],
                "image": "data\\images\\2393062.jpg"
            },
            {
                "VG_image_id": "2380520",
                "VG_object_id": "1345681",
                "bbox": [170, 306, 386, 374],
                "image": "data\\images\\2380520.jpg"
            }
        ],
        "questions_with_scores": [
            ["what room is it", 2],
            ["what room is the floor in", 1],
            ["what color is the sink", 1],
            ["What color is the floor", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["what is the floor made of", -1],
            ["what room is the floor in", 1],
            ["what is on the floor", -1],
            ["how many people are standing on the floor", -1],
            ["what color is the sink", 1],
            ["where is the floor", -1],
            ["where is the picture taken", -1],
            ["What color is the floor", 1],
            ["what is the flooring", -1],
            ["what kind of flooring is this", -1],
            ["what room is this", 1],
            ["what is covering the floor", -1],
            ["what room is it", 2]
        ],
        "context": [
            "a kitchen with a black refrigerator and a microwave.",
            "a bathroom with a sink, toilet, and shower."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2349047",
                "VG_object_id": "2051721",
                "bbox": [126, 179, 190, 252],
                "image": "data\\images\\2349047.jpg"
            },
            {
                "VG_image_id": "2380767",
                "VG_object_id": "1342597",
                "bbox": [157, 198, 305, 332],
                "image": "data\\images\\2380767.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["where is the picture taken", 1],
            ["Where is the person", 1],
            ["what are the people doing", 1],
            ["where is the person", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["who is wearing the shirt", -1],
            ["what is the person doing", 1],
            ["where is the picture taken", 1],
            ["how many people are there", -1],
            ["what is the gender of the person", -1],
            ["Where is the person", 1],
            ["what is the persion wearing", -1],
            ["who is in the picture", -1],
            ["what is the main color of the shirt", -1],
            ["what are the people doing", 1],
            ["where is the person", 1],
            ["what is the person wearing", -1],
            ["what is the shirt color", -1]
        ],
        "context": [
            "a man holding two tennis rackets in a room.",
            "a man in a red shirt and a yellow frisbee."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2372854",
                "VG_object_id": "3730870",
                "bbox": [186, 335, 236, 468],
                "image": "data\\images\\2372854.jpg"
            },
            {
                "VG_image_id": "2334009",
                "VG_object_id": "3201821",
                "bbox": [33, 94, 253, 239],
                "image": "data\\images\\2334009.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["What is child doing", 2],
            ["where is the child", 1],
            ["what is the person wearing", 1],
            ["what is the boy holding", 1],
            ["what is the boy doing", 1]
        ],
        "org_questions": [
            ["How many people are there", 2],
            ["What is child doing", 2],
            ["where is the child", 1],
            ["what is the person wearing", 1],
            ["what is the boy holding", 1],
            ["what is the boy doing", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["what is on the ground", -1],
            ["what is the color of the grass", -1]
        ],
        "context": [
            "a man and a child holding a kite in front of flags.",
            "a boy is playing soccer on the field."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2320112",
                "VG_object_id": "2918504",
                "bbox": [146, 117, 383, 191],
                "image": "data\\images\\2320112.jpg"
            },
            {
                "VG_image_id": "2332051",
                "VG_object_id": "2968331",
                "bbox": [140, 202, 210, 284],
                "image": "data\\images\\2332051.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many planes are there", 2],
            ["what color is the plane", 2],
            ["what is the main color of the airplane", 1],
            ["how many planes", 1]
        ],
        "org_questions": [
            ["how many planes are there", 2],
            ["what color is the plane", 2],
            ["when is the picture taken", -1],
            ["where is the plane", -1],
            ["what color is the sky", -1],
            ["what is the main color of the airplane", 1],
            ["what is flying in the sky", -1],
            ["what are the planes doing", -1],
            ["how many planes", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "two blue and yellow airplanes flying in formation.",
            "a plane flying in the sky with smoke coming out of it."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2387646",
                "VG_object_id": "511934",
                "bbox": [1, 3, 497, 373],
                "image": "data\\images\\2387646.jpg"
            },
            {
                "VG_image_id": "2330679",
                "VG_object_id": "3719581",
                "bbox": [13, 24, 482, 361],
                "image": "data\\images\\2330679.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 2],
            ["what is on the counter", 1],
            ["what color is the counter", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the wall", 2],
            ["what is on the counter", 1],
            ["what color is the counter", 1],
            ["what is the counter made of", -1],
            ["what is on the wall", -1],
            ["what color are the cabinet", -1],
            ["where was this taken", -1],
            ["what room is this", -1],
            ["who is in the photo", 1],
            ["what is in the kitchen", -1]
        ],
        "context": [
            "a young boy standing in a kitchen with an oven.",
            "a kitchen with a sink, microwave, and other items on the counter."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2331009",
                "VG_object_id": "3356353",
                "bbox": [88, 36, 334, 321],
                "image": "data\\images\\2331009.jpg"
            },
            {
                "VG_image_id": "2358542",
                "VG_object_id": "3768359",
                "bbox": [0, 166, 275, 392],
                "image": "data\\images\\2358542.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is child doing", 2],
            ["what are the children doing", 2],
            ["What is the gender of child", 1],
            ["What is child holding", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["What is the gender of child", 1],
            ["What is child doing", 2],
            ["What is child holding", 1],
            ["how many people are there", -1],
            ["what is the child on", -1],
            ["what are the children doing", 2],
            ["where is the picture taken", -1],
            ["who is in the photo", 1],
            ["where is the baby looking", -1]
        ],
        "context": [
            "a child sitting in a high chair eating cake.",
            "a child sleeping on a bed with stuffed animals."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2320463",
                "VG_object_id": "3036525",
                "bbox": [7, 10, 487, 490],
                "image": "data\\images\\2320463.jpg"
            },
            {
                "VG_image_id": "2352051",
                "VG_object_id": "1740384",
                "bbox": [0, 0, 372, 498],
                "image": "data\\images\\2352051.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 1],
            ["What is on the table", 1],
            ["what is on the wall", 1],
            ["what color are the cabinet", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["What is on the table", 1],
            ["what is on the wall", 1],
            ["what is the color of the wall", -1],
            ["what color are the cabinet", 1],
            ["where was this taken", -1],
            ["how many people are in the photo", -1],
            ["what room is this", -1],
            ["who is in the photo", -1],
            ["what is in the kitchen", -1]
        ],
        "context": [
            "a man making pizzas in a restaurant kitchen",
            "a person laying on the floor in a kitchen"
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2380130",
                "VG_object_id": "1350250",
                "bbox": [252, 308, 353, 382],
                "image": "data\\images\\2380130.jpg"
            },
            {
                "VG_image_id": "2358705",
                "VG_object_id": "799901",
                "bbox": [265, 219, 352, 285],
                "image": "data\\images\\2358705.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many buses are in the picture", 2],
            ["where is the bus", 1],
            ["what is in the distance", 1],
            ["where was the photo taken", 1],
            ["what is on the street", 1]
        ],
        "org_questions": [
            ["what color is the bus", -1],
            ["where is the bus", 1],
            ["how many buses are in the picture", 2],
            ["when is the picture taken", -1],
            ["what is in the distance", 1],
            ["what is the weather like", -1],
            ["what color is the ground", -1],
            ["what kind of vehicle is in the picture", -1],
            ["where was the photo taken", 1],
            ["what is on the street", 1]
        ],
        "context": [
            "a busy city street with a lot of people walking around.",
            "a group of pink buses parked in a parking lot."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2351164",
                "VG_object_id": "2034398",
                "bbox": [67, 156, 187, 325],
                "image": "data\\images\\2351164.jpg"
            },
            {
                "VG_image_id": "2335402",
                "VG_object_id": "963048",
                "bbox": [286, 11, 362, 240],
                "image": "data\\images\\2335402.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what is in the background", 2],
            ["what is the land made of", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["what is on the man's head", 1],
            ["what is the persion on the left doing", 1]
        ],
        "org_questions": [
            ["what is the man holding", 2],
            ["what is in the background", 2],
            ["what is the land made of", 1],
            ["how many people are there", -1],
            ["what color is his shirt", -1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["what is the man standing on", 1],
            ["what is on the man's head", 1],
            ["what is the persion on the left doing", 1]
        ],
        "context": [
            "a man walking down a street with an umbrella.",
            "a man in a white outfit standing on a rock."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2393999",
                "VG_object_id": "467884",
                "bbox": [4, 272, 430, 333],
                "image": "data\\images\\2393999.jpg"
            },
            {
                "VG_image_id": "2327599",
                "VG_object_id": "3294977",
                "bbox": [9, 370, 329, 498],
                "image": "data\\images\\2327599.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the land made of", 1],
            ["what is on the ground", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["what is the land made of", 1],
            ["what is on the ground", 1],
            ["what is the color of the ground", -1],
            ["how many people are there", -1],
            ["When is photo taken", -1],
            ["how many motorcycles are there on the ground", -1],
            ["where was this picture taken", 1],
            ["how is the weather", -1]
        ],
        "context": [
            "a yellow truck parked at a gas station.",
            "a clock tower on a sidewalk in a city."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316643",
                "VG_object_id": "3083987",
                "bbox": [26, 6, 344, 393],
                "image": "data\\images\\2316643.jpg"
            },
            {
                "VG_image_id": "2316789",
                "VG_object_id": "3094836",
                "bbox": [61, 100, 126, 307],
                "image": "data\\images\\2316789.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 2],
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["how many people are in the picture", 2],
            ["what animal is in the photo", -1],
            ["how is the weather", -1],
            ["what is the man wearing", -1],
            ["what is the man holding", 1],
            ["what color is the background", -1],
            ["where was the photo taken", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man holding a remote control and walking.",
            "two dogs on a leash standing next to a parking meter."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2326794",
                "VG_object_id": "2802099",
                "bbox": [5, 250, 497, 328],
                "image": "data\\images\\2326794.jpg"
            },
            {
                "VG_image_id": "2382619",
                "VG_object_id": "538139",
                "bbox": [19, 384, 261, 485],
                "image": "data\\images\\2382619.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["how many people are there", 1],
            ["where is the surfboard", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what are the people doing", 2],
            ["what kind of animal is on the beach", -1],
            ["where is the surfboard", 1],
            ["what is on the beach", -1],
            ["what is in the distance", -1],
            ["how is the water", -1],
            ["where was this picture taken", -1],
            ["where are the people", -1],
            ["where was the photo taken", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a man holding a surfboard on a beach.",
            "two people sitting on surfboards on the beach."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2386114",
                "VG_object_id": "3845222",
                "bbox": [134, 146, 199, 186],
                "image": "data\\images\\2386114.jpg"
            },
            {
                "VG_image_id": "2416870",
                "VG_object_id": "2829261",
                "bbox": [266, 36, 352, 101],
                "image": "data\\images\\2416870.jpg"
            }
        ],
        "questions_with_scores": [["what is behind the motorcycle", 1]],
        "org_questions": [
            ["where is the motorcycle", -1],
            ["what is the ground covered with", -1],
            ["what is behind the motorcycle", 1],
            ["how many people are there", -1],
            ["what color are the seats", -1],
            ["how many motorcycles are there", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is on the ground", -1],
            ["what is on the front of the motorcycle", -1]
        ],
        "context": [
            "a motorcycle parked in a parking lot next to a car.",
            "a motorcycle parked next to a sign on a floor."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2411176",
                "VG_object_id": "315784",
                "bbox": [28, 253, 100, 306],
                "image": "data\\images\\2411176.jpg"
            },
            {
                "VG_image_id": "2379480",
                "VG_object_id": "1357187",
                "bbox": [307, 222, 381, 360],
                "image": "data\\images\\2379480.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 2],
            ["what is on the ground", 2],
            ["what color is the box", 1],
            ["what is the floor made of", 1]
        ],
        "org_questions": [
            ["what color is the box", 1],
            ["how many draws are there", -1],
            ["what is the floor made of", 1],
            ["what is in the box", -1],
            ["What is the box on", -1],
            ["how many people are in the background", -1],
            ["where is the picture taken", 2],
            ["what is on the ground", 2]
        ],
        "context": [
            "a train is traveling down the tracks in the countryside.",
            "a kitchen with a microwave and a sink"
        ]
    },
    {
        "object_category": "skier",
        "images": [
            {
                "VG_image_id": "2413625",
                "VG_object_id": "169589",
                "bbox": [76, 192, 136, 282],
                "image": "data\\images\\2413625.jpg"
            },
            {
                "VG_image_id": "2340217",
                "VG_object_id": "3154736",
                "bbox": [44, 236, 114, 371],
                "image": "data\\images\\2340217.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the skier's clothes", 1],
            ["what is in the background", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the skier's clothes", 1],
            ["what is in the background", 1],
            ["what is the skier doing", -1],
            ["how many people are there", -1],
            ["what is the gender of the person", -1],
            ["where is the person", -1],
            ["how many skiers are there", -1],
            ["what color is the background", -1],
            ["when was this photo taken", -1],
            ["what is on the man's head", 1],
            ["who is in the photo", -1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a man riding skis down a snow covered slope.",
            "a man on skis standing on top of a snow covered slope."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2386333",
                "VG_object_id": "1287149",
                "bbox": [68, 227, 144, 310],
                "image": "data\\images\\2386333.jpg"
            },
            {
                "VG_image_id": "2402980",
                "VG_object_id": "384489",
                "bbox": [233, 289, 299, 346],
                "image": "data\\images\\2402980.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's trouser", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the man's trouser", 1],
            ["what is the man doing", -1],
            ["what is in the background", 1],
            ["what is the gender of the person wearing the trousers", -1],
            ["how many people are there in the picture", -1],
            ["when is this photo taken", -1],
            ["where is the man", -1],
            ["what is the man wearing", -1],
            ["who is in the photo", -1],
            ["what color is the sky", -1],
            ["when was the photo taken", -1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a snowboarder is doing a trick in the air.",
            "a man in a suit is parasailing in the air."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2393388",
                "VG_object_id": "1220435",
                "bbox": [169, 19, 442, 416],
                "image": "data\\images\\2393388.jpg"
            },
            {
                "VG_image_id": "2417263",
                "VG_object_id": "3328123",
                "bbox": [225, 121, 346, 312],
                "image": "data\\images\\2417263.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what color is the man's cap", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the man's cap", 1],
            ["what is the man holding", -1],
            ["How many people are there", -1],
            ["where is the man", -1],
            ["what is the weather like", -1],
            ["what is in the background", 1],
            ["What is man doing", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what sport is being played", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man wearing a white shirt and black pants.",
            "a baseball player swinging a bat on a field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2404473",
                "VG_object_id": "1115418",
                "bbox": [106, 286, 221, 441],
                "image": "data\\images\\2404473.jpg"
            },
            {
                "VG_image_id": "2411288",
                "VG_object_id": "359852",
                "bbox": [190, 225, 318, 330],
                "image": "data\\images\\2411288.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the jacket", 2],
            ["what gender is the person in the shirt", 2],
            ["who is wearing the shirt", 1],
            ["what is the persion doing", 1],
            ["what pattern is on the tie", 1],
            ["what is around the tie", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what color is the jacket", 2],
            ["what gender is the person in the shirt", 2],
            ["who is wearing the shirt", 1],
            ["what is the persion doing", 1],
            ["what pattern is on the tie", 1],
            ["what is on the shirt", -1],
            ["what is around the tie", 1]
        ],
        "context": [
            "a man in a suit and tie sitting on a couch.",
            "a little girl with a colorful neck tie and a brown and orange tie."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2326560",
                "VG_object_id": "982051",
                "bbox": [0, 1, 297, 378],
                "image": "data\\images\\2326560.jpg"
            },
            {
                "VG_image_id": "2342855",
                "VG_object_id": "3644290",
                "bbox": [201, 80, 350, 192],
                "image": "data\\images\\2342855.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's shirt", 1],
            ["what color is the table", 1]
        ],
        "org_questions": [
            ["what color is the boy's shirt", 1],
            ["what is the boy doing", -1],
            ["what color is the table", 1],
            ["how many children are there in the picture", -1],
            ["what is the child wearing on his head", -1],
            ["what is the child holding", -1],
            ["what is the boy wearing", -1],
            ["where was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the table", -1],
            ["what are the people eating", -1]
        ],
        "context": [
            "a young boy is eating a piece of pizza.",
            "a young boy eating pizza"
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2400937",
                "VG_object_id": "404057",
                "bbox": [111, 183, 374, 499],
                "image": "data\\images\\2400937.jpg"
            },
            {
                "VG_image_id": "2322285",
                "VG_object_id": "3529531",
                "bbox": [74, 247, 436, 373],
                "image": "data\\images\\2322285.jpg"
            }
        ],
        "questions_with_scores": [
            ["WHat color is the floor", 2],
            ["How many people are there", 1],
            ["what kind of floor is this", 1]
        ],
        "org_questions": [
            ["What is on the floor", -1],
            ["WHat color is the floor", 2],
            ["How many people are there", 1],
            ["Where is the picture taken", -1],
            ["what is the pattern of the floor", -1],
            ["what is the floor made of", -1],
            ["where is the floor", -1],
            ["what is the girl doing", -1],
            ["what kind of floor is this", 1]
        ],
        "context": [
            "a little girl playing a game on the television.",
            "a woman playing a video game in a living room."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2346026",
                "VG_object_id": "903107",
                "bbox": [167, 158, 249, 276],
                "image": "data\\images\\2346026.jpg"
            },
            {
                "VG_image_id": "2363137",
                "VG_object_id": "1841675",
                "bbox": [94, 149, 227, 486],
                "image": "data\\images\\2363137.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 1],
            ["what color is the sky", 1],
            ["what color is the clock on the building", 1]
        ],
        "org_questions": [
            ["what color is the building", 1],
            ["what color is the sky", 1],
            ["what color is the clock on the building", 1],
            ["how many clocks are on the tower", -1],
            ["where is the clock", -1],
            ["what is the weather like", -1],
            ["what shape is the clock", -1],
            ["what is on top of the tower", -1],
            ["when was the photo taken", -1],
            ["what is on the side of the building", -1]
        ],
        "context": [
            "a large building with a clock on the front of it.",
            "a clock tower in the middle of a city."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2348068",
                "VG_object_id": "3607591",
                "bbox": [163, 244, 230, 294],
                "image": "data\\images\\2348068.jpg"
            },
            {
                "VG_image_id": "2373456",
                "VG_object_id": "588760",
                "bbox": [117, 5, 226, 112],
                "image": "data\\images\\2373456.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the table", 1],
            ["what is in the container", 1],
            ["what is inside the container", 1],
            ["what is in the middle of the picture", 1]
        ],
        "org_questions": [
            ["what color is the container", -1],
            ["what is the container made of", -1],
            ["what color is the table", 1],
            ["where is the container", -1],
            ["what is in the container", 1],
            ["what is the container on", -1],
            ["what is inside the container", 1],
            ["how many people are there", 2],
            ["where was the photo taken", -1],
            ["what is on the counter", -1],
            ["what is in the middle of the picture", 1]
        ],
        "context": [
            "a woman in a red shirt",
            "a pan of blueberry muffins on a stove."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2410365",
                "VG_object_id": "219393",
                "bbox": [213, 156, 350, 311],
                "image": "data\\images\\2410365.jpg"
            },
            {
                "VG_image_id": "2318105",
                "VG_object_id": "1014038",
                "bbox": [27, 22, 479, 484],
                "image": "data\\images\\2318105.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are in the picture", 1],
            ["What food is on the plate", 1],
            ["what is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the plate", -1],
            ["how many plates are in the picture", 1],
            ["what shape is the plate", -1],
            ["What food is on the plate", 1],
            ["what is the table made of", -1],
            ["what is on the plate", 1],
            ["where is the plate", -1],
            ["what is the plate sitting on", -1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a laptop computer sitting next to a plate of pizza.",
            "a plate with some food on it"
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2358601",
                "VG_object_id": "3543873",
                "bbox": [102, 129, 295, 342],
                "image": "data\\images\\2358601.jpg"
            },
            {
                "VG_image_id": "2315988",
                "VG_object_id": "2802726",
                "bbox": [20, 129, 351, 454],
                "image": "data\\images\\2315988.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what sport is the person doing", 1],
            ["what color are the person's shorts", 1],
            ["where is the picture taken", 1],
            ["What is person doing", 1],
            ["what is the persion standing on", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what sport is the person doing", 1],
            ["what color are the person's shorts", 1],
            ["how many people are there", -1],
            ["where is the picture taken", 1],
            ["what is on the person's head", -1],
            ["What is person doing", 1],
            ["when was the photo taken", -1],
            ["who is in the air", -1],
            ["what is the persion standing on", 1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man is doing a trick on a skateboard.",
            "a man playing tennis on a court"
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2391216",
                "VG_object_id": "490678",
                "bbox": [137, 130, 191, 267],
                "image": "data\\images\\2391216.jpg"
            },
            {
                "VG_image_id": "2417423",
                "VG_object_id": "3426111",
                "bbox": [164, 174, 275, 373],
                "image": "data\\images\\2417423.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["what is the person doing", 1],
            ["what is in the background", 1],
            ["What is child doing", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["how many people are in the picture", 1],
            ["what is the person doing", 1],
            ["what is the man wearing on the head", -1],
            ["where is the photo taken", -1],
            ["What color is child's shirt", -1],
            ["what is in the background", 1],
            ["What is child doing", 1],
            ["what type of pants is the man wearing", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "two men sitting on a bench with a white horse.",
            "a boy is skateboarding in an empty skate park."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2354224",
                "VG_object_id": "840285",
                "bbox": [75, 176, 417, 320],
                "image": "data\\images\\2354224.jpg"
            },
            {
                "VG_image_id": "2350032",
                "VG_object_id": "2259954",
                "bbox": [130, 212, 313, 369],
                "image": "data\\images\\2350032.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the distance", 2],
            ["what color is the bench", 1],
            ["how many people are there", 1],
            ["what is on the bench", 1],
            ["what is next to the bench", 1]
        ],
        "org_questions": [
            ["what color is the bench", 1],
            ["where is the bench", -1],
            ["how many people are there", 1],
            ["When is photo taken", -1],
            ["what is on the bench", 1],
            ["what is the bench made of", -1],
            ["what is in the distance", 2],
            ["what is the bench sitting on", -1],
            ["what is next to the bench", 1],
            ["how many benches", -1],
            ["what is under the bench", -1]
        ],
        "context": [
            "a person sitting on a bench next to a train.",
            "a bench sitting on a boardwalk next to a beach."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2364457",
                "VG_object_id": "2398933",
                "bbox": [4, 12, 303, 380],
                "image": "data\\images\\2364457.jpg"
            },
            {
                "VG_image_id": "2368445",
                "VG_object_id": "1909190",
                "bbox": [0, 103, 161, 453],
                "image": "data\\images\\2368445.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's clothes", 2],
            ["what is the girl holding", 1],
            ["what is the gilr's posture", 1],
            ["how many people are there", 1],
            ["What is this person doing", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the girl's clothes", 2],
            ["what is the girl holding", 1],
            ["what is the gilr's posture", 1],
            ["how many people are there", 1],
            ["what is the persion wearing on the head", -1],
            ["where is the girl staying", -1],
            ["What is this person doing", 1],
            ["what is the ground covered with", 1],
            ["who is in the photo", -1],
            ["what is the girl wearing", -1],
            ["where is the woman", -1]
        ],
        "context": [
            "a little girl sitting at a table with a pizza.",
            "two women playing a game with remote controllers."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2400776",
                "VG_object_id": "405524",
                "bbox": [319, 83, 391, 145],
                "image": "data\\images\\2400776.jpg"
            },
            {
                "VG_image_id": "2378936",
                "VG_object_id": "1362836",
                "bbox": [244, 90, 402, 140],
                "image": "data\\images\\2378936.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the plane", 2],
            ["what color is the plane's tail", 2]
        ],
        "org_questions": [
            ["What color is the plane", 2],
            ["How many planes are there", -1],
            ["when is this photo taken", -1],
            ["what color is the sky", -1],
            ["what color is the plane's tail", 2],
            ["where are the planes", -1],
            ["what is flying in the sky", -1],
            ["what are the planes doing", -1],
            ["how many planes", -1],
            ["what is in the air", -1]
        ],
        "context": [
            "two red and white jets flying in the sky.",
            "a yellow plane flying through the air with a small plane in the sky."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2394818",
                "VG_object_id": "460739",
                "bbox": [308, 264, 497, 301],
                "image": "data\\images\\2394818.jpg"
            },
            {
                "VG_image_id": "2416292",
                "VG_object_id": "1056794",
                "bbox": [1, 373, 395, 499],
                "image": "data\\images\\2416292.jpg"
            }
        ],
        "questions_with_scores": [["what color is the man's hat", 1]],
        "org_questions": [
            ["what color is the dirt", -1],
            ["how many people are in the picture", -1],
            ["what color is the man's hat", 1],
            ["what animal is standing on the ground", -1],
            ["what is on the ground", -1],
            ["what is the ground covered with", -1],
            ["what color is the ground", -1],
            ["where was this photo taken", -1],
            ["where is the grass", -1],
            ["what is brown", -1]
        ],
        "context": [
            "a baseball player swinging a bat at a ball.",
            "a man in a baseball uniform is holding a bat."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2370743",
                "VG_object_id": "3856968",
                "bbox": [252, 279, 413, 424],
                "image": "data\\images\\2370743.jpg"
            },
            {
                "VG_image_id": "2402917",
                "VG_object_id": "1130034",
                "bbox": [114, 1, 436, 189],
                "image": "data\\images\\2402917.jpg"
            }
        ],
        "questions_with_scores": [["what is the ground covered with", 1]],
        "org_questions": [
            ["What color is the cabinet", -1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["where is the cabinet", -1],
            ["what room is this", -1],
            ["who is in the photo", -1],
            ["where was the photo taken", -1],
            ["what is in the kitchen", -1],
            ["what are the cabinets made out of", -1]
        ],
        "context": [
            "a kitchen with a sink and a stove",
            "a kitchen with a sink, microwave, and a refrigerator."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2376185",
                "VG_object_id": "575338",
                "bbox": [1, 0, 498, 129],
                "image": "data\\images\\2376185.jpg"
            },
            {
                "VG_image_id": "2382299",
                "VG_object_id": "539905",
                "bbox": [0, 0, 499, 176],
                "image": "data\\images\\2382299.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many buildings are there in the distance", 1],
            ["what is in the sky", 1]
        ],
        "org_questions": [
            ["what color is the sky", -1],
            ["how many buildings are there in the distance", 1],
            ["how many planes are there in the picture", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", -1],
            ["what is on the road", -1],
            ["how is the weather", -1],
            ["when was the picture taken", -1],
            ["what is behind the plane", -1],
            ["what is in the sky", 1]
        ],
        "context": [
            "a large jetliner sitting on top of an airport tarmac.",
            "a large jetliner sitting on top of an airport tarmac."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2372473",
                "VG_object_id": "1832954",
                "bbox": [93, 300, 182, 498],
                "image": "data\\images\\2372473.jpg"
            },
            {
                "VG_image_id": "2343493",
                "VG_object_id": "923127",
                "bbox": [104, 8, 242, 321],
                "image": "data\\images\\2343493.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's helmet", 2],
            ["what color is the man's shirt", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what color is the man's helmet", 2],
            ["what is in front of the person", -1],
            ["how many people are there", -1],
            ["where is the man", -1],
            ["What is the man doing", -1],
            ["what is the man wearing", 1],
            ["what is the ground covered with", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what kind of pants is the man wearing", -1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a group of people riding motorcycles down a street.",
            "a man and woman riding a yellow scooter."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2372883",
                "VG_object_id": "1717005",
                "bbox": [9, 19, 362, 496],
                "image": "data\\images\\2372883.jpg"
            },
            {
                "VG_image_id": "2350311",
                "VG_object_id": "3446346",
                "bbox": [146, 40, 393, 372],
                "image": "data\\images\\2350311.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's glasses", 1],
            ["where is the man sitting", 1],
            ["what is the man holding", 1],
            ["what is the man doing", 1],
            ["what color is the ground", 1],
            ["where is the man", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color are the man's glasses", 1],
            ["where is the man sitting", 1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["what is the man doing", 1],
            ["what color is the ground", 1],
            ["where is the man", 1],
            ["who is in the photo", -1],
            ["what is on the man's face", -1],
            ["what is in the background", 1],
            ["what is the man sitting on", -1]
        ],
        "context": [
            "a man eating a pink frosted donut with sprinkles.",
            "a man sitting on a toilet in a room."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2399236",
                "VG_object_id": "1171043",
                "bbox": [0, 263, 499, 426],
                "image": "data\\images\\2399236.jpg"
            },
            {
                "VG_image_id": "2347010",
                "VG_object_id": "3613809",
                "bbox": [62, 0, 496, 372],
                "image": "data\\images\\2347010.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the floor made of", 1],
            ["what color is the floor ", 1],
            ["what is on the ground", 1],
            ["where is the picture taken", 1],
            ["how many dogs are there on the floor", 1],
            ["what color is the animal", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the floor made of", 1],
            ["what color is the floor ", 1],
            ["what is on the ground", 1],
            ["how many people are there", -1],
            ["where is the picture taken", 1],
            ["how many dogs are there on the floor", 1],
            ["what color is the animal", 1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a dog standing on its hind legs looking at a television.",
            "three pigeons are standing on the sidewalk."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2319525",
                "VG_object_id": "2770631",
                "bbox": [4, 167, 498, 237],
                "image": "data\\images\\2319525.jpg"
            },
            {
                "VG_image_id": "2411166",
                "VG_object_id": "360221",
                "bbox": [2, 296, 331, 499],
                "image": "data\\images\\2411166.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["how many planes are there on the ground", 1],
            ["what is in the background", 1],
            ["what is the weather like", 1],
            ["what is the land made of", 1],
            ["what is on the ground", 1],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["how many planes are there on the ground", 1],
            ["what is in the background", 1],
            ["what is the weather like", 1],
            ["what is the land made of", 1],
            ["where was this picture taken", -1],
            ["what is on the ground", 1],
            ["how is the weather", 1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a large airplane is on the runway in the fog.",
            "a woman walking down a street holding an umbrella."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2351866",
                "VG_object_id": "1666314",
                "bbox": [223, 132, 413, 197],
                "image": "data\\images\\2351866.jpg"
            },
            {
                "VG_image_id": "2372015",
                "VG_object_id": "1861918",
                "bbox": [32, 135, 265, 331],
                "image": "data\\images\\2372015.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many ears are there", 2],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the person doing", -1],
            ["how many ears are there", 2],
            ["how many people are there", -1],
            ["what is in the background", 1],
            ["how is the weather", -1]
        ],
        "context": [
            "a woman holding a banana in her hand.",
            "a man looking at a giraffe in the background."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2367784",
                "VG_object_id": "3872274",
                "bbox": [11, 173, 373, 412],
                "image": "data\\images\\2367784.jpg"
            },
            {
                "VG_image_id": "2409893",
                "VG_object_id": "230934",
                "bbox": [223, 54, 288, 125],
                "image": "data\\images\\2409893.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the persion wearing", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["what gender is the person who wears the shirt", -1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1],
            ["what is the person holding", 1],
            ["what color is the shirt", -1]
        ],
        "context": [
            "a man holding a sandwich in his hands.",
            "a conveyor belt with donuts being made."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2333681",
                "VG_object_id": "3486736",
                "bbox": [62, 47, 334, 350],
                "image": "data\\images\\2333681.jpg"
            },
            {
                "VG_image_id": "2404739",
                "VG_object_id": "3814737",
                "bbox": [23, 3, 276, 498],
                "image": "data\\images\\2404739.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many players are there", 2],
            ["what is the player holding", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["how many players are there", 2],
            ["what is the player holding", 1],
            ["what is the man doing", -1],
            ["what gender is the player", -1],
            ["what color are the man's pants", -1],
            ["What is the gender of player", -1],
            ["how many people are there in the picture", 1],
            ["what is the player doing", -1],
            ["where was this photo taken", -1],
            ["when was the photo taken", -1],
            ["what sport is being played", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "baseball players celebrate after a game.",
            "a baseball player is getting ready to hit a ball."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2356409",
                "VG_object_id": "1936104",
                "bbox": [171, 55, 306, 297],
                "image": "data\\images\\2356409.jpg"
            },
            {
                "VG_image_id": "2373332",
                "VG_object_id": "3727854",
                "bbox": [315, 102, 437, 327],
                "image": "data\\images\\2373332.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the playing holding", 2],
            ["what is the playing holding in hang", 2],
            ["what is the player doing", 1],
            ["What is the person holding", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what is the playing holding", 2],
            ["what is the main color of the land", -1],
            ["what color is the player's uniform", -1],
            ["How many people are there", -1],
            ["what is the player doing", 1],
            ["what is in the background", -1],
            ["What color is the ground", -1],
            ["What is the person holding", 1],
            ["what is on the man's head", 1],
            ["who is in the picture", -1],
            ["where is the man standing", -1],
            ["what sport is the man playing", -1],
            ["what is the playing holding in hang", 2]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a baseball player is throwing a ball on a field."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2369185",
                "VG_object_id": "1907126",
                "bbox": [267, 149, 496, 329],
                "image": "data\\images\\2369185.jpg"
            },
            {
                "VG_image_id": "2409363",
                "VG_object_id": "243404",
                "bbox": [1, 126, 499, 331],
                "image": "data\\images\\2409363.jpg"
            }
        ],
        "questions_with_scores": [
            ["What sports is the person playing", 2],
            ["what sport are the people doing on the field", 2],
            ["What is the color of field", 1],
            ["what is the field made of", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["What is the color of field", 1],
            ["What sports is the person playing", 2],
            ["How many trains are there", -1],
            ["what is on the ground", -1],
            ["what season is this photo taken in", -1],
            ["what sport are the people doing on the field", 2],
            ["where was this photo taken", -1],
            ["how is the weather", -1],
            ["where is the grass", -1],
            ["what is the field made of", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a young boy swinging a bat at a baseball.",
            "person dribbles the ball down the field."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2407516",
                "VG_object_id": "366792",
                "bbox": [3, 306, 497, 372],
                "image": "data\\images\\2407516.jpg"
            },
            {
                "VG_image_id": "2407572",
                "VG_object_id": "1096901",
                "bbox": [27, 12, 486, 297],
                "image": "data\\images\\2407572.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the field", 2],
            ["what color is the grass", 2],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the field", 2],
            ["what is on the field", -1],
            ["what color is the grass", 2],
            ["how many horses are there on the field", -1],
            ["What is the background of photo", -1],
            ["when was the picture taken", 1],
            ["how is the weather", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a group of cows grazing in a field.",
            "a black bear eating berries in a field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2345307",
                "VG_object_id": "2731976",
                "bbox": [122, 135, 208, 192],
                "image": "data\\images\\2345307.jpg"
            },
            {
                "VG_image_id": "2408965",
                "VG_object_id": "251640",
                "bbox": [317, 126, 386, 202],
                "image": "data\\images\\2408965.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 1],
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["what is the ground covered with", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["what is the ground covered with", 1],
            ["where is the photo taken", 1],
            ["how many people are there", -1],
            ["when was the picture taken", -1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a woman riding a bike down a street.",
            "two people walking down a dirt road carrying luggage."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2394239",
                "VG_object_id": "1213016",
                "bbox": [141, 188, 262, 280],
                "image": "data\\images\\2394239.jpg"
            },
            {
                "VG_image_id": "2371999",
                "VG_object_id": "594768",
                "bbox": [301, 6, 478, 118],
                "image": "data\\images\\2371999.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the distance", 2],
            ["what color is the umbrella", 1],
            ["where is the umbrella", 1],
            ["what pattern is on the umbrella", 1],
            ["What color is umbrella", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 1],
            ["where is the umbrella", 1],
            ["what time is it", -1],
            ["How many umbrellas are there", -1],
            ["what pattern is on the umbrella", 1],
            ["what is in the distance", 2],
            ["How many people are there", -1],
            ["What color is umbrella", 1],
            ["what is the weather like", -1],
            ["what is the umbrella made of", -1],
            ["how are the umbrellas", -1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "a beach with people and umbrellas and a train on the tracks.",
            "a street scene with bicycles and people on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2373499",
                "VG_object_id": "2686808",
                "bbox": [136, 10, 245, 183],
                "image": "data\\images\\2373499.jpg"
            },
            {
                "VG_image_id": "2403816",
                "VG_object_id": "1121188",
                "bbox": [294, 62, 412, 298],
                "image": "data\\images\\2403816.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["what is in the background", 1],
            ["How many people are there", 1],
            ["what is the persion wearing on the head", 1],
            ["where is the woman", 1],
            ["What is woman holding", 1],
            ["what is the woman holding", 1],
            ["what is the man standing on", 1],
            ["what is the person wearing", 1],
            ["what is the weather like", 1],
            ["how many people are there in the picture", 1],
            ["What is the woman wearing on her head", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", -1],
            ["what is the woman doing", 1],
            ["what is in the background", 1],
            ["How many people are there", 1],
            ["what is the persion wearing on the head", 1],
            ["where is the woman", 1],
            ["What is woman holding", 1],
            ["what is the woman holding", 1],
            ["when was the photo taken", -1],
            ["what is the man standing on", 1],
            ["what is the person wearing", 1],
            ["what is the weather like", 1],
            ["how many people are there in the picture", 1],
            ["What is the woman wearing on her head", 1],
            ["when is the picture taken", -1]
        ],
        "context": [
            "a long row boat with a person rowing it.",
            "a person cross country skiing in the snow."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2409182",
                "VG_object_id": "247249",
                "bbox": [348, 202, 493, 253],
                "image": "data\\images\\2409182.jpg"
            },
            {
                "VG_image_id": "2329436",
                "VG_object_id": "2704506",
                "bbox": [249, 138, 341, 181],
                "image": "data\\images\\2329436.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sink", 1],
            ["what is the sink made of", 1],
            ["what pattern does the wall behind the wall have", 1],
            ["what shape is the sink", 1],
            ["what is on the stove", 1]
        ],
        "org_questions": [
            ["what color is the sink", 1],
            ["what is the sink made of", 1],
            ["what is beside the sink", -1],
            ["how many sinks are there", -1],
            ["what pattern does the wall behind the wall have", 1],
            ["what shape is the sink", 1],
            ["where is the picture taken", -1],
            ["what is the sink on", -1],
            ["what is on top of the counter", -1],
            ["where is the stove", -1],
            ["what is on the stove", 1],
            ["what is next to the counter", -1]
        ],
        "context": [
            "a kitchen with a wooden table and chairs.",
            "a kitchen with a stove, sink, and a dishwasher."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2391701",
                "VG_object_id": "486944",
                "bbox": [117, 56, 420, 302],
                "image": "data\\images\\2391701.jpg"
            },
            {
                "VG_image_id": "2345085",
                "VG_object_id": "2085793",
                "bbox": [54, 180, 270, 316],
                "image": "data\\images\\2345085.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there on the ground", 2],
            ["how many zebras are there in the picture", 2],
            ["what color is the ground", 1],
            ["what color is the background", 1],
            ["What is zebra doing", 1],
            ["what is in front of the zebras", 1],
            ["what is the land filled with", 1],
            ["what is behind the zebra", 1]
        ],
        "org_questions": [
            ["how many zebras are there on the ground", 2],
            ["what color is the ground", 1],
            ["what color is the background", 1],
            ["what animals are on the grass", -1],
            ["What is zebra doing", 1],
            ["where is the zebra", -1],
            ["what is in front of the zebras", 1],
            ["what is the land filled with", 1],
            ["when was this picture taken", -1],
            ["what are the zebras standing on", -1],
            ["what type of animal is shown", -1],
            ["what is behind the zebra", 1],
            ["how many zebras are there in the picture", 2]
        ],
        "context": [
            "a zebra grazing on grass in a field.",
            "a zebra standing next to a stone wall."
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2375270",
                "VG_object_id": "583271",
                "bbox": [16, 2, 496, 357],
                "image": "data\\images\\2375270.jpg"
            },
            {
                "VG_image_id": "2357367",
                "VG_object_id": "2345202",
                "bbox": [47, 173, 373, 285],
                "image": "data\\images\\2357367.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many plates are there", 2],
            ["what color is the table", 1],
            ["what color is the tray", 1],
            ["What food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what color is the tray", 1],
            ["what shape is the tray", -1],
            ["What food is on the plate", 1],
            ["where is the tray", -1],
            ["what is the tray made of", -1],
            ["how many plates are there", 2],
            ["what is the food on", -1],
            ["where was this picture taken", -1],
            ["what is on the table", -1],
            ["what is the shape of the table", -1]
        ],
        "context": [
            "a tray of food with a variety of foods.",
            "a table with a plate of food and a pizza"
        ]
    },
    {
        "object_category": "lady",
        "images": [
            {
                "VG_image_id": "2399092",
                "VG_object_id": "1172375",
                "bbox": [81, 171, 136, 318],
                "image": "data\\images\\2399092.jpg"
            },
            {
                "VG_image_id": "2366941",
                "VG_object_id": "625229",
                "bbox": [238, 327, 313, 482],
                "image": "data\\images\\2366941.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the lady's clothes", 1],
            ["what is the lady doing", 1],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["how many person's are there in the photo", 1],
            ["how many people are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the lady's clothes", 1],
            ["what is the lady doing", 1],
            ["how many people are there", -1],
            ["what is the weather like", -1],
            ["what is the woman wearing", 1],
            ["what is the woman holding", 1],
            ["how many person's are there in the photo", 1],
            ["how many people are there in the picture", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a person pushing a bike",
            "an elderly woman sitting on a bench in front of a house."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2411457",
                "VG_object_id": "311621",
                "bbox": [123, 88, 312, 233],
                "image": "data\\images\\2411457.jpg"
            },
            {
                "VG_image_id": "2387234",
                "VG_object_id": "1277791",
                "bbox": [146, 115, 385, 207],
                "image": "data\\images\\2387234.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bear are in the picture", 1],
            ["what is the color of the water", 1],
            ["what is the main color of the background", 1]
        ],
        "org_questions": [
            ["what color is the bear", -1],
            ["what is the bear doing", -1],
            ["how many bear are in the picture", 1],
            ["what is on the bear's neck", -1],
            ["what is the weather like", -1],
            ["what is the color of the water", 1],
            ["what is the main color of the background", 1],
            ["what type of animal is shown", -1],
            ["where was the picture taken", -1],
            ["who is in the water", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a brown bear swimming in a waterfall.",
            "three bears are crossing a river in the wild."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2345524",
                "VG_object_id": "3624007",
                "bbox": [14, 259, 479, 383],
                "image": "data\\images\\2345524.jpg"
            },
            {
                "VG_image_id": "2369889",
                "VG_object_id": "2296730",
                "bbox": [63, 141, 499, 331],
                "image": "data\\images\\2369889.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are on the bed", 2],
            ["what color is the blanket", 1],
            ["what is the woman doing", 1]
        ],
        "org_questions": [
            ["what color is the blanket", 1],
            ["how many people are on the bed", 2],
            ["what is the woman doing", 1],
            ["What is on the bed", -1],
            ["how many beds are in the picture", -1],
            ["where was the picture taken", -1],
            ["what is next to the bed", -1]
        ],
        "context": [
            "a woman standing on a bed while a man is on a laptop.",
            "a woman sitting on a bed using a laptop."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2363248",
                "VG_object_id": "766771",
                "bbox": [177, 176, 271, 339],
                "image": "data\\images\\2363248.jpg"
            },
            {
                "VG_image_id": "2384275",
                "VG_object_id": "692108",
                "bbox": [111, 240, 236, 400],
                "image": "data\\images\\2384275.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what color is the man's shirt", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what color is the man's shirt", 1],
            ["what is the man holding", 2],
            ["how many people are there", -1],
            ["what is the gender of the person wearing the trousers", -1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["what is on the man's feet", -1],
            ["what is on the man's head", -1],
            ["what color is the grass", -1]
        ],
        "context": [
            "a boy is throwing a baseball on a field.",
            "a baseball player swinging a bat on a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2371547",
                "VG_object_id": "2492941",
                "bbox": [111, 254, 207, 341],
                "image": "data\\images\\2371547.jpg"
            },
            {
                "VG_image_id": "2390619",
                "VG_object_id": "495466",
                "bbox": [206, 151, 262, 220],
                "image": "data\\images\\2390619.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's jacket", 1],
            ["What is the man doing", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["What is man doing", 1]
        ],
        "org_questions": [
            ["What color is the man's jacket", 1],
            ["What is the man doing", 1],
            ["What is the weather like", -1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["What is man doing", 1],
            ["what color are the pants", -1],
            ["when was this picture taken", -1],
            ["what kind of pants is the man wearing", -1],
            ["what is the man wearing on the head", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a man skiing down a snowy hill with a ski lift in the background.",
            "a man and a woman riding a motorcycle."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2368445",
                "VG_object_id": "1909190",
                "bbox": [0, 103, 161, 453],
                "image": "data\\images\\2368445.jpg"
            },
            {
                "VG_image_id": "2327448",
                "VG_object_id": "2741086",
                "bbox": [97, 35, 183, 241],
                "image": "data\\images\\2327448.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 2],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 2],
            ["what is the woman holding", 1],
            ["where is the woman", -1],
            ["what time is it", -1],
            ["what is the color of girl's pants", -1],
            ["what is the girl wearing on the head", -1],
            ["how old is the girl", -1],
            ["who is in the photo", -1],
            ["how many people are there", -1]
        ],
        "context": [
            "two women playing a game with remote controllers.",
            "a woman cutting a cake with green icing."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2381799",
                "VG_object_id": "1333999",
                "bbox": [207, 130, 321, 285],
                "image": "data\\images\\2381799.jpg"
            },
            {
                "VG_image_id": "2366617",
                "VG_object_id": "2076267",
                "bbox": [182, 132, 272, 244],
                "image": "data\\images\\2366617.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the shirt", 1],
            ["where is the picture taken", 1],
            ["what is the persion holding", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["how many people are there in the picture", 2],
            ["where is the picture taken", 1],
            ["what is the man doing", -1],
            ["what is the gender of the person", -1],
            ["what is the persion holding", 1],
            ["where is the man", 1],
            ["what is on the man's head", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a man carrying a surfboard in a parking garage.",
            "two men standing in a living room with a dog."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2384992",
                "VG_object_id": "1302347",
                "bbox": [162, 301, 237, 397],
                "image": "data\\images\\2384992.jpg"
            },
            {
                "VG_image_id": "2342134",
                "VG_object_id": "936886",
                "bbox": [76, 148, 201, 276],
                "image": "data\\images\\2342134.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["How many people are there", -1],
            ["what is the person wearing", -1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a group of people sitting at a table in a large room.",
            "a woman standing in a kitchen with a plate of food."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2412316",
                "VG_object_id": "197349",
                "bbox": [6, 1, 406, 306],
                "image": "data\\images\\2412316.jpg"
            },
            {
                "VG_image_id": "2412985",
                "VG_object_id": "182712",
                "bbox": [30, 77, 285, 265],
                "image": "data\\images\\2412985.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bear", 1],
            ["what is in front of the bear", 1],
            ["what color is the background", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the bear doing", -1],
            ["where is the bear", 1],
            ["what is in front of the bear", 1],
            ["how many bears are there", -1],
            ["what color is the background", 1],
            ["what is the weather like", -1],
            ["what is behind the bear", -1],
            ["when was this picture taken", -1],
            ["what type of bear is this", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a polar bear is standing on a rock.",
            "a polar bear in a cage with a ball"
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2350680",
                "VG_object_id": "2542757",
                "bbox": [377, 4, 462, 374],
                "image": "data\\images\\2350680.jpg"
            },
            {
                "VG_image_id": "2366879",
                "VG_object_id": "1812333",
                "bbox": [300, 8, 487, 306],
                "image": "data\\images\\2366879.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 2],
            ["what is under the curtain", 1],
            ["what room is this", 1],
            ["what is in the room", 1],
            ["what kind of room is this", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 2],
            ["where is the curtain", -1],
            ["what is under the curtain", 1],
            ["how many people are there", -1],
            ["what is behind the curtain", -1],
            ["where is the picture taken", -1],
            ["what room is this", 1],
            ["what is in the room", 1],
            ["what kind of room is this", 1]
        ],
        "context": [
            "a bathroom with a bathtub, a toilet and a sink.",
            "a bedroom with a bed, mirror, and a mirror."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2394927",
                "VG_object_id": "459583",
                "bbox": [1, 216, 331, 491],
                "image": "data\\images\\2394927.jpg"
            },
            {
                "VG_image_id": "2342104",
                "VG_object_id": "937259",
                "bbox": [0, 308, 488, 418],
                "image": "data\\images\\2342104.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 2],
            ["what is on the floor", 2],
            ["how many people are there in the picture", 2],
            ["how many dogs are there on the floor", 1],
            ["what room is the rug in", 1],
            ["what is on the left of the picture", 1],
            ["how many people are in the picture", 1],
            ["what is in the background", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the floor", 2],
            ["what is on the floor", 2],
            ["how many dogs are there on the floor", 1],
            ["what room is the rug in", 1],
            ["what shape is the rug", -1],
            ["where is the rug", -1],
            ["what is on the left of the picture", 1],
            ["how many people are in the picture", 1],
            ["what is the floor made of", -1],
            ["what is in the background", 1],
            ["where is the picture taken", 1],
            ["what is covering the floor", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a dog on a skateboard",
            "a man and a woman standing next to a motorcycle."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2396783",
                "VG_object_id": "440535",
                "bbox": [38, 40, 295, 171],
                "image": "data\\images\\2396783.jpg"
            },
            {
                "VG_image_id": "2342001",
                "VG_object_id": "2720282",
                "bbox": [92, 78, 344, 257],
                "image": "data\\images\\2342001.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["What the color of building", 2],
            ["how many people are there", 1],
            ["how many  windows does the building have", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the building", 2],
            ["what is in front of the building", -1],
            ["how many people are there", 1],
            ["how many  windows does the building have", 1],
            ["What the color of building", 2],
            ["How many flags are there", -1],
            ["where was this picture taken", -1],
            ["where are the trees", -1],
            ["what is the building made of", -1],
            ["what is on the ground", 1]
        ],
        "context": [
            "a man in a white shirt is playing baseball",
            "a busy street filled with lots of traffic."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2333529",
                "VG_object_id": "3469598",
                "bbox": [408, 137, 479, 180],
                "image": "data\\images\\2333529.jpg"
            },
            {
                "VG_image_id": "2333840",
                "VG_object_id": "3334660",
                "bbox": [338, 152, 455, 242],
                "image": "data\\images\\2333840.jpg"
            }
        ],
        "questions_with_scores": [
            ["what kind of room is the television placed in", 2],
            ["what color is the shelf under the television", 1],
            ["which room is the television placing in", 1],
            ["what is in front of the television", 1],
            ["what is the TV screen showing", 1],
            ["what is hanging on the wall", 1],
            ["what is above the tv", 1]
        ],
        "org_questions": [
            ["what color is the shelf under the television", 1],
            ["which room is the television placing in", 1],
            ["how many people are there in the picture", -1],
            ["where is the tv", -1],
            ["what is in front of the television", 1],
            ["what is the TV screen showing", 1],
            ["what color is the wall", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is hanging on the wall", 1],
            ["what is turned on", -1],
            ["what is above the tv", 1],
            ["what kind of room is the television placed in", 2]
        ],
        "context": [
            "a bedroom with a bed, a television, and a television.",
            "a living room with a table, chairs, a television and a table."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2374800",
                "VG_object_id": "3848569",
                "bbox": [43, 217, 130, 343],
                "image": "data\\images\\2374800.jpg"
            },
            {
                "VG_image_id": "2339161",
                "VG_object_id": "2472793",
                "bbox": [281, 136, 375, 181],
                "image": "data\\images\\2339161.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["What is man doing", 1],
            ["what is the person holding", 1],
            ["what gesture is the man in the shirt", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["How many people are there", -1],
            ["What color is man's shirt", 2],
            ["What is man doing", 1],
            ["where is the picture taken", -1],
            ["what is the person holding", 1],
            ["what gesture is the man in the shirt", 1],
            ["what is the persion doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", -1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a young boy flying a kite in a field.",
            "a group of men standing on top of a sandy beach."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2342687",
                "VG_object_id": "3645649",
                "bbox": [107, 37, 439, 281],
                "image": "data\\images\\2342687.jpg"
            },
            {
                "VG_image_id": "2315988",
                "VG_object_id": "3426615",
                "bbox": [31, 135, 341, 439],
                "image": "data\\images\\2315988.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what sport is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the field made of", 1],
            ["what is the person holding", 1],
            ["What is guy doing", 1],
            ["what is on the ground", 1],
            ["what game is being played", 1]
        ],
        "org_questions": [
            ["what sport is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["what is the field made of", 1],
            ["how many people are there", 2],
            ["what is the person holding", 1],
            ["what is the man wearing", -1],
            ["What is guy doing", 1],
            ["what is the color of man's pants", -1],
            ["where was this photo taken", -1],
            ["what is on the ground", 1],
            ["where is the man", -1],
            ["what game is being played", 1]
        ],
        "context": [
            "a man is diving for a frisbee in the grass.",
            "a man playing tennis on a court"
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2383687",
                "VG_object_id": "530904",
                "bbox": [5, 175, 162, 232],
                "image": "data\\images\\2383687.jpg"
            },
            {
                "VG_image_id": "2352382",
                "VG_object_id": "2750795",
                "bbox": [319, 233, 402, 327],
                "image": "data\\images\\2352382.jpg"
            }
        ],
        "questions_with_scores": [["how many pillows are there", 1]],
        "org_questions": [
            ["how many pillows are there", 1],
            ["What is on the bed", -1],
            ["where is the photo taken", -1],
            ["how many people are there in the picture", -1],
            ["where is the pillow placed on", -1],
            ["where are pillows", -1],
            ["what color are the pillows on the bed", -1],
            ["where are the pillows", -1],
            ["what color are the walls", -1]
        ],
        "context": [
            "a hotel room with a large bed and a chair.",
            "a bedroom with a bed and a chair."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2355546",
                "VG_object_id": "828700",
                "bbox": [163, 99, 420, 358],
                "image": "data\\images\\2355546.jpg"
            },
            {
                "VG_image_id": "2361020",
                "VG_object_id": "3218986",
                "bbox": [112, 32, 347, 382],
                "image": "data\\images\\2361020.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the bear smelling", 1],
            ["what color is the ground", 1],
            ["what is in front of the bear", 1]
        ],
        "org_questions": [
            ["what is the bear smelling", 1],
            ["what color is the ground", 1],
            ["how many bears are there in the picture", -1],
            ["what is the weather like", -1],
            ["what is behind the bear", -1],
            ["what is in front of the bear", 1],
            ["when was the picture taken", -1],
            ["what kind of animal is this", -1],
            ["where is the bear", -1],
            ["what is the bear standing on", -1],
            ["what is on the ground", -1]
        ],
        "context": [
            "a black bear is walking through the woods.",
            "a black bear standing next to a tree."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2317361",
                "VG_object_id": "3451931",
                "bbox": [10, 205, 107, 352],
                "image": "data\\images\\2317361.jpg"
            },
            {
                "VG_image_id": "42",
                "VG_object_id": "1061630",
                "bbox": [51, 404, 303, 599],
                "image": "data\\images\\42.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many chairs are there", 1],
            ["what is on the chair", 1],
            ["what is the floor made of", 1]
        ],
        "org_questions": [
            ["how many chairs are there", 1],
            ["what is the persion holding", -1],
            ["what is the table made of", -1],
            ["where is the chair", -1],
            ["what is the persion on the chair wearing", -1],
            ["what is the persion doing", -1],
            ["what room is this", -1],
            ["what is on the chair", 1],
            ["what material is the table made of", -1],
            ["what are the chairs made of", -1],
            ["what is the floor made of", 1]
        ],
        "context": [
            "a living room with a couch, table, and chairs.",
            "a room with a couch, chairs, and a television."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2389669",
                "VG_object_id": "3828508",
                "bbox": [129, 232, 184, 442],
                "image": "data\\images\\2389669.jpg"
            },
            {
                "VG_image_id": "2374174",
                "VG_object_id": "1942345",
                "bbox": [377, 87, 496, 331],
                "image": "data\\images\\2374174.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the towel", 2],
            ["what color is the towel on the shower", 1]
        ],
        "org_questions": [
            ["what color is the towel", 2],
            ["how many towels are there", -1],
            ["what room is the towel in", -1],
            ["what is behind the towel", -1],
            ["where is the towel", -1],
            ["where is the picture taken", -1],
            ["What color is the wall", -1],
            ["who is in the photo", -1],
            ["what is hanging on the wall", -1],
            ["what is on the towel", -1],
            ["where was the photo taken", -1],
            ["what color is the towel on the shower", 1]
        ],
        "context": [
            "a bathroom with a toilet, sink and a mirror.",
            "a woman is standing in a bathtub with her feet in the tub."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2415383",
                "VG_object_id": "3425251",
                "bbox": [52, 114, 153, 334],
                "image": "data\\images\\2415383.jpg"
            },
            {
                "VG_image_id": "2366605",
                "VG_object_id": "1967668",
                "bbox": [337, 0, 497, 352],
                "image": "data\\images\\2366605.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child doing", 1],
            ["where is the girl", 1],
            ["what is the child wearing", 1],
            ["what are the child playing", 1],
            ["who is in the photo", 1],
            ["where is the child", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", -1],
            ["what is the child doing", 1],
            ["how many people are in the picture", -1],
            ["where is the girl", 1],
            ["what is the child wearing", 1],
            ["what are the child playing", 1],
            ["who is in the photo", 1],
            ["where is the child", 1],
            ["How many people are there", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a little girl on the beach holding a kite.",
            "a child in a pink jacket and glasses playing with a broken toilet."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2408667",
                "VG_object_id": "3808848",
                "bbox": [64, 289, 179, 359],
                "image": "data\\images\\2408667.jpg"
            },
            {
                "VG_image_id": "2402996",
                "VG_object_id": "1129040",
                "bbox": [2, 260, 114, 301],
                "image": "data\\images\\2402996.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sink", 1],
            ["what is the sink made of", 1],
            ["where was the photo taken", 1],
            ["what is next to the sink", 1],
            ["what is above the sink", 1]
        ],
        "org_questions": [
            ["what color is the sink", 1],
            ["what is the sink made of", 1],
            ["what color is the wall behind the sink", -1],
            ["how many sinks are there", -1],
            ["what shape is the sink", -1],
            ["what pattern does the wall behind the wall have", -1],
            ["what is placed on the sink", -1],
            ["what is the sink on", -1],
            ["where was the photo taken", 1],
            ["what is next to the sink", 1],
            ["what is above the sink", 1]
        ],
        "context": [
            "a bathroom with a sink, mirror, and a bathtub.",
            "a kitchen with a sink, refrigerator, and a window."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2369377",
                "VG_object_id": "2074306",
                "bbox": [413, 15, 499, 137],
                "image": "data\\images\\2369377.jpg"
            },
            {
                "VG_image_id": "2341652",
                "VG_object_id": "940877",
                "bbox": [160, 180, 231, 422],
                "image": "data\\images\\2341652.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man riding", 1],
            ["what is on the man's head", 1],
            ["what color is the man's shirt", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man riding", 1],
            ["what is on the man's head", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what color is the man's shirt", 1],
            ["what gesture is the man", 1],
            ["what is the man wearing", 1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1],
            ["what type of pants is the man wearing", -1],
            ["where is the man", -1]
        ],
        "context": [
            "a person is skateboarding on the steps.",
            "a man on a motorcycle with a dog on the back."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2402053",
                "VG_object_id": "393222",
                "bbox": [252, 27, 500, 375],
                "image": "data\\images\\2402053.jpg"
            },
            {
                "VG_image_id": "2364399",
                "VG_object_id": "641303",
                "bbox": [12, 92, 101, 375],
                "image": "data\\images\\2364399.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is woman doing", 2],
            ["what is the woman holding", 1],
            ["what is the woman wearing", 1],
            ["where is the woman", 1],
            ["what is the persion standing on", 1],
            ["what is the woman looking at", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 1],
            ["what is the woman wearing", 1],
            ["what gesture is the woman", -1],
            ["How many people are there", -1],
            ["where is the woman", 1],
            ["What color is woman's hair", -1],
            ["What is woman doing", 2],
            ["what is the persion standing on", 1],
            ["who is in the photo", -1],
            ["what is on the woman's face", -1],
            ["what is the woman looking at", 1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a man and a woman sitting on a train looking at a cell phone.",
            "a woman looking at a display case filled with cakes."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2374059",
                "VG_object_id": "2031336",
                "bbox": [60, 64, 309, 372],
                "image": "data\\images\\2374059.jpg"
            },
            {
                "VG_image_id": "2358402",
                "VG_object_id": "803182",
                "bbox": [243, 119, 316, 206],
                "image": "data\\images\\2358402.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what is on the table", -1],
            ["what color is the man's shirt", 2],
            ["Where is the man", -1],
            ["What is the man doing", -1],
            ["what is the man wearing on his face", -1],
            ["what is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man sitting on", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the man looking at", -1]
        ],
        "context": [
            "a man sitting at a table with a pizza in front of him.",
            "a group of people sitting around a table eating food."
        ]
    },
    {
        "object_category": "court",
        "images": [
            {
                "VG_image_id": "2346359",
                "VG_object_id": "3617764",
                "bbox": [1, 127, 483, 246],
                "image": "data\\images\\2346359.jpg"
            },
            {
                "VG_image_id": "2359066",
                "VG_object_id": "3100323",
                "bbox": [1, 406, 446, 498],
                "image": "data\\images\\2359066.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there on the court", 1],
            ["what is the man wearing on his head", 1]
        ],
        "org_questions": [
            ["how many people are there on the court", 1],
            ["what color is the man's shirt", -1],
            ["what is the man wearing on his head", 1],
            ["what is the ground covered with", -1],
            ["What color is the court", -1],
            ["how color is the player's shirt", -1],
            ["what color is the court ground", -1],
            ["what is the main color of the court", -1],
            ["where is this scene", -1],
            ["what is on the court", -1],
            ["where are the white lines", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a man swinging a tennis racket at a ball.",
            "a man hitting a tennis ball with a racquet."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2401444",
                "VG_object_id": "1147813",
                "bbox": [207, 107, 295, 332],
                "image": "data\\images\\2401444.jpg"
            },
            {
                "VG_image_id": "2378982",
                "VG_object_id": "1868114",
                "bbox": [268, 17, 463, 315],
                "image": "data\\images\\2378982.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man standing on", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what are the people doing", 1],
            ["what kind of clothes is the man wearing", 1],
            ["what sport is being played", 1],
            ["what is the man holding", 1],
            ["what sport is being played ", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man standing on", 1],
            ["what is in the background", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", -1],
            ["where is the photo taken", 1],
            ["what are the people doing", 1],
            ["what kind of clothes is the man wearing", 1],
            ["who is in the photo", -1],
            ["what sport is being played", 1],
            ["what is the man holding", 1],
            ["what is on the player's head", -1],
            ["what sport is being played ", 1],
            ["what is the man wearing on head", -1],
            ["where is the man", 1]
        ],
        "context": [
            "a baseball player walking on a field",
            "a man sitting on a tennis court holding a tennis racket."
        ]
    },
    {
        "object_category": "truck",
        "images": [
            {
                "VG_image_id": "2360589",
                "VG_object_id": "3891764",
                "bbox": [387, 257, 478, 352],
                "image": "data\\images\\2360589.jpg"
            },
            {
                "VG_image_id": "2372534",
                "VG_object_id": "591543",
                "bbox": [201, 246, 264, 312],
                "image": "data\\images\\2372534.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many trucks are there", 1],
            ["what is on the side of the car", 1]
        ],
        "org_questions": [
            ["what color is the truck", -1],
            ["where is the truck ", -1],
            ["how many trucks are there", 1],
            ["what is the ground covered with", -1],
            ["how is the weather", -1],
            ["what is in the distance", -1],
            ["what kind of land is the truck on", -1],
            ["what is on the truck", -1],
            ["when was the photo taken", -1],
            ["what kind of vehicle is this", -1],
            ["where was the photo taken", -1],
            ["what is on the side of the car", 1]
        ],
        "context": [
            "a model of a model train set.",
            "a large building with a lot of windows"
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2341127",
                "VG_object_id": "2483160",
                "bbox": [305, 176, 390, 285],
                "image": "data\\images\\2341127.jpg"
            },
            {
                "VG_image_id": "2394474",
                "VG_object_id": "2605004",
                "bbox": [67, 88, 194, 228],
                "image": "data\\images\\2394474.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is it", 2],
            ["what color is the woman's shirt", 1],
            ["what color are the woman's trousers", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what color are the woman's trousers", 1],
            ["what is the woman holding", 1],
            ["where is the photo taken", -1],
            ["what is the ground covered with", -1],
            ["what sport is it", 2],
            ["where is the person", -1],
            ["how many people are in the photo", -1],
            ["when was the photo taken", -1],
            ["what type of shirt is the woman wearing", -1]
        ],
        "context": [
            "a woman and a child playing with a ball in a field.",
            "a woman throwing a frisbee in a wooded area."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2364498",
                "VG_object_id": "2169919",
                "bbox": [1, 87, 110, 430],
                "image": "data\\images\\2364498.jpg"
            },
            {
                "VG_image_id": "2356812",
                "VG_object_id": "2143230",
                "bbox": [297, 7, 458, 334],
                "image": "data\\images\\2356812.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's coat", 1],
            ["what is behind the man", 1],
            ["How many people are there", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the man's coat", 1],
            ["what color is the man's trouser", -1],
            ["what is behind the man", 1],
            ["How many people are there", 1],
            ["what is the man wearing on head", -1],
            ["where is the photo taken", -1],
            ["what is the man doing", -1],
            ["who is holding the umbrella", -1],
            ["when was the picture taken", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a man and a woman walking across a crosswalk in the rain.",
            "a man holding an umbrella in front of a fruit stand."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2358224",
                "VG_object_id": "1989538",
                "bbox": [221, 81, 468, 159],
                "image": "data\\images\\2358224.jpg"
            },
            {
                "VG_image_id": "2315894",
                "VG_object_id": "3464373",
                "bbox": [93, 40, 428, 302],
                "image": "data\\images\\2315894.jpg"
            }
        ],
        "questions_with_scores": [["what color is the airplane", 1]],
        "org_questions": [
            ["what color is the airplane", 1],
            ["how many airplanes are in the picture", -1],
            ["when is this photo taken", -1],
            ["where is the plane", -1],
            ["what is in front of the plane", -1],
            ["what color is the sky", -1],
            ["how is the weather", -1],
            ["what is the plane doing", -1],
            ["what is flying in the sky", -1],
            ["what is flying", -1]
        ],
        "context": [
            "a large jetliner flying through a blue sky.",
            "a small airplane flying in the sky"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2355855",
                "VG_object_id": "3565716",
                "bbox": [3, 80, 55, 201],
                "image": "data\\images\\2355855.jpg"
            },
            {
                "VG_image_id": "2415457",
                "VG_object_id": "2959376",
                "bbox": [134, 26, 214, 178],
                "image": "data\\images\\2415457.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 2],
            ["what is the color of  the man's pants", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the color of  the man's pants", 1],
            ["how many women are there in the picture", -1],
            ["where is the picture taken", 2],
            ["what is the man wearing on his head", -1],
            ["what is the man doing", -1],
            ["who is wearing a black shirt", -1],
            ["when was the photo taken", -1],
            ["what are the people wearing", -1],
            ["what is the man on the left wearing", -1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a group of children riding bikes down a track.",
            "a woman sitting at a table talking on a cell phone."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2319420",
                "VG_object_id": "3028675",
                "bbox": [139, 20, 241, 332],
                "image": "data\\images\\2319420.jpg"
            },
            {
                "VG_image_id": "150482",
                "VG_object_id": "1075229",
                "bbox": [37, 394, 143, 680],
                "image": "data\\images\\150482.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's clothes", 2],
            ["what is the persion wearing on head", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding on her hand", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", 2],
            ["what is the persion wearing on head", 1],
            ["how many horses are there in the picture", -1],
            ["what is the woman doing", 1],
            ["where is the woman", -1],
            ["what is the woman holding on her hand", 1],
            ["what is on the woman's neck", -1],
            ["what is the gender of the person in the blue jacket", -1],
            ["when was the photo taken", -1],
            ["what is the persion in the blue shirt wearing", -1]
        ],
        "context": [
            "a man standing on the sidewalk with a woman on his phone.",
            "police officers on horses stand in the middle of a street."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2342787",
                "VG_object_id": "930804",
                "bbox": [92, 144, 145, 211],
                "image": "data\\images\\2342787.jpg"
            },
            {
                "VG_image_id": "2413902",
                "VG_object_id": "163840",
                "bbox": [188, 203, 312, 291],
                "image": "data\\images\\2413902.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is on the table", 1],
            ["how many people are in the picture", -1],
            ["what shape is the table", -1],
            ["what is the table made of", -1],
            ["what is beside the table", -1],
            ["how many tables are there", -1],
            ["what is next to the table", -1]
        ],
        "context": [
            "a living room with a couch, a lamp and a window.",
            "a living room with a couch, chairs, television and a tv."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2391032",
                "VG_object_id": "1244626",
                "bbox": [12, 98, 95, 153],
                "image": "data\\images\\2391032.jpg"
            },
            {
                "VG_image_id": "2367299",
                "VG_object_id": "1969871",
                "bbox": [273, 110, 351, 190],
                "image": "data\\images\\2367299.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["what is the gender of the person", 2],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 2],
            ["what is the gender of the person", 2],
            ["what is the person doing", -1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["how many children are there in the picture", -1],
            ["what color is the background", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a man flying through the air while riding a skateboard.",
            "two women on skis on a snowy mountain"
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2327909",
                "VG_object_id": "2865868",
                "bbox": [118, 361, 286, 455],
                "image": "data\\images\\2327909.jpg"
            },
            {
                "VG_image_id": "2339335",
                "VG_object_id": "3249390",
                "bbox": [170, 54, 408, 292],
                "image": "data\\images\\2339335.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the motorcycle", 2],
            ["what color is the rider's shirt", 1]
        ],
        "org_questions": [
            ["what is on the motorcycle", -1],
            ["what color is the motorcycle", 2],
            ["where is the motor cycle", -1],
            ["how many people are there", -1],
            ["what is the motorcycle doing", -1],
            ["what is in the distance", -1],
            ["how many dogs are there in the picture", -1],
            ["what color is the rider's shirt", 1],
            ["what type of vehicle is shown", -1],
            ["who is riding the motorcycle", -1],
            ["what is the man riding", -1],
            ["where was the picture taken", -1],
            ["where is the motorcycle", -1]
        ],
        "context": [
            "three people sitting on a motorcycle in front of a building.",
            "a group of police officers riding a motorcycle with a teddy bear on the side."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2363701",
                "VG_object_id": "3744331",
                "bbox": [0, 315, 331, 496],
                "image": "data\\images\\2363701.jpg"
            },
            {
                "VG_image_id": "2406943",
                "VG_object_id": "287818",
                "bbox": [4, 170, 496, 275],
                "image": "data\\images\\2406943.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing on the beach", 2],
            ["what is the man standing on", 1],
            ["how many people are there", 1],
            ["what is standing on the beach", 1],
            ["what is the person doing on the beach", 1]
        ],
        "org_questions": [
            ["what color is the beach", -1],
            ["what is on the beach", -1],
            ["what is the man standing on", 1],
            ["how many people are there", 1],
            ["what are the people doing on the beach", 2],
            ["what is standing on the beach", 1],
            ["what is the person doing on the beach", 1],
            ["where was this photo taken", -1],
            ["what is the ground covered with", -1]
        ],
        "context": [
            "a couple of people riding horses on a beach.",
            "a woman is throwing a frisbee on the beach."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2404050",
                "VG_object_id": "1119230",
                "bbox": [81, 6, 413, 310],
                "image": "data\\images\\2404050.jpg"
            },
            {
                "VG_image_id": "2373797",
                "VG_object_id": "2575329",
                "bbox": [231, 70, 489, 284],
                "image": "data\\images\\2373797.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the girl's shirt", 2],
            ["what is the girl doing", 1],
            ["Where is the girl", 1],
            ["what is the persion holding", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what color is the girl's hair", -1],
            ["what color is the girl's shirt", 2],
            ["Where is the girl", 1],
            ["what is the persion holding", 1],
            ["how many people are there", 1],
            ["what is the gender of the person in the photo", -1],
            ["who is in the photo", -1],
            ["what is the girl wearing", -1]
        ],
        "context": [
            "a little girl playing with a computer keyboard.",
            "a woman with a ponytail and a gray hoodie."
        ]
    },
    {
        "object_category": "boat",
        "images": [
            {
                "VG_image_id": "2335908",
                "VG_object_id": "2692320",
                "bbox": [59, 157, 258, 234],
                "image": "data\\images\\2335908.jpg"
            },
            {
                "VG_image_id": "2412674",
                "VG_object_id": "189309",
                "bbox": [17, 167, 414, 254],
                "image": "data\\images\\2412674.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the water under the boat", 1],
            ["what color is the sky", 1],
            ["what is the boat doing", 1],
            ["what is the boat made of", 1]
        ],
        "org_questions": [
            ["what color is the water under the boat", 1],
            ["what color is the sky", 1],
            ["how many people are there", -1],
            ["what is the boat doing", 1],
            ["what is the boat made of", 1],
            ["where is the boat", -1],
            ["What is the background of image", -1],
            ["when was the photo taken", -1],
            ["what is on the water", -1],
            ["what is in the background", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a group of boats sitting in the water.",
            "a group of sailboats sailing across a body of water."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2416801",
                "VG_object_id": "1057205",
                "bbox": [176, 124, 495, 365],
                "image": "data\\images\\2416801.jpg"
            },
            {
                "VG_image_id": "2354894",
                "VG_object_id": "1689644",
                "bbox": [0, 237, 371, 498],
                "image": "data\\images\\2354894.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is quilt", 1],
            ["what is on the bed", 1],
            ["how many pillows are there on the bed", 1]
        ],
        "org_questions": [
            ["what color is quilt", 1],
            ["what is on the bed", 1],
            ["how many people are there", 2],
            ["where is this bed", -1],
            ["how many pillows are there on the bed", 1],
            ["what room is this", -1],
            ["what is behind the bed", -1],
            ["what is covering the bed", -1]
        ],
        "context": [
            "a bed sitting in a bedroom next to a lamp.",
            "a little girl jumping on a bed with a white sheet."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "713143",
                "VG_object_id": "1579540",
                "bbox": [463, 637, 695, 765],
                "image": "data\\images\\713143.jpg"
            },
            {
                "VG_image_id": "2380079",
                "VG_object_id": "711355",
                "bbox": [272, 41, 360, 179],
                "image": "data\\images\\2380079.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the wall", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", -1],
            ["where is the cabinet", -1],
            ["what is hanging on the cabinet", -1],
            ["what room is this", -1],
            ["what are the cabinets made of", -1],
            ["where was this picture taken", -1],
            ["where was the photo taken", -1],
            ["what color is the wall", -1],
            ["what is on the wall", 1],
            ["how many people are there", 1],
            ["where is the picture taken", -1],
            ["where are the cabinets", -1]
        ],
        "context": [
            "a kitchen with a sink and a refrigerator.",
            "a woman and a girl are cooking in a kitchen."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2323224",
                "VG_object_id": "3012791",
                "bbox": [7, 185, 487, 328],
                "image": "data\\images\\2323224.jpg"
            },
            {
                "VG_image_id": "2398186",
                "VG_object_id": "1181853",
                "bbox": [4, 243, 500, 369],
                "image": "data\\images\\2398186.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is the color of the ground", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the color of the ground", 1],
            ["what is on the ground", -1],
            ["how many people are there in the picture", 2],
            ["where was the photo taken", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "a man riding on the back of a motorcycle in a field.",
            "a giraffe standing in a field"
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2316772",
                "VG_object_id": "3433964",
                "bbox": [247, 89, 280, 184],
                "image": "data\\images\\2316772.jpg"
            },
            {
                "VG_image_id": "713875",
                "VG_object_id": "1589848",
                "bbox": [535, 640, 642, 814],
                "image": "data\\images\\713875.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bottle", 2],
            ["what is behind the bottle", 1]
        ],
        "org_questions": [
            ["what color is the bottle", -1],
            ["where is the bottle", 2],
            ["how many cats are there in the picture", -1],
            ["what is behind the bottle", 1],
            ["what is he bottle made of", -1],
            ["what is in the jar", -1],
            ["what is next to the bottle", -1],
            ["what is the main color of the cup", -1],
            ["what is the color of the bottle", -1]
        ],
        "context": [
            "a plate of food with fries and a sandwich.",
            "a woman is looking inside a refrigerator."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2415730",
                "VG_object_id": "2945248",
                "bbox": [2, 40, 362, 312],
                "image": "data\\images\\2415730.jpg"
            },
            {
                "VG_image_id": "2352877",
                "VG_object_id": "2306489",
                "bbox": [6, 2, 495, 371],
                "image": "data\\images\\2352877.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is color of the building", 1],
            ["what is on the building", 1],
            ["what color is the sky", 1]
        ],
        "org_questions": [
            ["what is color of the building", 1],
            ["what is on the building", 1],
            ["how many people are there", -1],
            ["when is this picture taken", -1],
            ["what is the weather like", -1],
            ["what is the building made of", -1],
            ["where was the photo taken", -1],
            ["what is in the background", -1],
            ["what color is the sky", 1],
            ["when was the photo taken", -1],
            ["how many signs are in the picture", -1]
        ],
        "context": [
            "a telephone pole and a street sign in front of a house.",
            "a street sign on a pole in front of a tall building."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2396754",
                "VG_object_id": "1195383",
                "bbox": [26, 159, 499, 225],
                "image": "data\\images\\2396754.jpg"
            },
            {
                "VG_image_id": "2325875",
                "VG_object_id": "2892709",
                "bbox": [7, 182, 498, 358],
                "image": "data\\images\\2325875.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the table", 2],
            ["what color is the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what color is the keyboard", -1],
            ["how many plates are on the table", -1],
            ["what shape is the table", 2],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["what is sitting on the desk", -1],
            ["where was this photo taken", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a laptop computer sitting on top of a desk.",
            "a desk with two computer monitors and a keyboard."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2403273",
                "VG_object_id": "1126346",
                "bbox": [176, 158, 363, 374],
                "image": "data\\images\\2403273.jpg"
            },
            {
                "VG_image_id": "2345884",
                "VG_object_id": "2062270",
                "bbox": [77, 40, 329, 163],
                "image": "data\\images\\2345884.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["what is on the shelf", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the shelf", -1],
            ["how many people are in the picture", 1],
            ["where is the shelf", -1],
            ["what is on the shelf", 1],
            ["what is in the background", -1],
            ["how many books are there on the shelf", -1],
            ["who is in the photo", 1],
            ["where was this picture taken", -1],
            ["what is the shelf made of", -1],
            ["what color is the wall behind the shelf", -1],
            ["where is the shelf placing", -1],
            ["when was the picture taken", -1],
            ["what is this a picture of", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a man in a white shirt",
            "a commercial kitchen with a stove and ovens."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2344149",
                "VG_object_id": "2527785",
                "bbox": [88, 286, 141, 376],
                "image": "data\\images\\2344149.jpg"
            },
            {
                "VG_image_id": "2351522",
                "VG_object_id": "1889863",
                "bbox": [388, 203, 446, 280],
                "image": "data\\images\\2351522.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the dog", 1],
            ["where is the dog", 1],
            ["what is the ground the dog standing on made of", 1],
            ["what is the persion doing", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the dog", 1],
            ["where is the dog", 1],
            ["how many dogs are in the picture", -1],
            ["what is the ground the dog standing on made of", 1],
            ["what is the persion doing", 1],
            ["how is the weather", -1],
            ["what is in the background", 1],
            ["what kind of animal is in the picture", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is on the ground", -1],
            ["how many dogs", -1]
        ],
        "context": [
            "a man and a woman walking a dog on a sidewalk.",
            "three people riding horses on a beach with a dog"
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2404484",
                "VG_object_id": "342197",
                "bbox": [159, 73, 499, 246],
                "image": "data\\images\\2404484.jpg"
            },
            {
                "VG_image_id": "2358680",
                "VG_object_id": "2530349",
                "bbox": [282, 155, 424, 255],
                "image": "data\\images\\2358680.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the building made of", 1],
            ["where is the building", 1],
            ["What is ratio of size of building to size of image", 1],
            ["what is on the ground", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the building made of", 1],
            ["what color is the building's roof", -1],
            ["where is the building", 1],
            ["what shape is the roof of the building", -1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["What is ratio of size of building to size of image", 1],
            ["what is the weather like", -1],
            ["what is on the ground", 1],
            ["when was the picture taken", -1],
            ["where was the photo taken", 1],
            ["what is in the background", -1]
        ],
        "context": [
            "a house with a red roof sits on a river bank.",
            "a man walking across a street with a traffic light."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2367645",
                "VG_object_id": "2678031",
                "bbox": [282, 124, 391, 351],
                "image": "data\\images\\2367645.jpg"
            },
            {
                "VG_image_id": "2362427",
                "VG_object_id": "3750238",
                "bbox": [294, 61, 434, 217],
                "image": "data\\images\\2362427.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 1],
            ["What is the man doing", 1],
            ["how many people are there", 1],
            ["what is the land covered with", 1],
            ["what color is the background", 1]
        ],
        "org_questions": [
            ["What color is the man's shirt", 1],
            ["What is the man doing", 1],
            ["Where is the man", -1],
            ["how many people are there", 1],
            ["what is the man wearing", -1],
            ["what is the land covered with", 1],
            ["what color is the background", 1],
            ["when was the photo taken", -1],
            ["what is on the man's head", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a group of people standing next to an elephant.",
            "a man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2361594",
                "VG_object_id": "1848444",
                "bbox": [267, 4, 380, 198],
                "image": "data\\images\\2361594.jpg"
            },
            {
                "VG_image_id": "2317574",
                "VG_object_id": "3790947",
                "bbox": [239, 198, 334, 398],
                "image": "data\\images\\2317574.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["what is in the background", 1],
            ["Where is the person", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what is in the background", 1],
            ["how many people are there in the picture", -1],
            ["Where is the person", 1],
            ["What is the gender of the person", -1],
            ["what is the person on", -1],
            ["when was the photo taken", -1],
            ["what kind of pants is the man wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man riding a skateboard down a street.",
            "a man standing outside of a building talking on a cell phone."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2354548",
                "VG_object_id": "837628",
                "bbox": [1, 175, 500, 332],
                "image": "data\\images\\2354548.jpg"
            },
            {
                "VG_image_id": "2416160",
                "VG_object_id": "3410824",
                "bbox": [27, 97, 469, 263],
                "image": "data\\images\\2416160.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is behind the bench", 1],
            ["what is the person doing on the bench", 1],
            ["what is the bench made of", 1],
            ["What is on the bench", 1]
        ],
        "org_questions": [
            ["what is behind the bench", 1],
            ["what is the person doing on the bench", 1],
            ["how many children are on the bench", -1],
            ["When is photo taken", -1],
            ["what is the bench made of", 1],
            ["what is the weather like", -1],
            ["What is on the bench", 1],
            ["where was the picture taken", -1],
            ["what is the persion sitting on", -1]
        ],
        "context": [
            "a woman sitting on a bench looking at her cell phone.",
            "a pile of clothes and a backpack on a bench."
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2349084",
                "VG_object_id": "2374781",
                "bbox": [23, 17, 499, 205],
                "image": "data\\images\\2349084.jpg"
            },
            {
                "VG_image_id": "2367289",
                "VG_object_id": "623503",
                "bbox": [5, 12, 499, 373],
                "image": "data\\images\\2367289.jpg"
            }
        ],
        "questions_with_scores": [["what color is the table", 1]],
        "org_questions": [
            ["what color is the wall", -1],
            ["what color is the table", 1],
            ["how many televisions are there in the picture", -1],
            ["what is on the ground", -1],
            ["what is the floor made of", -1],
            ["how many people are there in the room", -1],
            ["what is the ground covered with", -1],
            ["what room is this", -1],
            ["when was the photo taken", -1],
            ["where is this picture taken", -1]
        ],
        "context": [
            "a living room with a fireplace and a chair.",
            "a living room with a couch, coffee table and a couch."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2357907",
                "VG_object_id": "807651",
                "bbox": [124, 50, 237, 324],
                "image": "data\\images\\2357907.jpg"
            },
            {
                "VG_image_id": "2352447",
                "VG_object_id": "3583339",
                "bbox": [78, 342, 130, 468],
                "image": "data\\images\\2352447.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 1],
            ["where is the woman", 1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["what is the persion standing on", 1],
            ["What is the background of image", 1],
            ["What is the person holding", 1],
            ["what is the persion on the left doing", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["How many people are there", -1],
            ["What is the gender of the person", -1],
            ["where is the woman", 1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion standing on", 1],
            ["who is in the picture", -1],
            ["how is the weather", -1],
            ["What is the background of image", 1],
            ["What is the person holding", 1],
            ["when was this photo taken", -1],
            ["what is the persion on the left doing", 1]
        ],
        "context": [
            "a woman on skis in the snow in the woods.",
            "a woman flying a kite in a field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2364433",
                "VG_object_id": "1690963",
                "bbox": [291, 103, 386, 246],
                "image": "data\\images\\2364433.jpg"
            },
            {
                "VG_image_id": "2366927",
                "VG_object_id": "2410435",
                "bbox": [230, 98, 314, 303],
                "image": "data\\images\\2366927.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the woman's head", 2],
            ["what is the woman doing", 1],
            ["what color is the woman's shirt", 1],
            ["what is the woman holding", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", -1],
            ["what is the woman doing", 1],
            ["what color is the woman's shirt", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["when was the picture taken", -1],
            ["what is the woman holding", 1],
            ["what is on the woman's head", 2],
            ["what is the persion on the left wearing", 1]
        ],
        "context": [
            "a man is kneeling down while holding a kite.",
            "a woman standing next to a horse in a field."
        ]
    },
    {
        "object_category": "lamp",
        "images": [
            {
                "VG_image_id": "2354539",
                "VG_object_id": "3776983",
                "bbox": [256, 79, 307, 125],
                "image": "data\\images\\2354539.jpg"
            },
            {
                "VG_image_id": "2316134",
                "VG_object_id": "3461985",
                "bbox": [206, 55, 258, 107],
                "image": "data\\images\\2316134.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is beside the lamp", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what shape is the lamp", -1],
            ["what color is the lamp", -1],
            ["where is the lamp", -1],
            ["how many lamps are there in the picture", -1],
            ["when is this picture taken", -1],
            ["what is the lamp putting on", -1],
            ["what is beside the lamp", 1],
            ["where is the picture taken", 1],
            ["who is in the picture", -1],
            ["what is hanging on the wall", -1],
            ["when was the photo taken", -1],
            ["what is on the wall", -1]
        ],
        "context": [
            "a hotel room with a desk, television, and a desk.",
            "a bed with a brown blanket and a picture of a bed."
        ]
    },
    {
        "object_category": "bear",
        "images": [
            {
                "VG_image_id": "2372006",
                "VG_object_id": "3853658",
                "bbox": [193, 115, 372, 330],
                "image": "data\\images\\2372006.jpg"
            },
            {
                "VG_image_id": "2408694",
                "VG_object_id": "1090835",
                "bbox": [185, 81, 359, 301],
                "image": "data\\images\\2408694.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the bear", 2],
            ["what color is the bear's shirt", 1],
            ["what is on the bear's head", 1],
            ["what is behind the bear", 1]
        ],
        "org_questions": [
            ["where is the bear", 2],
            ["what color is the bear's shirt", 1],
            ["how many bears are there", -1],
            ["what is the bear doing", -1],
            ["what is the bear wearing", -1],
            ["what is the bear on", -1],
            ["what is on the bear's head", 1],
            ["what is on the bear", -1],
            ["what is behind the bear", 1],
            ["what is the bear's color", -1],
            ["what is the main color of the bear", -1]
        ],
        "context": [
            "a baby laying in a crib next to a teddy bear.",
            "a teddy bear with a sign on it's back."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2370909",
                "VG_object_id": "1675677",
                "bbox": [4, 177, 498, 278],
                "image": "data\\images\\2370909.jpg"
            },
            {
                "VG_image_id": "2326802",
                "VG_object_id": "3197860",
                "bbox": [0, 222, 498, 297],
                "image": "data\\images\\2326802.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the street", 1],
            ["how many buses are there on the street", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the street", 1],
            ["what is on the ground", -1],
            ["how many buses are there on the street", 1],
            ["how is the weather", -1],
            ["what is the ground covered with", 1],
            ["how many people are on the street", -1],
            ["where was this photo taken", -1],
            ["what is the weather like", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a red double decker bus driving down a street.",
            "a street with traffic lights hanging from wires."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2382717",
                "VG_object_id": "537567",
                "bbox": [11, 112, 251, 272],
                "image": "data\\images\\2382717.jpg"
            },
            {
                "VG_image_id": "2317879",
                "VG_object_id": "3467774",
                "bbox": [1, 4, 331, 389],
                "image": "data\\images\\2317879.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing in the head", 2],
            ["what color is the man's shirt", 2],
            ["what color is the man's pants", 2],
            ["how many people are there", 1],
            ["what is on the boy's head", 1]
        ],
        "org_questions": [
            ["what is the man wearing in the head", 2],
            ["what color is the man's shirt", 2],
            ["how many people are there", 1],
            ["what is the boy holding", -1],
            ["where is the skateboard", -1],
            ["who is on the skateboard", -1],
            ["what is the boy on the skateboard doing", -1],
            ["what is in the background", -1],
            ["what is on the boy's head", 1],
            ["what color is the man's pants", 2]
        ],
        "context": [
            "a young man is doing a trick on a skateboard.",
            "a man doing a trick on a skateboard."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2357195",
                "VG_object_id": "814111",
                "bbox": [130, 90, 267, 204],
                "image": "data\\images\\2357195.jpg"
            },
            {
                "VG_image_id": "2407805",
                "VG_object_id": "3810094",
                "bbox": [83, 141, 237, 256],
                "image": "data\\images\\2407805.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is person's shirt", 2],
            ["who is wearing the shirt", 2],
            ["What sports is person doing", 1],
            ["what gender is the person in the shirt", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["What sports is person doing", 1],
            ["What color is person's shirt", 2],
            ["What color is the ground", -1],
            ["who is wearing the shirt", 2],
            ["what is the ground covered with", -1],
            ["what gender is the person in the shirt", 1],
            ["when was the photo taken", -1],
            ["how many people are there", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a young girl holding a tennis racquet and ball.",
            "a man is throwing a frisbee in a park."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2344016",
                "VG_object_id": "918167",
                "bbox": [160, 163, 219, 209],
                "image": "data\\images\\2344016.jpg"
            },
            {
                "VG_image_id": "2377858",
                "VG_object_id": "3201576",
                "bbox": [2, 57, 202, 324],
                "image": "data\\images\\2377858.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the land made of ", 2],
            ["where is the person", 1],
            ["where are the people staying", 1],
            ["what is on the ground", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the land made of ", 2],
            ["what gender is the person who wears the shirt", -1],
            ["where is the person", 1],
            ["what is the person wearing", -1],
            ["where are the people staying", 1],
            ["how many people are shown", -1],
            ["when was the photo taken", -1],
            ["what is on the ground", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a man standing on a sidewalk next to a wall.",
            "a man kneeling down next to a toothbrush."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2357496",
                "VG_object_id": "3553225",
                "bbox": [116, 77, 496, 373],
                "image": "data\\images\\2357496.jpg"
            },
            {
                "VG_image_id": "2416308",
                "VG_object_id": "3225937",
                "bbox": [21, 17, 484, 277],
                "image": "data\\images\\2416308.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many bags are there", 1],
            ["what is in front of the bag", 1]
        ],
        "org_questions": [
            ["how many bags are there", 1],
            ["what color is the bag", -1],
            ["where is the picture taken", -1],
            ["what is in front of the bag", 1],
            ["what is the bag made of", -1],
            ["who is in the picture", -1],
            ["where are the suitcases", -1],
            ["what is on top of the suitcase", -1],
            ["where is the backpack", -1]
        ],
        "context": [
            "a suitcase is packed with clothes and clothes.",
            "a black bag with a passport and a passport"
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2356918",
                "VG_object_id": "1903452",
                "bbox": [85, 94, 246, 331],
                "image": "data\\images\\2356918.jpg"
            },
            {
                "VG_image_id": "2351237",
                "VG_object_id": "2092365",
                "bbox": [99, 121, 211, 460],
                "image": "data\\images\\2351237.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["What is gender of person", 1],
            ["what is the persion wearing", 1],
            ["what is the persion doing", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What is gender of person", 1],
            ["what is the persion wearing", 1],
            ["what is the persion doing", 1],
            ["what is in the distance", -1],
            ["where is the person", -1],
            ["when was the photo taken", -1],
            ["how is the weather", -1],
            ["who is in the photo", 1]
        ],
        "context": [
            "two girls walking in the street with umbrellas.",
            "a man standing on a street corner next to a fire hydrant."
        ]
    },
    {
        "object_category": "bag",
        "images": [
            {
                "VG_image_id": "2364589",
                "VG_object_id": "2437681",
                "bbox": [142, 175, 309, 335],
                "image": "data\\images\\2364589.jpg"
            },
            {
                "VG_image_id": "2359333",
                "VG_object_id": "794220",
                "bbox": [7, 68, 244, 223],
                "image": "data\\images\\2359333.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the  bag", 1],
            ["what is the bag on", 1],
            ["what is on the bag", 1],
            ["where is the bag", 1],
            ["what is on the left side of the picture", 1],
            ["what is black and white", 1],
            ["what is on the left side", 1],
            ["what is on the side of the bag", 1]
        ],
        "org_questions": [
            ["what is the color of the  bag", 1],
            ["what is the bag on", 1],
            ["what is on the bag", 1],
            ["how many people are there", -1],
            ["where is the bag", 1],
            ["what is on the left side of the picture", 1],
            ["what is black and white", 1],
            ["what is on the left side", 1],
            ["what is on the side of the bag", 1]
        ],
        "context": [
            "a couple of bags that are sitting on the floor",
            "a laptop, camera, and other electronics on a couch."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2350569",
                "VG_object_id": "2412094",
                "bbox": [119, 77, 309, 212],
                "image": "data\\images\\2350569.jpg"
            },
            {
                "VG_image_id": "2408822",
                "VG_object_id": "254368",
                "bbox": [101, 86, 379, 331],
                "image": "data\\images\\2408822.jpg"
            }
        ],
        "questions_with_scores": [["where is the elephant", 1]],
        "org_questions": [
            ["where is the elephant", 1],
            ["how many people are in the picture", -1],
            ["what color is the background", -1],
            ["what is the elephant doing", -1],
            ["What is in the background", -1],
            ["how many teeth do the elephants have", -1],
            ["how many elephants are there", -1],
            ["what type of animal is shown", -1],
            ["what is on the elephant's head", -1],
            ["what are the elephants standing on", -1],
            ["what is the elephant holding", -1]
        ],
        "context": [
            "a group of people looking at an elephant at the zoo.",
            "a man standing next to an elephant in a forest."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2319186",
                "VG_object_id": "1003981",
                "bbox": [202, 146, 358, 309],
                "image": "data\\images\\2319186.jpg"
            },
            {
                "VG_image_id": "2393115",
                "VG_object_id": "474716",
                "bbox": [235, 32, 323, 203],
                "image": "data\\images\\2393115.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's helmet", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the man's helmet", 1],
            ["what is the main color of the ground", -1],
            ["who is playing tennis", -1],
            ["what sport is the person playing", -1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["what are the players doing", -1],
            ["when was the photo taken", -1],
            ["where is the batter", -1],
            ["what is on the batter's head", -1],
            ["what is the catcher wearing", -1]
        ],
        "context": [
            "a baseball player swinging a bat",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "plant",
        "images": [
            {
                "VG_image_id": "2408536",
                "VG_object_id": "259800",
                "bbox": [72, 146, 251, 283],
                "image": "data\\images\\2408536.jpg"
            },
            {
                "VG_image_id": "2321839",
                "VG_object_id": "3293706",
                "bbox": [384, 1, 481, 72],
                "image": "data\\images\\2321839.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is  the plant", 1],
            ["what color is the vase", 1]
        ],
        "org_questions": [
            ["what color is  the plant", 1],
            ["what color is the vase", 1],
            ["how many birds are there in the picture", -1],
            ["what room is the plant in", -1],
            ["where is the plate", -1],
            ["where is the photo taken", -1],
            ["what is in the background", -1],
            ["where is the plant", -1]
        ],
        "context": [
            "a vase with white flowers sitting on a counter.",
            "two women sitting at a table eating food."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2409287",
                "VG_object_id": "363542",
                "bbox": [3, 7, 489, 83],
                "image": "data\\images\\2409287.jpg"
            },
            {
                "VG_image_id": "2409276",
                "VG_object_id": "245170",
                "bbox": [1, 38, 492, 98],
                "image": "data\\images\\2409276.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["what can be seen in the background", 2],
            ["what color is the ground", 1],
            ["what is on the ground", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["where is the photo taken", 2],
            ["what color is the ground", 1],
            ["how many planes are there in the picture", -1],
            ["What time is it", -1],
            ["what is on the ground", 1],
            ["what is the ground covered with", 1],
            ["how is the weather", -1],
            ["when was this photo taken", -1],
            ["what can be seen in the background", 2]
        ],
        "context": [
            "a herd of sheep standing on top of a lush green field.",
            "a man walking on a beach with a surfboard."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2375743",
                "VG_object_id": "2214457",
                "bbox": [7, 211, 452, 373],
                "image": "data\\images\\2375743.jpg"
            },
            {
                "VG_image_id": "2378326",
                "VG_object_id": "1785262",
                "bbox": [7, 137, 498, 315],
                "image": "data\\images\\2378326.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many laptops are there on the table", 1],
            ["how many people are there", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["what color is the table", 2],
            ["how many laptops are there on the table", 1],
            ["what is the table made of", -1],
            ["what shape is the table", -1],
            ["how many people are there", 1],
            ["how many plates are there", -1],
            ["where was this picture taken", -1],
            ["what room is this", 1],
            ["what is on the table", -1],
            ["where is the laptop", -1]
        ],
        "context": [
            "a man sitting at a table with a laptop.",
            "a room with a laptop, headphones and headphones on a table."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2377475",
                "VG_object_id": "565423",
                "bbox": [3, 204, 497, 294],
                "image": "data\\images\\2377475.jpg"
            },
            {
                "VG_image_id": "2391792",
                "VG_object_id": "486173",
                "bbox": [1, 193, 498, 331],
                "image": "data\\images\\2391792.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many animals are there on the ground", 1],
            ["what animal is on the ground", 1],
            ["What is in the distance", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["what is on the ground", -1],
            ["how many animals are there on the ground", 1],
            ["what animal is on the ground", 1],
            ["what is the weather like", -1],
            ["What is in the distance", 1],
            ["how many zebras are there", -1],
            ["when was this picture taken", -1],
            ["where is the grass", -1],
            ["how is the grass", -1],
            ["where was this picture taken", 1]
        ],
        "context": [
            "a giraffe standing in a dirt area next to a rock wall.",
            "two cows grazing on grass near a body of water."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2332655",
                "VG_object_id": "2956652",
                "bbox": [334, 0, 489, 332],
                "image": "data\\images\\2332655.jpg"
            },
            {
                "VG_image_id": "2396464",
                "VG_object_id": "443996",
                "bbox": [93, 20, 256, 499],
                "image": "data\\images\\2396464.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's clothes", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", 1],
            ["what are the man holding", -1],
            ["what is the man wearing", -1],
            ["how many people are there", 1],
            ["what sport is the man doing", -1],
            ["what is the man wearing on his face", -1],
            ["what is the man doing", -1],
            ["where is the man standing", -1],
            ["who is in the picture", -1],
            ["what is the man standing on", -1],
            ["what type of pants is the man wearing", -1]
        ],
        "context": [
            "a man standing in a living room holding a wii remote.",
            "a man standing in a living room holding a wii controller."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2366279",
                "VG_object_id": "2890677",
                "bbox": [0, 3, 500, 373],
                "image": "data\\images\\2366279.jpg"
            },
            {
                "VG_image_id": "2373550",
                "VG_object_id": "731788",
                "bbox": [1, 1, 365, 498],
                "image": "data\\images\\2373550.jpg"
            }
        ],
        "questions_with_scores": [["what color is the wall", 2]],
        "org_questions": [
            ["what color is the wall", 2],
            ["what color is the table", -1],
            ["what is the floor made of", -1],
            ["what is on the counter", -1],
            ["what are the cabinets made of", -1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["what type of room is this", -1]
        ],
        "context": [
            "a kitchen with a table, refrigerator, and refrigerator.",
            "a kitchen with a island and a sink."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2342724",
                "VG_object_id": "931271",
                "bbox": [2, 215, 171, 368],
                "image": "data\\images\\2342724.jpg"
            },
            {
                "VG_image_id": "2320263",
                "VG_object_id": "994679",
                "bbox": [1, 162, 498, 333],
                "image": "data\\images\\2320263.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is on the ground", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["What is on the ground", 1],
            ["how many people are there", -1],
            ["what is the land made of", -1],
            ["What is the ground made of", -1],
            ["where was the photo taken", 1],
            ["what is the weather like", -1],
            ["when was the picture taken", -1],
            ["how is the weather", -1]
        ],
        "context": [
            "a large blue airplane is parked on the tarmac.",
            "a motorcycle parked on the side of a street."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2417689",
                "VG_object_id": "3457650",
                "bbox": [202, 130, 309, 226],
                "image": "data\\images\\2417689.jpg"
            },
            {
                "VG_image_id": "2378275",
                "VG_object_id": "2609410",
                "bbox": [66, 243, 177, 346],
                "image": "data\\images\\2378275.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the player's pants", 2],
            ["how many people are there", 2],
            ["what is the player holding", 1],
            ["what color is the trousers", 1],
            ["how many people are there in the photo", 1]
        ],
        "org_questions": [
            ["what color is the player's pants", 2],
            ["what is the player holding", 1],
            ["how many people are there", 2],
            ["what is the man wearing on head", -1],
            ["what is the ground covered with", -1],
            ["what color is the trousers", 1],
            ["how many people are there in the photo", 1],
            ["when was the picture taken", -1],
            ["what is the man doing", -1],
            ["what is the man standing on", -1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a baseball player sliding into a base while another player tries to tag him out.",
            "a young boy holding a baseball bat on a field."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2338607",
                "VG_object_id": "3930038",
                "bbox": [201, 67, 485, 329],
                "image": "data\\images\\2338607.jpg"
            },
            {
                "VG_image_id": "2410020",
                "VG_object_id": "228012",
                "bbox": [280, 58, 499, 383],
                "image": "data\\images\\2410020.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["where is the man", 2],
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["what is behind the man", 1],
            ["what is the man wearing", 1],
            ["what is the man looking at", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man holding", 2],
            ["how many people are there", 1],
            ["where is the man", 2],
            ["what is the man doing", 1],
            ["what color is the background", 1],
            ["who is in the picture", -1],
            ["what is on the man's head", -1],
            ["what is behind the man", 1],
            ["what is the man wearing", 1],
            ["what is the man looking at", 1]
        ],
        "context": [
            "a man in a suit and tie talking to a man in a suit.",
            "a man and a woman on a cell phone."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2403699",
                "VG_object_id": "350499",
                "bbox": [110, 253, 327, 375],
                "image": "data\\images\\2403699.jpg"
            },
            {
                "VG_image_id": "2383773",
                "VG_object_id": "1317230",
                "bbox": [134, 259, 480, 372],
                "image": "data\\images\\2383773.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor", 1],
            ["where is the photo taken", 1],
            ["what is on the floor", 1],
            ["what room is the floor in", 1],
            ["what is the floor made of", 1],
            ["what color is the wall", 1],
            ["what is the flooring", 1],
            ["what kind of flooring is this", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["what color is the floor", 1],
            ["where is the photo taken", 1],
            ["what is on the floor", 1],
            ["how many people are there", -1],
            ["what room is the floor in", 1],
            ["what is the pattern of the floor", -1],
            ["what is the floor made of", 1],
            ["what color is the wall", 1],
            ["what is the flooring", 1],
            ["what kind of flooring is this", 1],
            ["what room is this", 1]
        ],
        "context": [
            "a kitchen with a stove, sink, and refrigerator.",
            "a living room with a couch, television and a couch."
        ]
    },
    {
        "object_category": "shelf",
        "images": [
            {
                "VG_image_id": "2328408",
                "VG_object_id": "2985338",
                "bbox": [82, 171, 347, 341],
                "image": "data\\images\\2328408.jpg"
            },
            {
                "VG_image_id": "2394506",
                "VG_object_id": "463538",
                "bbox": [362, 205, 493, 278],
                "image": "data\\images\\2394506.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shelf", 1],
            ["what is in front of the shelf", 1]
        ],
        "org_questions": [
            ["what color is the shelf", 1],
            ["where is the shelf", -1],
            ["how many books are there on the shelf", -1],
            ["what is on the shelf", -1],
            ["what is the shelf made of", -1],
            ["what is in the background", -1],
            ["what is in front of the shelf", 1],
            ["how many people are in the picture", -1],
            ["what is on top of the table", -1],
            ["what is next to the desk", -1],
            ["where are the books", -1]
        ],
        "context": [
            "a television that is on a stand with a video game system.",
            "a living room with a table and a book shelf."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2405750",
                "VG_object_id": "3712923",
                "bbox": [173, 22, 498, 279],
                "image": "data\\images\\2405750.jpg"
            },
            {
                "VG_image_id": "2387004",
                "VG_object_id": "1280376",
                "bbox": [1, 1, 497, 205],
                "image": "data\\images\\2387004.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many umbrella are there", 2],
            ["what color is the umbrella", 1],
            ["what gender is the person holding the umbrella", 1],
            ["who is holding an umbrella", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", 1],
            ["how many umbrella are there", 2],
            ["what gender is the person holding the umbrella", 1],
            ["what is the persion doing", -1],
            ["what is the ground covered with", -1],
            ["Where is the umbrella", -1],
            ["what is under the umbrella", -1],
            ["who is holding an umbrella", 1],
            ["what are the people holding", -1],
            ["what is the weather like", -1]
        ],
        "context": [
            "a man holding an umbrella in the rain.",
            "a woman holding an umbrella while walking down the street."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2395472",
                "VG_object_id": "454257",
                "bbox": [182, 62, 293, 155],
                "image": "data\\images\\2395472.jpg"
            },
            {
                "VG_image_id": "2329852",
                "VG_object_id": "2824426",
                "bbox": [120, 51, 332, 186],
                "image": "data\\images\\2329852.jpg"
            }
        ],
        "questions_with_scores": [["what color is the desk", 2]],
        "org_questions": [
            ["what color is the desk", 2],
            ["how many computers are there", -1],
            ["what is next to the computer", -1],
            ["what device does the screen belong to", -1],
            ["What is on the table", -1],
            ["What creature is near the screen", -1],
            ["how many people are there in the picture", -1],
            ["where was the photo taken", -1],
            ["when was the photo taken", -1],
            ["what is the monitor on", -1],
            ["where is the computer monitor", -1]
        ],
        "context": [
            "a desk with a computer and a keyboard",
            "a computer monitor sitting on top of a wooden desk."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2396317",
                "VG_object_id": "1198944",
                "bbox": [101, 132, 272, 392],
                "image": "data\\images\\2396317.jpg"
            },
            {
                "VG_image_id": "2358145",
                "VG_object_id": "1728916",
                "bbox": [265, 80, 409, 317],
                "image": "data\\images\\2358145.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the horse", 2],
            ["what is the land made of", 2],
            ["How many horses are there", 1],
            ["What is horse doing", 1],
            ["what is on the horse", 1],
            ["what is the horse carrying", 1],
            ["what is the ground the horse standing on made of", 1],
            ["who is riding the horse", 1],
            ["what is the horse wearing", 1]
        ],
        "org_questions": [
            ["How many horses are there", 1],
            ["What is horse doing", 1],
            ["Where is the horse", 2],
            ["what is the land made of", 2],
            ["what is on the horse", 1],
            ["what is the horse carrying", 1],
            ["what is the ground the horse standing on made of", 1],
            ["what type of animal is shown", -1],
            ["who is riding the horse", 1],
            ["what is the horse wearing", 1],
            ["what is on the horse's head", -1]
        ],
        "context": [
            "a horse standing in a parking lot next to a car.",
            "a man riding on the back of a brown horse."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2342104",
                "VG_object_id": "937245",
                "bbox": [68, 209, 221, 256],
                "image": "data\\images\\2342104.jpg"
            },
            {
                "VG_image_id": "2345702",
                "VG_object_id": "3324535",
                "bbox": [120, 147, 255, 199],
                "image": "data\\images\\2345702.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many motorcycles are there", 1],
            ["where is the motorcycle", 1],
            ["how many people are there", 1],
            ["what is the ground covered with", 1],
            ["what is the color of the motorcycle", 1],
            ["what is in front of the motorcycle", 1]
        ],
        "org_questions": [
            ["how many motorcycles are there", 1],
            ["where is the motorcycle", 1],
            ["how many people are there", 1],
            ["what time is it", -1],
            ["what is the ground covered with", 1],
            ["what is the color of the motorcycle", 1],
            ["what is on the front of the motorcycle", -1],
            ["when was the photo taken", -1],
            ["what is in front of the motorcycle", 1],
            ["what is on the motorcycle", -1]
        ],
        "context": [
            "a man and a woman standing next to a motorcycle.",
            "two motorcycles parked next to each other on a street."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2375041",
                "VG_object_id": "584459",
                "bbox": [223, 196, 309, 267],
                "image": "data\\images\\2375041.jpg"
            },
            {
                "VG_image_id": "2338174",
                "VG_object_id": "2273356",
                "bbox": [75, 201, 240, 290],
                "image": "data\\images\\2338174.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the main color on the screen", 2],
            ["what is the main color of the monitor screen", 2],
            ["What color is the screen", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["Where is the screen", -1],
            ["What color is the screen", 1],
            ["how many people are there", -1],
            ["what is in front of the monitor", -1],
            ["How many screens are there", -1],
            ["what is the main color on the screen", 2],
            ["what is the main color of the monitor screen", 2],
            ["what is on the wall", 1],
            ["what is on the table", -1]
        ],
        "context": [
            "a desk with a television and a desk",
            "a woman leaning on a microwave in a kitchen."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2379393",
                "VG_object_id": "1358006",
                "bbox": [257, 129, 414, 330],
                "image": "data\\images\\2379393.jpg"
            },
            {
                "VG_image_id": "2330572",
                "VG_object_id": "3052724",
                "bbox": [141, 102, 469, 329],
                "image": "data\\images\\2330572.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child's posture", 1],
            ["what are the people doing", 1],
            ["what color is the clothes of the boy", 1],
            ["where is the child", 1],
            ["what is the boy holding", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what is the child's posture", 1],
            ["what are the people doing", 1],
            ["what color is the clothes of the boy", 1],
            ["where is the child", 1],
            ["what is the boy holding", 1],
            ["what gesture is the child", -1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["what is on the boy's face", -1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a group of soldiers playing soccer in a park.",
            "a man and two children on a boat."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2343933",
                "VG_object_id": "3013175",
                "bbox": [158, 32, 313, 225],
                "image": "data\\images\\2343933.jpg"
            },
            {
                "VG_image_id": "2335337",
                "VG_object_id": "2455526",
                "bbox": [246, 138, 390, 373],
                "image": "data\\images\\2335337.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is on the man's head", 2],
            ["what color is the man's shirt", 1],
            ["what is the man wearing on his back", 1],
            ["where is the picture taken", 1],
            ["What is the man holding", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 1],
            ["what is the man wearing on his back", 1],
            ["how many people are there", -1],
            ["where is the picture taken", 1],
            ["What is the man holding", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", 2],
            ["what is the persion on the left wearing", 1]
        ],
        "context": [
            "a man riding on the back of a horse in a rodeo.",
            "a group of people standing outside of a food truck."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2347628",
                "VG_object_id": "3610116",
                "bbox": [6, 16, 494, 361],
                "image": "data\\images\\2347628.jpg"
            },
            {
                "VG_image_id": "2401821",
                "VG_object_id": "1143723",
                "bbox": [27, 17, 499, 286],
                "image": "data\\images\\2401821.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 1],
            ["What color are the containers", 1],
            ["what is the plate made of", 1],
            ["what is the table made of", 1],
            ["what shape is the plate", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["What color are the containers", 1],
            ["What kind of food is it", -1],
            ["how many tables are there", -1],
            ["what is the plate made of", 1],
            ["where is the table", -1],
            ["what is on the table", -1],
            ["what is the table made of", 1],
            ["what shape is the plate", 1],
            ["what is the food on", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "two plates of food with rice and broccoli on them.",
            "a cup of coffee next to a plate of food."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2366021",
                "VG_object_id": "2656433",
                "bbox": [42, 25, 126, 480],
                "image": "data\\images\\2366021.jpg"
            },
            {
                "VG_image_id": "2344833",
                "VG_object_id": "912697",
                "bbox": [301, 128, 421, 282],
                "image": "data\\images\\2344833.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the guy wearing", 2],
            ["how many person are there", 2],
            ["where is the guy", 1],
            ["what is the guy doing", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["what color is the man's pants", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["what is the guy wearing", 2],
            ["where is the guy", 1],
            ["what is the guy doing", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["what color is the man's pants", 1],
            ["what are on the guy's feet", -1],
            ["what is the guy holding", -1],
            ["when was this photo taken", -1],
            ["who is in the picture", -1],
            ["what is the persion standing on", 1],
            ["what is on the man's head", -1],
            ["how many person are there", 2]
        ],
        "context": [
            "a group of men playing basketball on a street.",
            "a man sitting on a bench next to two giraffes."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2318077",
                "VG_object_id": "3680325",
                "bbox": [100, 185, 498, 324],
                "image": "data\\images\\2318077.jpg"
            },
            {
                "VG_image_id": "2381620",
                "VG_object_id": "703153",
                "bbox": [177, 193, 268, 323],
                "image": "data\\images\\2381620.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the back of the chair", 1],
            ["how many chairs are there in the picture", 1],
            ["what shape is the back of the chair", 1],
            ["what is the floor under the chair made of", 1]
        ],
        "org_questions": [
            ["what color is the back of the chair", 1],
            ["how many chairs are there in the picture", 1],
            ["what shape is the back of the chair", 1],
            ["what is the floor under the chair made of", 1],
            ["where is the chair", -1],
            ["What is on the chair", -1],
            ["who is sitting on the chair", -1],
            ["where was the photo taken", -1],
            ["what are the chairs made of", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a restaurant with a wine cooler and a counter.",
            "a man standing in a living room playing a video game."
        ]
    },
    {
        "object_category": "bus",
        "images": [
            {
                "VG_image_id": "2332584",
                "VG_object_id": "969196",
                "bbox": [12, 25, 239, 197],
                "image": "data\\images\\2332584.jpg"
            },
            {
                "VG_image_id": "2360747",
                "VG_object_id": "786207",
                "bbox": [34, 38, 452, 236],
                "image": "data\\images\\2360747.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the bus", 2],
            ["what is the weather like", 2],
            ["what is in the distance", 1],
            ["how many buses are there", 1],
            ["what is the bus doing", 1]
        ],
        "org_questions": [
            ["what color is the bus", 2],
            ["what is the weather like", 2],
            ["what is in the distance", 1],
            ["How many people are there", -1],
            ["where is the bus", -1],
            ["what is on the side of the bus", -1],
            ["how many buses are there", 1],
            ["when was the photo taken", -1],
            ["what is the bus doing", 1],
            ["what type of bus is this", -1],
            ["what is behind the bus", -1]
        ],
        "context": [
            "a bus splashing water on a street.",
            "a double decker bus is parked on the street."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2412142",
                "VG_object_id": "306993",
                "bbox": [42, 217, 417, 332],
                "image": "data\\images\\2412142.jpg"
            },
            {
                "VG_image_id": "2379166",
                "VG_object_id": "554568",
                "bbox": [1, 277, 482, 374],
                "image": "data\\images\\2379166.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the floor", 1],
            ["How many people are there", 1],
            ["what is covering the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is on the floor", 1],
            ["How many people are there", 1],
            ["where is the photo taken", -1],
            ["what is the floor made of", -1],
            ["where is the floor", -1],
            ["how many tables are there in the picture", -1],
            ["how is the floor made", -1],
            ["what is the flooring", -1],
            ["what type of flooring is shown", -1],
            ["what type of floor", -1],
            ["what is covering the floor", 1]
        ],
        "context": [
            "a living room with a couch, chair, and television.",
            "person and his father hang out the bed in the living room."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2405801",
                "VG_object_id": "371939",
                "bbox": [109, 15, 261, 305],
                "image": "data\\images\\2405801.jpg"
            },
            {
                "VG_image_id": "2415250",
                "VG_object_id": "143162",
                "bbox": [164, 75, 217, 204],
                "image": "data\\images\\2415250.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["how old is the girl", 1],
            ["where is the girl", 1],
            ["what is the girl sitting on", 1],
            ["what is the girl wearing", 1],
            ["what sport is the girl playing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["how old is the girl", 1],
            ["where is the girl", 1],
            ["what is the girl sitting on", 1],
            ["how many people are there", -1],
            ["what is the girl wearing", 1],
            ["what sport is the girl playing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a woman in a bikini riding a surfboard.",
            "a little girl riding a horse in a corral."
        ]
    },
    {
        "object_category": "basket",
        "images": [
            {
                "VG_image_id": "2373583",
                "VG_object_id": "588477",
                "bbox": [234, 154, 351, 275],
                "image": "data\\images\\2373583.jpg"
            },
            {
                "VG_image_id": "2320420",
                "VG_object_id": "3112688",
                "bbox": [126, 376, 184, 438],
                "image": "data\\images\\2320420.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the basket", 2],
            ["what color is the ground", 2],
            ["what is the color of the basket", 1],
            ["how many people are there", 1],
            ["what shape is the basket", 1],
            ["what is on the floor", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["what is the color of the basket", 1],
            ["where is the basket", 2],
            ["how many people are there", 1],
            ["what shape is the basket", 1],
            ["what is on the floor", 1],
            ["who is in the photo", 1],
            ["what is in the picture", -1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a man throwing a frisbee at a frisbee golf course.",
            "a bathroom with a sink, mirror, and a shower curtain."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2335073",
                "VG_object_id": "3211043",
                "bbox": [2, 300, 494, 342],
                "image": "data\\images\\2335073.jpg"
            },
            {
                "VG_image_id": "2341999",
                "VG_object_id": "2235467",
                "bbox": [15, 221, 485, 495],
                "image": "data\\images\\2341999.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 1],
            ["What is the background of photo", 1],
            ["what is the table sitting on", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the table", -1],
            ["what is on the table", 1],
            ["How many people are there", -1],
            ["what is the table made of", -1],
            ["What is the background of photo", 1],
            ["what is the table sitting on", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a sandwich with meat, cheese, and vegetables on a plate.",
            "a glass bottle on a table"
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2344780",
                "VG_object_id": "3629625",
                "bbox": [0, 1, 497, 355],
                "image": "data\\images\\2344780.jpg"
            },
            {
                "VG_image_id": "2343525",
                "VG_object_id": "922754",
                "bbox": [48, 208, 105, 305],
                "image": "data\\images\\2343525.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many persons are in the picture", 1],
            ["what color is the man's shirt", 1],
            ["what is the persion doing", 1],
            ["what is in the snow", 1]
        ],
        "org_questions": [
            ["what is on the ground", -1],
            ["how many persons are in the picture", 1],
            ["what color is the man's shirt", 1],
            ["what time is it", -1],
            ["where is the picture taken", -1],
            ["what is the persion doing", 1],
            ["what is the land made of", -1],
            ["how is the weather", -1],
            ["when was the picture taken", -1],
            ["what is in the snow", 1],
            ["where is the snow", -1]
        ],
        "context": [
            "two snowboarders are laying down in the snow.",
            "a man in a red jacket and goggles"
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2408790",
                "VG_object_id": "254986",
                "bbox": [100, 351, 326, 500],
                "image": "data\\images\\2408790.jpg"
            },
            {
                "VG_image_id": "150330",
                "VG_object_id": "1038217",
                "bbox": [25, 11, 903, 631],
                "image": "data\\images\\150330.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the vehicle", 1],
            ["how many vehicles are there on the road", 1],
            ["what type of vehicle is this", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the vehicle", 1],
            ["how many people are there in front of the vehicle", -1],
            ["how many people are there on the vehicle", -1],
            ["when is the picture taken", -1],
            ["what is on the vehicle", -1],
            ["where is the vehicle", -1],
            ["how is the weather", -1],
            ["how many vehicles are there on the road", 1],
            ["what type of vehicle is this", 1],
            ["where was this photo taken", -1],
            ["what is in the background", 1],
            ["what is on the side of the bus", -1]
        ],
        "context": [
            "a man riding a skateboard down a rail.",
            "a bus parked in a parking lot with people standing on it."
        ]
    },
    {
        "object_category": "light",
        "images": [
            {
                "VG_image_id": "2365525",
                "VG_object_id": "2446650",
                "bbox": [2, 9, 77, 59],
                "image": "data\\images\\2365525.jpg"
            },
            {
                "VG_image_id": "2365972",
                "VG_object_id": "755200",
                "bbox": [164, 262, 233, 339],
                "image": "data\\images\\2365972.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the light", 1],
            ["what is in the background", 1],
            ["What is the status of lamp", 1]
        ],
        "org_questions": [
            ["what color is the light", 1],
            ["what is in the background", 1],
            ["what time is it", -1],
            ["what is the traffic light on", -1],
            ["What is the status of lamp", 1],
            ["how many lights are there", -1],
            ["where is the traffic light", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a train that is sitting on the tracks.",
            "a traffic light with a bicycle on it."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2397070",
                "VG_object_id": "1192941",
                "bbox": [116, 69, 235, 292],
                "image": "data\\images\\2397070.jpg"
            },
            {
                "VG_image_id": "2338839",
                "VG_object_id": "3699441",
                "bbox": [9, 44, 369, 494],
                "image": "data\\images\\2338839.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man wearing", 2],
            ["what is on the man's face", 2],
            ["where is the photo taken", 1],
            ["What is man holding", 1],
            ["what is behind the man", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what is the man wearing", 2],
            ["where is the photo taken", 1],
            ["What is man holding", 1],
            ["what is the man wearing on his head", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is behind the man", 1],
            ["what is on the man's face", 2],
            ["what are the people doing", 1]
        ],
        "context": [
            "a man and woman sitting on a bench reading",
            "a man with a camera and a camera on his shoulder."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2329337",
                "VG_object_id": "3432940",
                "bbox": [9, 17, 139, 93],
                "image": "data\\images\\2329337.jpg"
            },
            {
                "VG_image_id": "2376741",
                "VG_object_id": "2684299",
                "bbox": [135, 212, 255, 303],
                "image": "data\\images\\2376741.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the car", 2],
            ["What color is the photo", 2],
            ["how many cars are there", 1],
            ["what is in front of the cars", 1],
            ["what kind of vehicle is shown", 1]
        ],
        "org_questions": [
            ["What color is the car", 2],
            ["What color is the photo", 2],
            ["What is the background of photo", -1],
            ["how many cars are there", 1],
            ["what is on the side of the car", -1],
            ["which part of the car can we see in the picture", -1],
            ["what time is it", -1],
            ["what is in front of the cars", 1],
            ["who is in the picture", -1],
            ["where is the car", -1],
            ["what kind of vehicle is shown", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a motorcycle parked on the grass with a yellow sign.",
            "a truck and a group of sheep grazing in a field."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2376584",
                "VG_object_id": "1841144",
                "bbox": [0, 1, 498, 371],
                "image": "data\\images\\2376584.jpg"
            },
            {
                "VG_image_id": "2408667",
                "VG_object_id": "3808842",
                "bbox": [1, 0, 332, 498],
                "image": "data\\images\\2408667.jpg"
            }
        ],
        "questions_with_scores": [["What color is floor", 1]],
        "org_questions": [
            ["What color is floor", 1],
            ["What color is the wall", -1],
            ["how many people are there", -1],
            ["what is on the wall", -1],
            ["what is the pattern of the wall", -1],
            ["how may toilets are there", -1],
            ["how many glasses are there", -1],
            ["where was this photo taken", -1],
            ["what room is this", -1],
            ["when was the picture taken", -1],
            ["what is the floor made of", -1],
            ["where is the toilet", -1]
        ],
        "context": [
            "a bathroom with a sink, mirror, and toilet.",
            "a bathroom with a sink, mirror, and a bathtub."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2330443",
                "VG_object_id": "3043337",
                "bbox": [8, 182, 394, 498],
                "image": "data\\images\\2330443.jpg"
            },
            {
                "VG_image_id": "2379078",
                "VG_object_id": "3835539",
                "bbox": [0, 152, 499, 329],
                "image": "data\\images\\2379078.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what is on the ground", 1],
            ["what color is the thing on the ground", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is the color of the ground", -1],
            ["what is on the ground", 1],
            ["what color is the thing on the ground", 1],
            ["how many motorcycles are there on the ground", -1],
            ["where is the picture taken", 1],
            ["what is the land made of", -1],
            ["how is the weather", -1],
            ["what is the weather like", -1],
            ["where is the shadow", -1],
            ["what is the ground covered with", -1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a fire hydrant on a sidewalk in a park.",
            "a person with a suitcase walking along a train track."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2374536",
                "VG_object_id": "1805992",
                "bbox": [0, 260, 499, 332],
                "image": "data\\images\\2374536.jpg"
            },
            {
                "VG_image_id": "2341127",
                "VG_object_id": "3922083",
                "bbox": [0, 151, 496, 369],
                "image": "data\\images\\2341127.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many balls are there on the field", 2],
            ["what are the people in the field doing", 2],
            ["how many people are there on the field", 1],
            ["what is the persion doing on the field", 1]
        ],
        "org_questions": [
            ["how many people are there on the field", 1],
            ["what is the field used for", -1],
            ["how many balls are there on the field", 2],
            ["what season is this photo taken in", -1],
            ["what color is the grass", -1],
            ["what is on the ground", -1],
            ["what is the persion doing on the field", 1],
            ["where is the grass", -1],
            ["where was the picture taken", -1],
            ["what are the people in the field doing", 2]
        ],
        "context": [
            "a woman riding a horse over a hurdle.",
            "a woman and a child playing with a ball in a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2322531",
                "VG_object_id": "3509296",
                "bbox": [199, 312, 305, 372],
                "image": "data\\images\\2322531.jpg"
            },
            {
                "VG_image_id": "2398840",
                "VG_object_id": "1175122",
                "bbox": [137, 300, 234, 452],
                "image": "data\\images\\2398840.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["what is the man doing", 1],
            ["what is behind the man", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what is the person holding", 1],
            ["where is the photo taken", 1],
            ["what gender is the person in the trousers", -1],
            ["what color is the trouser", -1],
            ["what is in the background", 1],
            ["what is the man doing", 1],
            ["what is color of the man's pants", -1],
            ["what type of pants is the man wearing", -1],
            ["what is on the man's face", -1],
            ["what is the persion wearing", -1],
            ["what is behind the man", 1]
        ],
        "context": [
            "a group of men playing a video game.",
            "a young boy holding an umbrella on a street."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2366610",
                "VG_object_id": "2074382",
                "bbox": [100, 356, 259, 460],
                "image": "data\\images\\2366610.jpg"
            },
            {
                "VG_image_id": "2322906",
                "VG_object_id": "2930220",
                "bbox": [85, 159, 300, 299],
                "image": "data\\images\\2322906.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the table", 1],
            ["What is the background of image", 1],
            ["what is the shape of the plate", 1]
        ],
        "org_questions": [
            ["what is in the plate", -1],
            ["what color is the table", 1],
            ["what is the table made of", -1],
            ["how many bowls are there on the table", -1],
            ["What is the background of image", 1],
            ["how many people are there", 2],
            ["what is the shape of the plate", 1],
            ["what is the food on", -1],
            ["where is the food", -1],
            ["what is on the table", -1]
        ],
        "context": [
            "a man cutting a pizza on a table.",
            "a bowl of fruit is sitting on a table."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2360238",
                "VG_object_id": "2139322",
                "bbox": [274, 241, 381, 352],
                "image": "data\\images\\2360238.jpg"
            },
            {
                "VG_image_id": "2412442",
                "VG_object_id": "194308",
                "bbox": [346, 62, 493, 193],
                "image": "data\\images\\2412442.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the plate", 2],
            ["what color is the table", 2],
            ["what is the container made of", 1]
        ],
        "org_questions": [
            ["what color is the plate", 2],
            ["what color is the table", 2],
            ["how many spoons are in the picture", -1],
            ["where is the food", -1],
            ["what is in the bowl", -1],
            ["what is the container made of", 1],
            ["what color is the food in the container", -1],
            ["what is on top of the plate", -1],
            ["what is next to the plate", -1],
            ["what is on the plate", -1],
            ["what is in the white container", -1],
            ["what is the main color of the container", -1]
        ],
        "context": [
            "a plate with a sandwich and chips on it.",
            "a plate with a sandwich and fries on it."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2412390",
                "VG_object_id": "195346",
                "bbox": [53, 310, 213, 455],
                "image": "data\\images\\2412390.jpg"
            },
            {
                "VG_image_id": "2409662",
                "VG_object_id": "236827",
                "bbox": [95, 336, 262, 499],
                "image": "data\\images\\2409662.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 1],
            ["what color is the man's top", 1],
            ["what is the person in the trousers doing", 1],
            ["What is man holding", 1]
        ],
        "org_questions": [
            ["what is the man holding", 1],
            ["what color is the man's top", 1],
            ["where is the man", -1],
            ["how many people are there in the photo", -1],
            ["who is wearing the trousers", -1],
            ["what is the person in the trousers doing", 1],
            ["What is man holding", 1],
            ["what type of pants is the man wearing", -1],
            ["what are the men wearing", -1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a man holding a frisbee while standing on a field.",
            "a man wearing a mask and a mask holding a cell phone."
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2348562",
                "VG_object_id": "1992571",
                "bbox": [137, 2, 279, 94],
                "image": "data\\images\\2348562.jpg"
            },
            {
                "VG_image_id": "2317901",
                "VG_object_id": "3688116",
                "bbox": [0, 63, 205, 159],
                "image": "data\\images\\2317901.jpg"
            }
        ],
        "questions_with_scores": [["what color is the car", 2]],
        "org_questions": [
            ["what color is the car", 2],
            ["how many cars are in the picture", -1],
            ["what is in the background", -1],
            ["when is the picture taken", -1],
            ["where is the vehicle", -1],
            ["how is the weather", -1],
            ["what is on the car", -1],
            ["when is this photo taken", -1],
            ["what is on the side of the car", -1],
            ["when was the picture taken", -1],
            ["where are the cars", -1]
        ],
        "context": [
            "a horse wearing a hat with a rope around its neck.",
            "a man in a green uniform is kneeling on the ground."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2373583",
                "VG_object_id": "588460",
                "bbox": [25, 114, 120, 330],
                "image": "data\\images\\2373583.jpg"
            },
            {
                "VG_image_id": "2398133",
                "VG_object_id": "1182492",
                "bbox": [275, 75, 329, 231],
                "image": "data\\images\\2398133.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what is the man wearing on head", 2],
            ["what is the color of the man's pants", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["What is the background of image", 1],
            ["where is the man", 1],
            ["who is wearing a hat", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", -1],
            ["what is the color of the man's pants", 1],
            ["how many people are there", 1],
            ["where is the photo taken", 1],
            ["what is the man wearing on head", 2],
            ["What is the background of image", 1],
            ["where is the man", 1],
            ["who is wearing a hat", 1],
            ["when was the picture taken", -1],
            ["what is the persion in the foreground wearing", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man throwing a frisbee at a frisbee golf course.",
            "a boy doing a trick on a skateboard at a skate park."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2322889",
                "VG_object_id": "3383234",
                "bbox": [88, 88, 270, 358],
                "image": "data\\images\\2322889.jpg"
            },
            {
                "VG_image_id": "2370380",
                "VG_object_id": "2427942",
                "bbox": [98, 97, 332, 420],
                "image": "data\\images\\2370380.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["what is the man wearing", 1],
            ["what are the people doing", 1],
            ["Where is the man", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["how is the weather", -1],
            ["what is the man riding", -1],
            ["what color is the floor", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", 1],
            ["what are the people doing", 1],
            ["what color is the background", -1],
            ["Where is the man", 1],
            ["what is the man holding", 1],
            ["what is the man standing on", -1]
        ],
        "context": [
            "a person jumping a skate board in the air",
            "a man in a pirate costume riding a skateboard."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2365424",
                "VG_object_id": "634279",
                "bbox": [178, 175, 290, 338],
                "image": "data\\images\\2365424.jpg"
            },
            {
                "VG_image_id": "2321274",
                "VG_object_id": "2933979",
                "bbox": [36, 26, 342, 343],
                "image": "data\\images\\2321274.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the girl", 2],
            ["what color is the girl's shirt", 1],
            ["what is the girl holding", 1],
            ["what is the girl wearing", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the girl's shirt", 1],
            ["what is the girl holding", 1],
            ["what is the girl wearing", 1],
            ["how many people are there", -1],
            ["who has a long hair", -1],
            ["where is the girl", 2],
            ["what is the weather like", -1],
            ["what is the persion doing", 1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a little girl sitting on a bench next to a group of teddy bears.",
            "a woman is talking on a cell phone."
        ]
    },
    {
        "object_category": "pillow",
        "images": [
            {
                "VG_image_id": "2366299",
                "VG_object_id": "2063744",
                "bbox": [1, 98, 137, 206],
                "image": "data\\images\\2366299.jpg"
            },
            {
                "VG_image_id": "2381287",
                "VG_object_id": "705704",
                "bbox": [98, 186, 287, 253],
                "image": "data\\images\\2381287.jpg"
            }
        ],
        "questions_with_scores": [["what color is the bed", 2]],
        "org_questions": [
            ["what color is the bed", 2],
            ["how many pillows are there", -1],
            ["what room is the pillow in", -1],
            ["where is the pillow", -1],
            ["what is the pillow on", -1],
            ["what is on top of the bed", -1],
            ["what is on the wall", -1],
            ["what color are the pillows on the bed", -1],
            ["where are the pillows", -1],
            ["what color are the walls", -1]
        ],
        "context": [
            "a bed with a white comforter and pillows in a small room.",
            "a bedroom with a large bed and a window."
        ]
    },
    {
        "object_category": "sofa",
        "images": [
            {
                "VG_image_id": "2328140",
                "VG_object_id": "978622",
                "bbox": [0, 201, 209, 334],
                "image": "data\\images\\2328140.jpg"
            },
            {
                "VG_image_id": "2350626",
                "VG_object_id": "866589",
                "bbox": [113, 184, 243, 286],
                "image": "data\\images\\2350626.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sofa", 1],
            ["what color is the floor", 1],
            ["what is the floor under the sofa made of", 1],
            ["What is in the back of sofa", 1],
            ["what is on the floor", 1],
            ["what is in front of the couch", 1]
        ],
        "org_questions": [
            ["what color is the sofa", 1],
            ["what color is the floor", 1],
            ["how many people are there", -1],
            ["what is the floor under the sofa made of", 1],
            ["What is next to the sofa", -1],
            ["What is in the back of sofa", 1],
            ["How many sofas are there", -1],
            ["what room is this", -1],
            ["what is on the floor", 1],
            ["what is in the room", -1],
            ["what is in front of the couch", 1]
        ],
        "context": [
            "a living room with a couch and a table",
            "a living room with a couch and a table."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2368361",
                "VG_object_id": "618283",
                "bbox": [0, 64, 278, 314],
                "image": "data\\images\\2368361.jpg"
            },
            {
                "VG_image_id": "2350513",
                "VG_object_id": "2128841",
                "bbox": [0, 81, 480, 368],
                "image": "data\\images\\2350513.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 1],
            ["how is the weather", 1],
            ["how many cars are in the picture", 1],
            ["what is on the left of the car", 1],
            ["what is the car doing", 1]
        ],
        "org_questions": [
            ["what color is the car", 1],
            ["how is the weather", 1],
            ["how many cars are in the picture", 1],
            ["when is the photo taken", -1],
            ["what is the ground covered with", -1],
            ["where is the car", -1],
            ["what is on the left of the car", 1],
            ["what is the car doing", 1],
            ["what kind of car is this", -1],
            ["what is parked on the car", -1],
            ["what is the car parked on", -1]
        ],
        "context": [
            "a woman walking in the rain with an umbrella.",
            "a man drives a car down a street."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2411525",
                "VG_object_id": "358854",
                "bbox": [238, 251, 291, 350],
                "image": "data\\images\\2411525.jpg"
            },
            {
                "VG_image_id": "2332769",
                "VG_object_id": "2890219",
                "bbox": [36, 109, 86, 163],
                "image": "data\\images\\2332769.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are in the picture", 1],
            ["what is the man doing", 1],
            ["how many shirt are there in the picture", 1],
            ["what is on the man's face", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", -1],
            ["how many people are in the picture", 1],
            ["what is the man doing", 1],
            ["where is the person", -1],
            ["how many shirt are there in the picture", 1],
            ["What is the main color of the shirt", -1],
            ["what color is the shirt", -1],
            ["what is on the man's face", 1],
            ["what is the man wearing", 1],
            ["where is the man", -1]
        ],
        "context": [
            "a man sitting at a table with a laptop and a bag.",
            "a couple standing next to a wedding cake."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2360375",
                "VG_object_id": "788314",
                "bbox": [0, 142, 499, 354],
                "image": "data\\images\\2360375.jpg"
            },
            {
                "VG_image_id": "2392708",
                "VG_object_id": "1226750",
                "bbox": [3, 252, 498, 373],
                "image": "data\\images\\2392708.jpg"
            }
        ],
        "questions_with_scores": [
            ["what animals are on the field", 2],
            ["what is the field made of", 1],
            ["how many bulls are there on the ground", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what animals are on the field", 2],
            ["what is the field made of", 1],
            ["how many bulls are there on the ground", 1],
            ["what is in the distance", -1],
            ["when was the photo taken", -1],
            ["how is the weather", -1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a herd of cattle grazing in a field of tall grass.",
            "a group of giraffes standing in a zoo enclosure."
        ]
    },
    {
        "object_category": "tray",
        "images": [
            {
                "VG_image_id": "2344727",
                "VG_object_id": "913647",
                "bbox": [114, 239, 402, 317],
                "image": "data\\images\\2344727.jpg"
            },
            {
                "VG_image_id": "2411906",
                "VG_object_id": "1078711",
                "bbox": [110, 317, 328, 371],
                "image": "data\\images\\2411906.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the tray", 2],
            ["what color is the table", 2],
            ["what is on the tray", 1],
            ["What is the man holding", 1],
            ["What food is on the plate", 1],
            ["what  color is the table the tray is placed on", 1],
            ["how many people are there", 1],
            ["what shape is the table", 1]
        ],
        "org_questions": [
            ["what is on the tray", 1],
            ["what color is the tray", 2],
            ["what color is the table", 2],
            ["where is the photo taken", -1],
            ["What is the man holding", 1],
            ["What food is on the plate", 1],
            ["what is in the plate", -1],
            ["what  color is the table the tray is placed on", 1],
            ["how many people are there", 1],
            ["what shape is the table", 1],
            ["what is the table made of", -1]
        ],
        "context": [
            "a man and two children sitting at a table with a cake.",
            "a man holding a baby while sitting at a table."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2397979",
                "VG_object_id": "1184331",
                "bbox": [83, 159, 202, 381],
                "image": "data\\images\\2397979.jpg"
            },
            {
                "VG_image_id": "2362021",
                "VG_object_id": "2207328",
                "bbox": [190, 65, 282, 227],
                "image": "data\\images\\2362021.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["what color is the person's clothes", 1],
            ["where is this photo taken", 1],
            ["What is man doing", 1],
            ["what is the person wearing", 1],
            ["where is the person", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what is in the background", 1],
            ["what color is the person's clothes", 1],
            ["how many people are there", -1],
            ["where is this photo taken", 1],
            ["What is man doing", 1],
            ["what is the person wearing", 1],
            ["where is the person", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's head", 1]
        ],
        "context": [
            "a couple standing next to each other holding an umbrella.",
            "a couple of people riding on the back of an elephant."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2414141",
                "VG_object_id": "159181",
                "bbox": [273, 206, 317, 311],
                "image": "data\\images\\2414141.jpg"
            },
            {
                "VG_image_id": "2341508",
                "VG_object_id": "2711394",
                "bbox": [233, 134, 292, 182],
                "image": "data\\images\\2341508.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's trousers", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the man's trousers", 1],
            ["how many people are there", 1],
            ["what is the weather like", -1],
            ["what is the man wearing on his head", -1],
            ["where is the man", -1],
            ["what is the man doing", 1],
            ["what is the ground covered with", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's feet", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a young boy riding a skateboard on a bridge.",
            "a snowboarder is jumping over a snow hill."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2357296",
                "VG_object_id": "1744377",
                "bbox": [11, 22, 486, 366],
                "image": "data\\images\\2357296.jpg"
            },
            {
                "VG_image_id": "2358695",
                "VG_object_id": "799991",
                "bbox": [39, 3, 142, 332],
                "image": "data\\images\\2358695.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the building", 2],
            ["How many clocks are there", 1]
        ],
        "org_questions": [
            ["What color is the building", 2],
            ["How many clocks are there", 1],
            ["when is this picture taken", -1],
            ["What is behind the building", -1],
            ["what is the weather like", -1],
            ["where is the building", -1],
            ["what is in front of the building", -1],
            ["who is in the photo", -1],
            ["what is the building made of", -1],
            ["what time is it", -1],
            ["what is on the building", -1],
            ["what is in the photo", -1]
        ],
        "context": [
            "a large brick building with a clock on the front.",
            "a flag and a clock tower with a flag on top."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2397453",
                "VG_object_id": "434032",
                "bbox": [7, 215, 500, 370],
                "image": "data\\images\\2397453.jpg"
            },
            {
                "VG_image_id": "2397802",
                "VG_object_id": "1186314",
                "bbox": [1, 313, 499, 373],
                "image": "data\\images\\2397802.jpg"
            }
        ],
        "questions_with_scores": [["what is the weather like", 2]],
        "org_questions": [
            ["what is beside the road", -1],
            ["what is the weather like", 2],
            ["what color is the sidewalk of the road", -1],
            ["How many buses are there", -1],
            ["when is the picture taken", -1],
            ["what is in the distance", -1],
            ["where was this photo taken", -1],
            ["what is the road made of", -1],
            ["where are the cars", -1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a wet city street with a traffic light and a traffic light.",
            "a large church with a tower in the background."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2380324",
                "VG_object_id": "710991",
                "bbox": [3, 2, 361, 289],
                "image": "data\\images\\2380324.jpg"
            },
            {
                "VG_image_id": "2320069",
                "VG_object_id": "3180980",
                "bbox": [65, 2, 420, 271],
                "image": "data\\images\\2320069.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many cats are there", 2],
            ["where is the cat", 2],
            ["what color is the background", 1],
            ["what is the cat sitting on", 1],
            ["what is beside the cat", 1],
            ["what is the cat looking at", 1]
        ],
        "org_questions": [
            ["What is cat doing ", -1],
            ["How many cats are there", 2],
            ["what color is the background", 1],
            ["where is the cat", 2],
            ["what is the cat sitting on", 1],
            ["what gesture is the cat", -1],
            ["what is beside the cat", 1],
            ["what type of animal is shown", -1],
            ["what is on the cat's head", -1],
            ["what is the cat looking at", 1],
            ["what is on the table", -1],
            ["where is the cat looking", -1]
        ],
        "context": [
            "a cat is playing with a glass of water.",
            "two cats are playing on a chair."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2316445",
                "VG_object_id": "3955341",
                "bbox": [95, 17, 291, 498],
                "image": "data\\images\\2316445.jpg"
            },
            {
                "VG_image_id": "2354154",
                "VG_object_id": "3574679",
                "bbox": [1, 3, 117, 312],
                "image": "data\\images\\2354154.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["how many people are there", 1],
            ["what is the man wearing on his face", -1],
            ["how is the weather", -1],
            ["where is the man", 1],
            ["what is the gender of the person in the picture", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1],
            ["what is on the man's head", -1],
            ["what is the man looking at", -1]
        ],
        "context": [
            "a man with a toothbrush in his mouth standing in a yard.",
            "a person blowing out a candle on a cake."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2375941",
                "VG_object_id": "1853477",
                "bbox": [3, 18, 498, 319],
                "image": "data\\images\\2375941.jpg"
            },
            {
                "VG_image_id": "2365179",
                "VG_object_id": "636058",
                "bbox": [0, 1, 499, 347],
                "image": "data\\images\\2365179.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many animals are there", 2],
            ["What color is the field", 1]
        ],
        "org_questions": [
            ["What color is the field", 1],
            ["How many animals are there", 2],
            ["What is the background of photo", -1],
            ["what is in the background", -1],
            ["how many horses are there on the field", -1],
            ["where is the grass", -1],
            ["when was this picture taken", -1],
            ["how is the weather", -1],
            ["where was this picture taken", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "two sheep are sitting in a field of grass.",
            "a small lamb standing in a grassy field."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2384265",
                "VG_object_id": "3847179",
                "bbox": [0, 37, 385, 374],
                "image": "data\\images\\2384265.jpg"
            },
            {
                "VG_image_id": "2349610",
                "VG_object_id": "2636731",
                "bbox": [146, 98, 219, 207],
                "image": "data\\images\\2349610.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the woman's shirt", 1],
            ["Where is the photo taken", 1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["What is the background of image", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["What color is the woman's shirt", 1],
            ["Where is the photo taken", 1],
            ["What is the woman doing", -1],
            ["how many people are there", 1],
            ["what is in the background", 1],
            ["what is the woman wearing", -1],
            ["What is the background of image", 1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is the woman standing on", -1],
            ["what is the woman holding", 1]
        ],
        "context": [
            "a woman holding a wii remote in her hand.",
            "a market with a variety of fruits and vegetables."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2336092",
                "VG_object_id": "2299507",
                "bbox": [1, 204, 88, 374],
                "image": "data\\images\\2336092.jpg"
            },
            {
                "VG_image_id": "713266",
                "VG_object_id": "1581355",
                "bbox": [637, 15, 904, 393],
                "image": "data\\images\\713266.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["what is the person wearing", 1],
            ["what is the person holding", 1],
            ["what is the persion on the left wearing", 1]
        ],
        "org_questions": [
            ["where is the person", 1],
            ["what is the person doing", 1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the person wearing", 1],
            ["what is the person holding", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion on the left wearing", 1]
        ],
        "context": [
            "a group of people riding bikes down a street.",
            "a group of people riding in a raft on a river."
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2411837",
                "VG_object_id": "208902",
                "bbox": [4, 355, 293, 470],
                "image": "data\\images\\2411837.jpg"
            },
            {
                "VG_image_id": "2401328",
                "VG_object_id": "3817348",
                "bbox": [5, 80, 497, 279],
                "image": "data\\images\\2401328.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing on the field", 2],
            ["how many people are there on the field", 2],
            ["what is the gender of the person", 1]
        ],
        "org_questions": [
            ["what is the person doing on the field", 2],
            ["how many people are there on the field", 2],
            ["what is the gender of the person", 1],
            ["what is the color of the grass", -1],
            ["what is in the background", -1],
            ["when was the picture taken", -1],
            ["where are the people", -1],
            ["what is the weather like", -1],
            ["what is the ground covered with", -1]
        ],
        "context": [
            "a girl flying a kite in a field.",
            "a man swinging a bat on top of a field."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2377259",
                "VG_object_id": "2629936",
                "bbox": [204, 304, 269, 349],
                "image": "data\\images\\2377259.jpg"
            },
            {
                "VG_image_id": "2317454",
                "VG_object_id": "3430171",
                "bbox": [289, 220, 402, 255],
                "image": "data\\images\\2317454.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 2],
            ["How many plates are there", 1],
            ["How many people are there", 1],
            ["What food  in the plate", 1],
            ["what is on the plates", 1]
        ],
        "org_questions": [
            ["What color is the table", 2],
            ["How many plates are there", 1],
            ["How many people are there", 1],
            ["what pattern is on the plate", -1],
            ["where is the plate", -1],
            ["what is the table under the plate made of", -1],
            ["What food  in the plate", 1],
            ["what is on the plates", 1],
            ["what kind of food is on the table", -1],
            ["what is next to the table", -1],
            ["what is the table made of", -1]
        ],
        "context": [
            "a man cutting a cake on a table",
            "a man and a child sitting at a table with pizza."
        ]
    },
    {
        "object_category": "basket",
        "images": [
            {
                "VG_image_id": "1159642",
                "VG_object_id": "1595717",
                "bbox": [82, 220, 213, 319],
                "image": "data\\images\\1159642.jpg"
            },
            {
                "VG_image_id": "2408993",
                "VG_object_id": "251077",
                "bbox": [1, 185, 248, 499],
                "image": "data\\images\\2408993.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the basket", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["what shape is the basket", 1],
            ["What is the background of image", 1],
            ["where is the basket", 1],
            ["what is on the floor", 1]
        ],
        "org_questions": [
            ["what is in the basket", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["what shape is the basket", 1],
            ["What is the background of image", 1],
            ["where is the basket", 1],
            ["what is on the floor", 1],
            ["what is in the center of the picture", -1]
        ],
        "context": [
            "a man pushing a cart with vegetables",
            "a doll sitting in a baby carriage next to a doll."
        ]
    },
    {
        "object_category": "box",
        "images": [
            {
                "VG_image_id": "2381625",
                "VG_object_id": "1335271",
                "bbox": [0, 0, 491, 398],
                "image": "data\\images\\2381625.jpg"
            },
            {
                "VG_image_id": "2331291",
                "VG_object_id": "3716666",
                "bbox": [162, 232, 308, 316],
                "image": "data\\images\\2331291.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the box", 1],
            ["where is the photo taken", 1],
            ["what is on the box", 1],
            ["what type of food is shown", 1],
            ["what is next to the box", 1],
            ["what kind of food is in the picture", 1]
        ],
        "org_questions": [
            ["what color is the box", 1],
            ["what is in the box", -1],
            ["how many people are there", -1],
            ["when is this photo taken", -1],
            ["where is the photo taken", 1],
            ["what is on the box", 1],
            ["what type of food is shown", 1],
            ["where is the food", -1],
            ["what is next to the box", 1],
            ["what kind of food is in the picture", 1],
            ["what is the box made of", -1]
        ],
        "context": [
            "a cake with a m & m face drawn on it.",
            "a dog is standing next to a crate of fruit."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2347151",
                "VG_object_id": "892987",
                "bbox": [213, 253, 330, 324],
                "image": "data\\images\\2347151.jpg"
            },
            {
                "VG_image_id": "2324314",
                "VG_object_id": "3421052",
                "bbox": [48, 0, 498, 82],
                "image": "data\\images\\2324314.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many cars are there", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["how many cars are there", 1],
            ["what is in the background", -1],
            ["WHat color is the building", -1],
            ["what is the building made of", -1],
            ["what is on the ground", 1],
            ["where was the photo taken", -1],
            ["when was this photo taken", -1],
            ["where are the buildings", -1],
            ["what is on the side of the road", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a street sign with a sticker on it",
            "a green bench sitting on the side of a street."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2315988",
                "VG_object_id": "2802726",
                "bbox": [20, 129, 351, 454],
                "image": "data\\images\\2315988.jpg"
            },
            {
                "VG_image_id": "2321759",
                "VG_object_id": "3454023",
                "bbox": [121, 173, 249, 496],
                "image": "data\\images\\2321759.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the man doing", 2],
            ["What color is man's shoes", 2],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["What is the man doing", 2],
            ["What color is man;s shirt", -1],
            ["What color is man's shoes", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["where is the person", 1],
            ["what is the person wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the man's feet", -1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man playing tennis on a court",
            "a man flying a kite in the air."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2330225",
                "VG_object_id": "3722205",
                "bbox": [173, 126, 231, 266],
                "image": "data\\images\\2330225.jpg"
            },
            {
                "VG_image_id": "2374220",
                "VG_object_id": "2378945",
                "bbox": [30, 24, 195, 435],
                "image": "data\\images\\2374220.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 1],
            ["where is the woman", 1],
            ["what is the woman wearing", 1],
            ["how is the weather", 1],
            ["What is woman doing", 1],
            ["What is the background of image", 1]
        ],
        "org_questions": [
            ["what color is the woman's umbrella", -1],
            ["what is the woman doing", -1],
            ["How many people are there", 1],
            ["where is the woman", 1],
            ["what is the woman holding", -1],
            ["who is holding a umbrella", -1],
            ["what is the woman wearing", 1],
            ["what is on the woman's head", -1],
            ["who is in the picture", -1],
            ["how is the weather", 1],
            ["What is woman doing", 1],
            ["What is the background of image", 1],
            ["What is the person holding", -1],
            ["when was the photo taken", -1],
            ["who is holding the umbrella", -1]
        ],
        "context": [
            "three people walking in the rain with umbrellas.",
            "a woman holding an umbrella and a man taking a picture."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2328879",
                "VG_object_id": "975776",
                "bbox": [101, 1, 221, 74],
                "image": "data\\images\\2328879.jpg"
            },
            {
                "VG_image_id": "2408098",
                "VG_object_id": "268277",
                "bbox": [128, 182, 246, 374],
                "image": "data\\images\\2408098.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman wearing", 1],
            ["what is the gesture of the woman", 1],
            ["how many people are there", 1],
            ["what is the woman doing", 1]
        ],
        "org_questions": [
            ["what is the woman wearing", 1],
            ["what is the gesture of the woman", 1],
            ["where is the woman", -1],
            ["how many people are there", 1],
            ["what is the woman doing", 1],
            ["what is the woman holding", -1],
            ["when was the photo taken", -1],
            ["what gender is the person in the photo", -1],
            ["what is on the woman's head", -1]
        ],
        "context": [
            "a bird is sitting on a plate with a sandwich.",
            "a restaurant with a menu on the wall."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2397784",
                "VG_object_id": "430763",
                "bbox": [177, 324, 374, 491],
                "image": "data\\images\\2397784.jpg"
            },
            {
                "VG_image_id": "2371488",
                "VG_object_id": "2187716",
                "bbox": [207, 222, 337, 339],
                "image": "data\\images\\2371488.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is behind the bench", 1],
            ["where is the bench", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the bench made of", -1],
            ["what is behind the bench", 1],
            ["how many benches are there", -1],
            ["where is the bench", 1],
            ["What is on the bench", -1],
            ["what is in the background", 1],
            ["how many people are sitting on the bench", -1],
            ["what is made of wood", -1],
            ["where is the picture taken", -1],
            ["what is the ground covered with", -1],
            ["what is next to the bench", -1]
        ],
        "context": [
            "a picture of a religious painting on a wall.",
            "a room with a table, chairs, and a table."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2395461",
                "VG_object_id": "1205200",
                "bbox": [229, 160, 362, 290],
                "image": "data\\images\\2395461.jpg"
            },
            {
                "VG_image_id": "2399720",
                "VG_object_id": "1167092",
                "bbox": [203, 110, 478, 365],
                "image": "data\\images\\2399720.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the laptop", 1],
            ["what is behind the laptop", 1],
            ["how many people are there", 1],
            ["what is the computer placed on", 1],
            ["what is the person holding", 1]
        ],
        "org_questions": [
            ["where is the laptop", 1],
            ["what is behind the laptop", 1],
            ["how many people are there", 1],
            ["what is on the laptop's screen", -1],
            ["what is the computer placed on", 1],
            ["what color is the laptop's screen", -1],
            ["How many laptops are there", -1],
            ["what is the man doing", -1],
            ["what is the person holding", 1]
        ],
        "context": [
            "a man standing in front of a laptop computer.",
            "two people are playing with a laptop computer."
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2406911",
                "VG_object_id": "288392",
                "bbox": [130, 41, 488, 214],
                "image": "data\\images\\2406911.jpg"
            },
            {
                "VG_image_id": "2411038",
                "VG_object_id": "317427",
                "bbox": [0, 2, 496, 374],
                "image": "data\\images\\2411038.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many televisions are in the room", 2],
            ["how many air conditioners are in the room", 1],
            ["how many lamps are in the room", 1],
            ["What color is the table", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["how many televisions are in the room", 2],
            ["how many air conditioners are in the room", 1],
            ["how many lamps are in the room", 1],
            ["What color is the table", 1],
            ["what is on the wall", 1],
            ["how many people are there in the picture", -1],
            ["what color is the wall", -1],
            ["where was this taken", -1],
            ["what room is this", -1],
            ["who is in the room", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a hotel room with a bed, desk, television and a desk.",
            "a living room with a couch, chair, and a painting."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2368639",
                "VG_object_id": "2162547",
                "bbox": [343, 12, 499, 78],
                "image": "data\\images\\2368639.jpg"
            },
            {
                "VG_image_id": "2412371",
                "VG_object_id": "195863",
                "bbox": [125, 0, 265, 80],
                "image": "data\\images\\2412371.jpg"
            }
        ],
        "questions_with_scores": [["how many cups are on the table", 1]],
        "org_questions": [
            ["what color is the chair", -1],
            ["how many cups are on the table", 1],
            ["how many people are there in the picture", -1],
            ["who is in the photo", -1],
            ["what is in the background", -1],
            ["what is in the photo", -1]
        ],
        "context": [
            "a pizza with basil leaves on a plate.",
            "a table with many plates of food on it"
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2372249",
                "VG_object_id": "592964",
                "bbox": [1, 13, 67, 119],
                "image": "data\\images\\2372249.jpg"
            },
            {
                "VG_image_id": "2365876",
                "VG_object_id": "755600",
                "bbox": [95, 146, 156, 311],
                "image": "data\\images\\2365876.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 1],
            ["What is in the foreground", 1]
        ],
        "org_questions": [
            ["what color is the building", 1],
            ["what is the building made of", -1],
            ["how many people are there", -1],
            ["Where is the building", -1],
            ["What is in the foreground", 1],
            ["what is on the wall", -1],
            ["when was this picture taken", -1],
            ["what is on the top of the building", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a red fire truck parked on a brick road.",
            "a traffic light on a pole in a city."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2389405",
                "VG_object_id": "3828639",
                "bbox": [1, 132, 497, 317],
                "image": "data\\images\\2389405.jpg"
            },
            {
                "VG_image_id": "2340531",
                "VG_object_id": "2198394",
                "bbox": [0, 444, 283, 499],
                "image": "data\\images\\2340531.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the people on the land doing", 1],
            ["how many people are there on the land", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what is the people on the land doing", 1],
            ["what is the land made of", -1],
            ["how many animals are on the land", -1],
            ["What is on the ground", -1],
            ["What is the background of image", -1],
            ["what is standing on the land", -1],
            ["where was this photo taken", -1],
            ["where is the grass", -1],
            ["what is green", -1],
            ["what is in the field", -1],
            ["what is on the land", -1],
            ["what is in the background", -1],
            ["how many people are there on the land", 1],
            ["where is the land", -1]
        ],
        "context": [
            "two people playing frisbee in a grassy field.",
            "a person flying a kite in the sky."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2368445",
                "VG_object_id": "1909190",
                "bbox": [0, 103, 161, 453],
                "image": "data\\images\\2368445.jpg"
            },
            {
                "VG_image_id": "2367964",
                "VG_object_id": "2415017",
                "bbox": [49, 13, 316, 497],
                "image": "data\\images\\2367964.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl wearing on the head", 2],
            ["what color is the girl's shirt", 2],
            ["how many people are there", 1],
            ["what are the people doing", 1],
            ["Where is the girl", 1],
            ["How old is the girl", 1],
            ["what is the persion holding", 1],
            ["what is the persion wearing", 1],
            ["what is the girl doing", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what are the people doing", 1],
            ["what is the girl wearing on the head", 2],
            ["what color is the girl's shirt", 2],
            ["Where is the girl", 1],
            ["How old is the girl", 1],
            ["who is in the photo", -1],
            ["what is the persion holding", 1],
            ["what is the persion wearing", 1],
            ["what is the girl doing", 1]
        ],
        "context": [
            "two women playing a game with remote controllers.",
            "a little girl holding a banana in her hand."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2405384",
                "VG_object_id": "1108274",
                "bbox": [91, 67, 319, 331],
                "image": "data\\images\\2405384.jpg"
            },
            {
                "VG_image_id": "2363604",
                "VG_object_id": "2406000",
                "bbox": [158, 139, 213, 204],
                "image": "data\\images\\2363604.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what shape is the collar of the man's shirt", 2],
            ["what shape is the man's collar", 1]
        ],
        "org_questions": [
            ["what is the man doing", -1],
            ["what shape is the man's collar", 1],
            ["what is the man holding", 2],
            ["how many men are there", -1],
            ["where is the photo taken", -1],
            ["What is man wearing on the face", -1],
            ["what is the persion wearing", -1],
            ["who is in the photo", -1],
            ["how is the man's hair", -1],
            ["what is the man wearing on his head", -1],
            ["what shape is the collar of the man's shirt", 2]
        ],
        "context": [
            "a man holding a tennis racket in his hand.",
            "a man playing basketball on a wooden court."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2358743",
                "VG_object_id": "2376959",
                "bbox": [50, 72, 157, 329],
                "image": "data\\images\\2358743.jpg"
            },
            {
                "VG_image_id": "2368392",
                "VG_object_id": "618093",
                "bbox": [174, 28, 337, 325],
                "image": "data\\images\\2368392.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is wearing shirt on the left", 2],
            ["what is the person doing", 1],
            ["what is the man holding", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["when is the photo taken", -1],
            ["what color is the person", -1],
            ["who is in the photo", -1],
            ["what kind of shirt is the man wearing", -1],
            ["what is on the man's head", -1],
            ["where is the man", 1],
            ["what is wearing shirt on the left", 2]
        ],
        "context": [
            "a woman swinging a tennis racket at a ball.",
            "a man and a woman riding a motorcycle."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2366701",
                "VG_object_id": "753778",
                "bbox": [2, 13, 500, 197],
                "image": "data\\images\\2366701.jpg"
            },
            {
                "VG_image_id": "713784",
                "VG_object_id": "1039630",
                "bbox": [758, 1, 1022, 263],
                "image": "data\\images\\713784.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in front of the building", 1],
            ["what is on the side of the building", 1]
        ],
        "org_questions": [
            ["what color is the building", -1],
            ["what is in front of the building", 1],
            ["how is the weather", -1],
            ["what is the building made of", -1],
            ["when was the picture taken", -1],
            ["what is on the side of the building", 1],
            ["what is the weather like", -1],
            ["what is in the background", -1]
        ],
        "context": [
            "a train is parked at a train station.",
            "a couple of buses that are sitting in the street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "1159877",
                "VG_object_id": "1598612",
                "bbox": [135, 297, 269, 497],
                "image": "data\\images\\1159877.jpg"
            },
            {
                "VG_image_id": "2350311",
                "VG_object_id": "3446346",
                "bbox": [146, 40, 393, 372],
                "image": "data\\images\\2350311.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the color of the man's pants", 1],
            ["where is the person", 1],
            ["who is in the photo", 1],
            ["where was this photo taken", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["What color is man's shirt", -1],
            ["what is the persion standing on", -1],
            ["what is the color of the man's pants", 1],
            ["where is the person", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", 1],
            ["what is the man holding", -1],
            ["where was this photo taken", 1]
        ],
        "context": [
            "a restaurant kitchen with a lot of people working.",
            "a man sitting on a toilet in a room."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2318915",
                "VG_object_id": "1006104",
                "bbox": [238, 59, 397, 169],
                "image": "data\\images\\2318915.jpg"
            },
            {
                "VG_image_id": "2377429",
                "VG_object_id": "565647",
                "bbox": [208, 0, 303, 239],
                "image": "data\\images\\2377429.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many curtains are there", 1],
            ["what is in the background", 1],
            ["where is the curtain", 1],
            ["what is on the window", 1]
        ],
        "org_questions": [
            ["what color is the curtain", -1],
            ["what color is the wall", -1],
            ["how many curtains are there", 1],
            ["what is in the background", 1],
            ["where is the curtain", 1],
            ["what is the pattern on the curtain", -1],
            ["what room is it", -1],
            ["what is hanging on the wall", -1],
            ["what is covering the window", -1],
            ["what is on the window", 1],
            ["where is the photo taken", -1],
            ["what room is this", -1]
        ],
        "context": [
            "a living room with a couch, coffee table and a fireplace.",
            "a living room with a couch and a mirror"
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2345974",
                "VG_object_id": "903482",
                "bbox": [28, 17, 451, 299],
                "image": "data\\images\\2345974.jpg"
            },
            {
                "VG_image_id": "2400635",
                "VG_object_id": "1156437",
                "bbox": [122, 136, 499, 314],
                "image": "data\\images\\2400635.jpg"
            }
        ],
        "questions_with_scores": [["what is the main color of the train", 2]],
        "org_questions": [
            ["what is the main color of the train", 2],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["what is the train doing", -1],
            ["what is the land made of", -1],
            ["what is on the side of the train", -1],
            ["what is in the front of the train", -1],
            ["where was this photo taken", -1],
            ["what is the train on", -1],
            ["where is the train", -1],
            ["what is on the tracks", -1],
            ["what is above the train", -1]
        ],
        "context": [
            "a train is traveling down the tracks near a bridge.",
            "a train is pulling into a train station."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2335982",
                "VG_object_id": "2658943",
                "bbox": [76, 123, 217, 413],
                "image": "data\\images\\2335982.jpg"
            },
            {
                "VG_image_id": "2417807",
                "VG_object_id": "2868397",
                "bbox": [168, 254, 224, 468],
                "image": "data\\images\\2417807.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many ties is the man wearing", 2],
            ["what color is the man's sleeves", 1],
            ["how many people are there", 1],
            ["what pattern is on the tie", 1]
        ],
        "org_questions": [
            ["how many ties is the man wearing", 2],
            ["what color is the man's sleeves", 1],
            ["who is wearing the necktie", -1],
            ["what is the man doing", -1],
            ["what is the man's hairstyle", -1],
            ["how many people are there", 1],
            ["what pattern is on the tie", 1],
            ["where is the tie", -1],
            ["what is around the man's neck", -1],
            ["what is the man wearing", -1],
            ["what is on the man's neck", -1]
        ],
        "context": [
            "a man with a tie on and a man in a white shirt.",
            "a man in a suit and tie posing for a picture."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2409129",
                "VG_object_id": "248402",
                "bbox": [31, 17, 185, 450],
                "image": "data\\images\\2409129.jpg"
            },
            {
                "VG_image_id": "2351087",
                "VG_object_id": "2492511",
                "bbox": [168, 137, 306, 494],
                "image": "data\\images\\2351087.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is the man holding", 2],
            ["how many people are there", 1],
            ["what is the man wearing on his head", -1],
            ["what is the man doing", 1],
            ["what is the man wearing on head", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is on the man's face", -1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "a man cutting a pizza on top of a stove.",
            "a group of people standing around a table with wine."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2372272",
                "VG_object_id": "592806",
                "bbox": [0, 163, 423, 374],
                "image": "data\\images\\2372272.jpg"
            },
            {
                "VG_image_id": "2380209",
                "VG_object_id": "1349343",
                "bbox": [3, 397, 153, 499],
                "image": "data\\images\\2380209.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the rug", 2],
            ["what is the color of the floor", 2],
            ["what is on the rug", 1],
            ["what is on the wall", 1],
            ["what pattern is on the floor", 1],
            ["what is the pattern on the floor", 1]
        ],
        "org_questions": [
            ["what color is the rug", 2],
            ["what is on the rug", 1],
            ["what is the floor made of", -1],
            ["what is on the ground", -1],
            ["what is the color of the floor", 2],
            ["what is on the wall", 1],
            ["how many pillows are there", -1],
            ["what pattern is on the floor", 1],
            ["what is covering the floor", -1],
            ["what is the pattern on the floor", 1]
        ],
        "context": [
            "a living room with a rug, chairs, and a table.",
            "a bed in a room with a window."
        ]
    },
    {
        "object_category": "guy",
        "images": [
            {
                "VG_image_id": "2332612",
                "VG_object_id": "3675723",
                "bbox": [3, 49, 217, 329],
                "image": "data\\images\\2332612.jpg"
            },
            {
                "VG_image_id": "2407911",
                "VG_object_id": "271456",
                "bbox": [3, 2, 371, 323],
                "image": "data\\images\\2407911.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the guy doing", 2],
            ["what color is the guy's shirt", 2],
            ["what is the guy holding", 1],
            ["how many people are there", 1],
            ["What is guy doing", 1],
            ["when was this photo taken", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["what is the guy doing", 2],
            ["what color is the guy's shirt", 2],
            ["what is the guy holding", 1],
            ["how many people are there", 1],
            ["What is guy doing", 1],
            ["when was this photo taken", 1],
            ["who is in the photo", -1],
            ["where is the man", 1],
            ["what is on the man's face", -1]
        ],
        "context": [
            "a man with a mustache is holding an umbrella.",
            "a man sitting at a table with two hot dogs and a drink."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2355154",
                "VG_object_id": "3491783",
                "bbox": [210, 112, 280, 189],
                "image": "data\\images\\2355154.jpg"
            },
            {
                "VG_image_id": "2367813",
                "VG_object_id": "3872163",
                "bbox": [12, 128, 107, 184],
                "image": "data\\images\\2367813.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["where is the picture taken", 2],
            ["what is the woman doing", 1],
            ["what color is the background", 1],
            ["what is the woman wearing", 1],
            ["what color are the clothes of the woman on the left", 1],
            ["where are the people", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 2],
            ["where is the picture taken", 2],
            ["what is the woman doing", 1],
            ["what color is the background", 1],
            ["what is the woman wearing", 1],
            ["what is the woman on the left holding", -1],
            ["what is above the woman", -1],
            ["what color are the clothes of the woman on the left", 1],
            ["when was the photo taken", -1],
            ["what is the gender of the person", -1],
            ["who is in the photo", -1],
            ["where are the people", 1]
        ],
        "context": [
            "a person riding a wave on a surfboard.",
            "a dog catching a frisbee in a dog park."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2368208",
                "VG_object_id": "2649685",
                "bbox": [210, 141, 371, 266],
                "image": "data\\images\\2368208.jpg"
            },
            {
                "VG_image_id": "2354698",
                "VG_object_id": "836387",
                "bbox": [226, 84, 441, 268],
                "image": "data\\images\\2354698.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is beside the cat", 1],
            ["where is the cat standing", 1],
            ["how many cat are there", 1],
            ["what is the cat doing", 1],
            ["What is the background of image", 1],
            ["what is the cat sitting on", 1],
            ["what is the cat on", 1],
            ["what is on the cat", 1],
            ["where is the cat", 1],
            ["how many cats are in the picture", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the cat", -1],
            ["what is beside the cat", 1],
            ["where is the cat standing", 1],
            ["how many cat are there", 1],
            ["what is the cat doing", 1],
            ["What is the background of image", 1],
            ["what is the cat sitting on", 1],
            ["what is the cat on", 1],
            ["what kind of animal is in the picture", -1],
            ["who is in the picture", -1],
            ["what is on the cat's head", -1],
            ["what is on the cat", 1],
            ["where is the cat", 1],
            ["how many cats are in the picture", 1],
            ["what is in the background", 1]
        ],
        "context": [
            "a girl laying in bed with a cat and book.",
            "two cats are laying on a bean bag bed."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2357056",
                "VG_object_id": "1922626",
                "bbox": [15, 195, 499, 262],
                "image": "data\\images\\2357056.jpg"
            },
            {
                "VG_image_id": "2323364",
                "VG_object_id": "2989410",
                "bbox": [199, 1, 447, 128],
                "image": "data\\images\\2323364.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the umbrellas", 2],
            ["where are the umbrellas", 1],
            ["what is the pattern on the umbrella", 1],
            ["What color is umbrella", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what color are the umbrellas", 2],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["where are the umbrellas", 1],
            ["what is the pattern on the umbrella", 1],
            ["What color is umbrella", 1],
            ["what is the building made of", -1],
            ["what are the people doing", 1],
            ["what color is the roof of the building", -1]
        ],
        "context": [
            "a bird flying over a restaurant with a bird on it.",
            "a group of people standing around each other."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2369719",
                "VG_object_id": "2293842",
                "bbox": [109, 168, 257, 293],
                "image": "data\\images\\2369719.jpg"
            },
            {
                "VG_image_id": "1592687",
                "VG_object_id": "2820826",
                "bbox": [361, 403, 498, 538],
                "image": "data\\images\\1592687.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["who is wearing a white shirt", 1]
        ],
        "org_questions": [
            ["what is the man doing", 2],
            ["what color is the man's shirt", 2],
            ["what gender is the person in the shirt", -1],
            ["where is the person", -1],
            ["what is the gender of the person", -1],
            ["how many people are in the photo", -1],
            ["when was the photo taken", -1],
            ["who is wearing a white shirt", 1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man sitting on the grass talking on a cell phone.",
            "a man riding a skateboard on a sidewalk."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2368441",
                "VG_object_id": "3869102",
                "bbox": [100, 0, 499, 202],
                "image": "data\\images\\2368441.jpg"
            },
            {
                "VG_image_id": "2376989",
                "VG_object_id": "2702218",
                "bbox": [0, 97, 106, 236],
                "image": "data\\images\\2376989.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 1],
            ["how many bicycles are there", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the car", 1],
            ["how many bicycles are there", 1],
            ["what time is it", -1],
            ["what is the ground covered with", -1],
            ["what is the weather like", -1],
            ["what is in the background", 1],
            ["when was this picture taken", -1],
            ["what is on the car", -1],
            ["where was the picture taken", -1],
            ["what is next to the car", -1]
        ],
        "context": [
            "a table with two hot dogs and a drink on it.",
            "a man in a hat and glasses riding a bike."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2341937",
                "VG_object_id": "938613",
                "bbox": [414, 239, 499, 331],
                "image": "data\\images\\2341937.jpg"
            },
            {
                "VG_image_id": "2390996",
                "VG_object_id": "1245119",
                "bbox": [1, 246, 212, 332],
                "image": "data\\images\\2390996.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 1],
            ["how many chairs are there", 1]
        ],
        "org_questions": [
            ["what color is the chair", 1],
            ["how many chairs are there", 1],
            ["what is the chair made of", -1],
            ["how many people are in  the picture", -1],
            ["what color is the ground", -1],
            ["what room is this", -1],
            ["where is the picture taken", -1],
            ["what is in the room", -1]
        ],
        "context": [
            "a dog standing in a kitchen next to a refrigerator.",
            "a kitchen with a stove, oven, sink and refrigerator."
        ]
    },
    {
        "object_category": "toilet",
        "images": [
            {
                "VG_image_id": "2390570",
                "VG_object_id": "1249008",
                "bbox": [206, 169, 308, 302],
                "image": "data\\images\\2390570.jpg"
            },
            {
                "VG_image_id": "2320357",
                "VG_object_id": "993909",
                "bbox": [122, 206, 201, 365],
                "image": "data\\images\\2320357.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the floor under the floor", 1],
            ["what color is the wall behind the floor", 1],
            ["What color is the floor", 1]
        ],
        "org_questions": [
            ["what color is the floor under the floor", 1],
            ["what color is the wall behind the floor", 1],
            ["what is on the wall", -1],
            ["What is the floor made of", -1],
            ["what is the toilet on", -1],
            ["What color is the floor", 1],
            ["how many toilets are there", -1],
            ["what room is this", -1],
            ["where is the toilet", -1],
            ["how is the toilet", -1],
            ["what kind of toilet is this", -1]
        ],
        "context": [
            "a toilet in a stall with a black base.",
            "a toilet in a small bathroom with a window."
        ]
    },
    {
        "object_category": "train",
        "images": [
            {
                "VG_image_id": "2366321",
                "VG_object_id": "754341",
                "bbox": [151, 104, 381, 228],
                "image": "data\\images\\2366321.jpg"
            },
            {
                "VG_image_id": "2342628",
                "VG_object_id": "2164973",
                "bbox": [170, 69, 426, 274],
                "image": "data\\images\\2342628.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the train", 2],
            ["what shape is the head of the train", 1]
        ],
        "org_questions": [
            ["what color is the train", 2],
            ["where is the photo taken", -1],
            ["what shape is the head of the train", 1],
            ["how many trains are there", -1],
            ["what is the train on", -1],
            ["where is the train", -1],
            ["how many carriages does the train have", -1],
            ["when was the picture taken", -1],
            ["what is on the side of the train", -1],
            ["what kind of train is this", -1],
            ["what is the train doing", -1],
            ["what is on the tracks", -1]
        ],
        "context": [
            "a train is going down the tracks under a bridge.",
            "a red train is pulling into a station."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2414473",
                "VG_object_id": "294851",
                "bbox": [0, 237, 499, 374],
                "image": "data\\images\\2414473.jpg"
            },
            {
                "VG_image_id": "2378139",
                "VG_object_id": "2242044",
                "bbox": [20, 187, 481, 482],
                "image": "data\\images\\2378139.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing on the beach", 2],
            ["how many people are on the beach", 2],
            ["What is the person wearing", 1],
            ["What is the person holding", 1],
            ["what is in the sky", 1]
        ],
        "org_questions": [
            ["what are the people doing on the beach", 2],
            ["how many people are on the beach", 2],
            ["what is in the background", -1],
            ["what color is the sky", -1],
            ["What is the person wearing", 1],
            ["What is the person holding", 1],
            ["what is on the sand", -1],
            ["what is in the sky", 1],
            ["where was this photo taken", -1],
            ["where is the sand", -1],
            ["where are the people standing", -1],
            ["what color is the sand", -1]
        ],
        "context": [
            "a person flying a kite on a beach.",
            "two people walking on the beach with surfboards."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2376201",
                "VG_object_id": "1726449",
                "bbox": [78, 139, 143, 271],
                "image": "data\\images\\2376201.jpg"
            },
            {
                "VG_image_id": "2361658",
                "VG_object_id": "2077082",
                "bbox": [6, 24, 241, 331],
                "image": "data\\images\\2361658.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what color is man's helmet", 1],
            ["what color is the man's shirt", 1],
            ["What is man doing ", 1]
        ],
        "org_questions": [
            ["what color is man's helmet", 1],
            ["what color is the man's shirt", 1],
            ["what is the man holding", 2],
            ["how many people are there", -1],
            ["what is the ground covered with", -1],
            ["Where is the man", -1],
            ["what is the man wearing on his neck", -1],
            ["What is man doing ", 1],
            ["who is wearing a helmet", -1],
            ["when was the photo taken", -1],
            ["what sport is being played", -1],
            ["what is the batter wearing", -1]
        ],
        "context": [
            "a baseball player holding a bat on a field.",
            "a baseball player holding a bat on a field."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2327426",
                "VG_object_id": "3320211",
                "bbox": [276, 6, 461, 183],
                "image": "data\\images\\2327426.jpg"
            },
            {
                "VG_image_id": "2328955",
                "VG_object_id": "3461762",
                "bbox": [125, 191, 197, 343],
                "image": "data\\images\\2328955.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["what color is the person's shirt", 1],
            ["what gender is the person", 1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["What color is the man's pair of trousers", 1],
            ["who is in the photo", 1],
            ["where are the people", 1],
            ["what are the people doing", 1]
        ],
        "org_questions": [
            ["what is the person doing", 2],
            ["what color is the person's shirt", 1],
            ["what gender is the person", 1],
            ["How many people are there", -1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["What color is the man's pair of trousers", 1],
            ["who is in the photo", 1],
            ["what is the persion wearing", -1],
            ["where are the people", 1],
            ["what are the people doing", 1]
        ],
        "context": [
            "a boy on a skateboard doing a trick.",
            "a bus is parked on the side of the road."
        ]
    },
    {
        "object_category": "animal",
        "images": [
            {
                "VG_image_id": "2417650",
                "VG_object_id": "3256840",
                "bbox": [98, 137, 220, 227],
                "image": "data\\images\\2417650.jpg"
            },
            {
                "VG_image_id": "2382002",
                "VG_object_id": "700026",
                "bbox": [264, 246, 341, 323],
                "image": "data\\images\\2382002.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the cow", 2],
            ["what is the animal doing", 1]
        ],
        "org_questions": [
            ["where is the cow", 2],
            ["what color is the cow", -1],
            ["what kinds of animal is it", -1],
            ["how many bears are there in the picture", -1],
            ["what is the animal doing", 1],
            ["what is the animal", -1],
            ["when was the picture taken", -1],
            ["who is in the picture", -1],
            ["what is the animal in the picture", -1],
            ["what animal is in the picture", -1],
            ["what animal is this", -1]
        ],
        "context": [
            "a man in the ocean with a bull in the water.",
            "a cow standing in a field of tall grass."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2371315",
                "VG_object_id": "598545",
                "bbox": [98, 102, 213, 180],
                "image": "data\\images\\2371315.jpg"
            },
            {
                "VG_image_id": "2382260",
                "VG_object_id": "1329888",
                "bbox": [107, 179, 196, 303],
                "image": "data\\images\\2382260.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what color is the shirt", 1],
            ["where is the person ", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the man holding", 2],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["who is wearing the shirt", -1],
            ["where is the person ", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1]
        ],
        "context": [
            "a man riding a horse in a rodeo",
            "a young boy holding a baseball bat on a baseball field."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2407805",
                "VG_object_id": "3810094",
                "bbox": [83, 141, 237, 256],
                "image": "data\\images\\2407805.jpg"
            },
            {
                "VG_image_id": "2329470",
                "VG_object_id": "3311255",
                "bbox": [25, 95, 268, 324],
                "image": "data\\images\\2329470.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the man holding", 2],
            ["what sport is the man playing", 1],
            ["what is the man doing", 1],
            ["what is the persion in the middle holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what sport is the man playing", 1],
            ["what is the man holding", 2],
            ["how many people are there", -1],
            ["where is the person", -1],
            ["what is the man doing", 1],
            ["what is the persion in the middle holding", 1],
            ["when was this photo taken", -1],
            ["what is the man wearing on his head", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a man is throwing a frisbee in a park.",
            "a man swinging a bat at a ball."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2351140",
                "VG_object_id": "1698378",
                "bbox": [138, 260, 245, 440],
                "image": "data\\images\\2351140.jpg"
            },
            {
                "VG_image_id": "2412461",
                "VG_object_id": "193896",
                "bbox": [0, 181, 162, 329],
                "image": "data\\images\\2412461.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the dog doing", 1],
            ["what is on the dog's neck", 1],
            ["what is the dog on", 1],
            ["what is in front of the dog", 1],
            ["how many dogs are in the picture", 1]
        ],
        "org_questions": [
            ["what color is the ground", -1],
            ["where is the dog staying", -1],
            ["what is in the background", -1],
            ["How many people are there", -1],
            ["What is the dog doing", 1],
            ["what is on the dog's neck", 1],
            ["what is the dog on", 1],
            ["what is in front of the dog", 1],
            ["what animal is in the picture", -1],
            ["what is the dog wearing", -1],
            ["what is the dog looking at", -1],
            ["what is the dog's color", -1],
            ["how many dogs are in the picture", 1],
            ["where is the dog", -1],
            ["how is the weather", -1]
        ],
        "context": [
            "a dog walking in a field with a horse behind it.",
            "a dog is looking at a herd of cows."
        ]
    },
    {
        "object_category": "keyboard",
        "images": [
            {
                "VG_image_id": "2317273",
                "VG_object_id": "1022199",
                "bbox": [147, 184, 377, 294],
                "image": "data\\images\\2317273.jpg"
            },
            {
                "VG_image_id": "2367931",
                "VG_object_id": "2788871",
                "bbox": [145, 251, 297, 315],
                "image": "data\\images\\2367931.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the keyboard", 2],
            ["what is under the desk", 1],
            ["what is the color of the keyboard", 1]
        ],
        "org_questions": [
            ["what color is the keyboard", 2],
            ["what color is the desk", -1],
            ["how many people are there", -1],
            ["where is the keyboard", -1],
            ["what is on the keyboard", -1],
            ["how many laptop are there", -1],
            ["what is the desk made of", -1],
            ["what is under the desk", 1],
            ["what is in front of the desk", -1],
            ["what is the color of the keyboard", 1]
        ],
        "context": [
            "a computer desk with a monitor and keyboard.",
            "a desk with two monitors and a microphone."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2315748",
                "VG_object_id": "3609777",
                "bbox": [1, 5, 332, 259],
                "image": "data\\images\\2315748.jpg"
            },
            {
                "VG_image_id": "2415847",
                "VG_object_id": "3163880",
                "bbox": [8, 6, 498, 267],
                "image": "data\\images\\2415847.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many doors are in the building", 2],
            ["how many people are in the picture", 2],
            ["how many doors are there", 2],
            ["what is the color of the wall", 1]
        ],
        "org_questions": [
            ["what is the color of the wall", 1],
            ["how many doors are in the building", 2],
            ["how many people are in the picture", 2],
            ["what is in the background", -1],
            ["when was the picture taken", -1],
            ["what is the building made of", -1],
            ["what is on the wall", -1],
            ["where was the photo taken", -1],
            ["how many doors are there", 2]
        ],
        "context": [
            "a person walking down a sidewalk holding an umbrella.",
            "a blue motorcycle parked in front of two garage doors."
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2349381",
                "VG_object_id": "1825985",
                "bbox": [297, 3, 451, 59],
                "image": "data\\images\\2349381.jpg"
            },
            {
                "VG_image_id": "2367440",
                "VG_object_id": "3874017",
                "bbox": [285, 248, 498, 328],
                "image": "data\\images\\2367440.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 2],
            ["how many people are there in the picture", 2],
            ["what is the ground covered with", 1],
            ["where are the cars", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the car", 2],
            ["what is in front of the car", -1],
            ["what color is the background", -1],
            ["how many vehicle are there in the photo", -1],
            ["what time is it", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", 1],
            ["what is the vehicle on", -1],
            ["where are the cars", 1],
            ["when was the photo taken", -1],
            ["what is on the car", -1],
            ["what is in the background", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "a group of people playing a game of soccer.",
            "a street sign in front of a brick building."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2386576",
                "VG_object_id": "517877",
                "bbox": [215, 269, 452, 331],
                "image": "data\\images\\2386576.jpg"
            },
            {
                "VG_image_id": "2320667",
                "VG_object_id": "3962674",
                "bbox": [67, 93, 461, 358],
                "image": "data\\images\\2320667.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 2],
            ["how many plates are there", 1]
        ],
        "org_questions": [
            ["how many plates are there", 1],
            ["what color is the table", 2],
            ["what shape is the plate", -1],
            ["what is on the plate", -1],
            ["what is the plate on ", -1],
            ["what is the pattern on the plate", -1],
            ["what is in the distance", -1],
            ["where was the photo taken", -1],
            ["what kind of food is on the table", -1],
            ["what is next to the plate", -1],
            ["what is the main color of the plate", -1]
        ],
        "context": [
            "a woman sitting at a table with food on it.",
            "a sandwich with meat and vegetables on a plate."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2328962",
                "VG_object_id": "3346052",
                "bbox": [3, 0, 498, 499],
                "image": "data\\images\\2328962.jpg"
            },
            {
                "VG_image_id": "2398227",
                "VG_object_id": "1181455",
                "bbox": [5, 231, 332, 499],
                "image": "data\\images\\2398227.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the plate", 1],
            ["what is beside the plate", 1],
            ["What food is on the plate", 1]
        ],
        "org_questions": [
            ["what is on the plate", 1],
            ["what color is the plate", -1],
            ["what is beside the plate", 1],
            ["how many people are there in the picture", -1],
            ["what shape is the plate", -1],
            ["what is the plate made of", -1],
            ["what is the plate on ", -1],
            ["What food is on the plate", 1],
            ["what is the food on", -1],
            ["what is under the plate", -1]
        ],
        "context": [
            "a piece of cake on a plate on a carpet.",
            "a bowl of carrots and a spoon with a spoon in it."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2378297",
                "VG_object_id": "3836717",
                "bbox": [109, 1, 352, 480],
                "image": "data\\images\\2378297.jpg"
            },
            {
                "VG_image_id": "2358695",
                "VG_object_id": "799991",
                "bbox": [39, 3, 142, 332],
                "image": "data\\images\\2358695.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the sky", 2],
            ["what color is the building with clock", 1],
            ["what is color of the building", 1]
        ],
        "org_questions": [
            ["how many building are there", -1],
            ["what color is the building with clock", 1],
            ["What is in the center of image", -1],
            ["what is the weather like", -1],
            ["What color is the sky", 2],
            ["How many people are there in the image", -1],
            ["what is color of the building", 1],
            ["when was this picture taken", -1],
            ["where is the clock", -1],
            ["what is the building made out of", -1],
            ["what time is it", -1]
        ],
        "context": [
            "a stone building with a clock on the front.",
            "a flag and a clock tower with a flag on top."
        ]
    },
    {
        "object_category": "giraffe",
        "images": [
            {
                "VG_image_id": "2376621",
                "VG_object_id": "2418801",
                "bbox": [139, 120, 237, 383],
                "image": "data\\images\\2376621.jpg"
            },
            {
                "VG_image_id": "2401266",
                "VG_object_id": "1149484",
                "bbox": [193, 41, 458, 310],
                "image": "data\\images\\2401266.jpg"
            }
        ],
        "questions_with_scores": [["how many giraffes are there", 1]],
        "org_questions": [
            ["how many giraffes are there", 1],
            ["what is the giraffe standing on", -1],
            ["what is the giraffe doing", -1],
            ["where are the giraffes", -1],
            ["what is in the distance", -1],
            ["what is the ground covered with", -1],
            ["what is behind the giraffe", -1],
            ["what is in front of the giraffe", -1],
            ["what color are the trees", -1],
            ["when was the picture taken", -1],
            ["what kind of animal is in the picture", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a giraffe standing next to a tree in a field.",
            "two giraffes and an ostrich in a field."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2373246",
                "VG_object_id": "2226883",
                "bbox": [243, 272, 303, 327],
                "image": "data\\images\\2373246.jpg"
            },
            {
                "VG_image_id": "2404895",
                "VG_object_id": "655529",
                "bbox": [393, 222, 492, 274],
                "image": "data\\images\\2404895.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the man doing", 2],
            ["what are the people doing in the picture", 1],
            ["what color is the trousers", 1],
            ["where is the picture taken", 1],
            ["what is on the man's head", 1],
            ["what is the person holding", 1],
            ["what is on the ground", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what are the people doing in the picture", 1],
            ["what color is the trousers", 1],
            ["where is the picture taken", 1],
            ["how many people are there", -1],
            ["what is on the man's head", 1],
            ["what is the person holding", 1],
            ["what sport is the man doing", 2],
            ["when was the photo taken", -1],
            ["what is on the ground", 1],
            ["what is the man wearing", 1]
        ],
        "context": [
            "two people posing for a picture while skiing.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2388541",
                "VG_object_id": "3829325",
                "bbox": [178, 149, 245, 201],
                "image": "data\\images\\2388541.jpg"
            },
            {
                "VG_image_id": "2330601",
                "VG_object_id": "2739488",
                "bbox": [275, 212, 330, 299],
                "image": "data\\images\\2330601.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the man's pants", 2],
            ["what is the man doing", 2],
            ["what is the man holding in his hand", 1],
            ["where is the photo taken", 1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["what is on the man's feet", 1]
        ],
        "org_questions": [
            ["what is the color of the man's pants", 2],
            ["what is the man doing", 2],
            ["what is the man holding in his hand", 1],
            ["how many people are there", -1],
            ["where is the photo taken", 1],
            ["what is on the person's head", -1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["when was the photo taken", -1],
            ["what is the man wearing", -1],
            ["what is on the man's feet", 1],
            ["when was this picture taken", -1]
        ],
        "context": [
            "a skier is skiing down a hill.",
            "a man riding a blue motorcycle on a muddy track."
        ]
    },
    {
        "object_category": "floor",
        "images": [
            {
                "VG_image_id": "2406753",
                "VG_object_id": "290651",
                "bbox": [26, 374, 332, 500],
                "image": "data\\images\\2406753.jpg"
            },
            {
                "VG_image_id": "2353017",
                "VG_object_id": "3781336",
                "bbox": [2, 216, 228, 331],
                "image": "data\\images\\2353017.jpg"
            }
        ],
        "questions_with_scores": [
            ["what room is it", 2],
            ["what room is the floor in", 1],
            ["what color is the floor", 1],
            ["what is the color of the wall", 1],
            ["where was the photo taken", 1],
            ["what room is this", 1]
        ],
        "org_questions": [
            ["what is the floor made of", -1],
            ["what room is the floor in", 1],
            ["what color is the floor", 1],
            ["how many people are there", -1],
            ["what is on the ground", -1],
            ["what is the color of the wall", 1],
            ["where was the photo taken", 1],
            ["where are the tiles", -1],
            ["what room is this", 1],
            ["what is covering the floor", -1],
            ["what room is it", 2]
        ],
        "context": [
            "a bathroom with two sinks and a bathtub.",
            "a kitchen with a dishwasher, sink, and window."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2358635",
                "VG_object_id": "1920208",
                "bbox": [57, 167, 183, 380],
                "image": "data\\images\\2358635.jpg"
            },
            {
                "VG_image_id": "2370381",
                "VG_object_id": "3858874",
                "bbox": [253, 107, 356, 215],
                "image": "data\\images\\2370381.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["what color is the girl's jacket", 1],
            ["where is the girl", 1],
            ["how many people are there in the picture", 1],
            ["how old is the girl", 1],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what color is the girl's jacket", 1],
            ["where is the girl", 1],
            ["how many people are there in the picture", 1],
            ["what is the woman wearing", -1],
            ["how old is the girl", 1],
            ["what is the persion doing", 1],
            ["what is the gender of the person in the picture", -1],
            ["when was the picture taken", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a little girl is playing with a kite.",
            "a dog sitting next to a girl on a bench."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2407815",
                "VG_object_id": "273126",
                "bbox": [92, 51, 190, 300],
                "image": "data\\images\\2407815.jpg"
            },
            {
                "VG_image_id": "2320819",
                "VG_object_id": "1049184",
                "bbox": [113, 70, 208, 260],
                "image": "data\\images\\2320819.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man staying on", 1],
            ["what color is the man's clothes", 1],
            ["how many people are there in the picture", 1],
            ["What is man doing", 1],
            ["What is the man holding", 1],
            ["what is beside the man", 1],
            ["when was the photo taken", 1],
            ["what is the man on the left wearing on the head", 1]
        ],
        "org_questions": [
            ["what is the man staying on", 1],
            ["what color is the man's clothes", 1],
            ["how many people are there in the picture", 1],
            ["Where is the man", -1],
            ["What is man doing", 1],
            ["What is the man holding", 1],
            ["what is beside the man", 1],
            ["where is the man facing to", -1],
            ["when was the photo taken", 1],
            ["what kind of pants is the man wearing", -1],
            ["what is the man on the left wearing on the head", 1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man and a woman riding a motorcycle with a dog in the side car.",
            "a man riding a skateboard down a street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2345923",
                "VG_object_id": "903923",
                "bbox": [0, 123, 369, 491],
                "image": "data\\images\\2345923.jpg"
            },
            {
                "VG_image_id": "2334645",
                "VG_object_id": "2384803",
                "bbox": [134, 241, 322, 474],
                "image": "data\\images\\2334645.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what color are the man's clothes", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what is the man doing", -1],
            ["how many people are there", 1],
            ["what is on the man's head", -1],
            ["what is the weather like", -1],
            ["What is man doing", -1],
            ["what color are the man's clothes", 1],
            ["who is holding a phone", -1],
            ["when was the photo taken", -1],
            ["what is in the man's hand", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a man talking on a cell phone while standing in the street.",
            "a woman holding up a cell phone to take a picture."
        ]
    },
    {
        "object_category": "cat",
        "images": [
            {
                "VG_image_id": "2337662",
                "VG_object_id": "3504782",
                "bbox": [32, 117, 210, 353],
                "image": "data\\images\\2337662.jpg"
            },
            {
                "VG_image_id": "2365899",
                "VG_object_id": "1972527",
                "bbox": [0, 123, 287, 343],
                "image": "data\\images\\2365899.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the cat on", 1],
            ["what is the cat doing", 1],
            ["where are the cats", 1],
            ["what gesture is the cat", 1],
            ["what is on the cat's head", 1],
            ["what is the cat looking at", 1],
            ["what is in front of the cat", 1],
            ["where is the cat", 1],
            ["what is on the wall", 1]
        ],
        "org_questions": [
            ["what color is the cat", -1],
            ["what is the cat on", 1],
            ["what is the cat doing", 1],
            ["how many cats together", -1],
            ["where are the cats", 1],
            ["what gesture is the cat", 1],
            ["how many cats are in the picture", -1],
            ["who is in the picture", -1],
            ["what is on the cat's head", 1],
            ["what is the cat looking at", 1],
            ["what is on the cat", -1],
            ["what is in front of the cat", 1],
            ["where is the cat", 1],
            ["who is in the photo", -1],
            ["what is on the wall", 1]
        ],
        "context": [
            "a cat sitting on a table watching a television.",
            "a black cat laying on a wooden shelf."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2390470",
                "VG_object_id": "1249913",
                "bbox": [47, 161, 320, 331],
                "image": "data\\images\\2390470.jpg"
            },
            {
                "VG_image_id": "2358520",
                "VG_object_id": "1814757",
                "bbox": [2, 4, 498, 297],
                "image": "data\\images\\2358520.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the table", 1],
            ["what is the table made of", 1],
            ["how many people are there", 1],
            ["how many plates are there on the table", 1]
        ],
        "org_questions": [
            ["what color is the table", 1],
            ["what is the table made of", 1],
            ["how many people are there", 1],
            ["what is on the table", -1],
            ["what pattern is the plate", -1],
            ["how many plates are there on the table", 1],
            ["where was this photo taken", -1],
            ["where is the plate", -1],
            ["what is the food on", -1],
            ["where is the picture taken", -1],
            ["what color is the plate", -1]
        ],
        "context": [
            "a group of people standing around a table filled with food.",
            "two plates of food on a table with drinks."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2346889",
                "VG_object_id": "1985105",
                "bbox": [89, 172, 404, 346],
                "image": "data\\images\\2346889.jpg"
            },
            {
                "VG_image_id": "2337664",
                "VG_object_id": "956794",
                "bbox": [139, 108, 292, 285],
                "image": "data\\images\\2337664.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is wearing the shirt", 2],
            ["what is in the background", 1],
            ["How many people are there", 1],
            ["who is in the photo", 1]
        ],
        "org_questions": [
            ["who is wearing the shirt", 2],
            ["where is the picture taken", -1],
            ["what is in the background", 1],
            ["How many people are there", 1],
            ["who is in the photo", 1]
        ],
        "context": [
            "a teddy bear wearing a t - shirt sitting on a bed.",
            "a woman standing in front of a counter with a cake on it."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2417910",
                "VG_object_id": "3417269",
                "bbox": [5, 202, 374, 497],
                "image": "data\\images\\2417910.jpg"
            },
            {
                "VG_image_id": "2319309",
                "VG_object_id": "3073791",
                "bbox": [229, 249, 461, 374],
                "image": "data\\images\\2319309.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the counter", 1],
            ["what is on the counter", 1],
            ["what is in the kitchen", 1],
            ["what is the persion sitting on", 1],
            ["what is sitting on the counter", 1]
        ],
        "org_questions": [
            ["what color is the counter", 1],
            ["what is on the counter", 1],
            ["how many bottles are there on the counter", -1],
            ["where is the counter", -1],
            ["what is the counter made of", -1],
            ["what is in the kitchen", 1],
            ["what is the persion sitting on", 1],
            ["where is the photo taken", -1],
            ["how many people are there", -1],
            ["what is sitting on the counter", 1]
        ],
        "context": [
            "a little girl sitting on a counter eating a piece of food.",
            "a woman is eating a slice of pizza while watching tv."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2391930",
                "VG_object_id": "484854",
                "bbox": [247, 70, 500, 295],
                "image": "data\\images\\2391930.jpg"
            },
            {
                "VG_image_id": "2348080",
                "VG_object_id": "2524259",
                "bbox": [4, 65, 372, 305],
                "image": "data\\images\\2348080.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 1],
            ["what is on the table", 1],
            ["what room is the curtain in", 1],
            ["when is this picture taken", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 1],
            ["what is on the table", 1],
            ["how many people are there", -1],
            ["what room is the curtain in", 1],
            ["where is the curtain", -1],
            ["when is this picture taken", 1],
            ["what is in the background", -1],
            ["where is the photo taken", -1],
            ["what type of room is this", -1],
            ["what is hanging on the wall", -1],
            ["what room is this", -1],
            ["where was this picture taken", -1]
        ],
        "context": [
            "a man sitting on a couch in a living room.",
            "a kitchen with a large window and a small table with a hot dog on it."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2341510",
                "VG_object_id": "942253",
                "bbox": [133, 210, 492, 360],
                "image": "data\\images\\2341510.jpg"
            },
            {
                "VG_image_id": "2368958",
                "VG_object_id": "614845",
                "bbox": [2, 372, 112, 498],
                "image": "data\\images\\2368958.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the blanket", 2],
            ["what blanket is on", 1],
            ["where was the photo taken", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the blanket", -1],
            ["what blanket is on", 1],
            ["how many cats are there on the bed", -1],
            ["when is this photo taken", -1],
            ["where is the blanket", 2],
            ["where was the photo taken", 1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a bedroom with a television and a bed",
            "a man holding a frisbee in a park."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2322552",
                "VG_object_id": "3503436",
                "bbox": [39, 33, 263, 260],
                "image": "data\\images\\2322552.jpg"
            },
            {
                "VG_image_id": "2368392",
                "VG_object_id": "618093",
                "bbox": [174, 28, 337, 325],
                "image": "data\\images\\2368392.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["what color is the man's shirt", 1],
            ["what is the man holding", 1],
            ["what is the man doing", 1],
            ["what is the main color of the shirt", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man holding", 1],
            ["what is the man doing", 1],
            ["how many people are there in the photo", 2],
            ["what time is it", -1],
            ["where is the person", -1],
            ["how is the weather", -1],
            ["who is in the picture", -1],
            ["what is the persion wearing", -1],
            ["when was the photo taken", -1],
            ["what is the main color of the shirt", 1]
        ],
        "context": [
            "a boy doing a trick on a skateboard on a sidewalk.",
            "a man and a woman riding a motorcycle."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2383188",
                "VG_object_id": "1323129",
                "bbox": [170, 129, 412, 303],
                "image": "data\\images\\2383188.jpg"
            },
            {
                "VG_image_id": "2323214",
                "VG_object_id": "3182654",
                "bbox": [138, 153, 285, 353],
                "image": "data\\images\\2323214.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what gender is the person in the shirt", 2],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1],
            ["what color is the shirt of the person on the right", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what color is the shirt", 2],
            ["what gender is the person in the shirt", 2],
            ["what is the persion doing", 1],
            ["what is the persion holding", 1],
            ["who is wearing a white shirt", -1],
            ["what is the person wearing", -1],
            ["what color is the shirt of the person on the right", 1]
        ],
        "context": [
            "a girl sitting at a table with a cake and candles.",
            "a man and a woman playing a video game."
        ]
    },
    {
        "object_category": "umbrella",
        "images": [
            {
                "VG_image_id": "2403312",
                "VG_object_id": "382367",
                "bbox": [192, 61, 464, 254],
                "image": "data\\images\\2403312.jpg"
            },
            {
                "VG_image_id": "2318509",
                "VG_object_id": "2714587",
                "bbox": [15, 3, 372, 429],
                "image": "data\\images\\2318509.jpg"
            }
        ],
        "questions_with_scores": [
            ["what pattern is on the umbrella", 2],
            ["who is holding the umbrella", 1],
            ["what is the person wearing", 1],
            ["where is the umbrella", 1],
            ["what is in the distance", 1],
            ["what is the person doing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what color is the umbrella", -1],
            ["who is holding the umbrella", 1],
            ["what is the person wearing", 1],
            ["what pattern is on the umbrella", 2],
            ["where is the umbrella", 1],
            ["how many umbrellas are there", -1],
            ["what is the weather like", -1],
            ["what is in the distance", 1],
            ["what is the umbrella made of", -1],
            ["what is the person doing", 1],
            ["how is the umbrella", -1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a little girl sitting next to a pink umbrella.",
            "a man laying on a beach under an umbrella."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2320536",
                "VG_object_id": "2827638",
                "bbox": [215, 104, 333, 349],
                "image": "data\\images\\2320536.jpg"
            },
            {
                "VG_image_id": "2341552",
                "VG_object_id": "3654540",
                "bbox": [226, 130, 292, 250],
                "image": "data\\images\\2341552.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 2],
            ["where is the man", 1],
            ["what is in the distance", 1],
            ["where is the person", 1],
            ["what sport is being played", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what is the man playing", -1],
            ["where is the man", 1],
            ["what color is the person's shirt", 2],
            ["what is in the distance", 1],
            ["what is the person doing", -1],
            ["what is the person wearing", -1],
            ["where is the person", 1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what sport is being played", 1]
        ],
        "context": [
            "a man is swinging a bat at a ball.",
            "a man swinging a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2401366",
                "VG_object_id": "1148469",
                "bbox": [381, 26, 484, 311],
                "image": "data\\images\\2401366.jpg"
            },
            {
                "VG_image_id": "2390207",
                "VG_object_id": "3828219",
                "bbox": [62, 73, 315, 382],
                "image": "data\\images\\2390207.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's jacket", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's jacket", 1],
            ["how many people are there", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a man standing in a bathroom next to a urinal.",
            "a man riding a horse next to a stop sign."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2407798",
                "VG_object_id": "1095932",
                "bbox": [98, 32, 222, 282],
                "image": "data\\images\\2407798.jpg"
            },
            {
                "VG_image_id": "2370530",
                "VG_object_id": "603440",
                "bbox": [178, 206, 231, 446],
                "image": "data\\images\\2370530.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's clothes", 1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["What is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", -1],
            ["what color is the woman's clothes", 1],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["where is the woman", -1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["What is the woman holding", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["what is on the woman's head", -1],
            ["where are the people", -1]
        ],
        "context": [
            "a woman standing on a sidewalk while using her cell phone.",
            "a woman standing in front of a bus."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2332410",
                "VG_object_id": "3475719",
                "bbox": [0, 263, 492, 365],
                "image": "data\\images\\2332410.jpg"
            },
            {
                "VG_image_id": "2347087",
                "VG_object_id": "893711",
                "bbox": [0, 315, 371, 494],
                "image": "data\\images\\2347087.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 2],
            ["what color is the ground", 1],
            ["what is the ground covered with", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is the ground covered with", 1],
            ["how many people are there", -1],
            ["when is this picture taken", -1],
            ["what animals are there", -1],
            ["what is in the distance", -1],
            ["what is on the ground", -1],
            ["how is the weather", -1],
            ["where is this scene", -1],
            ["what is the giraffe standing on", -1],
            ["what is covering the ground", 1],
            ["what is in the background", 2]
        ],
        "context": [
            "a baby giraffe standing next to a small giraffe.",
            "three giraffes standing in a dirt field next to a building."
        ]
    },
    {
        "object_category": "bathroom",
        "images": [
            {
                "VG_image_id": "2411420",
                "VG_object_id": "3813899",
                "bbox": [5, 57, 494, 326],
                "image": "data\\images\\2411420.jpg"
            },
            {
                "VG_image_id": "2406562",
                "VG_object_id": "368688",
                "bbox": [1, 0, 331, 499],
                "image": "data\\images\\2406562.jpg"
            }
        ],
        "questions_with_scores": [["what color is the ground", 1]],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is on the wall", -1],
            ["what color is the wall", -1],
            ["how many sinks are there", -1],
            ["what is the floor of the bathroom made of", -1],
            ["what is the pattern of the wall", -1],
            ["how may toilets are there", -1],
            ["what is the floor made of", -1],
            ["where was this taken", -1],
            ["what room is this", -1],
            ["where is the toilet", -1],
            ["what is in the bathroom", -1]
        ],
        "context": [
            "a bathroom with a tub, sink, and mirror.",
            "a bathroom with a sink, toilet, and mirror."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2397941",
                "VG_object_id": "429473",
                "bbox": [7, 351, 397, 500],
                "image": "data\\images\\2397941.jpg"
            },
            {
                "VG_image_id": "2415430",
                "VG_object_id": "2705879",
                "bbox": [13, 15, 491, 326],
                "image": "data\\images\\2415430.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what are the people doing", 1],
            ["what color is the ground", 1],
            ["where is the land", 1],
            ["what is the land made of", 1],
            ["where was this picture taken", 1],
            ["what is covering the ground", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what are the people doing", 1],
            ["what kind of animal is on the land", -1],
            ["what color is the ground", 1],
            ["where is the land", 1],
            ["what is the land made of", 1],
            ["what is on the land", -1],
            ["how is the weather", -1],
            ["where was this picture taken", 1],
            ["what is covering the ground", 1],
            ["what is in the background", 1],
            ["what is the weather like", -1]
        ],
        "context": [
            "two men skiing down a snow covered slope.",
            "a child is riding a skateboard down a ramp."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2373743",
                "VG_object_id": "730951",
                "bbox": [156, 298, 227, 353],
                "image": "data\\images\\2373743.jpg"
            },
            {
                "VG_image_id": "2402917",
                "VG_object_id": "1130023",
                "bbox": [190, 224, 287, 262],
                "image": "data\\images\\2402917.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many sinks are there", 1],
            ["what is the ground covered with", 1],
            ["what room is this", 1],
            ["what is next to the sink", 1]
        ],
        "org_questions": [
            ["how many sinks are there", 1],
            ["what is the ground covered with", 1],
            ["what shape is the sink", -1],
            ["where is the picture taken", -1],
            ["what color is the sink", -1],
            ["what is above the sink", -1],
            ["what room is this", 1],
            ["what is next to the sink", 1],
            ["where are the sinks", -1],
            ["what is in the sink", -1],
            ["what is the sink color", -1]
        ],
        "context": [
            "a bathroom with a sink, mirror and toilet.",
            "a kitchen with a sink, microwave, and a refrigerator."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2403496",
                "VG_object_id": "352518",
                "bbox": [2, 116, 143, 331],
                "image": "data\\images\\2403496.jpg"
            },
            {
                "VG_image_id": "2362219",
                "VG_object_id": "776162",
                "bbox": [225, 151, 322, 326],
                "image": "data\\images\\2362219.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man' s shirt", 2],
            ["how many persons are in the picture", 2],
            ["what color is the man's shirt", 2],
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["what kind of shirt is the man wearing", 1],
            ["what are the people wearing", 1]
        ],
        "org_questions": [
            ["what color is the man' s shirt", 2],
            ["what is the man doing", 1],
            ["how many persons are in the picture", 2],
            ["what is the man holding", 1],
            ["how many people are there", -1],
            ["what kind of shirt is the man wearing", 1],
            ["what are the people wearing", 1],
            ["what is on the man's face", -1],
            ["what color is the man's shirt", 2]
        ],
        "context": [
            "a man and woman holding champagne glasses in their hands.",
            "a man in a white apron is making a pizza."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2413801",
                "VG_object_id": "165965",
                "bbox": [113, 160, 299, 369],
                "image": "data\\images\\2413801.jpg"
            },
            {
                "VG_image_id": "2351134",
                "VG_object_id": "1969133",
                "bbox": [239, 255, 437, 370],
                "image": "data\\images\\2351134.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is in the photo", 2],
            ["what color is the shirt", 1],
            ["what is the person holding", 1],
            ["what color is the person's hair", 1],
            ["who is wearing the shirt", 1],
            ["what is the persion doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the person holding", 1],
            ["what color is the person's hair", 1],
            ["how many people are there", -1],
            ["who is wearing the shirt", 1],
            ["what is the persion doing", 1],
            ["what is the persion wearing", -1],
            ["who is in the photo", 2]
        ],
        "context": [
            "a man in a yellow shirt holding a plate of food.",
            "a little girl with her hand on her face"
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2359248",
                "VG_object_id": "1823266",
                "bbox": [3, 7, 483, 277],
                "image": "data\\images\\2359248.jpg"
            },
            {
                "VG_image_id": "2383601",
                "VG_object_id": "3848008",
                "bbox": [1, 0, 499, 220],
                "image": "data\\images\\2383601.jpg"
            }
        ],
        "questions_with_scores": [
            ["what are the people doing", 2],
            ["when was the photo taken", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what are the people doing", 2],
            ["what is on the wall", -1],
            ["when was the photo taken", 1],
            ["where are the people", -1],
            ["who is in the photo", -1],
            ["what is the building made of", -1],
            ["where was this picture taken", 1]
        ],
        "context": [
            "a man riding a skateboard down a sidewalk.",
            "a man sitting on a bench with a laptop."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "713018",
                "VG_object_id": "1076346",
                "bbox": [487, 197, 690, 605],
                "image": "data\\images\\713018.jpg"
            },
            {
                "VG_image_id": "2398158",
                "VG_object_id": "1182116",
                "bbox": [242, 168, 315, 375],
                "image": "data\\images\\2398158.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the woman", 1],
            ["How many people are there", 1]
        ],
        "org_questions": [
            ["What color is the woman's top", -1],
            ["Where is the woman", 1],
            ["How many people are there", 1],
            ["What is woman holding", -1],
            ["What is woman doing", -1],
            ["what is the woman wearing", -1],
            ["what is the woman's posture", -1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's head", -1],
            ["what is the woman carrying", -1],
            ["what are the people doing", -1]
        ],
        "context": [
            "a group of people at a train station",
            "a woman standing next to a small airplane."
        ]
    },
    {
        "object_category": "rug",
        "images": [
            {
                "VG_image_id": "2349084",
                "VG_object_id": "2388873",
                "bbox": [290, 172, 411, 206],
                "image": "data\\images\\2349084.jpg"
            },
            {
                "VG_image_id": "2351529",
                "VG_object_id": "2857992",
                "bbox": [3, 235, 253, 314],
                "image": "data\\images\\2351529.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the photo", 2],
            ["what color is the floor under the rug", 1],
            ["WHat is on the rug", 1],
            ["what type of flooring is shown", 1],
            ["what is the main color of the floor", 1]
        ],
        "org_questions": [
            ["what color is the rug", -1],
            ["what color is the floor under the rug", 1],
            ["how many people are there in the photo", 2],
            ["what is on the ground", -1],
            ["where is the rug", -1],
            ["what is in the background", -1],
            ["WHat is on the rug", 1],
            ["what type of flooring is shown", 1],
            ["what kind of flooring is on the floor", -1],
            ["what is the floor made of", -1],
            ["what is the main color of the floor", 1]
        ],
        "context": [
            "a living room with a fireplace and a chair.",
            "a group of people sitting on a couch playing a video game."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2356918",
                "VG_object_id": "2591889",
                "bbox": [99, 83, 384, 330],
                "image": "data\\images\\2356918.jpg"
            },
            {
                "VG_image_id": "2336787",
                "VG_object_id": "3139913",
                "bbox": [7, 44, 289, 329],
                "image": "data\\images\\2336787.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's clothes", 1],
            ["what is the woman holding", 1],
            ["what is the woman wearing on the head", 1],
            ["when was this picture taken", 1],
            ["how is the weather", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", 1],
            ["what is the woman holding", 1],
            ["what is the woman wearing on the head", 1],
            ["How many people are there", -1],
            ["where is the person", -1],
            ["WHat is woman doing", -1],
            ["what is the woman wearing", -1],
            ["when was this picture taken", 1],
            ["how is the weather", 1],
            ["who is in the picture", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "two girls walking in the street with umbrellas.",
            "a woman is standing on a sidewalk with a cell phone."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2338443",
                "VG_object_id": "2807351",
                "bbox": [154, 31, 324, 210],
                "image": "data\\images\\2338443.jpg"
            },
            {
                "VG_image_id": "2370313",
                "VG_object_id": "742082",
                "bbox": [185, 143, 317, 282],
                "image": "data\\images\\2370313.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's clothes", 2],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["how many people are there", -1],
            ["what color is the man's clothes", 2],
            ["where is the photo taken", -1],
            ["what is the man doing", -1],
            ["what is the weather like", -1],
            ["what is the man wearing", 1],
            ["where is the person", -1],
            ["who is surfing", -1],
            ["when was the picture taken", -1],
            ["what is the man standing on", -1],
            ["what is on the man's feet", -1],
            ["what is the persion riding", -1]
        ],
        "context": [
            "a man riding a wave on top of a surfboard.",
            "a man riding a wave on a surfboard."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2392356",
                "VG_object_id": "480756",
                "bbox": [42, 48, 297, 458],
                "image": "data\\images\\2392356.jpg"
            },
            {
                "VG_image_id": "2317454",
                "VG_object_id": "3184362",
                "bbox": [264, 165, 409, 296],
                "image": "data\\images\\2317454.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the child's shirt", 1],
            ["what are on the table", 1],
            ["what is the child wearing", 1],
            ["what is the little girl eating", 1],
            ["what is the person eating", 1]
        ],
        "org_questions": [
            ["what color is the child's shirt", 1],
            ["what are on the table", 1],
            ["How many people are there", -1],
            ["what gender is the child", -1],
            ["what are the children doing", -1],
            ["what is the child wearing", 1],
            ["What is girl doing", -1],
            ["who is in the photo", -1],
            ["what is the little girl eating", 1],
            ["what color is the table", -1],
            ["what is the person eating", 1]
        ],
        "context": [
            "a young girl eating a doughnut with a smile on her face.",
            "a man and a child sitting at a table with pizza."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2409492",
                "VG_object_id": "240601",
                "bbox": [134, 86, 270, 451],
                "image": "data\\images\\2409492.jpg"
            },
            {
                "VG_image_id": "150378",
                "VG_object_id": "1038350",
                "bbox": [40, 159, 171, 544],
                "image": "data\\images\\150378.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["what is the color of the man's pants", 2],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["what is the color of the man's pants", 2],
            ["how many people are there", -1],
            ["What is the man doing", -1],
            ["what is the ground covered with", -1],
            ["where is the man", -1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is skateboarding", -1],
            ["what is the man riding on", -1],
            ["what is on the man's feet", -1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a man riding a skateboard on a sidewalk.",
            "a man riding a skateboard down a sidewalk."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "713197",
                "VG_object_id": "1909968",
                "bbox": [161, 441, 592, 737],
                "image": "data\\images\\713197.jpg"
            },
            {
                "VG_image_id": "2317621",
                "VG_object_id": "1019112",
                "bbox": [302, 169, 394, 240],
                "image": "data\\images\\2317621.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person holding", 1],
            ["what is the person doing", 1],
            ["what color is the shirt", 1],
            ["who is in the photo", 1],
            ["what color is the boy's jacket", 1],
            ["what color is the shirt of the boy", 1]
        ],
        "org_questions": [
            ["what is the person holding", 1],
            ["what is the person doing", 1],
            ["how many people are there", -1],
            ["what color is the shirt", 1],
            ["when is this photo taken", -1],
            ["What is man wearing on his head", -1],
            ["where is the person", -1],
            ["what is the weather like", -1],
            ["who is in the photo", 1],
            ["what is the boy wearing", -1],
            ["what color is the boy's jacket", 1],
            ["what color is the shirt of the boy", 1]
        ],
        "context": [
            "a man sitting at a desk eating a piece of food.",
            "two young boys playing with a plastic bat."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2402141",
                "VG_object_id": "1139919",
                "bbox": [2, 234, 254, 430],
                "image": "data\\images\\2402141.jpg"
            },
            {
                "VG_image_id": "2317952",
                "VG_object_id": "1015833",
                "bbox": [240, 266, 373, 349],
                "image": "data\\images\\2317952.jpg"
            }
        ],
        "questions_with_scores": [["what is the ground covered with", 1]],
        "org_questions": [
            ["what is on the counter", -1],
            ["what color is the wall", -1],
            ["what is the ground covered with", 1],
            ["How many people are there", -1],
            ["what is the floor made of", -1],
            ["what color is the floor", -1],
            ["where is the picture taken", -1],
            ["what room is this", -1]
        ],
        "context": [
            "a kitchen with a stove, sink, and refrigerator.",
            "a kitchen with a stove, microwave and dishwasher."
        ]
    },
    {
        "object_category": "distance",
        "images": [
            {
                "VG_image_id": "2377657",
                "VG_object_id": "717112",
                "bbox": [1, 0, 187, 375],
                "image": "data\\images\\2377657.jpg"
            },
            {
                "VG_image_id": "2317756",
                "VG_object_id": "1017670",
                "bbox": [2, 3, 499, 202],
                "image": "data\\images\\2317756.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the background", 1],
            ["how many trees are there in the distance", 1],
            ["where are the trees", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 2],
            ["what color is the background", 1],
            ["how many trees are there in the distance", 1],
            ["what time is it", -1],
            ["where are the trees", 1],
            ["how is the weather", -1],
            ["when was this picture taken", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a large clock tower with a clock on it's side.",
            "a man and woman sitting on a bench in a park."
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2365946",
                "VG_object_id": "2105850",
                "bbox": [97, 163, 292, 297],
                "image": "data\\images\\2365946.jpg"
            },
            {
                "VG_image_id": "2401171",
                "VG_object_id": "1150822",
                "bbox": [191, 168, 341, 208],
                "image": "data\\images\\2401171.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many motorcycles are there", 2],
            ["what is the ground covered with", 1],
            ["what is in the distance", 1],
            ["what is on the side of the motorcycle", 1]
        ],
        "org_questions": [
            ["how many motorcycles are there", 2],
            ["what time is it", -1],
            ["what is the ground covered with", 1],
            ["where are the motor ", -1],
            ["what is in the distance", 1],
            ["where is the photo taken", -1],
            ["when was the picture taken", -1],
            ["what is on the side of the motorcycle", 1],
            ["what is parked on the ground", -1]
        ],
        "context": [
            "a motorcycle parked on the side of a street.",
            "a group of people standing around a car."
        ]
    },
    {
        "object_category": "cow",
        "images": [
            {
                "VG_image_id": "2364987",
                "VG_object_id": "2578237",
                "bbox": [116, 39, 289, 339],
                "image": "data\\images\\2364987.jpg"
            },
            {
                "VG_image_id": "2327409",
                "VG_object_id": "3229683",
                "bbox": [96, 146, 281, 261],
                "image": "data\\images\\2327409.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what is behind the cow", 1]
        ],
        "org_questions": [
            ["where is the cow", -1],
            ["what color is the ground", 1],
            ["how many cows are there", -1],
            ["What is cow doing", -1],
            ["What color is cow", -1],
            ["what is the cow doing", -1],
            ["what color are the cows", -1],
            ["what type of animal is shown", -1],
            ["what is on the cow's head", -1],
            ["when was the photo taken", -1],
            ["what is behind the cow", 1],
            ["what is the cow looking at", -1]
        ],
        "context": [
            "a cow standing on a beach next to a boat.",
            "a couple of cows standing in a field of grass."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2369480",
                "VG_object_id": "3171064",
                "bbox": [296, 145, 485, 305],
                "image": "data\\images\\2369480.jpg"
            },
            {
                "VG_image_id": "2326842",
                "VG_object_id": "2998578",
                "bbox": [33, 140, 301, 257],
                "image": "data\\images\\2326842.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the dog", 1],
            ["What is the dog sitting on", 1],
            ["Where is the photo taken", 1],
            ["what is behind the dog", 1],
            ["where is the dog sitting", 1]
        ],
        "org_questions": [
            ["What color is the dog", 1],
            ["What is the dog sitting on", 1],
            ["Where is the photo taken", 1],
            ["how many dogs are there in the picture", -1],
            ["what is the dog doing", -1],
            ["what is the dog wearing", -1],
            ["what gesture is the dog", -1],
            ["what animal is in the picture", -1],
            ["what is behind the dog", 1],
            ["what is on the dog's head", -1],
            ["where is the dog sitting", 1]
        ],
        "context": [
            "a white dog laying on a bench",
            "two dogs sleeping on a bed with pillows."
        ]
    },
    {
        "object_category": "zebra",
        "images": [
            {
                "VG_image_id": "2341439",
                "VG_object_id": "942757",
                "bbox": [221, 106, 431, 331],
                "image": "data\\images\\2341439.jpg"
            },
            {
                "VG_image_id": "2348992",
                "VG_object_id": "2048431",
                "bbox": [68, 125, 393, 325],
                "image": "data\\images\\2348992.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many zebras are there in the picture", 2],
            ["what is the land covered with", 1],
            ["where is the zebras", 1],
            ["what color is the background", 1],
            ["what is the ground covered with", 1],
            ["what are the zebras doing", 1]
        ],
        "org_questions": [
            ["how many zebras are there in the picture", 2],
            ["what is the land covered with", 1],
            ["what animals are on the grass", -1],
            ["where is the zebras", 1],
            ["what color is the background", 1],
            ["what is the ground covered with", 1],
            ["what is on the ground", -1],
            ["when was the photo taken", -1],
            ["what are the zebras doing", 1],
            ["what is the zebra standing on", -1],
            ["where was the photo taken", -1],
            ["what type of animals are shown", -1]
        ],
        "context": [
            "two zebras standing in a field of tall grass.",
            "a group of zebras standing around in a park."
        ]
    },
    {
        "object_category": "airplane",
        "images": [
            {
                "VG_image_id": "2363279",
                "VG_object_id": "2421954",
                "bbox": [4, 37, 496, 275],
                "image": "data\\images\\2363279.jpg"
            },
            {
                "VG_image_id": "2406781",
                "VG_object_id": "290351",
                "bbox": [108, 56, 449, 275],
                "image": "data\\images\\2406781.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many airplanes are there", 2],
            ["what are the airplanes doing", 2],
            ["what direction does the plane fly in", 1],
            ["What is the number of plane", 1]
        ],
        "org_questions": [
            ["how many airplanes are there", 2],
            ["what are the airplanes doing", 2],
            ["what color is the sky", -1],
            ["what direction does the plane fly in", 1],
            ["where is the plane", -1],
            ["What is the number of plane", 1],
            ["what color is the line on the plane", -1],
            ["when was the photo taken", -1],
            ["what is on the ground", -1],
            ["what is behind the plane", -1],
            ["where was this photo taken", -1],
            ["what is in the air", -1]
        ],
        "context": [
            "a large jetliner sitting on top of a runway.",
            "a couple of planes that are on a runway"
        ]
    },
    {
        "object_category": "motorcycle",
        "images": [
            {
                "VG_image_id": "2336736",
                "VG_object_id": "3107260",
                "bbox": [72, 129, 425, 366],
                "image": "data\\images\\2336736.jpg"
            },
            {
                "VG_image_id": "2390899",
                "VG_object_id": "493174",
                "bbox": [5, 154, 497, 307],
                "image": "data\\images\\2390899.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many motorcycles are there", 2],
            ["where are the motocycles", 1],
            ["what is the ground covered with", 1],
            ["what is beside the motorcycle", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["how many motorcycles are there", 2],
            ["where are the motocycles", 1],
            ["when is the picture taken", -1],
            ["what is the ground covered with", 1],
            ["what is in the background", -1],
            ["what is beside the motorcycle", 1],
            ["where was this picture taken", -1],
            ["what is parked on the street", -1],
            ["what is on the ground", 1],
            ["where is the motorcycle parked", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a motorcycle parked on the side of a street.",
            "a group of motorcycles parked in a parking lot."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2368973",
                "VG_object_id": "1863336",
                "bbox": [9, 159, 499, 328],
                "image": "data\\images\\2368973.jpg"
            },
            {
                "VG_image_id": "2359373",
                "VG_object_id": "2318039",
                "bbox": [1, 373, 374, 497],
                "image": "data\\images\\2359373.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 1],
            ["what is the weather like", 1],
            ["how many people on the ground", 1],
            ["what is on the land", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["what is the weather like", 1],
            ["how many people on the ground", 1],
            ["what is the land made of", -1],
            ["what is on the land", 1],
            ["where was the photo taken", -1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "a woman walking down a street holding an umbrella.",
            "a parking meter on the sidewalk next to a building."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2365474",
                "VG_object_id": "633978",
                "bbox": [88, 76, 283, 353],
                "image": "data\\images\\2365474.jpg"
            },
            {
                "VG_image_id": "2402355",
                "VG_object_id": "1137189",
                "bbox": [184, 258, 367, 461],
                "image": "data\\images\\2402355.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the chair", 2],
            ["what is on the chair", 1],
            ["where is the photo taken", 1],
            ["how many people are in  the picture", 1],
            ["who is sitting on the chair", 1],
            ["what is in front of the chair", 1]
        ],
        "org_questions": [
            ["what color is the chair", 2],
            ["what is on the chair", 1],
            ["where is the chair", -1],
            ["how many computers are there", -1],
            ["where is the photo taken", 1],
            ["how many people are in  the picture", 1],
            ["how many pillows are there on the chair", -1],
            ["who is sitting on the chair", 1],
            ["what is the chair made of", -1],
            ["what is the chair sitting on", -1],
            ["what is in front of the chair", 1]
        ],
        "context": [
            "a chair with a laptop on it next to a bed.",
            "a man sitting at a table using a laptop."
        ]
    },
    {
        "object_category": "cabinet",
        "images": [
            {
                "VG_image_id": "2405843",
                "VG_object_id": "371596",
                "bbox": [208, 81, 471, 182],
                "image": "data\\images\\2405843.jpg"
            },
            {
                "VG_image_id": "2363388",
                "VG_object_id": "1889387",
                "bbox": [0, 1, 371, 140],
                "image": "data\\images\\2363388.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 1],
            ["what is the main color of the wall", 1],
            ["who is in the photo", 1],
            ["where was this photo taken", 1],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", -1],
            ["how many people are there in the picture", 1],
            ["where is the cabinet", -1],
            ["what is the main color of the wall", 1],
            ["what room is this", -1],
            ["what is the wall made of", -1],
            ["who is in the photo", 1],
            ["where was this photo taken", 1],
            ["how many people are there", 1],
            ["what is on the wall", -1],
            ["where was the picture taken", -1]
        ],
        "context": [
            "a woman playing a video game with a remote.",
            "a pineapple with bananas on top of it"
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2359497",
                "VG_object_id": "792874",
                "bbox": [12, 35, 240, 94],
                "image": "data\\images\\2359497.jpg"
            },
            {
                "VG_image_id": "2316492",
                "VG_object_id": "3471688",
                "bbox": [180, 0, 497, 119],
                "image": "data\\images\\2316492.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the seats", 2],
            ["what color is the man's cap", 2],
            ["what color is the man's shirt", 1]
        ],
        "org_questions": [
            ["what color are the seats", 2],
            ["what color is the man's cap", 2],
            ["what color is the man's shirt", 1],
            ["how many motorcycles are there", -1],
            ["what shape is the seat", -1],
            ["where is the picture taken", -1],
            ["where is the chair", -1],
            ["What color is the floor", -1],
            ["what is in the background", -1],
            ["when was the picture taken", -1],
            ["what is behind the player", -1],
            ["where are the people", -1]
        ],
        "context": [
            "a man holding a tennis racquet on a tennis court.",
            "a man playing tennis on a tennis court."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "498012",
                "VG_object_id": "1039256",
                "bbox": [2, 141, 1021, 766],
                "image": "data\\images\\498012.jpg"
            },
            {
                "VG_image_id": "2326415",
                "VG_object_id": "3939625",
                "bbox": [2, 347, 291, 498],
                "image": "data\\images\\2326415.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt of the person sitting at the table", 2],
            ["where is the table", 1],
            ["what are on the table", 1],
            ["how many plates are there on the table", 1],
            ["what color is the table", 1],
            ["where is this scene", 1]
        ],
        "org_questions": [
            ["where is the table", 1],
            ["what color is the shirt of the person sitting at the table", 2],
            ["what are on the table", 1],
            ["How many people are there", -1],
            ["what is the table made of", -1],
            ["what is beside the table", -1],
            ["how many plates are there on the table", 1],
            ["what color is the table", 1],
            ["where is this scene", 1],
            ["what is in the table", -1],
            ["what is under the table", -1]
        ],
        "context": [
            "a picnic table with a person sitting at it",
            "a woman smiling at a table with a piece of cake."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2404936",
                "VG_object_id": "336411",
                "bbox": [311, 107, 396, 332],
                "image": "data\\images\\2404936.jpg"
            },
            {
                "VG_image_id": "2366734",
                "VG_object_id": "1863893",
                "bbox": [22, 12, 222, 354],
                "image": "data\\images\\2366734.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the woman's pair of trousers", 1],
            ["What color is the woman's hair", 1],
            ["what is the woman holding", 1],
            ["what type of pants is the woman wearing", 1]
        ],
        "org_questions": [
            ["What color is the woman's pair of trousers", 1],
            ["What is the woman doing", -1],
            ["What color is the woman's hair", 1],
            ["how many people are there", -1],
            ["what time is it", -1],
            ["what is the woman holding", 1],
            ["what gesture is the woman", -1],
            ["who is in the photo", -1],
            ["what is on the woman's head", -1],
            ["what type of pants is the woman wearing", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a woman feeding a horse a carrot in a field.",
            "a woman standing in a yard with a cell phone."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2367076",
                "VG_object_id": "2001497",
                "bbox": [125, 75, 277, 181],
                "image": "data\\images\\2367076.jpg"
            },
            {
                "VG_image_id": "2378023",
                "VG_object_id": "2648497",
                "bbox": [294, 49, 446, 184],
                "image": "data\\images\\2378023.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the tv", 1],
            ["what is the tv on", 1],
            ["which room is the television placing in", 1],
            ["where is the tv", 1],
            ["what is showing on the tv", 1],
            ["what is in front of the television", 1],
            ["where was the photo taken", 1],
            ["what is above the tv", 1],
            ["what is behind the tv", 1]
        ],
        "org_questions": [
            ["what is the color of the tv", 1],
            ["what is the tv on", 1],
            ["how many people are there in the picture", -1],
            ["which room is the television placing in", 1],
            ["where is the tv", 1],
            ["what is showing on the tv", 1],
            ["what is in front of the television", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", -1],
            ["where was the photo taken", 1],
            ["what is above the tv", 1],
            ["what is behind the tv", 1]
        ],
        "context": [
            "a christmas tree is in the window",
            "a man sitting on a couch watching television."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2362643",
                "VG_object_id": "2573749",
                "bbox": [333, 131, 404, 336],
                "image": "data\\images\\2362643.jpg"
            },
            {
                "VG_image_id": "2335223",
                "VG_object_id": "2800489",
                "bbox": [129, 15, 246, 272],
                "image": "data\\images\\2335223.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["What is the man holding", 1],
            ["when was the photo taken", 1]
        ],
        "org_questions": [
            ["how many people are there in the picture", 1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["What is the man wearing on his head", -1],
            ["what color are the man's shoes", -1],
            ["What is the man holding", 1],
            ["what is the man wearing", -1],
            ["when was the photo taken", 1]
        ],
        "context": [
            "a group of people on motorcycles in a street.",
            "a man riding a bike through a flooded street."
        ]
    },
    {
        "object_category": "bottle",
        "images": [
            {
                "VG_image_id": "2393524",
                "VG_object_id": "471895",
                "bbox": [222, 2, 274, 117],
                "image": "data\\images\\2393524.jpg"
            },
            {
                "VG_image_id": "2355036",
                "VG_object_id": "833407",
                "bbox": [199, 32, 326, 480],
                "image": "data\\images\\2355036.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many people are there", 2],
            ["what is the table the bottle on made of", 1],
            ["where is the bottle", 1],
            ["what is in the bottle", 1],
            ["what is behind the bottle", 1],
            ["what is he bottle made of", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the bottle", -1],
            ["what is the table the bottle on made of", 1],
            ["where is the bottle", 1],
            ["How many people are there", 2],
            ["what is in the bottle", 1],
            ["what is behind the bottle", 1],
            ["what is he bottle made of", 1],
            ["how many bottles are there", -1],
            ["where was the photo taken", 1]
        ],
        "context": [
            "a display case filled with chocolate chip cookies and chocolate chips.",
            "a man and a woman sitting at a table with a bottle of wine."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2352338",
                "VG_object_id": "1749420",
                "bbox": [5, 64, 312, 179],
                "image": "data\\images\\2352338.jpg"
            },
            {
                "VG_image_id": "2345154",
                "VG_object_id": "3626752",
                "bbox": [0, 1, 483, 116],
                "image": "data\\images\\2345154.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 1],
            ["what is in front of the building", 1],
            ["what is on the ground", 1],
            ["where was this picture taken", 1]
        ],
        "org_questions": [
            ["what color is the building", 1],
            ["how many people are there in front of the building", -1],
            ["what is in front of the building", 1],
            ["where is the building", -1],
            ["what is on the ground", 1],
            ["when was the picture taken", -1],
            ["what is the building made of", -1],
            ["where was this picture taken", 1]
        ],
        "context": [
            "three sheep standing in a field with a house in the background.",
            "a woman is talking on a cell phone."
        ]
    },
    {
        "object_category": "horse",
        "images": [
            {
                "VG_image_id": "2376921",
                "VG_object_id": "2458665",
                "bbox": [36, 63, 428, 265],
                "image": "data\\images\\2376921.jpg"
            },
            {
                "VG_image_id": "2315824",
                "VG_object_id": "3297482",
                "bbox": [231, 110, 491, 346],
                "image": "data\\images\\2315824.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is on the horse", 1],
            ["who is on the horses", 1]
        ],
        "org_questions": [
            ["what color is the horse", -1],
            ["what is the land made of", -1],
            ["what is in the background", -1],
            ["how many people are there", 1],
            ["what are the horses doing", -1],
            ["where is the man", 1],
            ["what is on the horse", 1],
            ["how many horses are in the picture", -1],
            ["when was the photo taken", -1],
            ["who is on the horses", 1],
            ["what are the horses walking on", -1]
        ],
        "context": [
            "a black and white photo of two horses pulling a carriage.",
            "a group of men riding horses down a street."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2383488",
                "VG_object_id": "532436",
                "bbox": [0, 177, 499, 329],
                "image": "data\\images\\2383488.jpg"
            },
            {
                "VG_image_id": "2363779",
                "VG_object_id": "3743690",
                "bbox": [3, 203, 499, 331],
                "image": "data\\images\\2363779.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what is on the land", -1],
            ["how many people are there", 2],
            ["where is the picture taken", -1],
            ["what is the ground covered with", 1],
            ["What is land made of", -1],
            ["how is the weather", -1],
            ["what is the train doing", -1],
            ["what is the train on", -1],
            ["what is next to the train", -1],
            ["where are the tracks", -1]
        ],
        "context": [
            "a train is pulling into a train station.",
            "a train engine with a man standing next to it."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2414143",
                "VG_object_id": "159134",
                "bbox": [305, 135, 383, 201],
                "image": "data\\images\\2414143.jpg"
            },
            {
                "VG_image_id": "2346996",
                "VG_object_id": "2525964",
                "bbox": [214, 60, 380, 243],
                "image": "data\\images\\2346996.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the player's shorts", 2],
            ["what color is the player's shirt", 1]
        ],
        "org_questions": [
            ["what color is the player's shirt", 1],
            ["what color is the player's shorts", 2],
            ["what gender is the person in the shirt", -1],
            ["what shape is the shirt's collar", -1],
            ["what is the man doing", -1],
            ["where is the man", -1],
            ["what is in the distance", -1],
            ["what is on the man's head", -1],
            ["what type of shirt is the man wearing", -1],
            ["what is the man wearing", -1],
            ["what is on the man's shirt", -1]
        ],
        "context": [
            "a man playing tennis on a clay court.",
            "a man swinging a tennis racket at a ball."
        ]
    },
    {
        "object_category": "bench",
        "images": [
            {
                "VG_image_id": "2370906",
                "VG_object_id": "3856033",
                "bbox": [22, 219, 371, 305],
                "image": "data\\images\\2370906.jpg"
            },
            {
                "VG_image_id": "2398854",
                "VG_object_id": "421452",
                "bbox": [3, 106, 500, 391],
                "image": "data\\images\\2398854.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are sitting on the bench", 1],
            ["what is behind the bench", 1]
        ],
        "org_questions": [
            ["what color is the bench", -1],
            ["what is the bench made of", -1],
            ["how many people are there", -1],
            ["when is the picture taken", -1],
            ["what is the person doing on the bench", -1],
            ["what gender is the person", -1],
            ["what is on the bench", -1],
            ["where are the people", -1],
            ["what is in front of the man", -1],
            ["what are the people doing", -1],
            ["how many people are sitting on the bench", 1],
            ["what color is the ground", -1],
            ["what season is the photo taken in", -1],
            ["what is behind the bench", 1],
            ["Where is the bench", -1]
        ],
        "context": [
            "a couple of people sitting on top of a wooden bench.",
            "a man and a woman sitting on a bench."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2373332",
                "VG_object_id": "1915533",
                "bbox": [302, 105, 433, 328],
                "image": "data\\images\\2373332.jpg"
            },
            {
                "VG_image_id": "2371390",
                "VG_object_id": "597934",
                "bbox": [120, 195, 274, 453],
                "image": "data\\images\\2371390.jpg"
            }
        ],
        "questions_with_scores": [
            ["what sport is the man playing", 2],
            ["what is the man holding", 2],
            ["What is man doing", 1],
            ["what is the man wearing on his head", 1]
        ],
        "org_questions": [
            ["what sport is the man playing", 2],
            ["what is on the man's shirt", -1],
            ["what is the main color of the floor", -1],
            ["How many people are there", -1],
            ["Where is the man", -1],
            ["What is man doing", 1],
            ["what is the man holding", 2],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1],
            ["what is the man wearing on his head", 1]
        ],
        "context": [
            "a baseball player is throwing a ball on a field.",
            "a man holding a tennis racquet on a tennis court."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2375081",
                "VG_object_id": "723759",
                "bbox": [191, 191, 304, 374],
                "image": "data\\images\\2375081.jpg"
            },
            {
                "VG_image_id": "2338731",
                "VG_object_id": "3078747",
                "bbox": [97, 22, 400, 331],
                "image": "data\\images\\2338731.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 1],
            ["what is the man holding", 1],
            ["what color is the man's trouser", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man wearing on his head", 1],
            ["what is the man doing", 1],
            ["what is on the man's wrist", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 1],
            ["what is the man holding", 1],
            ["what color is the man's trouser", 1],
            ["how many people are there", 1],
            ["where is the man", 1],
            ["what is the man wearing on his head", 1],
            ["what is the man doing", 1],
            ["what is the man's posture", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1],
            ["what is on the man's wrist", 1]
        ],
        "context": [
            "a man in a field with a frisbee.",
            "a man holding a tennis racket in his right hand."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2403415",
                "VG_object_id": "353330",
                "bbox": [246, 277, 368, 315],
                "image": "data\\images\\2403415.jpg"
            },
            {
                "VG_image_id": "2386102",
                "VG_object_id": "519825",
                "bbox": [388, 266, 446, 312],
                "image": "data\\images\\2386102.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what is on the ground", -1],
            ["what is in the background", 1],
            ["how many people are there on the land", -1],
            ["where is the photo taken", 1],
            ["what is the weather like", -1],
            ["when was the picture taken", -1],
            ["what is the ground covered with", -1],
            ["what is covering the ground", -1]
        ],
        "context": [
            "a zebra and a deer stand in a zoo enclosure.",
            "a baseball player swinging a bat at a ball."
        ]
    },
    {
        "object_category": "towel",
        "images": [
            {
                "VG_image_id": "2365273",
                "VG_object_id": "635447",
                "bbox": [0, 0, 85, 158],
                "image": "data\\images\\2365273.jpg"
            },
            {
                "VG_image_id": "2354384",
                "VG_object_id": "838952",
                "bbox": [0, 262, 120, 332],
                "image": "data\\images\\2354384.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is beside the towel", 2],
            ["what is on the side of the towel", 1],
            ["what is below the towel", 1]
        ],
        "org_questions": [
            ["what color is the towel", -1],
            ["what is on the side of the towel", 1],
            ["what is below the towel", 1],
            ["how many people are in the picture", -1],
            ["when was the picture taken", -1],
            ["where is the picture taken", -1],
            ["what is beside the towel", 2]
        ],
        "context": [
            "a kitchen sink with a bowl of flowers",
            "a black cat laying on a couch with a book in the background."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2380464",
                "VG_object_id": "1346285",
                "bbox": [255, 138, 312, 312],
                "image": "data\\images\\2380464.jpg"
            },
            {
                "VG_image_id": "2318898",
                "VG_object_id": "3079491",
                "bbox": [148, 220, 202, 330],
                "image": "data\\images\\2318898.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the person", 2],
            ["what color is the person's shirt", 1],
            ["what is the gender of the person", 1],
            ["how many people are there", 1],
            ["what is the person holding", 1],
            ["What is the background of image", 1],
            ["when was this photo taken", 1],
            ["who is in the photo", 1],
            ["what is the persion wearing", 1],
            ["when was the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what is the gender of the person", 1],
            ["where is the person", 2],
            ["how many people are there", 1],
            ["what is on the person's head", -1],
            ["what is the person holding", 1],
            ["What is the background of image", 1],
            ["when was this photo taken", 1],
            ["who is in the photo", 1],
            ["what is the persion wearing", 1],
            ["when was the picture taken", 1]
        ],
        "context": [
            "a man holding a kite on top of a lush green field.",
            "a group of people sitting on a bus."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2318020",
                "VG_object_id": "1015127",
                "bbox": [42, 13, 84, 53],
                "image": "data\\images\\2318020.jpg"
            },
            {
                "VG_image_id": "2318033",
                "VG_object_id": "1014971",
                "bbox": [184, 8, 320, 179],
                "image": "data\\images\\2318033.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the screen", 2],
            ["where is the cell phone", 1]
        ],
        "org_questions": [
            ["what color is the screen", 2],
            ["what color is the cell phone", -1],
            ["where is the cell phone", 1],
            ["how many phones are in the picture", -1],
            ["how large is the screen", -1],
            ["what is in front of the screen", -1],
            ["where is the screen", -1],
            ["what type of phone is this", -1],
            ["what is the phone color", -1]
        ],
        "context": [
            "a cell phone on a table",
            "a person holding a cell phone with a message on it."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2372349",
                "VG_object_id": "736744",
                "bbox": [144, 58, 234, 259],
                "image": "data\\images\\2372349.jpg"
            },
            {
                "VG_image_id": "2315910",
                "VG_object_id": "2744191",
                "bbox": [0, 57, 187, 347],
                "image": "data\\images\\2315910.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["where is the girl", 1],
            ["what color is the girl's pant", 1],
            ["What is woman holding", 1],
            ["What color is ground", 1],
            ["What is girl doing", 1],
            ["how many people are there", 1],
            ["what is the woman riding on", 1],
            ["what is the woman on the left wearing", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["where is the girl", 1],
            ["what color is the girl's pant", 1],
            ["What is woman wearing in her face", -1],
            ["What is woman holding", 1],
            ["What color is ground", 1],
            ["What is girl doing", 1],
            ["how many people are there", 1],
            ["when was the photo taken", -1],
            ["what is the woman riding on", 1],
            ["what is the woman on the left wearing", 1]
        ],
        "context": [
            "a group of people riding bicycles on a street.",
            "a young girl riding a skateboard on a ramp."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2404637",
                "VG_object_id": "376531",
                "bbox": [5, 36, 182, 366],
                "image": "data\\images\\2404637.jpg"
            },
            {
                "VG_image_id": "2336092",
                "VG_object_id": "2299507",
                "bbox": [1, 204, 88, 374],
                "image": "data\\images\\2336092.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 1],
            ["what color is the woman's shirt", 1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the person doing", 1],
            ["what color is the woman's shirt", 1],
            ["what is on the person's head", -1],
            ["what is the ground covered with", -1],
            ["where is the person", 1],
            ["what color is the background", 1],
            ["how many people are in the photo", -1],
            ["when was this photo taken", -1],
            ["where was the photo taken", 1],
            ["what is the person wearing", -1]
        ],
        "context": [
            "a group of people standing on a bridge next to a river.",
            "a group of people riding bikes down a street."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2347024",
                "VG_object_id": "3613664",
                "bbox": [14, 175, 132, 357],
                "image": "data\\images\\2347024.jpg"
            },
            {
                "VG_image_id": "2378867",
                "VG_object_id": "556837",
                "bbox": [4, 170, 499, 373],
                "image": "data\\images\\2378867.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the elephant", 2],
            ["how many elephants are there", 1],
            ["what is in the distance", 1],
            ["what is the persion standing on", 1]
        ],
        "org_questions": [
            ["how many elephants are there", 1],
            ["what is in the distance", 1],
            ["what is on the elephant", 2],
            ["what color is the elephant", -1],
            ["where are the elephants", -1],
            ["what are the elephants doing", -1],
            ["what is the ground the elephants standing on made of", -1],
            ["what is in front of the elephant", -1],
            ["when was the picture taken", -1],
            ["what animal is in the picture", -1],
            ["who is in the picture", -1],
            ["what is the persion standing on", 1]
        ],
        "context": [
            "a group of people riding on the back of an elephant.",
            "a herd of elephants walking through a river."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2368808",
                "VG_object_id": "615795",
                "bbox": [202, 143, 261, 309],
                "image": "data\\images\\2368808.jpg"
            },
            {
                "VG_image_id": "2412557",
                "VG_object_id": "191774",
                "bbox": [70, 24, 199, 307],
                "image": "data\\images\\2412557.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what is the woman doing", 1],
            ["what is the woman holding", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 1],
            ["what color is the woman's clothes", -1],
            ["how many people are there", 2],
            ["what is the persion wearing on head", -1],
            ["where is the woman", -1],
            ["what is the woman holding", 1],
            ["what is the woman wearing", -1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a woman standing next to a moving truck.",
            "a man and a woman sitting on a bench with a child."
        ]
    },
    {
        "object_category": "screen",
        "images": [
            {
                "VG_image_id": "2365474",
                "VG_object_id": "633981",
                "bbox": [114, 133, 180, 208],
                "image": "data\\images\\2365474.jpg"
            },
            {
                "VG_image_id": "2370701",
                "VG_object_id": "1900272",
                "bbox": [253, 103, 415, 196],
                "image": "data\\images\\2370701.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the laptop", 2],
            ["where is the screen", 1]
        ],
        "org_questions": [
            ["what color is the laptop", 2],
            ["how many computers are there", -1],
            ["where is the screen", 1],
            ["what is the main color of the screen", -1],
            ["when was the photo taken", -1],
            ["what is on top of the laptop", -1],
            ["what is the laptop on", -1],
            ["what is on the laptop", -1]
        ],
        "context": [
            "a chair with a laptop on it next to a bed.",
            "a laptop computer sitting on top of a table."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2344228",
                "VG_object_id": "917040",
                "bbox": [155, 45, 415, 409],
                "image": "data\\images\\2344228.jpg"
            },
            {
                "VG_image_id": "2318792",
                "VG_object_id": "3042849",
                "bbox": [112, 269, 248, 441],
                "image": "data\\images\\2318792.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many dogs are in the picture", 2],
            ["what is the dog doing", 1],
            ["what is the dog's posture", 1],
            ["where is the photo taken", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what is the dog doing", 1],
            ["how many dogs are in the picture", 2],
            ["what is the dog's posture", 1],
            ["where is the photo taken", 1],
            ["where is the dog", -1],
            ["what kind of animal is this", -1],
            ["when was the picture taken", -1],
            ["what is on the dog's head", -1],
            ["what is on the ground", -1],
            ["what is the ground covered with", 1]
        ],
        "context": [
            "three dogs in a bowl with a purple leash.",
            "a dog sitting on a beach with a bird flying in the background."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2375019",
                "VG_object_id": "3089760",
                "bbox": [1, 132, 496, 332],
                "image": "data\\images\\2375019.jpg"
            },
            {
                "VG_image_id": "2412729",
                "VG_object_id": "3019056",
                "bbox": [143, 205, 428, 305],
                "image": "data\\images\\2412729.jpg"
            }
        ],
        "questions_with_scores": [["how many pillows are on the bed", 1]],
        "org_questions": [
            ["what color is the bed", -1],
            ["what color is the pillow on the right", -1],
            ["how many cats are on the blanket", -1],
            ["when is this photo taken", -1],
            ["what is on the bed", -1],
            ["what kind of bed is this", -1],
            ["what is next to the bed", -1],
            ["where was the photo taken", -1],
            ["how many light sources are there", -1],
            ["what color is the blanket", -1],
            ["what room is this", -1],
            ["where are the pillows", -1],
            ["what color are the bed sheets", -1],
            ["how many pillows are on the bed", 1]
        ],
        "context": [
            "a bed with a book and a book on it",
            "a hotel room with a bed and a shower"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2338860",
                "VG_object_id": "2811013",
                "bbox": [34, 74, 211, 465],
                "image": "data\\images\\2338860.jpg"
            },
            {
                "VG_image_id": "2379706",
                "VG_object_id": "550931",
                "bbox": [373, 102, 445, 316],
                "image": "data\\images\\2379706.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the man's shirt", 2],
            ["when was this picture taken", 2],
            ["what is the man wearing", 2],
            ["how many people are there", 1],
            ["what are the people doing", 1],
            ["what is the man's posture", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what are the people doing", 1],
            ["What color is the man's shirt", 2],
            ["what is the man wearing around his neck", -1],
            ["where is the light", -1],
            ["what is the man's posture", 1],
            ["when was this picture taken", 2],
            ["what is the man standing on", -1],
            ["what is on the man's head", -1],
            ["what is the man holding", -1],
            ["what is the man wearing", 2]
        ],
        "context": [
            "a man standing on a skateboard on a street.",
            "a group of people getting on a train."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2376740",
                "VG_object_id": "3683209",
                "bbox": [220, 3, 374, 193],
                "image": "data\\images\\2376740.jpg"
            },
            {
                "VG_image_id": "2415585",
                "VG_object_id": "2768676",
                "bbox": [143, 53, 305, 263],
                "image": "data\\images\\2415585.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 2],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 2],
            ["what color is the ground", -1],
            ["what gender is the person in the shirt", -1],
            ["where is the picture taken", 1],
            ["what is the persion doing", -1],
            ["what is the land made of ", -1],
            ["how many people are there", -1],
            ["what is the persion wearing", -1],
            ["what is the girl holding", -1]
        ],
        "context": [
            "a blue tray with food on it and a girl eating a bowl of soup.",
            "a woman sitting on the floor with a suitcase."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2412851",
                "VG_object_id": "3423405",
                "bbox": [2, 202, 90, 365],
                "image": "data\\images\\2412851.jpg"
            },
            {
                "VG_image_id": "2337715",
                "VG_object_id": "2201539",
                "bbox": [230, 195, 317, 291],
                "image": "data\\images\\2337715.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the person's clothes", 1],
            ["where is the person", 1],
            ["What is man doing", 1],
            ["What color is person's shirt", 1],
            ["what is the persion wearing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["what gender is the person", -1],
            ["what color is the person's clothes", 1],
            ["where is the person", 1],
            ["how many people are there", 2],
            ["What is man doing", 1],
            ["what is the person doing", -1],
            ["What color is person's shirt", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1],
            ["what is the persion holding", 1]
        ],
        "context": [
            "a group of people waiting for their luggage.",
            "a man standing next to a large airplane."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2403997",
                "VG_object_id": "347275",
                "bbox": [24, 12, 479, 383],
                "image": "data\\images\\2403997.jpg"
            },
            {
                "VG_image_id": "2345702",
                "VG_object_id": "2071634",
                "bbox": [11, 13, 193, 109],
                "image": "data\\images\\2345702.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is beside the car", 1],
            ["how many cars are there", 1],
            ["where was the picture taken", 1],
            ["what type of car is this", 1],
            ["where is the car car", 1],
            ["what is in front of the car", 1],
            ["Where is photo taken", 1]
        ],
        "org_questions": [
            ["what color is the car", -1],
            ["what is beside the car", 1],
            ["what is the pattern on the car", -1],
            ["how many cars are there", 1],
            ["what time is it", -1],
            ["which part of the car can we see in the picture", -1],
            ["where was the picture taken", 1],
            ["what type of car is this", 1],
            ["when was the picture taken", -1],
            ["where is the car car", 1],
            ["what is in front of the car", 1],
            ["what is in the background", -1],
            ["how many people are there", -1],
            ["Where is photo taken", 1],
            ["what is the ground covered with", -1]
        ],
        "context": [
            "a truck with a skull and crossbones painted on it.",
            "two motorcycles parked next to each other on a street."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2380218",
                "VG_object_id": "546900",
                "bbox": [62, 37, 135, 243],
                "image": "data\\images\\2380218.jpg"
            },
            {
                "VG_image_id": "2337523",
                "VG_object_id": "3043917",
                "bbox": [38, 132, 205, 329],
                "image": "data\\images\\2337523.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 1],
            ["what is on the man's head", 1],
            ["what is behind the person", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the ground covered with", -1],
            ["what is the man doing", -1],
            ["how many people are there", 1],
            ["what is the man wearing on his neck", -1],
            ["Where is the man", -1],
            ["what is the man holding", -1],
            ["what kind of clothes is the man wearing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1],
            ["what is on the man's head", 1],
            ["When is photo taken", -1],
            ["what is behind the person", 1],
            ["where is the man", -1]
        ],
        "context": [
            "a young boy throwing a frisbee in a park.",
            "a man standing next to a brown horse."
        ]
    },
    {
        "object_category": "plate",
        "images": [
            {
                "VG_image_id": "2398414",
                "VG_object_id": "1180118",
                "bbox": [334, 242, 447, 280],
                "image": "data\\images\\2398414.jpg"
            },
            {
                "VG_image_id": "2334010",
                "VG_object_id": "2752675",
                "bbox": [143, 257, 210, 302],
                "image": "data\\images\\2334010.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the plate", 2],
            ["What color is the table", 2],
            ["what food is on the plate", 1],
            ["what kind of food is this", 1]
        ],
        "org_questions": [
            ["What is on the plate", -1],
            ["What color is the plate", 2],
            ["What color is the table", 2],
            ["how many glasses are there in the picture", -1],
            ["where is the plate", -1],
            ["what is the table made of", -1],
            ["what food is on the plate", 1],
            ["what is on the table", -1],
            ["what shape is the plate", -1],
            ["what is the plate on", -1],
            ["what is next to the plate", -1],
            ["what kind of food is this", 1]
        ],
        "context": [
            "two young boys sitting at a table with a green cup.",
            "a table with plates of food and cups of coffee."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2328123",
                "VG_object_id": "3429894",
                "bbox": [98, 92, 427, 184],
                "image": "data\\images\\2328123.jpg"
            },
            {
                "VG_image_id": "2377308",
                "VG_object_id": "1709025",
                "bbox": [196, 267, 394, 374],
                "image": "data\\images\\2377308.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the seat", 1],
            ["What is on the bench", 1]
        ],
        "org_questions": [
            ["What color is man's glasses", -1],
            ["What color is the seat", 1],
            ["Where is the seat", -1],
            ["how many people are there", -1],
            ["What is on the bench", 1],
            ["where is the photo taken", -1],
            ["What is next to the seat", -1],
            ["who is in the picture", -1],
            ["what is the man doing", -1],
            ["what is on the side of the car", -1],
            ["where is the man", -1]
        ],
        "context": [
            "a man in a car wearing a tie and a tie.",
            "a bus with a tv on the top of it"
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2370409",
                "VG_object_id": "2405459",
                "bbox": [92, 15, 429, 308],
                "image": "data\\images\\2370409.jpg"
            },
            {
                "VG_image_id": "2391666",
                "VG_object_id": "1238774",
                "bbox": [359, 177, 414, 220],
                "image": "data\\images\\2391666.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman doing", 2],
            ["what is the woman holding", 1],
            ["what is the woman looking at", 1]
        ],
        "org_questions": [
            ["what is the woman doing", 2],
            ["what is the woman holding", 1],
            ["where is the woman", -1],
            ["how many people are there", -1],
            ["What color is woman's shirt", -1],
            ["how is the weather", -1],
            ["what is the woman wearing", -1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["what is the woman looking at", 1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a woman sitting on a bench talking on a cell phone.",
            "a man holding a sign in front of a tree."
        ]
    },
    {
        "object_category": "container",
        "images": [
            {
                "VG_image_id": "2326024",
                "VG_object_id": "983897",
                "bbox": [314, 268, 377, 371],
                "image": "data\\images\\2326024.jpg"
            },
            {
                "VG_image_id": "2322081",
                "VG_object_id": "993626",
                "bbox": [322, 141, 406, 240],
                "image": "data\\images\\2322081.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the color of the pot", 2],
            ["what is behind the container", 1],
            ["what is on the right side of the picture", 1],
            ["what is in the middle of the photo", 1]
        ],
        "org_questions": [
            ["What is the color of the pot", 2],
            ["what shape is the container", -1],
            ["where is the container", -1],
            ["what is the container made of", -1],
            ["what is behind the container", 1],
            ["how many people are there", -1],
            ["what is on the right side of the picture", 1],
            ["what is in the middle of the photo", 1]
        ],
        "context": [
            "a microwave oven sitting on a counter top.",
            "a table with a large amount of food on it."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2342424",
                "VG_object_id": "2631248",
                "bbox": [224, 208, 320, 313],
                "image": "data\\images\\2342424.jpg"
            },
            {
                "VG_image_id": "2391185",
                "VG_object_id": "1243034",
                "bbox": [418, 203, 481, 345],
                "image": "data\\images\\2391185.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the wall", 2],
            ["what color is the chair", 2],
            ["who is sitting on the chair", 1],
            ["What is next to the chair", 1]
        ],
        "org_questions": [
            ["what color is the wall", 2],
            ["what color is the chair", 2],
            ["how many couches are there", -1],
            ["what is in the picture", -1],
            ["where is the chair", -1],
            ["who is sitting on the chair", 1],
            ["What is next to the chair", 1],
            ["where was the photo taken", -1],
            ["where is the picture taken", -1]
        ],
        "context": [
            "a bedroom with two beds and a window.",
            "a group of people sitting around a table."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2376678",
                "VG_object_id": "570692",
                "bbox": [91, 62, 396, 254],
                "image": "data\\images\\2376678.jpg"
            },
            {
                "VG_image_id": "2385552",
                "VG_object_id": "1295529",
                "bbox": [239, 29, 397, 128],
                "image": "data\\images\\2385552.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["what color is the shirt", 1],
            ["what is the man doing", 1],
            ["what is the color of the man's pants", 1],
            ["what kind of pants is the man wearing", 1],
            ["what type of shirt is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 1],
            ["what is the man doing", 1],
            ["what is the color of the man's pants", 1],
            ["how many people are there", -1],
            ["What time is it", -1],
            ["what is the gender of the person", -1],
            ["where is the man", -1],
            ["what is on the man's head", 2],
            ["what kind of pants is the man wearing", 1],
            ["what type of shirt is the man wearing", 1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a man hitting a tennis ball with a racquet.",
            "a young man riding a skateboard on a sidewalk."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2353075",
                "VG_object_id": "2307214",
                "bbox": [416, 201, 498, 290],
                "image": "data\\images\\2353075.jpg"
            },
            {
                "VG_image_id": "2396446",
                "VG_object_id": "1197897",
                "bbox": [182, 68, 236, 100],
                "image": "data\\images\\2396446.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 1],
            ["what is behind the car", 1],
            ["what is the ground covered with", 1]
        ],
        "org_questions": [
            ["what color is the car", 1],
            ["what is behind the car", 1],
            ["what is the ground covered with", 1],
            ["what is in the distance", -1],
            ["when was this picture taken", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a semi truck is driving down the street.",
            "a duck standing on a ledge next to a fountain."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2343895",
                "VG_object_id": "918972",
                "bbox": [269, 83, 333, 158],
                "image": "data\\images\\2343895.jpg"
            },
            {
                "VG_image_id": "2316182",
                "VG_object_id": "3535915",
                "bbox": [313, 19, 477, 496],
                "image": "data\\images\\2316182.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's clothes", 2],
            ["what is the person doing", 2],
            ["what is the person holding", 1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["how many people are there", 1],
            ["what is the man on the left doing", 1]
        ],
        "org_questions": [
            ["what color is the person's clothes", 2],
            ["what is the person holding", 1],
            ["what is the person doing", 2],
            ["what is the woman wearing on her face", -1],
            ["where is the woman", -1],
            ["what is the woman wearing", 1],
            ["What is woman holding", 1],
            ["how many people are there", 1],
            ["when was this photo taken", -1],
            ["what is on the woman's head", -1],
            ["what is the man on the left doing", 1]
        ],
        "context": [
            "a horse drawn carriage on a city street",
            "a woman walking down a sidewalk while talking on her cell phone."
        ]
    },
    {
        "object_category": "chair",
        "images": [
            {
                "VG_image_id": "2357404",
                "VG_object_id": "1876502",
                "bbox": [1, 265, 118, 359],
                "image": "data\\images\\2357404.jpg"
            },
            {
                "VG_image_id": "2353897",
                "VG_object_id": "1893996",
                "bbox": [137, 167, 473, 372],
                "image": "data\\images\\2353897.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is in the background", 1],
            ["what is the persion holding", 1],
            ["What is above the chair", 1],
            ["when was the photo taken", 1],
            ["what is on the chair", 1],
            ["what is in the chair", 1]
        ],
        "org_questions": [
            ["what color is the chair", -1],
            ["how many chair are there", -1],
            ["what is in the background", 1],
            ["what is the persion holding", 1],
            ["What is above the chair", 1],
            ["when was the photo taken", 1],
            ["where is the chair", -1],
            ["where was the photo taken", -1],
            ["what is on the chair", 1],
            ["what is the chair made of", -1],
            ["How many people are there", -1],
            ["how many chairs are there", -1],
            ["where was this picture taken", -1],
            ["what is in the chair", 1]
        ],
        "context": [
            "a man holding a banana in his hand.",
            "a man reading a book"
        ]
    },
    {
        "object_category": "field",
        "images": [
            {
                "VG_image_id": "2371515",
                "VG_object_id": "3520911",
                "bbox": [0, 325, 330, 496],
                "image": "data\\images\\2371515.jpg"
            },
            {
                "VG_image_id": "2352127",
                "VG_object_id": "2483217",
                "bbox": [54, 117, 498, 331],
                "image": "data\\images\\2352127.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the grass", 2],
            ["How many animals are there", 1]
        ],
        "org_questions": [
            ["What color is the grass", 2],
            ["How many animals are there", 1],
            ["What is the background of image", -1],
            ["what is in the distance", -1],
            ["how many horses are there on the field", -1],
            ["where was the photo taken", -1],
            ["how is the weather", -1],
            ["what is the ground covered with", -1],
            ["where is the grass", -1]
        ],
        "context": [
            "a giraffe standing in a field with trees in the background.",
            "a giraffe standing next to a tree in a field."
        ]
    },
    {
        "object_category": "computer",
        "images": [
            {
                "VG_image_id": "2363550",
                "VG_object_id": "1950176",
                "bbox": [185, 23, 487, 280],
                "image": "data\\images\\2363550.jpg"
            },
            {
                "VG_image_id": "2339271",
                "VG_object_id": "951906",
                "bbox": [83, 39, 287, 218],
                "image": "data\\images\\2339271.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the computer", 1],
            ["how many computers are there", 1],
            ["what is the table made of", 1],
            ["what color is the table under the computer", 1]
        ],
        "org_questions": [
            ["what color is the computer", 1],
            ["how many computers are there", 1],
            ["what is the table made of", 1],
            ["where is the computer", -1],
            ["what is on the desk", -1],
            ["how many people are there", -1],
            ["what color is the table under the computer", 1],
            ["who is in the photo", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a laptop computer sitting next to a desktop computer.",
            "a computer monitor sitting on top of a glass desk."
        ]
    },
    {
        "object_category": "elephant",
        "images": [
            {
                "VG_image_id": "2395429",
                "VG_object_id": "454602",
                "bbox": [28, 126, 393, 226],
                "image": "data\\images\\2395429.jpg"
            },
            {
                "VG_image_id": "2362207",
                "VG_object_id": "2757885",
                "bbox": [154, 196, 491, 338],
                "image": "data\\images\\2362207.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants are there", 2],
            ["how many elephants", 2]
        ],
        "org_questions": [
            ["how many elephants are there", 2],
            ["where is the elephant", -1],
            ["what is the ground the elephants standing on made of", -1],
            ["what are the elephants doing", -1],
            ["what color is the elephant", -1],
            ["what is on the elephant", -1],
            ["what is in front of the elephants", -1],
            ["what animal is in the picture", -1],
            ["when was the photo taken", -1],
            ["what is in the water", -1],
            ["how many elephants", 2]
        ],
        "context": [
            "an elephant is swimming in a river with its trunk in the water.",
            "a herd of elephants walking across a river."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2410979",
                "VG_object_id": "318124",
                "bbox": [227, 130, 322, 416],
                "image": "data\\images\\2410979.jpg"
            },
            {
                "VG_image_id": "2356429",
                "VG_object_id": "2373377",
                "bbox": [265, 45, 333, 239],
                "image": "data\\images\\2356429.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the player's shirt", 2],
            ["how many players are there in the picture", 2],
            ["how many people are there", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["what color is the player's shirt", 2],
            ["what sport is the person playing", -1],
            ["what is the gender of the person", -1],
            ["what is the player wearing", -1],
            ["how many players are there in the picture", 2],
            ["what color is the ground", -1],
            ["where is the ball", -1],
            ["what is the man standing on", -1],
            ["who is in the photo", -1],
            ["what is the player doing", -1],
            ["what is the man holding", -1]
        ],
        "context": [
            "a baseball player standing on a field with a glove.",
            "the team is getting ready for the season."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2394621",
                "VG_object_id": "1211130",
                "bbox": [57, 98, 215, 290],
                "image": "data\\images\\2394621.jpg"
            },
            {
                "VG_image_id": "2338976",
                "VG_object_id": "952682",
                "bbox": [118, 128, 486, 286],
                "image": "data\\images\\2338976.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is man doing", 2],
            ["what is in the man's hand", 2],
            ["How many people are there", 1],
            ["What color is man's shirt", 1],
            ["where is the photo taken", 1],
            ["where are the people staying", 1],
            ["where is the man", 1]
        ],
        "org_questions": [
            ["How many people are there", 1],
            ["What color is man's shirt", 1],
            ["What is man doing", 2],
            ["where is the photo taken", 1],
            ["where are the people staying", 1],
            ["where is the man", 1],
            ["what is in the man's hand", 2],
            ["what type of shirt is the man wearing", -1],
            ["what is on the man's face", -1],
            ["what is the persion on the right wearing", -1]
        ],
        "context": [
            "two people playing a video game with nintendo wii controllers.",
            "a man sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "beach",
        "images": [
            {
                "VG_image_id": "2375094",
                "VG_object_id": "3183335",
                "bbox": [0, 0, 497, 274],
                "image": "data\\images\\2375094.jpg"
            },
            {
                "VG_image_id": "2416619",
                "VG_object_id": "2808308",
                "bbox": [1, 29, 499, 307],
                "image": "data\\images\\2416619.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many birds are there", 2],
            ["how many birds", 2],
            ["what color is the bird", 1]
        ],
        "org_questions": [
            ["how many birds are there", 2],
            ["what is the bird doing", -1],
            ["what is in the background", -1],
            ["what color is the bird", 1],
            ["when was this photo taken", -1],
            ["what is the bird standing on", -1],
            ["where was this picture taken", -1],
            ["how many birds", 2],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a small bird walking on a beach near the water.",
            "a flock of birds sitting on a beach."
        ]
    },
    {
        "object_category": "sink",
        "images": [
            {
                "VG_image_id": "2322671",
                "VG_object_id": "990893",
                "bbox": [129, 253, 234, 304],
                "image": "data\\images\\2322671.jpg"
            },
            {
                "VG_image_id": "2415877",
                "VG_object_id": "3082876",
                "bbox": [333, 198, 499, 252],
                "image": "data\\images\\2415877.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sink", 1],
            ["how many sinks are there", 1],
            ["what color is the cabinet", 1]
        ],
        "org_questions": [
            ["what color is the sink", 1],
            ["how many sinks are there", 1],
            ["what color is the cabinet", 1],
            ["what shape is the sink", -1],
            ["what is on the wall", -1],
            ["what is the sink on", -1],
            ["how many people are there in the picture", -1],
            ["what room is this", -1],
            ["where is this scene", -1],
            ["where is the sink", -1]
        ],
        "context": [
            "a kitchen with a sink, cabinets and a window.",
            "a kitchen with a refrigerator, cabinets, and a counter top."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2407798",
                "VG_object_id": "1095937",
                "bbox": [417, 43, 468, 82],
                "image": "data\\images\\2407798.jpg"
            },
            {
                "VG_image_id": "2343958",
                "VG_object_id": "918534",
                "bbox": [0, 373, 331, 447],
                "image": "data\\images\\2343958.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car on the road", 1],
            ["what is the weather like", 1]
        ],
        "org_questions": [
            ["how many cars are there on the road", -1],
            ["what color is the car on the road", 1],
            ["what color is the road", -1],
            ["what is in the distance", -1],
            ["what is the weather like", 1],
            ["when was the picture taken", -1],
            ["where is the car", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a woman standing on a sidewalk while using her cell phone.",
            "a city street with cars driving down it."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2334034",
                "VG_object_id": "3296265",
                "bbox": [253, 29, 340, 104],
                "image": "data\\images\\2334034.jpg"
            },
            {
                "VG_image_id": "2398823",
                "VG_object_id": "1175307",
                "bbox": [225, 224, 316, 328],
                "image": "data\\images\\2398823.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's shirt", 2],
            ["how many giraffes are there in the picture", 2],
            ["when is this photo taken", 1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the persion holding", 1],
            ["what is on the man's head", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", 2],
            ["when is this photo taken", 1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the persion holding", 1],
            ["what is on the man's head", 1],
            ["what kind of shirt is the man wearing", -1],
            ["what is the person wearing", -1],
            ["how many giraffes are there in the picture", 2]
        ],
        "context": [
            "a man jumping a skateboard over some steps.",
            "a man is petting a giraffe at a zoo."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2405094",
                "VG_object_id": "1110537",
                "bbox": [1, 175, 500, 330],
                "image": "data\\images\\2405094.jpg"
            },
            {
                "VG_image_id": "2327095",
                "VG_object_id": "3654925",
                "bbox": [0, 225, 497, 371],
                "image": "data\\images\\2327095.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the motorcycle", 2],
            ["what is the color of the road", 1],
            ["when is the picture taken", 1],
            ["what time is it", 1]
        ],
        "org_questions": [
            ["what is the color of the road", 1],
            ["what is on the motorcycle", 2],
            ["when is the picture taken", 1],
            ["how many buses are there on the street", -1],
            ["what kind of animal is on the road", -1],
            ["what kind of vehicle is in the photo", -1],
            ["what time is it", 1],
            ["where was this photo taken", -1],
            ["what is the road made of", -1],
            ["where are the motorcycles", -1]
        ],
        "context": [
            "a couple of cops riding on the back of motorcycles.",
            "a motorcycle parked on the side of the road."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2352894",
                "VG_object_id": "1707862",
                "bbox": [140, 117, 231, 214],
                "image": "data\\images\\2352894.jpg"
            },
            {
                "VG_image_id": "2372692",
                "VG_object_id": "590680",
                "bbox": [63, 193, 140, 236],
                "image": "data\\images\\2372692.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["where is the person", 2],
            ["what color are the person's trousers", 1],
            ["what is the person wearing", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what is the person doing", 2],
            ["where is the person", 2],
            ["how many persons are there", -1],
            ["what color are the person's trousers", 1],
            ["what is the man holding", -1],
            ["How many people are there", -1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", 1],
            ["where is the picture taken", 1],
            ["what color is the sky", -1]
        ],
        "context": [
            "a man flying through the air while riding a snowboard.",
            "a group of people laying on the beach."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2370742",
                "VG_object_id": "2509394",
                "bbox": [1, 61, 498, 371],
                "image": "data\\images\\2370742.jpg"
            },
            {
                "VG_image_id": "2368658",
                "VG_object_id": "616782",
                "bbox": [70, 141, 367, 362],
                "image": "data\\images\\2368658.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the sheet on  bed", 2],
            ["how many lights are there around the bed", 2]
        ],
        "org_questions": [
            ["what color is the sheet on  bed", 2],
            ["how many lights are there around the bed", 2],
            ["what is the floor under the bed made of", -1],
            ["what is on the bed besides pillows", -1],
            ["how many people are there", -1],
            ["what is on the bed", -1],
            ["where was this picture taken", -1],
            ["what room is this", -1],
            ["where are the pillows", -1],
            ["what is behind the bed", -1]
        ],
        "context": [
            "a bed with a white blanket and pillows on it.",
            "a bedroom with a bed, dresser, and a window."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2382873",
                "VG_object_id": "695588",
                "bbox": [197, 0, 417, 279],
                "image": "data\\images\\2382873.jpg"
            },
            {
                "VG_image_id": "2395608",
                "VG_object_id": "1204290",
                "bbox": [128, 102, 314, 490],
                "image": "data\\images\\2395608.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the building", 2],
            ["how many cars are there", 2],
            ["how is the weather", 1]
        ],
        "org_questions": [
            ["what is the building made of", -1],
            ["what color is the building", 2],
            ["how many cars are there", 2],
            ["when is the picture taken", -1],
            ["how is the weather", 1],
            ["what is on the ground", -1],
            ["where was this photo taken", -1],
            ["where are the buildings", -1],
            ["what is in the background", -1],
            ["what color is the sky", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a street with a lot of cars and a traffic light.",
            "a sign that is on a pole in a city."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2360010",
                "VG_object_id": "2014713",
                "bbox": [222, 219, 315, 289],
                "image": "data\\images\\2360010.jpg"
            },
            {
                "VG_image_id": "2326192",
                "VG_object_id": "983128",
                "bbox": [22, 213, 97, 352],
                "image": "data\\images\\2326192.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the shirt", 2],
            ["what is the person in the shirt doing", 1],
            ["what is in front of the person", 1],
            ["what is the persion wearing on his head", 1]
        ],
        "org_questions": [
            ["what color is the shirt", 2],
            ["what is the person in the shirt doing", 1],
            ["HOw many people are there", -1],
            ["What time is it", -1],
            ["what is in front of the person", 1],
            ["what is the persion wearing on his head", 1],
            ["when was the photo taken", -1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a skateboarder is doing a trick at a skate park.",
            "a girl and a boy sitting on top of an elephant statue."
        ]
    },
    {
        "object_category": "bowl",
        "images": [
            {
                "VG_image_id": "2352074",
                "VG_object_id": "1708223",
                "bbox": [316, 203, 379, 261],
                "image": "data\\images\\2352074.jpg"
            },
            {
                "VG_image_id": "2342288",
                "VG_object_id": "2176598",
                "bbox": [117, 60, 173, 130],
                "image": "data\\images\\2342288.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the table made of", 1],
            ["what color is the table", 1],
            ["what is in the bowl", 1],
            ["how many bowls are in the picture", 1],
            ["what color is the food in the bowl", 1],
            ["what is the bowl made of", 1],
            ["what kind of food is on the table", 1],
            ["what is on top of the table", 1]
        ],
        "org_questions": [
            ["what is the table made of", 1],
            ["what color is the table", 1],
            ["what is in the bowl", 1],
            ["how many bowls are in the picture", 1],
            ["where is the bowl", -1],
            ["what color is the food in the bowl", 1],
            ["what is the bowl made of", 1],
            ["when was the picture taken", -1],
            ["what kind of food is on the table", 1],
            ["what is on top of the table", 1]
        ],
        "context": [
            "two women in a kitchen preparing food.",
            "a magazine with a picture of food on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2345745",
                "VG_object_id": "905064",
                "bbox": [6, 27, 149, 366],
                "image": "data\\images\\2345745.jpg"
            },
            {
                "VG_image_id": "2365434",
                "VG_object_id": "634133",
                "bbox": [158, 7, 342, 255],
                "image": "data\\images\\2365434.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the woman's shirt", 1],
            ["what is the woman's posture", 1],
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["what is the woman doing", 1],
            ["What is woman holding", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", 1],
            ["what is the woman's posture", 1],
            ["what color is the table", 1],
            ["how many people are there", 1],
            ["where is the woman", -1],
            ["what is the woman doing", 1],
            [" what is the woman wearing", -1],
            ["What is woman holding", 1],
            ["who is in the photo", -1],
            ["where is the picture taken", -1],
            ["where are the people", -1],
            ["what are the people doing", -1]
        ],
        "context": [
            "a group of people sitting around a table with umbrellas.",
            "a woman sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "bird",
        "images": [
            {
                "VG_image_id": "2346788",
                "VG_object_id": "3615077",
                "bbox": [287, 111, 343, 173],
                "image": "data\\images\\2346788.jpg"
            },
            {
                "VG_image_id": "2316095",
                "VG_object_id": "2996253",
                "bbox": [87, 69, 354, 252],
                "image": "data\\images\\2316095.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many birds are there", 1],
            ["how many birds are there in the sky", 1],
            ["how large is the bird", 1],
            ["how many animals are there in the picture", 1]
        ],
        "org_questions": [
            ["what color is the sky", -1],
            ["how many birds are there", 1],
            ["what is in the background", -1],
            ["what gesture is the bird", -1],
            ["what is the main color of the background", -1],
            ["when was the photo taken", -1],
            ["where are the birds", -1],
            ["what are the birds doing", -1],
            ["what is flying in the sky", -1],
            ["how many birds are there in the sky", 1],
            ["what is the weather like", -1],
            ["how large is the bird", 1],
            ["what color is the background", -1],
            ["how many animals are there in the picture", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a bird flying over a bridge over a body of water.",
            "a boat is docked in a harbor with birds flying around."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "713811",
                "VG_object_id": "1589068",
                "bbox": [0, 324, 130, 764],
                "image": "data\\images\\713811.jpg"
            },
            {
                "VG_image_id": "2398739",
                "VG_object_id": "1176531",
                "bbox": [359, 138, 419, 272],
                "image": "data\\images\\2398739.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is man's shirt", 2],
            ["What color is the truck", 1],
            ["What color is the lane", 1],
            ["what shape is the man's collar", 1],
            ["how many men are there", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["What color is man's shirt", 2],
            ["What color is the truck", 1],
            ["What color is the lane", 1],
            ["what shape is the man's collar", 1],
            ["how many men are there", 1],
            ["What is man wearing on the face", -1],
            ["Where is the man", -1],
            ["What is man doing", -1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man holding", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a large bus is driving down the street.",
            "a man standing next to a bus on a field."
        ]
    },
    {
        "object_category": "television",
        "images": [
            {
                "VG_image_id": "2407757",
                "VG_object_id": "274020",
                "bbox": [77, 156, 223, 350],
                "image": "data\\images\\2407757.jpg"
            },
            {
                "VG_image_id": "2391528",
                "VG_object_id": "1240152",
                "bbox": [80, 3, 491, 351],
                "image": "data\\images\\2391528.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is showing on the tv", 2],
            ["where is the tv", 2],
            ["what is in front of the tv", 1],
            ["which room is the television placing in", 1],
            ["what is on the television", 1],
            ["where was the photo taken", 1],
            ["what is under the tv", 1]
        ],
        "org_questions": [
            ["what is showing on the tv", 2],
            ["what is in front of the tv", 1],
            ["where is the tv", 2],
            ["how many people are there in the picture", -1],
            ["which room is the television placing in", 1],
            ["what is on the television", 1],
            ["when was the photo taken", -1],
            ["what is the tv sitting on", -1],
            ["where was the photo taken", 1],
            ["what is under the tv", 1]
        ],
        "context": [
            "a television set on the ground with a box on the ground.",
            "a cat is watching a lion on the television."
        ]
    },
    {
        "object_category": "tower",
        "images": [
            {
                "VG_image_id": "2355906",
                "VG_object_id": "2774780",
                "bbox": [194, 131, 333, 320],
                "image": "data\\images\\2355906.jpg"
            },
            {
                "VG_image_id": "2410535",
                "VG_object_id": "215151",
                "bbox": [77, 19, 281, 492],
                "image": "data\\images\\2410535.jpg"
            }
        ],
        "questions_with_scores": [
            ["how tall is the tower", 1],
            ["what time is it", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["how tall is the tower", 1],
            ["what color is the sky", -1],
            ["what is clock  on", -1],
            ["what time is it", 1],
            ["where is the clock", -1],
            ["what is on the top of the tower", -1],
            ["what color is the clock on the tower", -1],
            ["what is the building made of", -1],
            ["how many clocks are there", -1],
            ["when was the photo taken", -1],
            ["what is in the background", 1],
            ["what is on the building", -1]
        ],
        "context": [
            "a clock tower in the middle of a park.",
            "a large clock tower with a steeple and a clock face."
        ]
    },
    {
        "object_category": "vehicle",
        "images": [
            {
                "VG_image_id": "2376383",
                "VG_object_id": "3685678",
                "bbox": [167, 216, 265, 268],
                "image": "data\\images\\2376383.jpg"
            },
            {
                "VG_image_id": "2320005",
                "VG_object_id": "996753",
                "bbox": [0, 37, 87, 119],
                "image": "data\\images\\2320005.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the ground", 2],
            ["what is on the vehicle", 1],
            ["what is in front of the car", 1]
        ],
        "org_questions": [
            ["what color is the car", -1],
            ["what color is the ground", 2],
            ["how many cars are there", -1],
            ["what is on the side of the vehicle", -1],
            ["where is the vehicle", -1],
            ["how is the weather", -1],
            ["what is on the vehicle", 1],
            ["when was this photo taken", -1],
            ["what is in the background", -1],
            ["what is in front of the car", 1],
            ["where are the cars parked", -1],
            ["what is next to the car", -1]
        ],
        "context": [
            "a red traffic light sitting on top of a snow covered street.",
            "a purple motorcycle with a purple seat and seat."
        ]
    },
    {
        "object_category": "girl",
        "images": [
            {
                "VG_image_id": "2367026",
                "VG_object_id": "2324067",
                "bbox": [116, 69, 276, 329],
                "image": "data\\images\\2367026.jpg"
            },
            {
                "VG_image_id": "2417534",
                "VG_object_id": "2875860",
                "bbox": [155, 193, 277, 498],
                "image": "data\\images\\2417534.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the girl doing", 1],
            ["what color is the girl's clothes", 1],
            ["where is the girl", 1],
            ["how many people are there", 1],
            ["what is the girl holding", 1],
            ["how many people are there in the picture", 1],
            ["what is the persion wearing", 1]
        ],
        "org_questions": [
            ["what is the girl doing", 1],
            ["what color is the girl's clothes", 1],
            ["where is the girl", 1],
            ["how many people are there", 1],
            ["what is the girl holding", 1],
            ["how many people are there in the picture", 1],
            ["when was this picture taken", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1]
        ],
        "context": [
            "a family poses for a picture on a snowy mountain.",
            "a woman standing next to a ladder with an apple tree in the background."
        ]
    },
    {
        "object_category": "soil",
        "images": [
            {
                "VG_image_id": "2362015",
                "VG_object_id": "3752304",
                "bbox": [33, 246, 411, 342],
                "image": "data\\images\\2362015.jpg"
            },
            {
                "VG_image_id": "2405966",
                "VG_object_id": "327609",
                "bbox": [0, 407, 331, 499],
                "image": "data\\images\\2405966.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many elephants are there on the ground", 2],
            ["how many elephants are there in the picture", 2],
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["what is on the ground", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["how many people are there on the ground", 1],
            ["how many elephants are there on the ground", 2],
            ["what is on the ground", 1],
            ["what is the weather like", -1],
            ["where is the grass", -1],
            ["what is in the background", -1],
            ["what is the elephant standing on", -1],
            ["how many elephants are there in the picture", 2]
        ],
        "context": [
            "two elephants standing in a dirt field with people watching.",
            "an elephant standing in a field of grass."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2340224",
                "VG_object_id": "2473154",
                "bbox": [254, 226, 324, 322],
                "image": "data\\images\\2340224.jpg"
            },
            {
                "VG_image_id": "2334077",
                "VG_object_id": "966118",
                "bbox": [88, 184, 189, 309],
                "image": "data\\images\\2334077.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's jacket", 2],
            ["what is in the background", 1],
            ["what is on the man's face", 1]
        ],
        "org_questions": [
            ["what color is the man's jacket", 2],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["Where is the photo taken", -1],
            ["what is the person holding", -1],
            ["where is the man", -1],
            ["what is on the man's face", 1],
            ["when was the photo taken", -1],
            ["what kind of pants is the man wearing", -1],
            ["what is the man doing", -1],
            ["what is the skier wearing", -1]
        ],
        "context": [
            "a man riding skis down a snow covered slope.",
            "a man on skis jumping in the air"
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2363319",
                "VG_object_id": "3746436",
                "bbox": [133, 318, 339, 437],
                "image": "data\\images\\2363319.jpg"
            },
            {
                "VG_image_id": "2381775",
                "VG_object_id": "701923",
                "bbox": [143, 380, 267, 463],
                "image": "data\\images\\2381775.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the table", 1],
            ["how many people are there", 1],
            ["what shape is the table", 1]
        ],
        "org_questions": [
            ["What color is the table", 1],
            ["What is the table made of", -1],
            ["What is on the table", -1],
            ["how many people are there", 1],
            ["where is the table", -1],
            ["what is beside the table", -1],
            ["how many bottles are there on the table", -1],
            ["what is placed on the table", -1],
            ["what shape is the table", 1],
            ["what is covering the table", -1],
            ["what is on top of the table", -1]
        ],
        "context": [
            "a bride and groom cutting a cake together.",
            "a man and woman playing a video game."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2321207",
                "VG_object_id": "3630003",
                "bbox": [3, 190, 485, 321],
                "image": "data\\images\\2321207.jpg"
            },
            {
                "VG_image_id": "2392941",
                "VG_object_id": "476098",
                "bbox": [10, 358, 328, 494],
                "image": "data\\images\\2392941.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the ground covered with", 1],
            ["what color is the ground", 1],
            ["what is on the land", 1],
            ["where is this scene", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what is the ground covered with", 1],
            ["what is the weather like", -1],
            ["what color is the ground", 1],
            ["how many people are there", -1],
            ["what is on the land", 1],
            ["when was the picture taken", -1],
            ["where is this scene", 1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a snow plow is being used to plow a driveway.",
            "a man riding a skateboard on top of a cement wall."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2408819",
                "VG_object_id": "1091460",
                "bbox": [4, 187, 357, 497],
                "image": "data\\images\\2408819.jpg"
            },
            {
                "VG_image_id": "2368435",
                "VG_object_id": "2658736",
                "bbox": [1, 159, 374, 499],
                "image": "data\\images\\2368435.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is gender of person", 2],
            ["What color is person's trouser", 2],
            ["Where is photo taken", 1],
            ["what is the ground covered with", 1],
            ["what is on the land", 1],
            ["how is the weather", 1],
            ["when was the picture taken", 1],
            ["what is the man standing on", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["What is gender of person", 2],
            ["What color is person's trouser", 2],
            ["Where is photo taken", 1],
            ["how many people are there", -1],
            ["what is the ground covered with", 1],
            ["what is on the land", 1],
            ["how is the weather", 1],
            ["when was the picture taken", 1],
            ["what is the man standing on", 1],
            ["what is covering the ground", 1]
        ],
        "context": [
            "a man riding a skateboard on top of a wooden ramp.",
            "a woman walking a dog on a trail in the woods."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2336636",
                "VG_object_id": "2168315",
                "bbox": [395, 167, 497, 294],
                "image": "data\\images\\2336636.jpg"
            },
            {
                "VG_image_id": "2339331",
                "VG_object_id": "2674833",
                "bbox": [208, 110, 370, 308],
                "image": "data\\images\\2339331.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man holding", 2],
            ["what is the man doing", 2],
            ["what is the man wearing on his head", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["Where is the man", 1],
            ["what gesture is the man", 1]
        ],
        "org_questions": [
            ["what is the man holding", 2],
            ["what is the man doing", 2],
            ["what is the man wearing on his head", 1],
            ["how many people are there", 1],
            ["what color is the background", 1],
            ["Where is the man", 1],
            ["how old is the man", -1],
            ["what gesture is the man", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a bus parked at a bus stop",
            "a group of bicyclists riding down a street."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2344789",
                "VG_object_id": "2682059",
                "bbox": [198, 85, 332, 260],
                "image": "data\\images\\2344789.jpg"
            },
            {
                "VG_image_id": "2409434",
                "VG_object_id": "648845",
                "bbox": [264, 175, 391, 317],
                "image": "data\\images\\2409434.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the man", 1],
            ["how many people are there", 1],
            ["how many children are there in the picture", 1],
            ["where is the photo taken", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", -1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the person wearing", -1],
            ["what is the person holding", -1],
            ["how many children are there in the picture", 1],
            ["what type of shirt is the man wearing", -1],
            ["who is in the picture", -1],
            ["what color is the shirt", -1],
            ["where is the photo taken", 1],
            ["what is the man wearing on his head", -1],
            ["what is the man holding", -1],
            ["what is the boy wearing", -1],
            ["what color is the shirt of the boy on the right", -1]
        ],
        "context": [
            "a man and a boy playing a video game",
            "a man and a woman playing a video game."
        ]
    },
    {
        "object_category": "kitchen",
        "images": [
            {
                "VG_image_id": "2320463",
                "VG_object_id": "3036525",
                "bbox": [7, 10, 487, 490],
                "image": "data\\images\\2320463.jpg"
            },
            {
                "VG_image_id": "2355597",
                "VG_object_id": "828251",
                "bbox": [13, 8, 498, 371],
                "image": "data\\images\\2355597.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 1],
            ["what color is the wall", 1]
        ],
        "org_questions": [
            ["what color is the cabinet", -1],
            ["how many people are there in the picture", 1],
            ["what color is the wall", 1],
            ["where was this taken", -1],
            ["when was the photo taken", -1],
            ["what room is this", -1],
            ["what is in the kitchen", -1]
        ],
        "context": [
            "a man making pizzas in a restaurant kitchen",
            "a kitchen with a stove, oven, and a stove."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2360226",
                "VG_object_id": "3762291",
                "bbox": [256, 198, 314, 317],
                "image": "data\\images\\2360226.jpg"
            },
            {
                "VG_image_id": "2341671",
                "VG_object_id": "940760",
                "bbox": [154, 235, 212, 372],
                "image": "data\\images\\2341671.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color are the man's pants", 2],
            ["where is the man", 2],
            ["what is in the background", 1],
            ["what is the person holding", 1],
            ["what is the man wearing", 1]
        ],
        "org_questions": [
            ["what color are the man's pants", 2],
            ["what is the man doing", -1],
            ["where is the man", 2],
            ["how many people are there in the picture", -1],
            ["what is the man wearing on his neck", -1],
            ["what is in the background", 1],
            ["what is the person holding", 1],
            ["what is the man wearing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", -1],
            ["what is on the man's head", -1]
        ],
        "context": [
            "a man and woman loading luggage onto a train.",
            "a group of people flying kites on a beach."
        ]
    },
    {
        "object_category": "bicycle",
        "images": [
            {
                "VG_image_id": "2376987",
                "VG_object_id": "718907",
                "bbox": [62, 164, 223, 358],
                "image": "data\\images\\2376987.jpg"
            },
            {
                "VG_image_id": "2409760",
                "VG_object_id": "234223",
                "bbox": [241, 166, 414, 300],
                "image": "data\\images\\2409760.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the bike doing", 1],
            ["what is the ground covered with", 1],
            ["what is in front of the bike", 1]
        ],
        "org_questions": [
            ["what is the bike doing", 1],
            ["how many bikes are in the picture", -1],
            ["what is the ground covered with", 1],
            ["What is background of image", -1],
            ["when was the photo taken", -1],
            ["where was the photo taken", -1],
            ["where is the bike", -1],
            ["what is in front of the bike", 1]
        ],
        "context": [
            "a group of men walking down a sidewalk.",
            "a bike is laying in a garden with flowers."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2350099",
                "VG_object_id": "1731044",
                "bbox": [14, 131, 186, 351],
                "image": "data\\images\\2350099.jpg"
            },
            {
                "VG_image_id": "2347307",
                "VG_object_id": "3612040",
                "bbox": [172, 60, 327, 415],
                "image": "data\\images\\2347307.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["how many men are there", 1],
            ["what color is the man's shirt", 1],
            ["what is the woman wearing", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is the man holding", 1],
            ["how many men are there", 1],
            ["where is the photo taken", -1],
            ["what color is the man's shirt", 1],
            ["when was the picture taken", -1],
            ["what is the woman wearing", 1],
            ["how many people are in the photo", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "a group of people sitting around a table with wine glasses.",
            "a bride and groom standing next to a wedding cake."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2416694",
                "VG_object_id": "3400405",
                "bbox": [4, 236, 329, 497],
                "image": "data\\images\\2416694.jpg"
            },
            {
                "VG_image_id": "2400335",
                "VG_object_id": "3818423",
                "bbox": [33, 238, 369, 372],
                "image": "data\\images\\2400335.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the photo taken", 2],
            ["what color is the ground", 1],
            ["how many tables are there in the picture", 1],
            ["what is the land made of", 1],
            ["What is the on the land", 1],
            ["what is covering the ground", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["where is the photo taken", 2],
            ["how many tables are there in the picture", 1],
            ["what time is it", -1],
            ["what is the land made of", 1],
            ["what is the weather like", -1],
            ["What is the on the land", 1],
            ["what is covering the ground", 1],
            ["how many people are in the photo", -1]
        ],
        "context": [
            "a toilet seat sitting in the middle of a garden.",
            "a living room with a couch, coffee table and a television."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2327646",
                "VG_object_id": "3358238",
                "bbox": [226, 157, 314, 262],
                "image": "data\\images\\2327646.jpg"
            },
            {
                "VG_image_id": "2323710",
                "VG_object_id": "2703318",
                "bbox": [32, 79, 141, 201],
                "image": "data\\images\\2323710.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the person's shirt", 1],
            ["what color are the person's trousers", 1],
            ["what is the person doing", 1],
            ["where is the person", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what color is the person's shirt", 1],
            ["what color are the person's trousers", 1],
            ["what is the person doing", 1],
            ["what gender is the person in the shirt", -1],
            ["where is the person", 1],
            ["what is the weather like", -1],
            ["what is the gender of the person", -1],
            ["how many people are in the photo", 1],
            ["when was this photo taken", -1]
        ],
        "context": [
            "two children playing with tennis rackets in a playground.",
            "a man kneeling down next to a motorcycle."
        ]
    },
    {
        "object_category": "bed",
        "images": [
            {
                "VG_image_id": "2317977",
                "VG_object_id": "3405817",
                "bbox": [0, 206, 281, 490],
                "image": "data\\images\\2317977.jpg"
            },
            {
                "VG_image_id": "2366034",
                "VG_object_id": "2268942",
                "bbox": [62, 51, 499, 373],
                "image": "data\\images\\2366034.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the pattern of the sheet", 2],
            ["how many people are there in the picture", 2],
            ["what color is the wall", 1],
            ["how many pillows are there on the bed", 1],
            ["what color are the pillows on the bed", 1],
            ["what is the main color of the bed", 1],
            ["who is in the room", 1]
        ],
        "org_questions": [
            ["what is the pattern of the sheet", 2],
            ["what color is the wall", 1],
            ["how many people are there in the picture", 2],
            ["What is on the bed", -1],
            ["how many pillows are there on the bed", 1],
            ["what color are the pillows on the bed", 1],
            ["what is the main color of the bed", 1],
            ["what color is the bed", -1],
            ["where was the photo taken", -1],
            ["what room is this", -1],
            ["what is the bed made of", -1],
            ["who is in the room", 1]
        ],
        "context": [
            "a bedroom with a bed, nightstand, lamp and nightstand.",
            "a woman sitting on a bed using a laptop."
        ]
    },
    {
        "object_category": "building",
        "images": [
            {
                "VG_image_id": "2352338",
                "VG_object_id": "1749420",
                "bbox": [5, 64, 312, 179],
                "image": "data\\images\\2352338.jpg"
            },
            {
                "VG_image_id": "2415103",
                "VG_object_id": "145924",
                "bbox": [2, 57, 500, 222],
                "image": "data\\images\\2415103.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in front of the building", 1],
            ["what is in front of the building", 1],
            ["What transportation are there", 1],
            ["where was this photo taken", 1],
            ["how many people are in the photo", 1]
        ],
        "org_questions": [
            ["what is the main color of the building", -1],
            ["how many people are there in front of the building", 1],
            ["what is in front of the building", 1],
            ["Where is the building", -1],
            ["What transportation are there", 1],
            ["what is the building made of", -1],
            ["what is on the wall", -1],
            ["when was the photo taken", -1],
            ["where was this photo taken", 1],
            ["what is in the background", -1],
            ["how many people are in the photo", 1]
        ],
        "context": [
            "three sheep standing in a field with a house in the background.",
            "a man riding a skateboard down a sidewalk."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2380218",
                "VG_object_id": "546917",
                "bbox": [73, 68, 135, 154],
                "image": "data\\images\\2380218.jpg"
            },
            {
                "VG_image_id": "2369858",
                "VG_object_id": "3861304",
                "bbox": [135, 51, 306, 179],
                "image": "data\\images\\2369858.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["how many shirts are there", 2],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what is the man doing", -1],
            ["how many people are there", 2],
            ["what is the persion in the shirt wearing on head", -1],
            ["what color is the background", 1],
            ["when was the picture taken", -1],
            ["where is the man", 1],
            ["what kind of shirt is the man wearing", -1],
            ["what is in the background", 1],
            ["how many shirts are there", 2]
        ],
        "context": [
            "a young boy throwing a frisbee in a park.",
            "a man is shaving a sheep on the ground."
        ]
    },
    {
        "object_category": "curtain",
        "images": [
            {
                "VG_image_id": "2377429",
                "VG_object_id": "565647",
                "bbox": [208, 0, 303, 239],
                "image": "data\\images\\2377429.jpg"
            },
            {
                "VG_image_id": "2385384",
                "VG_object_id": "1297570",
                "bbox": [344, 0, 401, 210],
                "image": "data\\images\\2385384.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the curtain", 1],
            ["where is the picture taken", 1],
            ["what is behind the curtain", 1],
            ["what is in front of the curtain", 1]
        ],
        "org_questions": [
            ["what color is the curtain", 1],
            ["how many people are there", -1],
            ["where is the picture taken", 1],
            ["what is behind the curtain", 1],
            ["what is in front of the curtain", 1],
            ["what is hanging on the window", -1],
            ["what is covering the window", -1]
        ],
        "context": [
            "a living room with a couch and a mirror",
            "a bathroom with a sink and a mirror"
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2351168",
                "VG_object_id": "1873049",
                "bbox": [161, 203, 253, 496],
                "image": "data\\images\\2351168.jpg"
            },
            {
                "VG_image_id": "2392244",
                "VG_object_id": "1232304",
                "bbox": [44, 8, 238, 191],
                "image": "data\\images\\2392244.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the child holding", 2],
            ["what is the little girl holding", 2],
            ["what color is the girl's shirt", 1],
            ["what is the child doing", 1]
        ],
        "org_questions": [
            ["what is the child holding", 2],
            ["what color is the girl's shirt", 1],
            ["how many people are there", -1],
            ["what is the child wearing on head", -1],
            ["what is the child doing", 1],
            ["what is the girl wearing", -1],
            ["who is in the photo", -1],
            ["what is on the girl's head", -1],
            ["what is the little girl holding", 2]
        ],
        "context": [
            "a little girl holding an umbrella in a kitchen.",
            "a child at a table with a plate of food."
        ]
    },
    {
        "object_category": "room",
        "images": [
            {
                "VG_image_id": "2351727",
                "VG_object_id": "2490467",
                "bbox": [3, 3, 497, 372],
                "image": "data\\images\\2351727.jpg"
            },
            {
                "VG_image_id": "2382697",
                "VG_object_id": "1326776",
                "bbox": [3, 5, 496, 332],
                "image": "data\\images\\2382697.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the wall", 1],
            ["what is the color of the chair", 1]
        ],
        "org_questions": [
            ["what color is the floor", -1],
            ["what is on the wall", 1],
            ["what is the color of the chair", 1],
            ["when is this photo taken", -1],
            ["how many lamps are in the room", -1],
            ["how many people are there in the picture", -1],
            ["what kind of room is this", -1],
            ["where was the picture taken", -1],
            ["what is in the room", -1],
            ["what room is this", -1]
        ],
        "context": [
            "a room with a book shelf and a plant",
            "a living room with a couch, desk and chair."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2361800",
                "VG_object_id": "2870515",
                "bbox": [0, 92, 138, 335],
                "image": "data\\images\\2361800.jpg"
            },
            {
                "VG_image_id": "2399803",
                "VG_object_id": "1166099",
                "bbox": [104, 0, 363, 41],
                "image": "data\\images\\2399803.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the color of the man's shirt", 1],
            ["how many people are there in the picture", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what is the color of the man's shirt", 1],
            ["how many people are there in the picture", 1],
            ["where is the photo taken", -1],
            ["what is the persion doing", -1],
            ["where is the person", -1],
            ["what is in the background", 1],
            ["what are the people sitting on", -1],
            ["where are the people sitting", -1],
            ["where are the people", -1],
            ["where was this photo taken", -1]
        ],
        "context": [
            "a dog sitting on a sidewalk next to a bar.",
            "a table with a sandwich, fries, and a drink."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2364433",
                "VG_object_id": "2094439",
                "bbox": [122, 55, 281, 258],
                "image": "data\\images\\2364433.jpg"
            },
            {
                "VG_image_id": "2373784",
                "VG_object_id": "2683345",
                "bbox": [0, 150, 128, 395],
                "image": "data\\images\\2373784.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the man's head", 2],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["What is man doing", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's shirt", -1],
            ["what is the man doing", 1],
            ["where is the man", 1],
            ["how many people are there", 1],
            ["what is the man wearing", -1],
            ["what gesture is the man", -1],
            ["What is man doing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man holding", 1],
            ["what is on the man's head", 2]
        ],
        "context": [
            "a man is kneeling down while holding a kite.",
            "a man sitting on the ground next to a bunch of motorcycles."
        ]
    },
    {
        "object_category": "player",
        "images": [
            {
                "VG_image_id": "2390514",
                "VG_object_id": "496596",
                "bbox": [378, 161, 445, 292],
                "image": "data\\images\\2390514.jpg"
            },
            {
                "VG_image_id": "2368707",
                "VG_object_id": "3273368",
                "bbox": [45, 108, 462, 341],
                "image": "data\\images\\2368707.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the player doing", 1],
            ["what is the player wearing", 1],
            ["what color is the player's shorts", 1],
            ["what is the main color of the ground", 1],
            ["what color is the ground", 1],
            ["what is the man doing", 1],
            ["what game is being played", 1]
        ],
        "org_questions": [
            ["what is the player doing", 1],
            ["what is the player wearing", 1],
            ["what color is the player's shorts", 1],
            ["how many people are there", -1],
            ["what is the gender of the person", -1],
            ["what is the main color of the ground", 1],
            ["what color is the ground", 1],
            ["what is the man doing", 1],
            ["where was this photo taken", -1],
            ["when was the picture taken", -1],
            ["what game is being played", 1],
            ["what is the number of people", -1]
        ],
        "context": [
            "a football game is being played on the field.",
            "a group of young men playing a game of frisbee."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2348255",
                "VG_object_id": "1960172",
                "bbox": [92, 112, 194, 372],
                "image": "data\\images\\2348255.jpg"
            },
            {
                "VG_image_id": "2347611",
                "VG_object_id": "2091539",
                "bbox": [2, 158, 305, 488],
                "image": "data\\images\\2347611.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["what are the people holding", 1],
            ["what gesture is the man", 1],
            ["what is the persion sitting on", 1],
            ["what is the man looking at", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what color is the man's shirt", 1],
            ["where is the man", 1],
            ["how many people are there", -1],
            ["what is the man wearing on his head", -1],
            ["how is the weather", -1],
            ["what are the people holding", 1],
            ["what gesture is the man", 1],
            ["who is in the photo", -1],
            ["what is the persion sitting on", 1],
            ["what is the man looking at", 1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a group of men standing around a tv playing a video game.",
            "two men sitting on a bench looking at a cell phone."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2358404",
                "VG_object_id": "803149",
                "bbox": [374, 113, 430, 207],
                "image": "data\\images\\2358404.jpg"
            },
            {
                "VG_image_id": "2416894",
                "VG_object_id": "2861419",
                "bbox": [262, 120, 313, 215],
                "image": "data\\images\\2416894.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["what color are the person's clothes", 1],
            ["what color is the ground", 1],
            ["where is the person", 1],
            ["what is the weather like", 1],
            ["what is the person holding", 1],
            ["where is the picture taken", 1]
        ],
        "org_questions": [
            ["what color are the person's clothes", 1],
            ["what color is the ground", 1],
            ["what is the person doing", 2],
            ["How many people are there", -1],
            ["what is the person wearing on the head", -1],
            ["where is the person", 1],
            ["what is the weather like", 1],
            ["what is the person holding", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is the person wearing", -1],
            ["where is the picture taken", 1]
        ],
        "context": [
            "a man riding a motorcycle down a dirt road.",
            "a person skiing down a snowy hill."
        ]
    },
    {
        "object_category": "child",
        "images": [
            {
                "VG_image_id": "2369160",
                "VG_object_id": "2179816",
                "bbox": [405, 95, 499, 370],
                "image": "data\\images\\2369160.jpg"
            },
            {
                "VG_image_id": "2360093",
                "VG_object_id": "2423393",
                "bbox": [23, 40, 291, 449],
                "image": "data\\images\\2360093.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the boy's clothes", 2],
            ["how many people are there", 2],
            ["what color is the ground", 1],
            ["what is the persion doing", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the ground", 1],
            ["where is the child", -1],
            ["what color is the boy's clothes", 2],
            ["what is the persion doing", 1],
            ["What is the gender of person", -1],
            ["where is the picture taken", -1],
            ["what color is the child's hair", -1],
            ["how many people are there", 2],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is in the background", 1]
        ],
        "context": [
            "two giraffes are standing in a zoo enclosure.",
            "a young boy with a blue jacket eating a donut."
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2367726",
                "VG_object_id": "750493",
                "bbox": [102, 36, 242, 291],
                "image": "data\\images\\2367726.jpg"
            },
            {
                "VG_image_id": "2376714",
                "VG_object_id": "1975379",
                "bbox": [78, 39, 302, 264],
                "image": "data\\images\\2376714.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["what is the man wearing", 1],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["what sport is the man playing", 1],
            ["what is the man standing on", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what is the man doing", 1],
            ["what is on the man's head", 1],
            ["what is the man wearing", 1],
            ["how many people are there", -1],
            ["how is the weather", -1],
            ["what color is the background", 1],
            ["where is the man", 1],
            ["what sport is the man playing", 1],
            ["when was the photo taken", -1],
            ["who is in the photo", -1],
            ["what is the man standing on", 1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man riding a scooter down a street.",
            "a man on a skateboard jumping over a rock."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2409897",
                "VG_object_id": "230810",
                "bbox": [72, 20, 258, 277],
                "image": "data\\images\\2409897.jpg"
            },
            {
                "VG_image_id": "2389191",
                "VG_object_id": "1263222",
                "bbox": [85, 120, 227, 374],
                "image": "data\\images\\2389191.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the woman holding", 2],
            ["what is the woman's posture", 2],
            ["what color is the woman's hair", 2],
            ["what color is the woman's clothes", 1],
            ["where is the woman", 1],
            ["what is the persion doing", 1],
            ["where was the photo taken", 1]
        ],
        "org_questions": [
            ["what is the woman holding", 2],
            ["what is the woman's posture", 2],
            ["what color is the woman's clothes", 1],
            ["How many people are there", -1],
            ["where is the woman", 1],
            ["what is the woman wearing on her face", -1],
            ["what is the woman wearing", -1],
            ["What color is the ground", -1],
            ["who is in the photo", -1],
            ["what is the persion doing", 1],
            ["where was the photo taken", 1],
            ["what color is the woman's hair", 2]
        ],
        "context": [
            "a woman with a teddy bear in a basket.",
            "a woman playing a video game in a living room."
        ]
    },
    {
        "object_category": "necktie",
        "images": [
            {
                "VG_image_id": "2414854",
                "VG_object_id": "150606",
                "bbox": [104, 117, 153, 431],
                "image": "data\\images\\2414854.jpg"
            },
            {
                "VG_image_id": "2401534",
                "VG_object_id": "398454",
                "bbox": [171, 277, 247, 499],
                "image": "data\\images\\2401534.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's necktie", 2],
            ["what color is the man's hair", 2],
            ["where is the man", 1],
            ["What is the color of tie", 1],
            ["what pattern is on the tie", 1],
            ["what type of tie is the man wearing", 1]
        ],
        "org_questions": [
            ["what color is the man's necktie", 2],
            ["what color is the man's hair", 2],
            ["what is the gender of the person", -1],
            ["where is the man", 1],
            ["What is the color of tie", 1],
            ["who is wearing a tie", -1],
            ["what is around the man's neck", -1],
            ["what pattern is on the tie", 1],
            ["what is the man doing", -1],
            ["what type of tie is the man wearing", 1]
        ],
        "context": [
            "a man holding a tie in his hands.",
            "a man wearing a plaid shirt and a black tie."
        ]
    },
    {
        "object_category": "land",
        "images": [
            {
                "VG_image_id": "2343967",
                "VG_object_id": "918466",
                "bbox": [0, 264, 500, 374],
                "image": "data\\images\\2343967.jpg"
            },
            {
                "VG_image_id": "2416270",
                "VG_object_id": "1056782",
                "bbox": [0, 138, 500, 331],
                "image": "data\\images\\2416270.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the ground covered with", 1],
            ["How many people are there", 1],
            ["what is the land made of", 1]
        ],
        "org_questions": [
            ["what color is the land", -1],
            ["what is the ground covered with", 1],
            ["How many people are there", 1],
            ["where is te floor", -1],
            ["what is on the land", -1],
            ["what is the land made of", 1],
            ["where was this photo taken", -1],
            ["where are the planes", -1],
            ["where is this scene", -1],
            ["where is the plane parked", -1],
            ["where is the plane", -1]
        ],
        "context": [
            "a fighter jet sitting on top of a dirt field.",
            "a large passenger jet sitting on top of an airport tarmac."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "2402355",
                "VG_object_id": "1137178",
                "bbox": [207, 163, 324, 317],
                "image": "data\\images\\2402355.jpg"
            },
            {
                "VG_image_id": "2385193",
                "VG_object_id": "1299758",
                "bbox": [3, 225, 82, 374],
                "image": "data\\images\\2385193.jpg"
            }
        ],
        "questions_with_scores": [
            ["who is wearing the shirt", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what gender is the person", 1],
            ["what is the person in the shirt holding", 1],
            ["what is the gender of the person", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["who is wearing the shirt", 1],
            ["what is the person doing", 1],
            ["how many people are there", 1],
            ["where is the person", 1],
            ["what is the person wearing", -1],
            ["what gender is the person", 1],
            ["what is the person in the shirt holding", 1],
            ["how many players are there in the picture", -1],
            ["what is the gender of the person", 1],
            ["what is the persion wearing", -1]
        ],
        "context": [
            "a man sitting at a table using a laptop.",
            "a woman is painting an elephant on a wooden fence."
        ]
    },
    {
        "object_category": "food",
        "images": [
            {
                "VG_image_id": "2408746",
                "VG_object_id": "255806",
                "bbox": [283, 258, 360, 306],
                "image": "data\\images\\2408746.jpg"
            },
            {
                "VG_image_id": "2318846",
                "VG_object_id": "3321451",
                "bbox": [95, 250, 248, 320],
                "image": "data\\images\\2318846.jpg"
            }
        ],
        "questions_with_scores": [
            ["what shape is the plate", 2],
            ["what color is the table under the plate", 2],
            ["how old is the person", 2],
            ["what color is the food", 1],
            ["who is in front of the food", 1],
            ["what kind of food is on the plate", 1],
            ["what food is on the plate", 1]
        ],
        "org_questions": [
            ["what color is the food", 1],
            ["what shape is the plate", 2],
            ["who is in front of the food", 1],
            ["what is the food placing on", -1],
            ["how many people are there", -1],
            ["what is on the plate", -1],
            ["what is beside the food", -1],
            ["what color is the table under the plate", 2],
            ["where is the plate", -1],
            ["what kind of food is on the plate", 1],
            ["what is in the bowl", -1],
            ["what food is on the plate", 1],
            ["how old is the person", 2]
        ],
        "context": [
            "a young boy sitting at a table with a plate of food.",
            "a man sitting at a table with a plate of food."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2377958",
                "VG_object_id": "1911900",
                "bbox": [246, 96, 318, 266],
                "image": "data\\images\\2377958.jpg"
            },
            {
                "VG_image_id": "2362687",
                "VG_object_id": "2600466",
                "bbox": [194, 17, 277, 128],
                "image": "data\\images\\2362687.jpg"
            }
        ],
        "questions_with_scores": [
            ["what  is the woman holding", 1],
            ["Where is the woman", 1],
            ["what sport is the woman playing", 1],
            ["what is the woman doing", 1],
            ["what is in front of the woman", 1],
            ["who is in the picture", 1],
            ["what is on the woman's head", 1],
            ["where was the photo taken", 1],
            ["what is the woman standing on", 1],
            ["where is the woman", 1]
        ],
        "org_questions": [
            ["what color is the woman's shirt", -1],
            ["what  is the woman holding", 1],
            ["what is the woman wearing", -1],
            ["how many people are there", -1],
            ["Where is the woman", 1],
            ["what sport is the woman playing", 1],
            ["what is the woman doing", 1],
            ["what is in front of the woman", 1],
            ["when was the photo taken", -1],
            ["who is in the picture", 1],
            ["what is on the woman's head", 1],
            ["where was the photo taken", 1],
            ["what is the woman standing on", 1],
            ["where is the woman", 1],
            ["what kind of clothes is the woman wearing", -1]
        ],
        "context": [
            "a woman flying a kite in a field.",
            "a woman riding a horse over a jump."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2361383",
                "VG_object_id": "2569114",
                "bbox": [230, 110, 398, 280],
                "image": "data\\images\\2361383.jpg"
            },
            {
                "VG_image_id": "2331684",
                "VG_object_id": "2963901",
                "bbox": [196, 78, 491, 317],
                "image": "data\\images\\2331684.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many sheeps are there in the picture", 2],
            ["what color is the dog's head", 1],
            ["what is behind the dog", 1],
            ["what is the dog doing", 1],
            ["where is the dog", 1]
        ],
        "org_questions": [
            ["what color is the dog's head", 1],
            ["what color is the background", -1],
            ["what is behind the dog", 1],
            ["how many dogs are there in the picture", -1],
            ["what is the dog doing", 1],
            ["what is the ground under the dog made of", -1],
            ["where is the dog", 1],
            ["what kind of dog is it", -1],
            ["when was the picture taken", -1],
            ["what animal is in the picture", -1],
            ["who is in the photo", -1],
            ["how many sheeps are there in the picture", 2]
        ],
        "context": [
            "a dog standing in the grass near a lake.",
            "a dog is sitting in a pile of sheep."
        ]
    },
    {
        "object_category": "laptop",
        "images": [
            {
                "VG_image_id": "2356191",
                "VG_object_id": "822659",
                "bbox": [120, 114, 298, 205],
                "image": "data\\images\\2356191.jpg"
            },
            {
                "VG_image_id": "2330779",
                "VG_object_id": "3094010",
                "bbox": [104, 174, 242, 245],
                "image": "data\\images\\2330779.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there", 2],
            ["what color is the laptop", 1],
            ["where is the laptop", 1],
            ["what is on the table", 1],
            ["What is laptop on", 1],
            ["what is on the man's lap", 1]
        ],
        "org_questions": [
            ["what color is the laptop", 1],
            ["where is the laptop", 1],
            ["what is on the table", 1],
            ["how many people are there", 2],
            ["what are the people doing", -1],
            ["What is laptop on", 1],
            ["how many laptops are there", -1],
            ["what kind of laptop is the man using", -1],
            ["what is the man wearing", -1],
            ["what is the man looking at", -1],
            ["what is on the man's lap", 1]
        ],
        "context": [
            "a young man sitting on a couch with a laptop.",
            "three men sitting on a couch with a laptop."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2403346",
                "VG_object_id": "1125603",
                "bbox": [1, 133, 236, 477],
                "image": "data\\images\\2403346.jpg"
            },
            {
                "VG_image_id": "2389709",
                "VG_object_id": "1258124",
                "bbox": [189, 67, 290, 252],
                "image": "data\\images\\2389709.jpg"
            }
        ],
        "questions_with_scores": [
            ["What is the background of image", 2],
            ["Where is the woman", 2],
            ["What color is woman's shirt", 1],
            ["How many people are there", 1],
            ["What is the person holding", 1]
        ],
        "org_questions": [
            ["What color is woman's shirt", 1],
            ["What is the background of image", 2],
            ["Where is the woman", 2],
            ["How many people are there", 1],
            ["what is the woman wearing", -1],
            ["what is the woman doing", -1],
            ["What is the person holding", 1],
            ["who is in the photo", -1],
            ["what is the woman standing on", -1],
            ["what is on the woman's face", -1],
            ["what is the woman looking at", -1]
        ],
        "context": [
            "a woman cooking food in a kitchen with a stove.",
            "a woman selling food on a street cart."
        ]
    },
    {
        "object_category": "table",
        "images": [
            {
                "VG_image_id": "2406208",
                "VG_object_id": "325800",
                "bbox": [15, 327, 488, 498],
                "image": "data\\images\\2406208.jpg"
            },
            {
                "VG_image_id": "2370836",
                "VG_object_id": "1783805",
                "bbox": [18, 7, 495, 326],
                "image": "data\\images\\2370836.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is on the table", 2],
            ["what color is the table ", 1]
        ],
        "org_questions": [
            ["what color is the table ", 1],
            ["what is on the table", 2],
            ["how many people are there", -1],
            ["where is the table", -1],
            ["what is the table made of", -1],
            ["how many plates are there on the table", -1],
            ["where was the photo taken", -1]
        ],
        "context": [
            "a small vase with a red flower in it",
            "a couple of wine glasses sitting on top of a table."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2359055",
                "VG_object_id": "1913841",
                "bbox": [161, 202, 254, 424],
                "image": "data\\images\\2359055.jpg"
            },
            {
                "VG_image_id": "2357268",
                "VG_object_id": "2233484",
                "bbox": [193, 72, 257, 178],
                "image": "data\\images\\2357268.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's pants", 1],
            ["what is the man wearing on the head", 1],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the man's pants", 1],
            ["what is the man wearing on the head", 1],
            ["what is in the background", 1],
            ["how many people are there", -1],
            ["where is the photo taken", -1],
            ["what is the person doing", -1],
            ["What is child doing", -1],
            ["what color are the clothes of the man on the right", -1],
            ["who is on the skateboard", -1],
            ["when was the picture taken", -1],
            ["what is the man holding", -1],
            ["what is the man wearing", -1]
        ],
        "context": [
            "a boy riding a skateboard down a street.",
            "a boy is skateboarding on a stone ledge."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2343540",
                "VG_object_id": "2650779",
                "bbox": [107, 169, 243, 244],
                "image": "data\\images\\2343540.jpg"
            },
            {
                "VG_image_id": "2352127",
                "VG_object_id": "2460300",
                "bbox": [256, 90, 321, 125],
                "image": "data\\images\\2352127.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the car", 2],
            ["what color is the ground", 2],
            ["where is the car", 1],
            ["what is the ground covered with", 1],
            ["what is on the left of the car", 1],
            ["what is in the distance", 1],
            ["where are the cars parked", 1],
            ["what color is the car on the right", 1]
        ],
        "org_questions": [
            ["what color is the car", 2],
            ["where is the car", 1],
            ["what is the ground covered with", 1],
            ["how many cars are there", -1],
            ["what time is it", -1],
            ["how is the weather", -1],
            ["what is on the left of the car", 1],
            ["what is in the distance", 1],
            ["when was the photo taken", -1],
            ["what is on the car", -1],
            ["where are the cars parked", 1],
            ["what color is the car on the right", 1],
            ["what color is the ground", 2]
        ],
        "context": [
            "a motorcycle towing a car on a road.",
            "a giraffe standing next to a tree in a field."
        ]
    },
    {
        "object_category": "car",
        "images": [
            {
                "VG_image_id": "2376741",
                "VG_object_id": "2684299",
                "bbox": [135, 212, 255, 303],
                "image": "data\\images\\2376741.jpg"
            },
            {
                "VG_image_id": "2383338",
                "VG_object_id": "533390",
                "bbox": [8, 131, 169, 206],
                "image": "data\\images\\2383338.jpg"
            }
        ],
        "questions_with_scores": [
            ["How many cars are there", 2],
            ["How many people are there ", 1],
            ["What is the background of image", 1],
            ["What color is the sky", 1],
            ["where is the car", 1],
            ["what type of vehicle is shown", 1]
        ],
        "org_questions": [
            ["How many cars are there", 2],
            ["How many people are there ", 1],
            ["What is the background of image", 1],
            ["What color is the sky", 1],
            ["how is the weather", -1],
            ["When is the photo taken", -1],
            ["what shape is the car", -1],
            ["where is the car", 1],
            ["what type of vehicle is shown", 1]
        ],
        "context": [
            "a truck and a group of sheep grazing in a field.",
            "a man on a skateboard doing a trick."
        ]
    },
    {
        "object_category": "shirt",
        "images": [
            {
                "VG_image_id": "713197",
                "VG_object_id": "1909968",
                "bbox": [161, 441, 592, 737],
                "image": "data\\images\\713197.jpg"
            },
            {
                "VG_image_id": "2349794",
                "VG_object_id": "2584094",
                "bbox": [3, 143, 273, 329],
                "image": "data\\images\\2349794.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the picture taken", 1],
            ["where is the person", 1],
            ["what is the person holding", 1],
            ["what is the person doing", 1]
        ],
        "org_questions": [
            ["what color is the shirt", -1],
            ["what gender is the person in the shirt", -1],
            ["how many people are there", -1],
            ["how long is the sleeves of the shirt", -1],
            ["What is the background of image", -1],
            ["What is the gender of person", -1],
            ["who is in the photo", -1],
            ["where is the picture taken", 1],
            ["where is the person", 1],
            ["what shape is the collar of the shirt", -1],
            ["what is the person holding", 1],
            ["what is the person wearing", -1],
            ["who is wearing the shirt", -1],
            ["what is the person doing", 1]
        ],
        "context": [
            "a man sitting at a desk eating a piece of food.",
            "a man holding a chocolate cake with a message on it."
        ]
    },
    {
        "object_category": "woman",
        "images": [
            {
                "VG_image_id": "2331183",
                "VG_object_id": "3288374",
                "bbox": [258, 31, 362, 291],
                "image": "data\\images\\2331183.jpg"
            },
            {
                "VG_image_id": "2345977",
                "VG_object_id": "2453515",
                "bbox": [0, 94, 324, 427],
                "image": "data\\images\\2345977.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many people are there in the picture", 2],
            ["what color is the woman's clothes", 1],
            ["what is the woman wearing", 1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the persion holding", 1],
            ["what is on the woman's face", 1],
            ["what is on the woman's head", 1]
        ],
        "org_questions": [
            ["what color is the woman's clothes", 1],
            ["what is the woman wearing", 1],
            ["what color is the background", 1],
            ["how many people are there", 1],
            ["where is the woman", 1],
            ["what is the woman doing", 1],
            ["what is the persion holding", 1],
            ["when was this photo taken", -1],
            ["who is in the photo", -1],
            ["what is on the woman's face", 1],
            ["what is on the woman's head", 1],
            ["how many people are there in the picture", 2]
        ],
        "context": [
            "two people standing on skis in the snow.",
            "a woman wearing a headdress."
        ]
    },
    {
        "object_category": "counter",
        "images": [
            {
                "VG_image_id": "2401563",
                "VG_object_id": "659386",
                "bbox": [80, 143, 499, 276],
                "image": "data\\images\\2401563.jpg"
            },
            {
                "VG_image_id": "2350198",
                "VG_object_id": "1668747",
                "bbox": [214, 221, 479, 331],
                "image": "data\\images\\2350198.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the counter", 2],
            ["what color are the walls", 2]
        ],
        "org_questions": [
            ["What color is the counter", 2],
            ["What is on the counter", -1],
            ["where is the counter", -1],
            ["What is next to the counter", -1],
            ["what on the counter", -1],
            ["how many sinks are there", -1],
            ["what room is this", -1],
            ["what is above the sink", -1],
            ["what is under the sink", -1],
            ["what color are the walls", 2]
        ],
        "context": [
            "a kitchen with a microwave, sink, and microwave.",
            "a bathroom with a sink, mirror, and a mirror."
        ]
    },
    {
        "object_category": "blanket",
        "images": [
            {
                "VG_image_id": "2406899",
                "VG_object_id": "651787",
                "bbox": [364, 211, 499, 273],
                "image": "data\\images\\2406899.jpg"
            },
            {
                "VG_image_id": "2371315",
                "VG_object_id": "598555",
                "bbox": [78, 216, 114, 274],
                "image": "data\\images\\2371315.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the blanket", 2],
            ["what is on the blanket", 2],
            ["what color is the blanket", 1],
            ["where was the photo taken", 1],
            ["what is in the photo", 1]
        ],
        "org_questions": [
            ["what color is the blanket", 1],
            ["where is the blanket", 2],
            ["what is on the blanket", 2],
            ["when is this photo taken", -1],
            ["what is the weather like", -1],
            ["how many people are there", -1],
            ["where was the photo taken", 1],
            ["what is in the photo", 1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a man sitting on the grass with a bag of food.",
            "a man riding a horse in a rodeo"
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2319172",
                "VG_object_id": "1004125",
                "bbox": [153, 297, 252, 374],
                "image": "data\\images\\2319172.jpg"
            },
            {
                "VG_image_id": "2394923",
                "VG_object_id": "459619",
                "bbox": [74, 251, 121, 306],
                "image": "data\\images\\2394923.jpg"
            }
        ],
        "questions_with_scores": [
            ["Where is the person", 1],
            ["How old is the person", 1],
            ["What is the person doing", 1]
        ],
        "org_questions": [
            ["Where is the person", 1],
            ["How old is the person", 1],
            ["What is the person doing", 1],
            ["Who is wearing those trousers", -1],
            ["what is behind the trousers", -1],
            ["how many people are there", -1],
            ["what is the main color of the pants", -1]
        ],
        "context": [
            "two men playing a video game in a room.",
            "a little boy flying a kite on a beach."
        ]
    },
    {
        "object_category": "male_child",
        "images": [
            {
                "VG_image_id": "2342853",
                "VG_object_id": "2013554",
                "bbox": [196, 149, 247, 221],
                "image": "data\\images\\2342853.jpg"
            },
            {
                "VG_image_id": "2346330",
                "VG_object_id": "1891238",
                "bbox": [118, 244, 177, 456],
                "image": "data\\images\\2346330.jpg"
            }
        ],
        "questions_with_scores": [
            ["how many children are there in the picture", 2],
            ["what is the child doing", 1],
            ["what color is the child's shirt", 1]
        ],
        "org_questions": [
            ["what is the child doing", 1],
            ["what color is the child's shirt", 1],
            ["how many children are there in the picture", 2],
            ["what is the boy wearing", -1],
            ["when was this photo taken", -1],
            ["what is the man holding", -1],
            ["who is in the photo", -1]
        ],
        "context": [
            "a person sitting on a horse on a beach.",
            "two children flying a kite on a beach."
        ]
    },
    {
        "object_category": "street",
        "images": [
            {
                "VG_image_id": "2370773",
                "VG_object_id": "3856814",
                "bbox": [3, 260, 496, 330],
                "image": "data\\images\\2370773.jpg"
            },
            {
                "VG_image_id": "2326383",
                "VG_object_id": "3373087",
                "bbox": [8, 225, 483, 373],
                "image": "data\\images\\2326383.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the road made of", 1],
            ["what is the main color of the road", 1],
            ["what is on the road", 1],
            ["how many people are there in the street", 1],
            ["what color is the ground of the street", 1]
        ],
        "org_questions": [
            ["what is the road made of", 1],
            ["what is the main color of the road", 1],
            ["what is on the road", 1],
            ["how many cars are there", -1],
            ["how is the weather", -1],
            ["how many people are there in the street", 1],
            ["how many vehicles are there on the road", -1],
            ["what color is the ground of the street", 1],
            ["where was this picture taken", -1],
            ["when was the photo taken", -1]
        ],
        "context": [
            "a horse drawn carriage on a city street.",
            "a school bus driving down a street next to a building."
        ]
    },
    {
        "object_category": "trouser",
        "images": [
            {
                "VG_image_id": "2326328",
                "VG_object_id": "3002557",
                "bbox": [160, 192, 313, 287],
                "image": "data\\images\\2326328.jpg"
            },
            {
                "VG_image_id": "2322143",
                "VG_object_id": "993248",
                "bbox": [101, 174, 220, 450],
                "image": "data\\images\\2322143.jpg"
            }
        ],
        "questions_with_scores": [
            ["what is the person doing", 2],
            ["what is in the background", 1]
        ],
        "org_questions": [
            ["what color is the pants", -1],
            ["what is the person doing", 2],
            ["what is the person wearing", -1],
            ["how many trousers are there in the photo", -1],
            ["what time is it", -1],
            ["where is the person", -1],
            ["what is in the background", 1],
            ["what type of pants is the person wearing", -1],
            ["how many people are there", -1],
            ["what color are the jeans", -1]
        ],
        "context": [
            "a little girl sitting on a skateboard in the grass.",
            "a man wearing a red jacket and gray pants."
        ]
    },
    {
        "object_category": "seat",
        "images": [
            {
                "VG_image_id": "2344098",
                "VG_object_id": "2153565",
                "bbox": [186, 281, 262, 328],
                "image": "data\\images\\2344098.jpg"
            },
            {
                "VG_image_id": "2402573",
                "VG_object_id": "1134227",
                "bbox": [83, 114, 289, 203],
                "image": "data\\images\\2402573.jpg"
            }
        ],
        "questions_with_scores": [
            ["What color is the bench", 2],
            ["What is on the bench", 2],
            ["What is next to the seat", 1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["where is the chair", 1],
            ["what is behind the seat", 1],
            ["what is on the right side of the bench", 1]
        ],
        "org_questions": [
            ["What color is the bench", 2],
            ["What is on the bench", 2],
            ["What is next to the seat", 1],
            ["how many motorcycles are there", -1],
            ["where is the photo taken", 1],
            ["what is in the background", 1],
            ["where is the chair", 1],
            ["what is behind the seat", 1],
            ["what material is the bench made of", -1],
            ["what is made of wood", -1],
            ["what is under the bench", -1],
            ["what is on the right side of the bench", 1]
        ],
        "context": [
            "a kitchen with a table, chairs, and a table.",
            "a blue book on a bench"
        ]
    },
    {
        "object_category": "man",
        "images": [
            {
                "VG_image_id": "2397075",
                "VG_object_id": "437643",
                "bbox": [59, 18, 234, 263],
                "image": "data\\images\\2397075.jpg"
            },
            {
                "VG_image_id": "2372468",
                "VG_object_id": "2100000",
                "bbox": [20, 13, 480, 483],
                "image": "data\\images\\2372468.jpg"
            }
        ],
        "questions_with_scores": [
            ["what color is the man's clothes", 2],
            ["what is the man wearing on the face", 1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is behind the man", 1],
            ["what is the man holding", 1]
        ],
        "org_questions": [
            ["what color is the man's clothes", 2],
            ["what is the man wearing on the face", 1],
            ["how many people are there", -1],
            ["where is the man", 1],
            ["what is the man doing", 1],
            ["what is the man wearing", 1],
            ["what is on the man's head", -1],
            ["when was the picture taken", -1],
            ["who is in the photo", -1],
            ["what is behind the man", 1],
            ["what is the man holding", 1]
        ],
        "context": [
            "a man sitting at a table with a pizza and a drink.",
            "a man in a suit and tie holding a cell phone."
        ]
    },
    {
        "object_category": "person",
        "images": [
            {
                "VG_image_id": "2359957",
                "VG_object_id": "1824386",
                "bbox": [102, 187, 177, 315],
                "image": "data\\images\\2359957.jpg"
            },
            {
                "VG_image_id": "2331836",
                "VG_object_id": "2829388",
                "bbox": [18, 129, 279, 485],
                "image": "data\\images\\2331836.jpg"
            }
        ],
        "questions_with_scores": [
            ["where are the people", 2],
            ["What is the man doing", 2],
            ["how many people are there", 1],
            ["what is the persion wearing", 1],
            ["what is the persion holding", 1]
        ],
        "org_questions": [
            ["how many people are there", 1],
            ["where are the people", 2],
            ["what is on the person's head", -1],
            ["What color is person's shirt", -1],
            ["What is the man doing", 2],
            ["what is in the distance", -1],
            ["what is the gender of the person in the photo", -1],
            ["who is in the photo", -1],
            ["what is the persion wearing", 1],
            ["what is the persion holding", 1],
            ["what is on the man's feet", -1]
        ],
        "context": [
            "a man holding a surfboard on top of a body of water.",
            "two men sitting on a train with their heads down."
        ]
    },
    {
        "object_category": "dog",
        "images": [
            {
                "VG_image_id": "2409618",
                "VG_object_id": "237736",
                "bbox": [320, 2, 416, 117],
                "image": "data\\images\\2409618.jpg"
            },
            {
                "VG_image_id": "2354357",
                "VG_object_id": "839195",
                "bbox": [280, 367, 456, 499],
                "image": "data\\images\\2354357.jpg"
            }
        ],
        "questions_with_scores": [
            ["where is the dog", 2],
            ["what is in front of the dog", 1],
            ["what color is the ground", 1],
            ["what is on the dog's neck", 1],
            ["what color are the dog's feet", 1]
        ],
        "org_questions": [
            ["where is the dog", 2],
            ["what is in front of the dog", 1],
            ["what color is the ground", 1],
            ["how many dogs are there", -1],
            ["what is the dog doing", -1],
            ["what is on the dog's neck", 1],
            ["what color are the dog's feet", 1],
            ["what kind of animal is in the picture", -1],
            ["when was the picture taken", -1]
        ],
        "context": [
            "a woman riding a pink bike with a dog in a basket.",
            "a man walking a dog on a leash next to a tree."
        ]
    }
]
